# Ollama LangExtract PII/PHI Recognition Configuration
# https://github.com/google/langextract
#
# This configuration is specifically for OllamaLangExtractRecognizer.
# Class hierarchy: LMRecognizer -> LangExtractRecognizer -> OllamaLangExtractRecognizer
#
# Detects both:
# - PII (Personally Identifiable Information): names, emails, phone numbers, SSN, etc.
# - PHI (Protected Health Information): medical records, health identifiers, etc.
#
# Installation:
# follow this link https://github.com/google/langextract#using-local-llms-with-ollama

# LangExtract shared configuration (used by all LangExtract-based recognizers)
langextract:
  # Prompt and examples (shared across all LangExtract recognizers)
  prompt_file: "langextract_prompts/default_pii_phi_prompt.txt"
  examples_file: "langextract_prompts/default_pii_examples.yaml"
  
  # Entity mappings: LangExtract entity -> Presidio entity type (shared)
  entity_mappings:
    person: PERSON
    full_name: PERSON
    name_first: PERSON  # First name component
    name_last: PERSON   # Last name component
    name_middle: PERSON # Middle name component
    location: LOCATION
    address: LOCATION
    organization: ORGANIZATION
    phone: PHONE_NUMBER
    phone_number: PHONE_NUMBER  # Alternative phone number extraction class
    email: EMAIL_ADDRESS
    date: DATE_TIME
    ssn: US_SSN
    identification_number: US_SSN  # Generic identification number
    credit_card: CREDIT_CARD
    medical_record: MEDICAL_LICENSE  # PHI
    ip_address: IP_ADDRESS
    url: URL
    iban: IBAN_CODE
    payment_status: IGNORE  # Not a PII/PHI entity - will be filtered out
  
  # Supported PII/PHI entity types (shared)
  supported_entities:
    - PERSON
    - LOCATION
    - ORGANIZATION
    - PHONE_NUMBER
    - EMAIL_ADDRESS
    - DATE_TIME
    - US_SSN
    - CREDIT_CARD
    - MEDICAL_LICENSE  # PHI
    - IP_ADDRESS
    - URL
    - IBAN_CODE
  
  min_score: 0.5

# Ollama-specific configuration (only for OllamaLangExtractRecognizer)
ollama:
  # Ollama server and model settings
  model_id: "gemma2:2b"
  model_url: "http://localhost:11434"
  temperature: null  # Optional: Set to 0.0-1.0 for deterministic/creative output
