package job

// Copyright (c) Microsoft and contributors.  All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//
// See the License for the specific language governing permissions and
// limitations under the License.
//
// Code generated by Microsoft (R) AutoRest Code Generator.
// Changes may cause incorrect behavior and will be lost if the code is regenerated.

import (
	"encoding/json"
	"github.com/Azure/go-autorest/autorest"
	"github.com/Azure/go-autorest/autorest/azure"
	"github.com/Azure/go-autorest/autorest/date"
	"github.com/Azure/go-autorest/autorest/to"
	"github.com/satori/go.uuid"
	"net/http"
)

// CompileMode enumerates the values for compile mode.
type CompileMode string

const (
	// Full ...
	Full CompileMode = "Full"
	// Semantic ...
	Semantic CompileMode = "Semantic"
	// SingleBox ...
	SingleBox CompileMode = "SingleBox"
)

// PossibleCompileModeValues returns an array of possible values for the CompileMode const type.
func PossibleCompileModeValues() []CompileMode {
	return []CompileMode{Full, Semantic, SingleBox}
}

// ResourceType enumerates the values for resource type.
type ResourceType string

const (
	// JobManagerResource ...
	JobManagerResource ResourceType = "JobManagerResource"
	// JobManagerResourceInUserFolder ...
	JobManagerResourceInUserFolder ResourceType = "JobManagerResourceInUserFolder"
	// StatisticsResource ...
	StatisticsResource ResourceType = "StatisticsResource"
	// StatisticsResourceInUserFolder ...
	StatisticsResourceInUserFolder ResourceType = "StatisticsResourceInUserFolder"
	// VertexResource ...
	VertexResource ResourceType = "VertexResource"
	// VertexResourceInUserFolder ...
	VertexResourceInUserFolder ResourceType = "VertexResourceInUserFolder"
)

// PossibleResourceTypeValues returns an array of possible values for the ResourceType const type.
func PossibleResourceTypeValues() []ResourceType {
	return []ResourceType{JobManagerResource, JobManagerResourceInUserFolder, StatisticsResource, StatisticsResourceInUserFolder, VertexResource, VertexResourceInUserFolder}
}

// Result enumerates the values for result.
type Result string

const (
	// Cancelled ...
	Cancelled Result = "Cancelled"
	// Failed ...
	Failed Result = "Failed"
	// None ...
	None Result = "None"
	// Succeeded ...
	Succeeded Result = "Succeeded"
)

// PossibleResultValues returns an array of possible values for the Result const type.
func PossibleResultValues() []Result {
	return []Result{Cancelled, Failed, None, Succeeded}
}

// SeverityTypes enumerates the values for severity types.
type SeverityTypes string

const (
	// Deprecated ...
	Deprecated SeverityTypes = "Deprecated"
	// Error ...
	Error SeverityTypes = "Error"
	// Info ...
	Info SeverityTypes = "Info"
	// SevereWarning ...
	SevereWarning SeverityTypes = "SevereWarning"
	// UserWarning ...
	UserWarning SeverityTypes = "UserWarning"
	// Warning ...
	Warning SeverityTypes = "Warning"
)

// PossibleSeverityTypesValues returns an array of possible values for the SeverityTypes const type.
func PossibleSeverityTypesValues() []SeverityTypes {
	return []SeverityTypes{Deprecated, Error, Info, SevereWarning, UserWarning, Warning}
}

// State enumerates the values for state.
type State string

const (
	// StateAccepted ...
	StateAccepted State = "Accepted"
	// StateCompiling ...
	StateCompiling State = "Compiling"
	// StateEnded ...
	StateEnded State = "Ended"
	// StateNew ...
	StateNew State = "New"
	// StatePaused ...
	StatePaused State = "Paused"
	// StateQueued ...
	StateQueued State = "Queued"
	// StateRunning ...
	StateRunning State = "Running"
	// StateScheduling ...
	StateScheduling State = "Scheduling"
	// StateStarting ...
	StateStarting State = "Starting"
	// StateWaitingForCapacity ...
	StateWaitingForCapacity State = "WaitingForCapacity"
)

// PossibleStateValues returns an array of possible values for the State const type.
func PossibleStateValues() []State {
	return []State{StateAccepted, StateCompiling, StateEnded, StateNew, StatePaused, StateQueued, StateRunning, StateScheduling, StateStarting, StateWaitingForCapacity}
}

// Type enumerates the values for type.
type Type string

const (
	// TypeHive ...
	TypeHive Type = "Hive"
	// TypeJobProperties ...
	TypeJobProperties Type = "JobProperties"
	// TypeScope ...
	TypeScope Type = "Scope"
	// TypeUSQL ...
	TypeUSQL Type = "USql"
)

// PossibleTypeValues returns an array of possible values for the Type const type.
func PossibleTypeValues() []Type {
	return []Type{TypeHive, TypeJobProperties, TypeScope, TypeUSQL}
}

// TypeBasicCreateJobProperties enumerates the values for type basic create job properties.
type TypeBasicCreateJobProperties string

const (
	// TypeBasicCreateJobPropertiesTypeCreateJobProperties ...
	TypeBasicCreateJobPropertiesTypeCreateJobProperties TypeBasicCreateJobProperties = "CreateJobProperties"
	// TypeBasicCreateJobPropertiesTypeScope ...
	TypeBasicCreateJobPropertiesTypeScope TypeBasicCreateJobProperties = "Scope"
	// TypeBasicCreateJobPropertiesTypeUSQL ...
	TypeBasicCreateJobPropertiesTypeUSQL TypeBasicCreateJobProperties = "USql"
)

// PossibleTypeBasicCreateJobPropertiesValues returns an array of possible values for the TypeBasicCreateJobProperties const type.
func PossibleTypeBasicCreateJobPropertiesValues() []TypeBasicCreateJobProperties {
	return []TypeBasicCreateJobProperties{TypeBasicCreateJobPropertiesTypeCreateJobProperties, TypeBasicCreateJobPropertiesTypeScope, TypeBasicCreateJobPropertiesTypeUSQL}
}

// TypeEnum enumerates the values for type enum.
type TypeEnum string

const (
	// Hive ...
	Hive TypeEnum = "Hive"
	// Scope ...
	Scope TypeEnum = "Scope"
	// USQL ...
	USQL TypeEnum = "USql"
)

// PossibleTypeEnumValues returns an array of possible values for the TypeEnum const type.
func PossibleTypeEnumValues() []TypeEnum {
	return []TypeEnum{Hive, Scope, USQL}
}

// BaseJobParameters data Lake Analytics Job Parameters base class for build and submit.
type BaseJobParameters struct {
	// Type - the job type of the current job (Hive, USql, or Scope (for internal use only)). Possible values include: 'USQL', 'Hive', 'Scope'
	Type TypeEnum `json:"type,omitempty"`
	// Properties - the job specific properties.
	Properties BasicCreateJobProperties `json:"properties,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for BaseJobParameters struct.
func (bjp *BaseJobParameters) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "type":
			if v != nil {
				var typeVar TypeEnum
				err = json.Unmarshal(*v, &typeVar)
				if err != nil {
					return err
				}
				bjp.Type = typeVar
			}
		case "properties":
			if v != nil {
				properties, err := unmarshalBasicCreateJobProperties(*v)
				if err != nil {
					return err
				}
				bjp.Properties = properties
			}
		}
	}

	return nil
}

// BuildJobParameters the parameters used to build a new Data Lake Analytics job.
type BuildJobParameters struct {
	// Name - the friendly name of the job to build.
	Name *string `json:"name,omitempty"`
	// Type - the job type of the current job (Hive, USql, or Scope (for internal use only)). Possible values include: 'USQL', 'Hive', 'Scope'
	Type TypeEnum `json:"type,omitempty"`
	// Properties - the job specific properties.
	Properties BasicCreateJobProperties `json:"properties,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for BuildJobParameters struct.
func (bjp *BuildJobParameters) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				bjp.Name = &name
			}
		case "type":
			if v != nil {
				var typeVar TypeEnum
				err = json.Unmarshal(*v, &typeVar)
				if err != nil {
					return err
				}
				bjp.Type = typeVar
			}
		case "properties":
			if v != nil {
				properties, err := unmarshalBasicCreateJobProperties(*v)
				if err != nil {
					return err
				}
				bjp.Properties = properties
			}
		}
	}

	return nil
}

// CancelFuture an abstraction for monitoring and retrieving the results of a long-running operation.
type CancelFuture struct {
	azure.Future
	req *http.Request
}

// Result returns the result of the asynchronous operation.
// If the operation has not completed it will return an error.
func (future CancelFuture) Result(client Client) (ar autorest.Response, err error) {
	var done bool
	done, err = future.Done(client)
	if err != nil {
		err = autorest.NewErrorWithError(err, "job.CancelFuture", "Result", future.Response(), "Polling failure")
		return
	}
	if !done {
		return ar, azure.NewAsyncOpIncompleteError("job.CancelFuture")
	}
	if future.PollingMethod() == azure.PollingLocation {
		ar, err = client.CancelResponder(future.Response())
		if err != nil {
			err = autorest.NewErrorWithError(err, "job.CancelFuture", "Result", future.Response(), "Failure responding to request")
		}
		return
	}
	var req *http.Request
	var resp *http.Response
	if future.PollingURL() != "" {
		req, err = http.NewRequest(http.MethodGet, future.PollingURL(), nil)
		if err != nil {
			return
		}
	} else {
		req = autorest.ChangeToGet(future.req)
	}
	resp, err = autorest.SendWithSender(client, req,
		autorest.DoRetryForStatusCodes(client.RetryAttempts, client.RetryDuration, autorest.StatusCodesForRetry...))
	if err != nil {
		err = autorest.NewErrorWithError(err, "job.CancelFuture", "Result", resp, "Failure sending request")
		return
	}
	ar, err = client.CancelResponder(resp)
	if err != nil {
		err = autorest.NewErrorWithError(err, "job.CancelFuture", "Result", resp, "Failure responding to request")
	}
	return
}

// CreateJobParameters the parameters used to submit a new Data Lake Analytics job.
type CreateJobParameters struct {
	// Name - the friendly name of the job to submit.
	Name *string `json:"name,omitempty"`
	// DegreeOfParallelism - the degree of parallelism to use for this job. This must be greater than 0, if set to less than 0 it will default to 1.
	DegreeOfParallelism *int32 `json:"degreeOfParallelism,omitempty"`
	// Priority - the priority value to use for the current job. Lower numbers have a higher priority. By default, a job has a priority of 1000. This must be greater than 0.
	Priority *int32 `json:"priority,omitempty"`
	// LogFilePatterns - the list of log file name patterns to find in the logFolder. '*' is the only matching character allowed. Example format: jobExecution*.log or *mylog*.txt
	LogFilePatterns *[]string `json:"logFilePatterns,omitempty"`
	// Related - the recurring job relationship information properties.
	Related *RelationshipProperties `json:"related,omitempty"`
	// Type - the job type of the current job (Hive, USql, or Scope (for internal use only)). Possible values include: 'USQL', 'Hive', 'Scope'
	Type TypeEnum `json:"type,omitempty"`
	// Properties - the job specific properties.
	Properties BasicCreateJobProperties `json:"properties,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for CreateJobParameters struct.
func (cjp *CreateJobParameters) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				cjp.Name = &name
			}
		case "degreeOfParallelism":
			if v != nil {
				var degreeOfParallelism int32
				err = json.Unmarshal(*v, &degreeOfParallelism)
				if err != nil {
					return err
				}
				cjp.DegreeOfParallelism = &degreeOfParallelism
			}
		case "priority":
			if v != nil {
				var priority int32
				err = json.Unmarshal(*v, &priority)
				if err != nil {
					return err
				}
				cjp.Priority = &priority
			}
		case "logFilePatterns":
			if v != nil {
				var logFilePatterns []string
				err = json.Unmarshal(*v, &logFilePatterns)
				if err != nil {
					return err
				}
				cjp.LogFilePatterns = &logFilePatterns
			}
		case "related":
			if v != nil {
				var related RelationshipProperties
				err = json.Unmarshal(*v, &related)
				if err != nil {
					return err
				}
				cjp.Related = &related
			}
		case "type":
			if v != nil {
				var typeVar TypeEnum
				err = json.Unmarshal(*v, &typeVar)
				if err != nil {
					return err
				}
				cjp.Type = typeVar
			}
		case "properties":
			if v != nil {
				properties, err := unmarshalBasicCreateJobProperties(*v)
				if err != nil {
					return err
				}
				cjp.Properties = properties
			}
		}
	}

	return nil
}

// BasicCreateJobProperties the common Data Lake Analytics job properties for job submission.
type BasicCreateJobProperties interface {
	AsCreateUSQLJobProperties() (*CreateUSQLJobProperties, bool)
	AsCreateScopeJobProperties() (*CreateScopeJobProperties, bool)
	AsCreateJobProperties() (*CreateJobProperties, bool)
}

// CreateJobProperties the common Data Lake Analytics job properties for job submission.
type CreateJobProperties struct {
	// RuntimeVersion - the runtime version of the Data Lake Analytics engine to use for the specific type of job being run.
	RuntimeVersion *string `json:"runtimeVersion,omitempty"`
	// Script - the script to run. Please note that the maximum script size is 3 MB.
	Script *string `json:"script,omitempty"`
	// Type - Possible values include: 'TypeBasicCreateJobPropertiesTypeCreateJobProperties', 'TypeBasicCreateJobPropertiesTypeUSQL', 'TypeBasicCreateJobPropertiesTypeScope'
	Type TypeBasicCreateJobProperties `json:"type,omitempty"`
}

func unmarshalBasicCreateJobProperties(body []byte) (BasicCreateJobProperties, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["type"] {
	case string(TypeBasicCreateJobPropertiesTypeUSQL):
		var cusjp CreateUSQLJobProperties
		err := json.Unmarshal(body, &cusjp)
		return cusjp, err
	case string(TypeBasicCreateJobPropertiesTypeScope):
		var csjp CreateScopeJobProperties
		err := json.Unmarshal(body, &csjp)
		return csjp, err
	default:
		var cjp CreateJobProperties
		err := json.Unmarshal(body, &cjp)
		return cjp, err
	}
}
func unmarshalBasicCreateJobPropertiesArray(body []byte) ([]BasicCreateJobProperties, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	cjpArray := make([]BasicCreateJobProperties, len(rawMessages))

	for index, rawMessage := range rawMessages {
		cjp, err := unmarshalBasicCreateJobProperties(*rawMessage)
		if err != nil {
			return nil, err
		}
		cjpArray[index] = cjp
	}
	return cjpArray, nil
}

// MarshalJSON is the custom marshaler for CreateJobProperties.
func (cjp CreateJobProperties) MarshalJSON() ([]byte, error) {
	cjp.Type = TypeBasicCreateJobPropertiesTypeCreateJobProperties
	objectMap := make(map[string]interface{})
	if cjp.RuntimeVersion != nil {
		objectMap["runtimeVersion"] = cjp.RuntimeVersion
	}
	if cjp.Script != nil {
		objectMap["script"] = cjp.Script
	}
	if cjp.Type != "" {
		objectMap["type"] = cjp.Type
	}
	return json.Marshal(objectMap)
}

// AsCreateUSQLJobProperties is the BasicCreateJobProperties implementation for CreateJobProperties.
func (cjp CreateJobProperties) AsCreateUSQLJobProperties() (*CreateUSQLJobProperties, bool) {
	return nil, false
}

// AsCreateScopeJobProperties is the BasicCreateJobProperties implementation for CreateJobProperties.
func (cjp CreateJobProperties) AsCreateScopeJobProperties() (*CreateScopeJobProperties, bool) {
	return nil, false
}

// AsCreateJobProperties is the BasicCreateJobProperties implementation for CreateJobProperties.
func (cjp CreateJobProperties) AsCreateJobProperties() (*CreateJobProperties, bool) {
	return &cjp, true
}

// AsBasicCreateJobProperties is the BasicCreateJobProperties implementation for CreateJobProperties.
func (cjp CreateJobProperties) AsBasicCreateJobProperties() (BasicCreateJobProperties, bool) {
	return &cjp, true
}

// CreateScopeJobParameters the parameters used to submit a new Data Lake Analytics Scope job. (Only for use
// internally with Scope job type.)
type CreateScopeJobParameters struct {
	// Tags - the key-value pairs used to add additional metadata to the job information. (Only for use internally with Scope job type.)
	Tags map[string]*string `json:"tags"`
	// Name - the friendly name of the job to submit.
	Name *string `json:"name,omitempty"`
	// DegreeOfParallelism - the degree of parallelism to use for this job. This must be greater than 0, if set to less than 0 it will default to 1.
	DegreeOfParallelism *int32 `json:"degreeOfParallelism,omitempty"`
	// Priority - the priority value to use for the current job. Lower numbers have a higher priority. By default, a job has a priority of 1000. This must be greater than 0.
	Priority *int32 `json:"priority,omitempty"`
	// LogFilePatterns - the list of log file name patterns to find in the logFolder. '*' is the only matching character allowed. Example format: jobExecution*.log or *mylog*.txt
	LogFilePatterns *[]string `json:"logFilePatterns,omitempty"`
	// Related - the recurring job relationship information properties.
	Related *RelationshipProperties `json:"related,omitempty"`
	// Type - the job type of the current job (Hive, USql, or Scope (for internal use only)). Possible values include: 'USQL', 'Hive', 'Scope'
	Type TypeEnum `json:"type,omitempty"`
	// Properties - the job specific properties.
	Properties BasicCreateJobProperties `json:"properties,omitempty"`
}

// MarshalJSON is the custom marshaler for CreateScopeJobParameters.
func (csjp CreateScopeJobParameters) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if csjp.Tags != nil {
		objectMap["tags"] = csjp.Tags
	}
	if csjp.Name != nil {
		objectMap["name"] = csjp.Name
	}
	if csjp.DegreeOfParallelism != nil {
		objectMap["degreeOfParallelism"] = csjp.DegreeOfParallelism
	}
	if csjp.Priority != nil {
		objectMap["priority"] = csjp.Priority
	}
	if csjp.LogFilePatterns != nil {
		objectMap["logFilePatterns"] = csjp.LogFilePatterns
	}
	if csjp.Related != nil {
		objectMap["related"] = csjp.Related
	}
	if csjp.Type != "" {
		objectMap["type"] = csjp.Type
	}
	objectMap["properties"] = csjp.Properties
	return json.Marshal(objectMap)
}

// UnmarshalJSON is the custom unmarshaler for CreateScopeJobParameters struct.
func (csjp *CreateScopeJobParameters) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "tags":
			if v != nil {
				var tags map[string]*string
				err = json.Unmarshal(*v, &tags)
				if err != nil {
					return err
				}
				csjp.Tags = tags
			}
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				csjp.Name = &name
			}
		case "degreeOfParallelism":
			if v != nil {
				var degreeOfParallelism int32
				err = json.Unmarshal(*v, &degreeOfParallelism)
				if err != nil {
					return err
				}
				csjp.DegreeOfParallelism = &degreeOfParallelism
			}
		case "priority":
			if v != nil {
				var priority int32
				err = json.Unmarshal(*v, &priority)
				if err != nil {
					return err
				}
				csjp.Priority = &priority
			}
		case "logFilePatterns":
			if v != nil {
				var logFilePatterns []string
				err = json.Unmarshal(*v, &logFilePatterns)
				if err != nil {
					return err
				}
				csjp.LogFilePatterns = &logFilePatterns
			}
		case "related":
			if v != nil {
				var related RelationshipProperties
				err = json.Unmarshal(*v, &related)
				if err != nil {
					return err
				}
				csjp.Related = &related
			}
		case "type":
			if v != nil {
				var typeVar TypeEnum
				err = json.Unmarshal(*v, &typeVar)
				if err != nil {
					return err
				}
				csjp.Type = typeVar
			}
		case "properties":
			if v != nil {
				properties, err := unmarshalBasicCreateJobProperties(*v)
				if err != nil {
					return err
				}
				csjp.Properties = properties
			}
		}
	}

	return nil
}

// CreateScopeJobProperties scope job properties used when submitting Scope jobs.
type CreateScopeJobProperties struct {
	// Resources - the list of resources that are required by the job.
	Resources *[]ScopeJobResource `json:"resources,omitempty"`
	// Notifier - the list of email addresses, separated by semi-colons, to notify when the job reaches a terminal state.
	Notifier *string `json:"notifier,omitempty"`
	// RuntimeVersion - the runtime version of the Data Lake Analytics engine to use for the specific type of job being run.
	RuntimeVersion *string `json:"runtimeVersion,omitempty"`
	// Script - the script to run. Please note that the maximum script size is 3 MB.
	Script *string `json:"script,omitempty"`
	// Type - Possible values include: 'TypeBasicCreateJobPropertiesTypeCreateJobProperties', 'TypeBasicCreateJobPropertiesTypeUSQL', 'TypeBasicCreateJobPropertiesTypeScope'
	Type TypeBasicCreateJobProperties `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for CreateScopeJobProperties.
func (csjp CreateScopeJobProperties) MarshalJSON() ([]byte, error) {
	csjp.Type = TypeBasicCreateJobPropertiesTypeScope
	objectMap := make(map[string]interface{})
	if csjp.Resources != nil {
		objectMap["resources"] = csjp.Resources
	}
	if csjp.Notifier != nil {
		objectMap["notifier"] = csjp.Notifier
	}
	if csjp.RuntimeVersion != nil {
		objectMap["runtimeVersion"] = csjp.RuntimeVersion
	}
	if csjp.Script != nil {
		objectMap["script"] = csjp.Script
	}
	if csjp.Type != "" {
		objectMap["type"] = csjp.Type
	}
	return json.Marshal(objectMap)
}

// AsCreateUSQLJobProperties is the BasicCreateJobProperties implementation for CreateScopeJobProperties.
func (csjp CreateScopeJobProperties) AsCreateUSQLJobProperties() (*CreateUSQLJobProperties, bool) {
	return nil, false
}

// AsCreateScopeJobProperties is the BasicCreateJobProperties implementation for CreateScopeJobProperties.
func (csjp CreateScopeJobProperties) AsCreateScopeJobProperties() (*CreateScopeJobProperties, bool) {
	return &csjp, true
}

// AsCreateJobProperties is the BasicCreateJobProperties implementation for CreateScopeJobProperties.
func (csjp CreateScopeJobProperties) AsCreateJobProperties() (*CreateJobProperties, bool) {
	return nil, false
}

// AsBasicCreateJobProperties is the BasicCreateJobProperties implementation for CreateScopeJobProperties.
func (csjp CreateScopeJobProperties) AsBasicCreateJobProperties() (BasicCreateJobProperties, bool) {
	return &csjp, true
}

// CreateUSQLJobProperties u-SQL job properties used when submitting U-SQL jobs.
type CreateUSQLJobProperties struct {
	// CompileMode - the specific compilation mode for the job used during execution. If this is not specified during submission, the server will determine the optimal compilation mode. Possible values include: 'Semantic', 'Full', 'SingleBox'
	CompileMode CompileMode `json:"compileMode,omitempty"`
	// RuntimeVersion - the runtime version of the Data Lake Analytics engine to use for the specific type of job being run.
	RuntimeVersion *string `json:"runtimeVersion,omitempty"`
	// Script - the script to run. Please note that the maximum script size is 3 MB.
	Script *string `json:"script,omitempty"`
	// Type - Possible values include: 'TypeBasicCreateJobPropertiesTypeCreateJobProperties', 'TypeBasicCreateJobPropertiesTypeUSQL', 'TypeBasicCreateJobPropertiesTypeScope'
	Type TypeBasicCreateJobProperties `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for CreateUSQLJobProperties.
func (cusjp CreateUSQLJobProperties) MarshalJSON() ([]byte, error) {
	cusjp.Type = TypeBasicCreateJobPropertiesTypeUSQL
	objectMap := make(map[string]interface{})
	if cusjp.CompileMode != "" {
		objectMap["compileMode"] = cusjp.CompileMode
	}
	if cusjp.RuntimeVersion != nil {
		objectMap["runtimeVersion"] = cusjp.RuntimeVersion
	}
	if cusjp.Script != nil {
		objectMap["script"] = cusjp.Script
	}
	if cusjp.Type != "" {
		objectMap["type"] = cusjp.Type
	}
	return json.Marshal(objectMap)
}

// AsCreateUSQLJobProperties is the BasicCreateJobProperties implementation for CreateUSQLJobProperties.
func (cusjp CreateUSQLJobProperties) AsCreateUSQLJobProperties() (*CreateUSQLJobProperties, bool) {
	return &cusjp, true
}

// AsCreateScopeJobProperties is the BasicCreateJobProperties implementation for CreateUSQLJobProperties.
func (cusjp CreateUSQLJobProperties) AsCreateScopeJobProperties() (*CreateScopeJobProperties, bool) {
	return nil, false
}

// AsCreateJobProperties is the BasicCreateJobProperties implementation for CreateUSQLJobProperties.
func (cusjp CreateUSQLJobProperties) AsCreateJobProperties() (*CreateJobProperties, bool) {
	return nil, false
}

// AsBasicCreateJobProperties is the BasicCreateJobProperties implementation for CreateUSQLJobProperties.
func (cusjp CreateUSQLJobProperties) AsBasicCreateJobProperties() (BasicCreateJobProperties, bool) {
	return &cusjp, true
}

// DataPath a Data Lake Analytics job data path item.
type DataPath struct {
	autorest.Response `json:"-"`
	// JobID - the id of the job this data is for.
	JobID *uuid.UUID `json:"jobId,omitempty"`
	// Command - the command that this job data relates to.
	Command *string `json:"command,omitempty"`
	// Paths - the list of paths to all of the job data.
	Paths *[]string `json:"paths,omitempty"`
}

// Diagnostics error diagnostic information for failed jobs.
type Diagnostics struct {
	// ColumnNumber - the column where the error occured.
	ColumnNumber *int32 `json:"columnNumber,omitempty"`
	// End - the ending index of the error.
	End *int32 `json:"end,omitempty"`
	// LineNumber - the line number the error occured on.
	LineNumber *int32 `json:"lineNumber,omitempty"`
	// Message - the error message.
	Message *string `json:"message,omitempty"`
	// Severity - the severity of the error. Possible values include: 'Warning', 'Error', 'Info', 'SevereWarning', 'Deprecated', 'UserWarning'
	Severity SeverityTypes `json:"severity,omitempty"`
	// Start - the starting index of the error.
	Start *int32 `json:"start,omitempty"`
}

// ErrorDetails the Data Lake Analytics job error details.
type ErrorDetails struct {
	// Description - the error message description
	Description *string `json:"description,omitempty"`
	// Details - the details of the error message.
	Details *string `json:"details,omitempty"`
	// EndOffset - the end offset in the job where the error was found.
	EndOffset *int32 `json:"endOffset,omitempty"`
	// ErrorID - the specific identifier for the type of error encountered in the job.
	ErrorID *string `json:"errorId,omitempty"`
	// FilePath - the path to any supplemental error files, if any.
	FilePath *string `json:"filePath,omitempty"`
	// HelpLink - the link to MSDN or Azure help for this type of error, if any.
	HelpLink *string `json:"helpLink,omitempty"`
	// InternalDiagnostics - the internal diagnostic stack trace if the user requesting the job error details has sufficient permissions it will be retrieved, otherwise it will be empty.
	InternalDiagnostics *string `json:"internalDiagnostics,omitempty"`
	// LineNumber - the specific line number in the job where the error occured.
	LineNumber *int32 `json:"lineNumber,omitempty"`
	// Message - the user friendly error message for the failure.
	Message *string `json:"message,omitempty"`
	// Resolution - the recommended resolution for the failure, if any.
	Resolution *string `json:"resolution,omitempty"`
	// InnerError - the inner error of this specific job error message, if any.
	InnerError *InnerError `json:"innerError,omitempty"`
	// Severity - the severity level of the failure. Possible values include: 'Warning', 'Error', 'Info', 'SevereWarning', 'Deprecated', 'UserWarning'
	Severity SeverityTypes `json:"severity,omitempty"`
	// Source - the ultimate source of the failure (usually either SYSTEM or USER).
	Source *string `json:"source,omitempty"`
	// StartOffset - the start offset in the job where the error was found
	StartOffset *int32 `json:"startOffset,omitempty"`
}

// HiveJobProperties hive job properties used when retrieving Hive jobs.
type HiveJobProperties struct {
	// LogsLocation - the Hive logs location
	LogsLocation *string `json:"logsLocation,omitempty"`
	// OutputLocation - the location of Hive job output files (both execution output and results)
	OutputLocation *string `json:"outputLocation,omitempty"`
	// StatementCount - the number of statements that will be run based on the script
	StatementCount *int32 `json:"statementCount,omitempty"`
	// ExecutedStatementCount - the number of statements that have been run based on the script
	ExecutedStatementCount *int32 `json:"executedStatementCount,omitempty"`
	// RuntimeVersion - the runtime version of the Data Lake Analytics engine to use for the specific type of job being run.
	RuntimeVersion *string `json:"runtimeVersion,omitempty"`
	// Script - the script to run. Please note that the maximum script size is 3 MB.
	Script *string `json:"script,omitempty"`
	// Type - Possible values include: 'TypeJobProperties', 'TypeUSQL', 'TypeScope', 'TypeHive'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for HiveJobProperties.
func (hjp HiveJobProperties) MarshalJSON() ([]byte, error) {
	hjp.Type = TypeHive
	objectMap := make(map[string]interface{})
	if hjp.LogsLocation != nil {
		objectMap["logsLocation"] = hjp.LogsLocation
	}
	if hjp.OutputLocation != nil {
		objectMap["outputLocation"] = hjp.OutputLocation
	}
	if hjp.StatementCount != nil {
		objectMap["statementCount"] = hjp.StatementCount
	}
	if hjp.ExecutedStatementCount != nil {
		objectMap["executedStatementCount"] = hjp.ExecutedStatementCount
	}
	if hjp.RuntimeVersion != nil {
		objectMap["runtimeVersion"] = hjp.RuntimeVersion
	}
	if hjp.Script != nil {
		objectMap["script"] = hjp.Script
	}
	if hjp.Type != "" {
		objectMap["type"] = hjp.Type
	}
	return json.Marshal(objectMap)
}

// AsUSQLJobProperties is the BasicProperties implementation for HiveJobProperties.
func (hjp HiveJobProperties) AsUSQLJobProperties() (*USQLJobProperties, bool) {
	return nil, false
}

// AsScopeJobProperties is the BasicProperties implementation for HiveJobProperties.
func (hjp HiveJobProperties) AsScopeJobProperties() (*ScopeJobProperties, bool) {
	return nil, false
}

// AsHiveJobProperties is the BasicProperties implementation for HiveJobProperties.
func (hjp HiveJobProperties) AsHiveJobProperties() (*HiveJobProperties, bool) {
	return &hjp, true
}

// AsProperties is the BasicProperties implementation for HiveJobProperties.
func (hjp HiveJobProperties) AsProperties() (*Properties, bool) {
	return nil, false
}

// AsBasicProperties is the BasicProperties implementation for HiveJobProperties.
func (hjp HiveJobProperties) AsBasicProperties() (BasicProperties, bool) {
	return &hjp, true
}

// InfoListResult list of JobInfo items.
type InfoListResult struct {
	autorest.Response `json:"-"`
	// Value - the list of JobInfo items.
	Value *[]InformationBasic `json:"value,omitempty"`
	// NextLink - the link (url) to the next page of results.
	NextLink *string `json:"nextLink,omitempty"`
}

// InfoListResultIterator provides access to a complete listing of InformationBasic values.
type InfoListResultIterator struct {
	i    int
	page InfoListResultPage
}

// Next advances to the next value.  If there was an error making
// the request the iterator does not advance and the error is returned.
func (iter *InfoListResultIterator) Next() error {
	iter.i++
	if iter.i < len(iter.page.Values()) {
		return nil
	}
	err := iter.page.Next()
	if err != nil {
		iter.i--
		return err
	}
	iter.i = 0
	return nil
}

// NotDone returns true if the enumeration should be started or is not yet complete.
func (iter InfoListResultIterator) NotDone() bool {
	return iter.page.NotDone() && iter.i < len(iter.page.Values())
}

// Response returns the raw server response from the last page request.
func (iter InfoListResultIterator) Response() InfoListResult {
	return iter.page.Response()
}

// Value returns the current value or a zero-initialized value if the
// iterator has advanced beyond the end of the collection.
func (iter InfoListResultIterator) Value() InformationBasic {
	if !iter.page.NotDone() {
		return InformationBasic{}
	}
	return iter.page.Values()[iter.i]
}

// IsEmpty returns true if the ListResult contains no values.
func (ilr InfoListResult) IsEmpty() bool {
	return ilr.Value == nil || len(*ilr.Value) == 0
}

// infoListResultPreparer prepares a request to retrieve the next set of results.
// It returns nil if no more results exist.
func (ilr InfoListResult) infoListResultPreparer() (*http.Request, error) {
	if ilr.NextLink == nil || len(to.String(ilr.NextLink)) < 1 {
		return nil, nil
	}
	return autorest.Prepare(&http.Request{},
		autorest.AsJSON(),
		autorest.AsGet(),
		autorest.WithBaseURL(to.String(ilr.NextLink)))
}

// InfoListResultPage contains a page of InformationBasic values.
type InfoListResultPage struct {
	fn  func(InfoListResult) (InfoListResult, error)
	ilr InfoListResult
}

// Next advances to the next page of values.  If there was an error making
// the request the page does not advance and the error is returned.
func (page *InfoListResultPage) Next() error {
	next, err := page.fn(page.ilr)
	if err != nil {
		return err
	}
	page.ilr = next
	return nil
}

// NotDone returns true if the page enumeration should be started or is not yet complete.
func (page InfoListResultPage) NotDone() bool {
	return !page.ilr.IsEmpty()
}

// Response returns the raw server response from the last page request.
func (page InfoListResultPage) Response() InfoListResult {
	return page.ilr
}

// Values returns the slice of values for the current page or nil if there are no values.
func (page InfoListResultPage) Values() []InformationBasic {
	if page.ilr.IsEmpty() {
		return nil
	}
	return *page.ilr.Value
}

// Information the extended Data Lake Analytics job information properties returned when retrieving a specific job.
type Information struct {
	autorest.Response `json:"-"`
	// ErrorMessage - the error message details for the job, if the job failed.
	ErrorMessage *[]ErrorDetails `json:"errorMessage,omitempty"`
	// StateAuditRecords - the job state audit records, indicating when various operations have been performed on this job.
	StateAuditRecords *[]StateAuditRecord `json:"stateAuditRecords,omitempty"`
	// Properties - the job specific properties.
	Properties BasicProperties `json:"properties,omitempty"`
	// JobID - the job's unique identifier (a GUID).
	JobID *uuid.UUID `json:"jobId,omitempty"`
	// Name - the friendly name of the job.
	Name *string `json:"name,omitempty"`
	// Type - the job type of the current job (Hive, USql, or Scope (for internal use only)). Possible values include: 'USQL', 'Hive', 'Scope'
	Type TypeEnum `json:"type,omitempty"`
	// Submitter - the user or account that submitted the job.
	Submitter *string `json:"submitter,omitempty"`
	// DegreeOfParallelism - the degree of parallelism used for this job. This must be greater than 0, if set to less than 0 it will default to 1.
	DegreeOfParallelism *int32 `json:"degreeOfParallelism,omitempty"`
	// Priority - the priority value for the current job. Lower numbers have a higher priority. By default, a job has a priority of 1000. This must be greater than 0.
	Priority *int32 `json:"priority,omitempty"`
	// SubmitTime - the time the job was submitted to the service.
	SubmitTime *date.Time `json:"submitTime,omitempty"`
	// StartTime - the start time of the job.
	StartTime *date.Time `json:"startTime,omitempty"`
	// EndTime - the completion time of the job.
	EndTime *date.Time `json:"endTime,omitempty"`
	// State - the job state. When the job is in the Ended state, refer to Result and ErrorMessage for details. Possible values include: 'StateAccepted', 'StateCompiling', 'StateEnded', 'StateNew', 'StateQueued', 'StateRunning', 'StateScheduling', 'StateStarting', 'StatePaused', 'StateWaitingForCapacity'
	State State `json:"state,omitempty"`
	// Result - the result of job execution or the current result of the running job. Possible values include: 'None', 'Succeeded', 'Cancelled', 'Failed'
	Result Result `json:"result,omitempty"`
	// LogFolder - the log folder path to use in the following format: adl://<accountName>.azuredatalakestore.net/system/jobservice/jobs/Usql/2016/03/13/17/18/5fe51957-93bc-4de0-8ddc-c5a4753b068b/logs/.
	LogFolder *string `json:"logFolder,omitempty"`
	// LogFilePatterns - the list of log file name patterns to find in the logFolder. '*' is the only matching character allowed. Example format: jobExecution*.log or *mylog*.txt
	LogFilePatterns *[]string `json:"logFilePatterns,omitempty"`
	// Related - the recurring job relationship information properties.
	Related *RelationshipProperties `json:"related,omitempty"`
	// Tags - the key-value pairs used to add additional metadata to the job information. (Only for use internally with Scope job type.)
	Tags map[string]*string `json:"tags"`
}

// MarshalJSON is the custom marshaler for Information.
func (i Information) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if i.ErrorMessage != nil {
		objectMap["errorMessage"] = i.ErrorMessage
	}
	if i.StateAuditRecords != nil {
		objectMap["stateAuditRecords"] = i.StateAuditRecords
	}
	objectMap["properties"] = i.Properties
	if i.JobID != nil {
		objectMap["jobId"] = i.JobID
	}
	if i.Name != nil {
		objectMap["name"] = i.Name
	}
	if i.Type != "" {
		objectMap["type"] = i.Type
	}
	if i.Submitter != nil {
		objectMap["submitter"] = i.Submitter
	}
	if i.DegreeOfParallelism != nil {
		objectMap["degreeOfParallelism"] = i.DegreeOfParallelism
	}
	if i.Priority != nil {
		objectMap["priority"] = i.Priority
	}
	if i.SubmitTime != nil {
		objectMap["submitTime"] = i.SubmitTime
	}
	if i.StartTime != nil {
		objectMap["startTime"] = i.StartTime
	}
	if i.EndTime != nil {
		objectMap["endTime"] = i.EndTime
	}
	if i.State != "" {
		objectMap["state"] = i.State
	}
	if i.Result != "" {
		objectMap["result"] = i.Result
	}
	if i.LogFolder != nil {
		objectMap["logFolder"] = i.LogFolder
	}
	if i.LogFilePatterns != nil {
		objectMap["logFilePatterns"] = i.LogFilePatterns
	}
	if i.Related != nil {
		objectMap["related"] = i.Related
	}
	if i.Tags != nil {
		objectMap["tags"] = i.Tags
	}
	return json.Marshal(objectMap)
}

// UnmarshalJSON is the custom unmarshaler for Information struct.
func (i *Information) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "errorMessage":
			if v != nil {
				var errorMessage []ErrorDetails
				err = json.Unmarshal(*v, &errorMessage)
				if err != nil {
					return err
				}
				i.ErrorMessage = &errorMessage
			}
		case "stateAuditRecords":
			if v != nil {
				var stateAuditRecords []StateAuditRecord
				err = json.Unmarshal(*v, &stateAuditRecords)
				if err != nil {
					return err
				}
				i.StateAuditRecords = &stateAuditRecords
			}
		case "properties":
			if v != nil {
				properties, err := unmarshalBasicProperties(*v)
				if err != nil {
					return err
				}
				i.Properties = properties
			}
		case "jobId":
			if v != nil {
				var jobID uuid.UUID
				err = json.Unmarshal(*v, &jobID)
				if err != nil {
					return err
				}
				i.JobID = &jobID
			}
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				i.Name = &name
			}
		case "type":
			if v != nil {
				var typeVar TypeEnum
				err = json.Unmarshal(*v, &typeVar)
				if err != nil {
					return err
				}
				i.Type = typeVar
			}
		case "submitter":
			if v != nil {
				var submitter string
				err = json.Unmarshal(*v, &submitter)
				if err != nil {
					return err
				}
				i.Submitter = &submitter
			}
		case "degreeOfParallelism":
			if v != nil {
				var degreeOfParallelism int32
				err = json.Unmarshal(*v, &degreeOfParallelism)
				if err != nil {
					return err
				}
				i.DegreeOfParallelism = &degreeOfParallelism
			}
		case "priority":
			if v != nil {
				var priority int32
				err = json.Unmarshal(*v, &priority)
				if err != nil {
					return err
				}
				i.Priority = &priority
			}
		case "submitTime":
			if v != nil {
				var submitTime date.Time
				err = json.Unmarshal(*v, &submitTime)
				if err != nil {
					return err
				}
				i.SubmitTime = &submitTime
			}
		case "startTime":
			if v != nil {
				var startTime date.Time
				err = json.Unmarshal(*v, &startTime)
				if err != nil {
					return err
				}
				i.StartTime = &startTime
			}
		case "endTime":
			if v != nil {
				var endTime date.Time
				err = json.Unmarshal(*v, &endTime)
				if err != nil {
					return err
				}
				i.EndTime = &endTime
			}
		case "state":
			if v != nil {
				var state State
				err = json.Unmarshal(*v, &state)
				if err != nil {
					return err
				}
				i.State = state
			}
		case "result":
			if v != nil {
				var resultVar Result
				err = json.Unmarshal(*v, &resultVar)
				if err != nil {
					return err
				}
				i.Result = resultVar
			}
		case "logFolder":
			if v != nil {
				var logFolder string
				err = json.Unmarshal(*v, &logFolder)
				if err != nil {
					return err
				}
				i.LogFolder = &logFolder
			}
		case "logFilePatterns":
			if v != nil {
				var logFilePatterns []string
				err = json.Unmarshal(*v, &logFilePatterns)
				if err != nil {
					return err
				}
				i.LogFilePatterns = &logFilePatterns
			}
		case "related":
			if v != nil {
				var related RelationshipProperties
				err = json.Unmarshal(*v, &related)
				if err != nil {
					return err
				}
				i.Related = &related
			}
		case "tags":
			if v != nil {
				var tags map[string]*string
				err = json.Unmarshal(*v, &tags)
				if err != nil {
					return err
				}
				i.Tags = tags
			}
		}
	}

	return nil
}

// InformationBasic the common Data Lake Analytics job information properties.
type InformationBasic struct {
	// JobID - the job's unique identifier (a GUID).
	JobID *uuid.UUID `json:"jobId,omitempty"`
	// Name - the friendly name of the job.
	Name *string `json:"name,omitempty"`
	// Type - the job type of the current job (Hive, USql, or Scope (for internal use only)). Possible values include: 'USQL', 'Hive', 'Scope'
	Type TypeEnum `json:"type,omitempty"`
	// Submitter - the user or account that submitted the job.
	Submitter *string `json:"submitter,omitempty"`
	// DegreeOfParallelism - the degree of parallelism used for this job. This must be greater than 0, if set to less than 0 it will default to 1.
	DegreeOfParallelism *int32 `json:"degreeOfParallelism,omitempty"`
	// Priority - the priority value for the current job. Lower numbers have a higher priority. By default, a job has a priority of 1000. This must be greater than 0.
	Priority *int32 `json:"priority,omitempty"`
	// SubmitTime - the time the job was submitted to the service.
	SubmitTime *date.Time `json:"submitTime,omitempty"`
	// StartTime - the start time of the job.
	StartTime *date.Time `json:"startTime,omitempty"`
	// EndTime - the completion time of the job.
	EndTime *date.Time `json:"endTime,omitempty"`
	// State - the job state. When the job is in the Ended state, refer to Result and ErrorMessage for details. Possible values include: 'StateAccepted', 'StateCompiling', 'StateEnded', 'StateNew', 'StateQueued', 'StateRunning', 'StateScheduling', 'StateStarting', 'StatePaused', 'StateWaitingForCapacity'
	State State `json:"state,omitempty"`
	// Result - the result of job execution or the current result of the running job. Possible values include: 'None', 'Succeeded', 'Cancelled', 'Failed'
	Result Result `json:"result,omitempty"`
	// LogFolder - the log folder path to use in the following format: adl://<accountName>.azuredatalakestore.net/system/jobservice/jobs/Usql/2016/03/13/17/18/5fe51957-93bc-4de0-8ddc-c5a4753b068b/logs/.
	LogFolder *string `json:"logFolder,omitempty"`
	// LogFilePatterns - the list of log file name patterns to find in the logFolder. '*' is the only matching character allowed. Example format: jobExecution*.log or *mylog*.txt
	LogFilePatterns *[]string `json:"logFilePatterns,omitempty"`
	// Related - the recurring job relationship information properties.
	Related *RelationshipProperties `json:"related,omitempty"`
	// Tags - the key-value pairs used to add additional metadata to the job information. (Only for use internally with Scope job type.)
	Tags map[string]*string `json:"tags"`
}

// MarshalJSON is the custom marshaler for InformationBasic.
func (ib InformationBasic) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if ib.JobID != nil {
		objectMap["jobId"] = ib.JobID
	}
	if ib.Name != nil {
		objectMap["name"] = ib.Name
	}
	if ib.Type != "" {
		objectMap["type"] = ib.Type
	}
	if ib.Submitter != nil {
		objectMap["submitter"] = ib.Submitter
	}
	if ib.DegreeOfParallelism != nil {
		objectMap["degreeOfParallelism"] = ib.DegreeOfParallelism
	}
	if ib.Priority != nil {
		objectMap["priority"] = ib.Priority
	}
	if ib.SubmitTime != nil {
		objectMap["submitTime"] = ib.SubmitTime
	}
	if ib.StartTime != nil {
		objectMap["startTime"] = ib.StartTime
	}
	if ib.EndTime != nil {
		objectMap["endTime"] = ib.EndTime
	}
	if ib.State != "" {
		objectMap["state"] = ib.State
	}
	if ib.Result != "" {
		objectMap["result"] = ib.Result
	}
	if ib.LogFolder != nil {
		objectMap["logFolder"] = ib.LogFolder
	}
	if ib.LogFilePatterns != nil {
		objectMap["logFilePatterns"] = ib.LogFilePatterns
	}
	if ib.Related != nil {
		objectMap["related"] = ib.Related
	}
	if ib.Tags != nil {
		objectMap["tags"] = ib.Tags
	}
	return json.Marshal(objectMap)
}

// InnerError the Data Lake Analytics job error details.
type InnerError struct {
	// DiagnosticCode - the diagnostic error code.
	DiagnosticCode *int32 `json:"diagnosticCode,omitempty"`
	// Severity - the severity level of the failure. Possible values include: 'Warning', 'Error', 'Info', 'SevereWarning', 'Deprecated', 'UserWarning'
	Severity SeverityTypes `json:"severity,omitempty"`
	// Details - the details of the error message.
	Details *string `json:"details,omitempty"`
	// Component - the component that failed.
	Component *string `json:"component,omitempty"`
	// ErrorID - the specific identifier for the type of error encountered in the job.
	ErrorID *string `json:"errorId,omitempty"`
	// HelpLink - the link to MSDN or Azure help for this type of error, if any.
	HelpLink *string `json:"helpLink,omitempty"`
	// InternalDiagnostics - the internal diagnostic stack trace if the user requesting the job error details has sufficient permissions it will be retrieved, otherwise it will be empty.
	InternalDiagnostics *string `json:"internalDiagnostics,omitempty"`
	// Message - the user friendly error message for the failure.
	Message *string `json:"message,omitempty"`
	// Resolution - the recommended resolution for the failure, if any.
	Resolution *string `json:"resolution,omitempty"`
	// Source - the ultimate source of the failure (usually either SYSTEM or USER).
	Source *string `json:"source,omitempty"`
	// Description - the error message description
	Description *string `json:"description,omitempty"`
	// InnerError - the inner error of this specific job error message, if any.
	InnerError *InnerError `json:"innerError,omitempty"`
}

// PipelineInformation job Pipeline Information, showing the relationship of jobs and recurrences of those jobs in
// a pipeline.
type PipelineInformation struct {
	autorest.Response `json:"-"`
	// PipelineID - the job relationship pipeline identifier (a GUID).
	PipelineID *uuid.UUID `json:"pipelineId,omitempty"`
	// PipelineName - the friendly name of the job relationship pipeline, which does not need to be unique.
	PipelineName *string `json:"pipelineName,omitempty"`
	// PipelineURI - the pipeline uri, unique, links to the originating service for this pipeline.
	PipelineURI *string `json:"pipelineUri,omitempty"`
	// NumJobsFailed - the number of jobs in this pipeline that have failed.
	NumJobsFailed *int32 `json:"numJobsFailed,omitempty"`
	// NumJobsCanceled - the number of jobs in this pipeline that have been canceled.
	NumJobsCanceled *int32 `json:"numJobsCanceled,omitempty"`
	// NumJobsSucceeded - the number of jobs in this pipeline that have succeeded.
	NumJobsSucceeded *int32 `json:"numJobsSucceeded,omitempty"`
	// AuHoursFailed - the number of job execution hours that resulted in failed jobs.
	AuHoursFailed *float64 `json:"auHoursFailed,omitempty"`
	// AuHoursCanceled - the number of job execution hours that resulted in canceled jobs.
	AuHoursCanceled *float64 `json:"auHoursCanceled,omitempty"`
	// AuHoursSucceeded - the number of job execution hours that resulted in successful jobs.
	AuHoursSucceeded *float64 `json:"auHoursSucceeded,omitempty"`
	// LastSubmitTime - the last time a job in this pipeline was submitted.
	LastSubmitTime *date.Time `json:"lastSubmitTime,omitempty"`
	// Runs - the list of recurrence identifiers representing each run of this pipeline.
	Runs *[]PipelineRunInformation `json:"runs,omitempty"`
	// Recurrences - the list of recurrence identifiers representing each run of this pipeline.
	Recurrences *[]uuid.UUID `json:"recurrences,omitempty"`
}

// PipelineInformationListResult list of job pipeline information items.
type PipelineInformationListResult struct {
	autorest.Response `json:"-"`
	// Value - the list of job pipeline information items.
	Value *[]PipelineInformation `json:"value,omitempty"`
	// NextLink - the link (url) to the next page of results.
	NextLink *string `json:"nextLink,omitempty"`
}

// PipelineInformationListResultIterator provides access to a complete listing of PipelineInformation values.
type PipelineInformationListResultIterator struct {
	i    int
	page PipelineInformationListResultPage
}

// Next advances to the next value.  If there was an error making
// the request the iterator does not advance and the error is returned.
func (iter *PipelineInformationListResultIterator) Next() error {
	iter.i++
	if iter.i < len(iter.page.Values()) {
		return nil
	}
	err := iter.page.Next()
	if err != nil {
		iter.i--
		return err
	}
	iter.i = 0
	return nil
}

// NotDone returns true if the enumeration should be started or is not yet complete.
func (iter PipelineInformationListResultIterator) NotDone() bool {
	return iter.page.NotDone() && iter.i < len(iter.page.Values())
}

// Response returns the raw server response from the last page request.
func (iter PipelineInformationListResultIterator) Response() PipelineInformationListResult {
	return iter.page.Response()
}

// Value returns the current value or a zero-initialized value if the
// iterator has advanced beyond the end of the collection.
func (iter PipelineInformationListResultIterator) Value() PipelineInformation {
	if !iter.page.NotDone() {
		return PipelineInformation{}
	}
	return iter.page.Values()[iter.i]
}

// IsEmpty returns true if the ListResult contains no values.
func (pilr PipelineInformationListResult) IsEmpty() bool {
	return pilr.Value == nil || len(*pilr.Value) == 0
}

// pipelineInformationListResultPreparer prepares a request to retrieve the next set of results.
// It returns nil if no more results exist.
func (pilr PipelineInformationListResult) pipelineInformationListResultPreparer() (*http.Request, error) {
	if pilr.NextLink == nil || len(to.String(pilr.NextLink)) < 1 {
		return nil, nil
	}
	return autorest.Prepare(&http.Request{},
		autorest.AsJSON(),
		autorest.AsGet(),
		autorest.WithBaseURL(to.String(pilr.NextLink)))
}

// PipelineInformationListResultPage contains a page of PipelineInformation values.
type PipelineInformationListResultPage struct {
	fn   func(PipelineInformationListResult) (PipelineInformationListResult, error)
	pilr PipelineInformationListResult
}

// Next advances to the next page of values.  If there was an error making
// the request the page does not advance and the error is returned.
func (page *PipelineInformationListResultPage) Next() error {
	next, err := page.fn(page.pilr)
	if err != nil {
		return err
	}
	page.pilr = next
	return nil
}

// NotDone returns true if the page enumeration should be started or is not yet complete.
func (page PipelineInformationListResultPage) NotDone() bool {
	return !page.pilr.IsEmpty()
}

// Response returns the raw server response from the last page request.
func (page PipelineInformationListResultPage) Response() PipelineInformationListResult {
	return page.pilr
}

// Values returns the slice of values for the current page or nil if there are no values.
func (page PipelineInformationListResultPage) Values() []PipelineInformation {
	if page.pilr.IsEmpty() {
		return nil
	}
	return *page.pilr.Value
}

// PipelineRunInformation run info for a specific job pipeline.
type PipelineRunInformation struct {
	// RunID - the run identifier of an instance of pipeline executions (a GUID).
	RunID *uuid.UUID `json:"runId,omitempty"`
	// LastSubmitTime - the time this instance was last submitted.
	LastSubmitTime *date.Time `json:"lastSubmitTime,omitempty"`
}

// BasicProperties the common Data Lake Analytics job properties.
type BasicProperties interface {
	AsUSQLJobProperties() (*USQLJobProperties, bool)
	AsScopeJobProperties() (*ScopeJobProperties, bool)
	AsHiveJobProperties() (*HiveJobProperties, bool)
	AsProperties() (*Properties, bool)
}

// Properties the common Data Lake Analytics job properties.
type Properties struct {
	// RuntimeVersion - the runtime version of the Data Lake Analytics engine to use for the specific type of job being run.
	RuntimeVersion *string `json:"runtimeVersion,omitempty"`
	// Script - the script to run. Please note that the maximum script size is 3 MB.
	Script *string `json:"script,omitempty"`
	// Type - Possible values include: 'TypeJobProperties', 'TypeUSQL', 'TypeScope', 'TypeHive'
	Type Type `json:"type,omitempty"`
}

func unmarshalBasicProperties(body []byte) (BasicProperties, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["type"] {
	case string(TypeUSQL):
		var usjp USQLJobProperties
		err := json.Unmarshal(body, &usjp)
		return usjp, err
	case string(TypeScope):
		var sjp ScopeJobProperties
		err := json.Unmarshal(body, &sjp)
		return sjp, err
	case string(TypeHive):
		var hjp HiveJobProperties
		err := json.Unmarshal(body, &hjp)
		return hjp, err
	default:
		var p Properties
		err := json.Unmarshal(body, &p)
		return p, err
	}
}
func unmarshalBasicPropertiesArray(body []byte) ([]BasicProperties, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	pArray := make([]BasicProperties, len(rawMessages))

	for index, rawMessage := range rawMessages {
		p, err := unmarshalBasicProperties(*rawMessage)
		if err != nil {
			return nil, err
		}
		pArray[index] = p
	}
	return pArray, nil
}

// MarshalJSON is the custom marshaler for Properties.
func (p Properties) MarshalJSON() ([]byte, error) {
	p.Type = TypeJobProperties
	objectMap := make(map[string]interface{})
	if p.RuntimeVersion != nil {
		objectMap["runtimeVersion"] = p.RuntimeVersion
	}
	if p.Script != nil {
		objectMap["script"] = p.Script
	}
	if p.Type != "" {
		objectMap["type"] = p.Type
	}
	return json.Marshal(objectMap)
}

// AsUSQLJobProperties is the BasicProperties implementation for Properties.
func (p Properties) AsUSQLJobProperties() (*USQLJobProperties, bool) {
	return nil, false
}

// AsScopeJobProperties is the BasicProperties implementation for Properties.
func (p Properties) AsScopeJobProperties() (*ScopeJobProperties, bool) {
	return nil, false
}

// AsHiveJobProperties is the BasicProperties implementation for Properties.
func (p Properties) AsHiveJobProperties() (*HiveJobProperties, bool) {
	return nil, false
}

// AsProperties is the BasicProperties implementation for Properties.
func (p Properties) AsProperties() (*Properties, bool) {
	return &p, true
}

// AsBasicProperties is the BasicProperties implementation for Properties.
func (p Properties) AsBasicProperties() (BasicProperties, bool) {
	return &p, true
}

// RecurrenceInformation recurrence job information for a specific recurrence.
type RecurrenceInformation struct {
	autorest.Response `json:"-"`
	// RecurrenceID - the recurrence identifier (a GUID), unique per activity/script, regardless of iterations. This is something to link different occurrences of the same job together.
	RecurrenceID *uuid.UUID `json:"recurrenceId,omitempty"`
	// RecurrenceName - the recurrence name, user friendly name for the correlation between jobs.
	RecurrenceName *string `json:"recurrenceName,omitempty"`
	// NumJobsFailed - the number of jobs in this recurrence that have failed.
	NumJobsFailed *int32 `json:"numJobsFailed,omitempty"`
	// NumJobsCanceled - the number of jobs in this recurrence that have been canceled.
	NumJobsCanceled *int32 `json:"numJobsCanceled,omitempty"`
	// NumJobsSucceeded - the number of jobs in this recurrence that have succeeded.
	NumJobsSucceeded *int32 `json:"numJobsSucceeded,omitempty"`
	// AuHoursFailed - the number of job execution hours that resulted in failed jobs.
	AuHoursFailed *float64 `json:"auHoursFailed,omitempty"`
	// AuHoursCanceled - the number of job execution hours that resulted in canceled jobs.
	AuHoursCanceled *float64 `json:"auHoursCanceled,omitempty"`
	// AuHoursSucceeded - the number of job execution hours that resulted in successful jobs.
	AuHoursSucceeded *float64 `json:"auHoursSucceeded,omitempty"`
	// LastSubmitTime - the last time a job in this recurrence was submitted.
	LastSubmitTime *date.Time `json:"lastSubmitTime,omitempty"`
}

// RecurrenceInformationListResult list of job recurrence information items.
type RecurrenceInformationListResult struct {
	autorest.Response `json:"-"`
	// Value - the list of job recurrence information items.
	Value *[]RecurrenceInformation `json:"value,omitempty"`
	// NextLink - the link (url) to the next page of results.
	NextLink *string `json:"nextLink,omitempty"`
}

// RecurrenceInformationListResultIterator provides access to a complete listing of RecurrenceInformation values.
type RecurrenceInformationListResultIterator struct {
	i    int
	page RecurrenceInformationListResultPage
}

// Next advances to the next value.  If there was an error making
// the request the iterator does not advance and the error is returned.
func (iter *RecurrenceInformationListResultIterator) Next() error {
	iter.i++
	if iter.i < len(iter.page.Values()) {
		return nil
	}
	err := iter.page.Next()
	if err != nil {
		iter.i--
		return err
	}
	iter.i = 0
	return nil
}

// NotDone returns true if the enumeration should be started or is not yet complete.
func (iter RecurrenceInformationListResultIterator) NotDone() bool {
	return iter.page.NotDone() && iter.i < len(iter.page.Values())
}

// Response returns the raw server response from the last page request.
func (iter RecurrenceInformationListResultIterator) Response() RecurrenceInformationListResult {
	return iter.page.Response()
}

// Value returns the current value or a zero-initialized value if the
// iterator has advanced beyond the end of the collection.
func (iter RecurrenceInformationListResultIterator) Value() RecurrenceInformation {
	if !iter.page.NotDone() {
		return RecurrenceInformation{}
	}
	return iter.page.Values()[iter.i]
}

// IsEmpty returns true if the ListResult contains no values.
func (rilr RecurrenceInformationListResult) IsEmpty() bool {
	return rilr.Value == nil || len(*rilr.Value) == 0
}

// recurrenceInformationListResultPreparer prepares a request to retrieve the next set of results.
// It returns nil if no more results exist.
func (rilr RecurrenceInformationListResult) recurrenceInformationListResultPreparer() (*http.Request, error) {
	if rilr.NextLink == nil || len(to.String(rilr.NextLink)) < 1 {
		return nil, nil
	}
	return autorest.Prepare(&http.Request{},
		autorest.AsJSON(),
		autorest.AsGet(),
		autorest.WithBaseURL(to.String(rilr.NextLink)))
}

// RecurrenceInformationListResultPage contains a page of RecurrenceInformation values.
type RecurrenceInformationListResultPage struct {
	fn   func(RecurrenceInformationListResult) (RecurrenceInformationListResult, error)
	rilr RecurrenceInformationListResult
}

// Next advances to the next page of values.  If there was an error making
// the request the page does not advance and the error is returned.
func (page *RecurrenceInformationListResultPage) Next() error {
	next, err := page.fn(page.rilr)
	if err != nil {
		return err
	}
	page.rilr = next
	return nil
}

// NotDone returns true if the page enumeration should be started or is not yet complete.
func (page RecurrenceInformationListResultPage) NotDone() bool {
	return !page.rilr.IsEmpty()
}

// Response returns the raw server response from the last page request.
func (page RecurrenceInformationListResultPage) Response() RecurrenceInformationListResult {
	return page.rilr
}

// Values returns the slice of values for the current page or nil if there are no values.
func (page RecurrenceInformationListResultPage) Values() []RecurrenceInformation {
	if page.rilr.IsEmpty() {
		return nil
	}
	return *page.rilr.Value
}

// RelationshipProperties job relationship information properties including pipeline information, correlation
// information, etc.
type RelationshipProperties struct {
	// PipelineID - the job relationship pipeline identifier (a GUID).
	PipelineID *uuid.UUID `json:"pipelineId,omitempty"`
	// PipelineName - the friendly name of the job relationship pipeline, which does not need to be unique.
	PipelineName *string `json:"pipelineName,omitempty"`
	// PipelineURI - the pipeline uri, unique, links to the originating service for this pipeline.
	PipelineURI *string `json:"pipelineUri,omitempty"`
	// RunID - the run identifier (a GUID), unique identifier of the iteration of this pipeline.
	RunID *uuid.UUID `json:"runId,omitempty"`
	// RecurrenceID - the recurrence identifier (a GUID), unique per activity/script, regardless of iterations. This is something to link different occurrences of the same job together.
	RecurrenceID *uuid.UUID `json:"recurrenceId,omitempty"`
	// RecurrenceName - the recurrence name, user friendly name for the correlation between jobs.
	RecurrenceName *string `json:"recurrenceName,omitempty"`
}

// Resource the Data Lake Analytics job resources.
type Resource struct {
	// Name - the name of the resource.
	Name *string `json:"name,omitempty"`
	// ResourcePath - the path to the resource.
	ResourcePath *string `json:"resourcePath,omitempty"`
	// Type - the job resource type. Possible values include: 'VertexResource', 'JobManagerResource', 'StatisticsResource', 'VertexResourceInUserFolder', 'JobManagerResourceInUserFolder', 'StatisticsResourceInUserFolder'
	Type ResourceType `json:"type,omitempty"`
}

// ResourceUsageStatistics the statistics information for resource usage.
type ResourceUsageStatistics struct {
	// Average - the average value.
	Average *float64 `json:"average,omitempty"`
	// Minimum - the minimum value.
	Minimum *int64 `json:"minimum,omitempty"`
	// Maximum - the maximum value.
	Maximum *int64 `json:"maximum,omitempty"`
}

// ScopeJobProperties scope job properties used when submitting and retrieving Scope jobs. (Only for use internally
// with Scope job type.)
type ScopeJobProperties struct {
	// Resources - the list of resources that are required by the job
	Resources *[]ScopeJobResource `json:"resources,omitempty"`
	// UserAlgebraPath - the algebra file path after the job has completed
	UserAlgebraPath *string `json:"userAlgebraPath,omitempty"`
	// Notifier - the list of email addresses, separated by semi-colons, to notify when the job reaches a terminal state.
	Notifier *string `json:"notifier,omitempty"`
	// TotalCompilationTime - the total time this job spent compiling. This value should not be set by the user and will be ignored if it is.
	TotalCompilationTime *string `json:"totalCompilationTime,omitempty"`
	// TotalPausedTime - the total time this job spent paused. This value should not be set by the user and will be ignored if it is.
	TotalPausedTime *string `json:"totalPausedTime,omitempty"`
	// TotalQueuedTime - the total time this job spent queued. This value should not be set by the user and will be ignored if it is.
	TotalQueuedTime *string `json:"totalQueuedTime,omitempty"`
	// TotalRunningTime - the total time this job spent executing. This value should not be set by the user and will be ignored if it is.
	TotalRunningTime *string `json:"totalRunningTime,omitempty"`
	// RootProcessNodeID - the ID used to identify the job manager coordinating job execution. This value should not be set by the user and will be ignored if it is.
	RootProcessNodeID *string `json:"rootProcessNodeId,omitempty"`
	// YarnApplicationID - the ID used to identify the yarn application executing the job. This value should not be set by the user and will be ignored if it is.
	YarnApplicationID *string `json:"yarnApplicationId,omitempty"`
	// RuntimeVersion - the runtime version of the Data Lake Analytics engine to use for the specific type of job being run.
	RuntimeVersion *string `json:"runtimeVersion,omitempty"`
	// Script - the script to run. Please note that the maximum script size is 3 MB.
	Script *string `json:"script,omitempty"`
	// Type - Possible values include: 'TypeJobProperties', 'TypeUSQL', 'TypeScope', 'TypeHive'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for ScopeJobProperties.
func (sjp ScopeJobProperties) MarshalJSON() ([]byte, error) {
	sjp.Type = TypeScope
	objectMap := make(map[string]interface{})
	if sjp.Resources != nil {
		objectMap["resources"] = sjp.Resources
	}
	if sjp.UserAlgebraPath != nil {
		objectMap["userAlgebraPath"] = sjp.UserAlgebraPath
	}
	if sjp.Notifier != nil {
		objectMap["notifier"] = sjp.Notifier
	}
	if sjp.TotalCompilationTime != nil {
		objectMap["totalCompilationTime"] = sjp.TotalCompilationTime
	}
	if sjp.TotalPausedTime != nil {
		objectMap["totalPausedTime"] = sjp.TotalPausedTime
	}
	if sjp.TotalQueuedTime != nil {
		objectMap["totalQueuedTime"] = sjp.TotalQueuedTime
	}
	if sjp.TotalRunningTime != nil {
		objectMap["totalRunningTime"] = sjp.TotalRunningTime
	}
	if sjp.RootProcessNodeID != nil {
		objectMap["rootProcessNodeId"] = sjp.RootProcessNodeID
	}
	if sjp.YarnApplicationID != nil {
		objectMap["yarnApplicationId"] = sjp.YarnApplicationID
	}
	if sjp.RuntimeVersion != nil {
		objectMap["runtimeVersion"] = sjp.RuntimeVersion
	}
	if sjp.Script != nil {
		objectMap["script"] = sjp.Script
	}
	if sjp.Type != "" {
		objectMap["type"] = sjp.Type
	}
	return json.Marshal(objectMap)
}

// AsUSQLJobProperties is the BasicProperties implementation for ScopeJobProperties.
func (sjp ScopeJobProperties) AsUSQLJobProperties() (*USQLJobProperties, bool) {
	return nil, false
}

// AsScopeJobProperties is the BasicProperties implementation for ScopeJobProperties.
func (sjp ScopeJobProperties) AsScopeJobProperties() (*ScopeJobProperties, bool) {
	return &sjp, true
}

// AsHiveJobProperties is the BasicProperties implementation for ScopeJobProperties.
func (sjp ScopeJobProperties) AsHiveJobProperties() (*HiveJobProperties, bool) {
	return nil, false
}

// AsProperties is the BasicProperties implementation for ScopeJobProperties.
func (sjp ScopeJobProperties) AsProperties() (*Properties, bool) {
	return nil, false
}

// AsBasicProperties is the BasicProperties implementation for ScopeJobProperties.
func (sjp ScopeJobProperties) AsBasicProperties() (BasicProperties, bool) {
	return &sjp, true
}

// ScopeJobResource the Scope job resources. (Only for use internally with Scope job type.)
type ScopeJobResource struct {
	// Name - the name of the resource.
	Name *string `json:"name,omitempty"`
	// Path - the path to the resource.
	Path *string `json:"path,omitempty"`
}

// StateAuditRecord the Data Lake Analytics job state audit records for tracking the lifecycle of a job.
type StateAuditRecord struct {
	// NewState - the new state the job is in.
	NewState *string `json:"newState,omitempty"`
	// TimeStamp - the time stamp that the state change took place.
	TimeStamp *date.Time `json:"timeStamp,omitempty"`
	// RequestedByUser - the user who requests the change.
	RequestedByUser *string `json:"requestedByUser,omitempty"`
	// Details - the details of the audit log.
	Details *string `json:"details,omitempty"`
}

// Statistics the Data Lake Analytics job execution statistics.
type Statistics struct {
	autorest.Response `json:"-"`
	// LastUpdateTimeUtc - the last update time for the statistics.
	LastUpdateTimeUtc *date.Time `json:"lastUpdateTimeUtc,omitempty"`
	// FinalizingTimeUtc - the job finalizing start time.
	FinalizingTimeUtc *date.Time `json:"finalizingTimeUtc,omitempty"`
	// Stages - the list of stages for the job.
	Stages *[]StatisticsVertexStage `json:"stages,omitempty"`
}

// StatisticsVertex the detailed information for a vertex.
type StatisticsVertex struct {
	// Name - the name of the vertex.
	Name *string `json:"name,omitempty"`
	// VertexID - the id of the vertex.
	VertexID *uuid.UUID `json:"vertexId,omitempty"`
	// ExecutionTime - the amount of execution time of the vertex.
	ExecutionTime *string `json:"executionTime,omitempty"`
	// DataRead - the amount of data read of the vertex, in bytes.
	DataRead *int64 `json:"dataRead,omitempty"`
	// PeakMemUsage - the amount of peak memory usage of the vertex, in bytes.
	PeakMemUsage *int64 `json:"peakMemUsage,omitempty"`
}

// StatisticsVertexStage the Data Lake Analytics job statistics vertex stage information.
type StatisticsVertexStage struct {
	// DataRead - the amount of data read, in bytes.
	DataRead *int64 `json:"dataRead,omitempty"`
	// DataReadCrossPod - the amount of data read across multiple pods, in bytes.
	DataReadCrossPod *int64 `json:"dataReadCrossPod,omitempty"`
	// DataReadIntraPod - the amount of data read in one pod, in bytes.
	DataReadIntraPod *int64 `json:"dataReadIntraPod,omitempty"`
	// DataToRead - the amount of data remaining to be read, in bytes.
	DataToRead *int64 `json:"dataToRead,omitempty"`
	// DataWritten - the amount of data written, in bytes.
	DataWritten *int64 `json:"dataWritten,omitempty"`
	// DuplicateDiscardCount - the number of duplicates that were discarded.
	DuplicateDiscardCount *int32 `json:"duplicateDiscardCount,omitempty"`
	// FailedCount - the number of failures that occured in this stage.
	FailedCount *int32 `json:"failedCount,omitempty"`
	// MaxVertexDataRead - the maximum amount of data read in a single vertex, in bytes.
	MaxVertexDataRead *int64 `json:"maxVertexDataRead,omitempty"`
	// MinVertexDataRead - the minimum amount of data read in a single vertex, in bytes.
	MinVertexDataRead *int64 `json:"minVertexDataRead,omitempty"`
	// ReadFailureCount - the number of read failures in this stage.
	ReadFailureCount *int32 `json:"readFailureCount,omitempty"`
	// RevocationCount - the number of vertices that were revoked during this stage.
	RevocationCount *int32 `json:"revocationCount,omitempty"`
	// RunningCount - the number of currently running vertices in this stage.
	RunningCount *int32 `json:"runningCount,omitempty"`
	// ScheduledCount - the number of currently scheduled vertices in this stage
	ScheduledCount *int32 `json:"scheduledCount,omitempty"`
	// StageName - the name of this stage in job execution.
	StageName *string `json:"stageName,omitempty"`
	// SucceededCount - the number of vertices that succeeded in this stage.
	SucceededCount *int32 `json:"succeededCount,omitempty"`
	// TempDataWritten - the amount of temporary data written, in bytes.
	TempDataWritten *int64 `json:"tempDataWritten,omitempty"`
	// TotalCount - the total vertex count for this stage.
	TotalCount *int32 `json:"totalCount,omitempty"`
	// TotalFailedTime - the amount of time that failed vertices took up in this stage.
	TotalFailedTime *string `json:"totalFailedTime,omitempty"`
	// TotalProgress - the current progress of this stage, as a percentage.
	TotalProgress *int32 `json:"totalProgress,omitempty"`
	// TotalSucceededTime - the amount of time all successful vertices took in this stage.
	TotalSucceededTime *string `json:"totalSucceededTime,omitempty"`
	// TotalPeakMemUsage - the sum of the peak memory usage of all the vertices in the stage, in bytes.
	TotalPeakMemUsage *int64 `json:"totalPeakMemUsage,omitempty"`
	// TotalExecutionTime - the sum of the total execution time of all the vertices in the stage.
	TotalExecutionTime *string `json:"totalExecutionTime,omitempty"`
	// MaxDataReadVertex - the vertex with the maximum amount of data read.
	MaxDataReadVertex *StatisticsVertex `json:"maxDataReadVertex,omitempty"`
	// MaxExecutionTimeVertex - the vertex with the maximum execution time.
	MaxExecutionTimeVertex *StatisticsVertex `json:"maxExecutionTimeVertex,omitempty"`
	// MaxPeakMemUsageVertex - the vertex with the maximum peak memory usage.
	MaxPeakMemUsageVertex *StatisticsVertex `json:"maxPeakMemUsageVertex,omitempty"`
	// EstimatedVertexCPUCoreCount - the estimated vertex CPU core count.
	EstimatedVertexCPUCoreCount *int32 `json:"estimatedVertexCpuCoreCount,omitempty"`
	// EstimatedVertexPeakCPUCoreCount - the estimated vertex peak CPU core count.
	EstimatedVertexPeakCPUCoreCount *int32 `json:"estimatedVertexPeakCpuCoreCount,omitempty"`
	// EstimatedVertexMemSize - the estimated vertex memory size, in bytes.
	EstimatedVertexMemSize *int64 `json:"estimatedVertexMemSize,omitempty"`
	// AllocatedContainerCPUCoreCount - the statistics information for the allocated container CPU core count.
	AllocatedContainerCPUCoreCount *ResourceUsageStatistics `json:"allocatedContainerCpuCoreCount,omitempty"`
	// AllocatedContainerMemSize - the statistics information for the allocated container memory size.
	AllocatedContainerMemSize *ResourceUsageStatistics `json:"allocatedContainerMemSize,omitempty"`
	// UsedVertexCPUCoreCount - the statistics information for the used vertex CPU core count.
	UsedVertexCPUCoreCount *ResourceUsageStatistics `json:"usedVertexCpuCoreCount,omitempty"`
	// UsedVertexPeakMemSize - the statistics information for the used vertex peak memory size.
	UsedVertexPeakMemSize *ResourceUsageStatistics `json:"usedVertexPeakMemSize,omitempty"`
}

// UpdateFuture an abstraction for monitoring and retrieving the results of a long-running operation.
type UpdateFuture struct {
	azure.Future
	req *http.Request
}

// Result returns the result of the asynchronous operation.
// If the operation has not completed it will return an error.
func (future UpdateFuture) Result(client Client) (i Information, err error) {
	var done bool
	done, err = future.Done(client)
	if err != nil {
		err = autorest.NewErrorWithError(err, "job.UpdateFuture", "Result", future.Response(), "Polling failure")
		return
	}
	if !done {
		return i, azure.NewAsyncOpIncompleteError("job.UpdateFuture")
	}
	if future.PollingMethod() == azure.PollingLocation {
		i, err = client.UpdateResponder(future.Response())
		if err != nil {
			err = autorest.NewErrorWithError(err, "job.UpdateFuture", "Result", future.Response(), "Failure responding to request")
		}
		return
	}
	var req *http.Request
	var resp *http.Response
	if future.PollingURL() != "" {
		req, err = http.NewRequest(http.MethodGet, future.PollingURL(), nil)
		if err != nil {
			return
		}
	} else {
		req = autorest.ChangeToGet(future.req)
	}
	resp, err = autorest.SendWithSender(client, req,
		autorest.DoRetryForStatusCodes(client.RetryAttempts, client.RetryDuration, autorest.StatusCodesForRetry...))
	if err != nil {
		err = autorest.NewErrorWithError(err, "job.UpdateFuture", "Result", resp, "Failure sending request")
		return
	}
	i, err = client.UpdateResponder(resp)
	if err != nil {
		err = autorest.NewErrorWithError(err, "job.UpdateFuture", "Result", resp, "Failure responding to request")
	}
	return
}

// UpdateJobParameters the parameters that can be used to update existing Data Lake Analytics job information
// properties. (Only for use internally with Scope job type.)
type UpdateJobParameters struct {
	// DegreeOfParallelism - the degree of parallelism used for this job. This must be greater than 0, if set to less than 0 it will default to 1.
	DegreeOfParallelism *int32 `json:"degreeOfParallelism,omitempty"`
	// Priority - the priority value for the current job. Lower numbers have a higher priority. By default, a job has a priority of 1000. This must be greater than 0.
	Priority *int32 `json:"priority,omitempty"`
	// Tags - the key-value pairs used to add additional metadata to the job information. (Only for use internally with Scope job type.)
	Tags map[string]*string `json:"tags"`
}

// MarshalJSON is the custom marshaler for UpdateJobParameters.
func (ujp UpdateJobParameters) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if ujp.DegreeOfParallelism != nil {
		objectMap["degreeOfParallelism"] = ujp.DegreeOfParallelism
	}
	if ujp.Priority != nil {
		objectMap["priority"] = ujp.Priority
	}
	if ujp.Tags != nil {
		objectMap["tags"] = ujp.Tags
	}
	return json.Marshal(objectMap)
}

// USQLJobProperties u-SQL job properties used when retrieving U-SQL jobs.
type USQLJobProperties struct {
	// Resources - the list of resources that are required by the job
	Resources *[]Resource `json:"resources,omitempty"`
	// Statistics - the job specific statistics.
	Statistics *Statistics `json:"statistics,omitempty"`
	// DebugData - the job specific debug data locations.
	DebugData *DataPath `json:"debugData,omitempty"`
	// Diagnostics - the diagnostics for the job.
	Diagnostics *[]Diagnostics `json:"diagnostics,omitempty"`
	// AlgebraFilePath - the algebra file path after the job has completed
	AlgebraFilePath *string `json:"algebraFilePath,omitempty"`
	// TotalCompilationTime - the total time this job spent compiling. This value should not be set by the user and will be ignored if it is.
	TotalCompilationTime *string `json:"totalCompilationTime,omitempty"`
	// TotalPausedTime - the total time this job spent paused. This value should not be set by the user and will be ignored if it is.
	TotalPausedTime *string `json:"totalPausedTime,omitempty"`
	// TotalQueuedTime - the total time this job spent queued. This value should not be set by the user and will be ignored if it is.
	TotalQueuedTime *string `json:"totalQueuedTime,omitempty"`
	// TotalRunningTime - the total time this job spent executing. This value should not be set by the user and will be ignored if it is.
	TotalRunningTime *string `json:"totalRunningTime,omitempty"`
	// RootProcessNodeID - the ID used to identify the job manager coordinating job execution. This value should not be set by the user and will be ignored if it is.
	RootProcessNodeID *string `json:"rootProcessNodeId,omitempty"`
	// YarnApplicationID - the ID used to identify the yarn application executing the job. This value should not be set by the user and will be ignored if it is.
	YarnApplicationID *string `json:"yarnApplicationId,omitempty"`
	// YarnApplicationTimeStamp - the timestamp (in ticks) for the yarn application executing the job. This value should not be set by the user and will be ignored if it is.
	YarnApplicationTimeStamp *int64 `json:"yarnApplicationTimeStamp,omitempty"`
	// CompileMode - the specific compilation mode for the job used during execution. If this is not specified during submission, the server will determine the optimal compilation mode. Possible values include: 'Semantic', 'Full', 'SingleBox'
	CompileMode CompileMode `json:"compileMode,omitempty"`
	// RuntimeVersion - the runtime version of the Data Lake Analytics engine to use for the specific type of job being run.
	RuntimeVersion *string `json:"runtimeVersion,omitempty"`
	// Script - the script to run. Please note that the maximum script size is 3 MB.
	Script *string `json:"script,omitempty"`
	// Type - Possible values include: 'TypeJobProperties', 'TypeUSQL', 'TypeScope', 'TypeHive'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for USQLJobProperties.
func (usjp USQLJobProperties) MarshalJSON() ([]byte, error) {
	usjp.Type = TypeUSQL
	objectMap := make(map[string]interface{})
	if usjp.Resources != nil {
		objectMap["resources"] = usjp.Resources
	}
	if usjp.Statistics != nil {
		objectMap["statistics"] = usjp.Statistics
	}
	if usjp.DebugData != nil {
		objectMap["debugData"] = usjp.DebugData
	}
	if usjp.Diagnostics != nil {
		objectMap["diagnostics"] = usjp.Diagnostics
	}
	if usjp.AlgebraFilePath != nil {
		objectMap["algebraFilePath"] = usjp.AlgebraFilePath
	}
	if usjp.TotalCompilationTime != nil {
		objectMap["totalCompilationTime"] = usjp.TotalCompilationTime
	}
	if usjp.TotalPausedTime != nil {
		objectMap["totalPausedTime"] = usjp.TotalPausedTime
	}
	if usjp.TotalQueuedTime != nil {
		objectMap["totalQueuedTime"] = usjp.TotalQueuedTime
	}
	if usjp.TotalRunningTime != nil {
		objectMap["totalRunningTime"] = usjp.TotalRunningTime
	}
	if usjp.RootProcessNodeID != nil {
		objectMap["rootProcessNodeId"] = usjp.RootProcessNodeID
	}
	if usjp.YarnApplicationID != nil {
		objectMap["yarnApplicationId"] = usjp.YarnApplicationID
	}
	if usjp.YarnApplicationTimeStamp != nil {
		objectMap["yarnApplicationTimeStamp"] = usjp.YarnApplicationTimeStamp
	}
	if usjp.CompileMode != "" {
		objectMap["compileMode"] = usjp.CompileMode
	}
	if usjp.RuntimeVersion != nil {
		objectMap["runtimeVersion"] = usjp.RuntimeVersion
	}
	if usjp.Script != nil {
		objectMap["script"] = usjp.Script
	}
	if usjp.Type != "" {
		objectMap["type"] = usjp.Type
	}
	return json.Marshal(objectMap)
}

// AsUSQLJobProperties is the BasicProperties implementation for USQLJobProperties.
func (usjp USQLJobProperties) AsUSQLJobProperties() (*USQLJobProperties, bool) {
	return &usjp, true
}

// AsScopeJobProperties is the BasicProperties implementation for USQLJobProperties.
func (usjp USQLJobProperties) AsScopeJobProperties() (*ScopeJobProperties, bool) {
	return nil, false
}

// AsHiveJobProperties is the BasicProperties implementation for USQLJobProperties.
func (usjp USQLJobProperties) AsHiveJobProperties() (*HiveJobProperties, bool) {
	return nil, false
}

// AsProperties is the BasicProperties implementation for USQLJobProperties.
func (usjp USQLJobProperties) AsProperties() (*Properties, bool) {
	return nil, false
}

// AsBasicProperties is the BasicProperties implementation for USQLJobProperties.
func (usjp USQLJobProperties) AsBasicProperties() (BasicProperties, bool) {
	return &usjp, true
}

// YieldFuture an abstraction for monitoring and retrieving the results of a long-running operation.
type YieldFuture struct {
	azure.Future
	req *http.Request
}

// Result returns the result of the asynchronous operation.
// If the operation has not completed it will return an error.
func (future YieldFuture) Result(client Client) (ar autorest.Response, err error) {
	var done bool
	done, err = future.Done(client)
	if err != nil {
		err = autorest.NewErrorWithError(err, "job.YieldFuture", "Result", future.Response(), "Polling failure")
		return
	}
	if !done {
		return ar, azure.NewAsyncOpIncompleteError("job.YieldFuture")
	}
	if future.PollingMethod() == azure.PollingLocation {
		ar, err = client.YieldResponder(future.Response())
		if err != nil {
			err = autorest.NewErrorWithError(err, "job.YieldFuture", "Result", future.Response(), "Failure responding to request")
		}
		return
	}
	var req *http.Request
	var resp *http.Response
	if future.PollingURL() != "" {
		req, err = http.NewRequest(http.MethodGet, future.PollingURL(), nil)
		if err != nil {
			return
		}
	} else {
		req = autorest.ChangeToGet(future.req)
	}
	resp, err = autorest.SendWithSender(client, req,
		autorest.DoRetryForStatusCodes(client.RetryAttempts, client.RetryDuration, autorest.StatusCodesForRetry...))
	if err != nil {
		err = autorest.NewErrorWithError(err, "job.YieldFuture", "Result", resp, "Failure sending request")
		return
	}
	ar, err = client.YieldResponder(resp)
	if err != nil {
		err = autorest.NewErrorWithError(err, "job.YieldFuture", "Result", resp, "Failure responding to request")
	}
	return
}
