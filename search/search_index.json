{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Presidio: Data Protection and De-identification SDK","text":"<p>Presidio (Origin from Latin praesidium \u2018protection, garrison\u2019) helps to ensure sensitive data is properly managed and governed. It provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.</p>"},{"location":"#goals","title":"Goals","text":"<ul> <li>Allow organizations to preserve privacy in a simpler way by democratizing de-identification technologies and introducing transparency in decisions.</li> <li>Embrace extensibility and customizability to a specific business need.</li> <li>Facilitate both fully automated and semi-automated PII de-identification flows on multiple platforms.</li> </ul>"},{"location":"#how-it-works","title":"How it works","text":""},{"location":"#main-features","title":"Main features","text":"<ol> <li>Predefined or custom PII recognizers leveraging Named Entity Recognition, regular expressions, rule based logic and checksum with relevant context in multiple languages.</li> <li>Options for connecting to external PII detection models.</li> <li>Multiple usage options, from Python or PySpark workloads through Docker to Kubernetes.</li> <li>Customizability in PII identification and anonymization.</li> <li>Module for redacting PII text in images.</li> </ol> <p>Warning</p> <p>Presidio can help identify sensitive/PII data in un/structured text. However, because it is using automated detection mechanisms, there is no guarantee that Presidio will find all sensitive information. Consequently, additional systems and protections should be employed.</p>"},{"location":"#demo","title":"Demo","text":"<p>Link to demo: https://aka.ms/presidio-demo</p>"},{"location":"#provide-feedback","title":"Provide feedback","text":"<p>Are you using Presidio? We'd love to know how! Please help us improve by taking this short anonymous survey.</p>"},{"location":"#presidios-modules","title":"Presidio's modules","text":"<ol> <li>Presidio analyzer: PII identification in text</li> <li>Presidio anonymizer: De-identify detected PII entities using different operators</li> <li>Presidio image redactor: Redact PII entities from images using OCR and PII identification</li> <li>Presidio structured: PII identification in structured/semi-structured data</li> </ol>"},{"location":"#installing-presidio","title":"Installing Presidio","text":"<ol> <li>Supported Python Versions</li> <li>Using pip</li> <li>Using Docker</li> <li>From source</li> <li>Migrating from V1 to V2</li> </ol>"},{"location":"#running-presidio","title":"Running Presidio","text":"<ol> <li>Samples for running Presidio via code</li> <li>Running Presidio as an HTTP service</li> <li>Setting up a development environment</li> <li>Perform PII identification using presidio-analyzer</li> <li>Perform PII de-identification using presidio-anonymizer</li> <li>Perform PII identification and redaction in images using presidio-image-redactor</li> <li>Example deployments</li> </ol>"},{"location":"#support","title":"Support","text":"<ul> <li>Before you submit an issue, please go over the documentation. For general discussions, please use the Github repo's discussion board.</li> <li>If you have a usage question, found a bug or have a suggestion for improvement, please file a Github issue.</li> <li>For other matters, please email presidio@microsoft.com.</li> </ul>"},{"location":"ahds_integration/","title":"Azure Health Data Services De-identification Service Integration","text":"<p>Presidio supports integration with Azure Health Data Services (AHDS) de-identification service for both entity recognition and anonymization with realistic surrogate generation.</p>"},{"location":"ahds_integration/#resources","title":"Resources","text":"<ul> <li>Azure Health Data Services Documentation</li> <li>Azure Health Deidentification Python SDK</li> </ul>"},{"location":"ahds_integration/#overview","title":"Overview","text":"<p>The AHDS de-identification service integration provides two main capabilities:</p> <ol> <li>AHDS Recognizer (in presidio-analyzer): Detects PHI entities using the Azure Health Data Services de-identification service</li> <li>AHDS Surrogate Operator (in presidio-anonymizer): Replaces PHI entities with realistic surrogates using the de-identification service</li> </ol>"},{"location":"ahds_integration/#benefits-of-surrogation","title":"Benefits of Surrogation","text":"<ul> <li>Maintains Data Utility: Preserves structure and format for downstream analytics</li> <li>Realistic Healthcare Context: Generates medically plausible names, dates, and identifiers</li> <li>Consistent Cross-References: Same entity gets same surrogate throughout document</li> <li>Format Preservation: Maintains original formatting and linguistic patterns</li> </ul>"},{"location":"ahds_integration/#installation","title":"Installation","text":""},{"location":"ahds_integration/#for-ahds-recognizer","title":"For AHDS Recognizer","text":"<pre><code>pip install presidio-analyzer[ahds]\n</code></pre>"},{"location":"ahds_integration/#for-ahds-surrogate-operator","title":"For AHDS Surrogate Operator","text":"<pre><code>pip install presidio-anonymizer[ahds]\n</code></pre>"},{"location":"ahds_integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure Health Data Services de-identification service endpoint</li> <li>Azure role-based access control configured</li> <li>Environment variables:</li> <li><code>AHDS_ENDPOINT</code>: Your AHDS de-identification service endpoint</li> </ul>"},{"location":"ahds_integration/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code>import os\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\n\n# Set your AHDS endpoint\nos.environ[\"AHDS_ENDPOINT\"] = \"https://your-ahds-endpoint.api.eus001.deid.azure.com\"\n\n# Initialize engines\nanalyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n\n# Medical text with PHI\ntext = \"Patient John Doe was seen by Dr. Smith on 2024-01-15 for diabetes treatment.\"\n\n# Step 1: Detect entities using AHDS recognizer\nanalyzer_results = analyzer.analyze(\n    text=text,\n    entities=[\"PATIENT\", \"DOCTOR\", \"DATE\"],\n    language=\"en\"\n)\n\n# Step 2: Anonymize using AHDS surrogate generation\nresult = anonymizer.anonymize(\n    text=text,\n    analyzer_results=analyzer_results,\n    operators={\n        \"DEFAULT\": OperatorConfig(\"surrogate\", {\n            \"entities\": analyzer_results,\n            \"input_locale\": \"en-US\",\n            \"surrogate_locale\": \"en-US\"\n        })\n    },\n)\n\nprint(f\"Original: {text}\")\nprint(f\"Anonymized: {result.text}\")\n# Output: \"Patient Michael Johnson was seen by Dr. Brown on 1987-06-23 for diabetes treatment.\"\n</code></pre> <p>Note: This example uses the Azure Health Data Services de-identification service surrogation, which provides superior data utility by generating realistic, medically-appropriate surrogates while maintaining document structure and relationships.</p>"},{"location":"ahds_integration/#configuration-options","title":"Configuration Options","text":""},{"location":"ahds_integration/#ahds-surrogate-operator-parameters","title":"AHDS Surrogate Operator Parameters","text":"<ul> <li><code>endpoint</code>: AHDS de-identification service endpoint (optional, uses <code>AHDS_ENDPOINT</code> env var)</li> <li><code>entities</code>: List of entities detected by analyzer</li> <li><code>input_locale</code>: Input locale (default: \"en-US\")</li> <li><code>surrogate_locale</code>: Surrogate locale (default: \"en-US\")</li> </ul>"},{"location":"ahds_integration/#authentication","title":"Authentication","text":"<p>The AHDS de-identification service integration uses Azure's <code>DefaultAzureCredential</code>, which supports multiple authentication methods:</p> <ol> <li>Environment variables (Service Principal)</li> <li>Managed Identity (when running on Azure)</li> <li>Azure CLI (<code>az login</code>)</li> <li>Visual Studio/VS Code credentials</li> <li>Interactive browser login</li> </ol> <p>For production deployments, we recommend using Service Principal or Managed Identity.</p>"},{"location":"ahds_integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ahds_integration/#common-issues","title":"Common Issues","text":"<ol> <li> <p>ModuleNotFoundError: Install the AHDS optional dependencies    <pre><code>pip install presidio-analyzer[ahds] presidio-anonymizer[ahds]\n</code></pre></p> </li> <li> <p>Authentication errors: Ensure Azure credentials are properly configured    <pre><code>az login  # For local development\n</code></pre></p> </li> <li> <p>Endpoint not found: Verify the <code>AHDS_ENDPOINT</code> environment variable is set correctly</p> </li> </ol>"},{"location":"ahds_integration/#testing-without-ahds","title":"Testing without AHDS","text":"<p>The AHDS operators gracefully handle missing dependencies and will be skipped if not available. Tests will be automatically skipped if the <code>AHDS_ENDPOINT</code> environment variable is not set.</p>"},{"location":"ahds_integration/#see-also","title":"See Also","text":"<ul> <li>Presidio Analyzer</li> <li>Presidio Anonymizer</li> <li>Azure Health Data Services de-identification service documentation</li> </ul>"},{"location":"api/","title":"Presidio API","text":"<p>Api reference for Presidio's main python modules</p> <ul> <li>Presidio analyzer Python API reference</li> <li>Presidio anonymizer Python API reference</li> <li>Presidio image redactor Python API reference</li> <li>Presidio structured Python API reference</li> </ul>"},{"location":"build_release/","title":"Build and release process","text":"<p>Presidio leverages Azure DevOps YAML pipelines to validate, build, release and deliver presidio. The pipelines make use of templates for code reuse using YAML Schema.</p>"},{"location":"build_release/#description","title":"Description","text":"<p>The following pipelines are provided and maintained as part of presidio development process:</p> <ul> <li>PR Validation - used to validate pull requests.<ul> <li>Linting</li> <li>Security and compliance analysis</li> <li>Unit tests</li> <li>E2E tests</li> </ul> </li> <li>CI - triggered on merge to main branch.<ul> <li>Linting</li> <li>Security and compliance analysis</li> <li>Unit tests</li> <li>E2E tests</li> <li>deploys the artifacts to an internal dev environment.</li> </ul> </li> <li>Release - manually triggered.<ul> <li>releases presidio official artifacts<ul> <li>pypi</li> <li>Microsoft container registry (and docker hub)</li> <li>GitHub</li> </ul> </li> <li>updates the official demo environment.</li> </ul> </li> </ul>"},{"location":"build_release/#variables-used-by-the-pipelines","title":"Variables used by the pipelines","text":""},{"location":"build_release/#ci-pipeline","title":"CI Pipeline","text":"<ul> <li>ACR_AZURE_SUBSCRIPTION - Service connection to Azure subscription where Azure Container Registry is.</li> <li>ACR_REGISTRY_NAME - Name of Azure Container Registry.</li> <li>ANALYZER_DEV_APP_NAME - Name of existing App Service for Analyzer (development environment).</li> <li>ANONYMIZER_DEV_APP_NAME - Name of existing App Service for Anonymizer (development environment).</li> <li>IMAGE_REDACTOR_DEV_APP_NAME - Name of existing App Service for Image Redactor (development environment).</li> <li>DEV_AZURE_SUBSCRIPTION - Service connection to Azure subscription where App Services are (development environment).</li> <li>DEV_RESOURCE_GROUP_NAME - Name of resource group where App Services are (development environment).</li> </ul>"},{"location":"build_release/#release-pipeline","title":"Release Pipeline","text":"<ul> <li>ACR_AZURE_SUBSCRIPTION - Service connection to Azure subscription where Azure Container Registry is.</li> <li>ACR_REGISTRY_NAME - Name of Azure Container Registry.</li> <li>ANALYZER_PROD_APP_NAME - Name of existing App Service for Analyzer (production environment).</li> <li>ANONYMIZER_PROD_APP_NAME - Name of existing App Service for Anonymizer (production environment).</li> <li>PROD_AZURE_SUBSCRIPTION - Service connection to Azure subscription where App Services are (production environment).</li> <li>PROD_RESOURCE_GROUP_NAME - Name of resource group where App Services are (production environment).</li> </ul>"},{"location":"build_release/#import-a-pipeline-to-azure-devops","title":"Import a pipeline to Azure Devops","text":"<ul> <li>Sign in to your Azure DevOps organization and navigate to your project.</li> <li>In your project, navigate to the Pipelines page. Then choose the action to create a new pipeline.</li> <li>Walk through the steps of the wizard by first selecting 'Use the classic editor, and select GitHub as the location of your source code.</li> <li>You might be redirected to GitHub to sign in. If so, enter your GitHub credentials.</li> <li>When the list of repositories appears, select presidio repository.</li> <li>Point Azure Pipelines to the relevant yaml definition you'd like to import.     Set the pipeline's name, the required triggers and variables and Select Save and run.</li> <li>A new run is started. Wait for the run to finish.</li> </ul>"},{"location":"build_release/#pypi-publishing-with-oidc","title":"PyPI Publishing with OIDC","text":"<p>The GitHub Actions release workflow uses OIDC (OpenID Connect) trusted publishing to PyPI, which provides enhanced security by eliminating the need to manage PyPI API tokens. This requires:</p> <ol> <li>PyPI Configuration: Each package (presidio_analyzer, presidio_anonymizer, etc.) must be configured on PyPI to trust the GitHub repository and workflow.</li> <li>GitHub Workflow: The workflow uses <code>pypa/gh-action-pypi-publish@release/v1</code> with <code>id-token: write</code> permissions.</li> <li>No Secrets Required: No PyPI API tokens need to be stored as GitHub secrets.</li> </ol> <p>Benefits of OIDC: - Enhanced security through short-lived tokens - No manual token management - Automatic token rotation - Audit trail of publishing activities</p> <p>Note: The Azure DevOps pipeline continues to use traditional PyPI authentication with service connections.</p>"},{"location":"community/","title":"Presidio eco-system","text":"<p>This section collects different resources developed with Presidio.</p>"},{"location":"community/#resources","title":"Resources","text":"Resource Description HashiCorp Vault Operator A library that allows to integrate Presidio with HashiCorp Vault for anonymization and deanonymization. Rasa bot framework Use Presidio to de-identify chat bot messages in Rasa. LangChain De-identification and reversible anonymization within LangChain. LlamaIndex De-identification and reversible anonymization within LlamaIndex. LiteLLM Integrate Presidio into LiteLLM Guardrails-ai Use Presidio as an LLM guardrails using the Guardrails AI suite. Presidio in LLMGuard Integrate Presidio into LLM Guard - The Security Toolkit for LLM Interactions. Privy Integrate Presidio into Privy. Huggingface Automatic PII detection on Huggingface datasets. KNIME Use Presidio within the KNIME framework. OpenMetadata Auto PII tagging for Sensitive/NonSensitive at the column level. dataiku PII detection in the LLM Mesh can detect various forms of PII in your prompts and queries, and either block or redact the queries. Obsei Obsei is an open-source, low-code, AI powered automation tool data-describe data-describe is a Python toolkit for Exploratory Data Analysis (EDA). It aims to accelerate data exploration and analysis by providing automated and polished analysis widgets. Azure Search Power Skills Power Skills are a collection of useful functions to be deployed as custom skills for Azure Cognitive Search. The skills can be used as templates or starting points for your own custom skills, or they can be deployed and used as they are if they happen to meet your requirements. DataOps for the Modern Data Warehouse Contains numerous code samples and artifacts on how to apply DevOps principles to data pipelines built according to the Modern Data Warehouse (MDW) architectural pattern on Microsoft Azure. Extending Power BI with Python and R Code repository for Extending Power BI with Python and R, published by Packt. HebSafeHarbor Clinical notes anonymization in Hebrew. Presidio Github Action Github Action that analyzes text for PII entities with Microsoft's Presidio framework. <ul> <li>Please create a PR if you're interested in adding your tool to this list.</li> </ul>"},{"location":"design/","title":"Presidio design","text":""},{"location":"design/#analyzer","title":"Analyzer","text":""},{"location":"design/#anonymizer","title":"Anonymizer","text":""},{"location":"design/#image-redactor","title":"Image Redactor","text":""},{"location":"design/#standard-image-types","title":"Standard Image Types","text":""},{"location":"design/#dicom-images","title":"DICOM Images","text":""},{"location":"development/","title":"Setting Up a Development Environment","text":""},{"location":"development/#getting-started","title":"Getting started","text":""},{"location":"development/#cloning-the-repo","title":"Cloning the repo","text":"<p>To create a local copy of Presidio repository, follow Github instructions on how to clone a project using git. The project is structured so that:</p> <ul> <li>Each Presidio service has a designated directory:</li> <li>The service logic.</li> <li>Tests, both unit and integration.</li> <li>Serving it as an HTTP service (found in app.py).</li> <li>Python Packaging setup script (setup.py).</li> <li>In the project root directory, you will find common code for using, serving and testing Presidio     as a cluster of services, as well as CI/CD pipelines codebase and documentation.</li> </ul>"},{"location":"development/#setting-up-poetry","title":"Setting up Poetry","text":"<p>Poetry is Python package manager. It is used to manage dependencies and virtual environments for Presidio services. Follow these steps when starting to work on a Presidio service with poetry:</p> <ol> <li> <p>Install poetry</p> <ul> <li> <p>Using Pip</p> <pre><code>pip install poetry\n</code></pre> </li> <li> <p>Using Homebrew (in MacOS)</p> <pre><code>brew install poetry\n</code></pre> </li> </ul> <p>Additional installation instructions for poetry: https://python-poetry.org/docs/#installation</p> </li> <li> <p>Have poetry create a virtualenv for the project and install all requirements in the pyproject.toml,     including dev requirements.</p> <p>For example, in the <code>presidio-analyzer</code> folder, run:</p> <pre><code>poetry install --all-extras\n</code></pre> </li> <li> <p>Run all tests:</p> <pre><code>poetry run pytest\n</code></pre> </li> <li> <p>To run arbitrary scripts within the virtual env, start the command with     <code>poetry run</code>. For example:</p> <ol> <li><code>poetry run ruff check</code></li> <li><code>poetry run pip freeze</code></li> <li><code>poetry run python -m spacy download en_core_web_lg</code></li> </ol> <p>Command 3 downloads the default spacy model needed for Presidio Analyzer.`</p> </li> </ol>"},{"location":"development/#alternatively-activate-the-virtual-environment-and-use-the-commands-using-this-method","title":"Alternatively, activate the virtual environment and use the commands using this method.","text":""},{"location":"development/#development-guidelines","title":"Development guidelines","text":"<ul> <li>A Github issue suggesting the change should be opened prior to a PR.</li> <li>All contributions should be documented, tested and linted. Please verify that all tests and lint checks pass successfully before proposing a change.</li> <li>To make the linting process easier, you can use pre-commit hooks to verify and automatically format code upon a git commit</li> <li>In order for a pull request to be accepted, the CI (containing unit tests, e2e tests and linting) needs to succeed, in addition to approvals from two maintainers.</li> <li>PRs should be small and solve/improve one issue at a time. If you have multiple suggestions for improvement, please open multiple PRs.</li> </ul>"},{"location":"development/#local-build-process","title":"Local build process","text":"<p>After modifying presidio codebase, you might want to build presidio cluster locally, and run tests to spot regressions. The recommended way of doing so is using docker-compose (bundled with 'Docker Desktop' for Windows and Mac systems, more information can be found here). Once installed, to start presidio cluster with all of its services in HTTP mode, run from the project root:</p> <pre><code>docker-compose up --build -d\n</code></pre> <p>Note</p> <p>Building for the first time might take some time, mainly on downloading the default spacy models.</p> <p>To validate that the services were built and started successfully, and to see the designated port for each, use docker-compose ps:</p> <pre><code>&gt;docker-compose ps\nCONTAINER ID   IMAGE                       COMMAND                  CREATED         STATUS         PORTS                    NAMES\n6d5a258d19c2   presidio-anonymizer         \"/bin/sh -c 'poetry \u2026\"   6 minutes ago   Up 6 minutes   0.0.0.0:5001-&gt;5001/tcp   presidio_presidio-anonymizer_1\n9aad2b68f93c   presidio-analyzer           \"/bin/sh -c 'poetry \u2026\"   2 days ago      Up 6 minutes   0.0.0.0:5002-&gt;5001/tcp   presidio_presidio-analyzer_1\n1448dfb3ec2b   presidio-image-redactor     \"/bin/sh -c 'poetry \u2026\"   2 seconds ago   Up 2 seconds   0.0.0.0:5003-&gt;5001/tcp   presidio_presidio-image-redactor_1\n</code></pre> <p>Edit docker-compose.yml configuration file to change the default ports.</p> <p>Starting part of the cluster, or one service only, can be done by stating its image name as argument for docker-compose. For example for analyzer service:</p> <pre><code>docker-compose up --build -d presidio-analyzer\n</code></pre>"},{"location":"development/#testing","title":"Testing","text":"<p>We strive to have a full test coverage in Presidio, and expect every pull request to include tests.</p> <p>In each service directory, a 'test' directory can be found. In it, both unit tests, for testing single files or classes, and integration tests, for testing integration between the service components, or integration with external packages.</p>"},{"location":"development/#basic-conventions","title":"Basic conventions","text":"<p>For tests to be consistent and predictable, we use the following basic conventions:</p> <ol> <li>Treat tests as production code. Keep the tests concise and readable, with descriptive namings.</li> <li>Assert on one behavior at a time in each test.</li> <li>Test names should follow a pattern of <code>test_when_[condition_to_test]_then_[expected_behavior]</code>.    For example: <code>test_given_an_unknown_entity_then_anonymize_uses_defaults</code>.</li> <li>Use test doubles and mocks    when writing unit tests. Make less use of them when writing integration tests.</li> </ol>"},{"location":"development/#running-tests","title":"Running tests","text":"<p>Presidio uses the pytest framework for testing. See the pytest documentation for more information.</p> <p>Running the tests locally can be done in two ways:</p> <ol> <li> <p>Using cli, from each service directory, run:</p> <pre><code>poetry run pytest\n</code></pre> </li> <li> <p>Using your IDE.    See configuration examples for    JetBrains PyCharm / IntelliJ IDEA    and Visual Studio Code</p> </li> </ol>"},{"location":"development/#end-to-end-tests","title":"End-to-end tests","text":"<p>Since Presidio services can function as HTTP servers, Presidio uses an additional end-to-end (e2e) testing layer to test their REST APIs. This e2e test framework is located under 'e2e-tests' directory. In it, you can also find test scenarios testing the integration between Presidio services through REST API. These tests should be annotated with 'integration' pytest marker <code>@pytest.mark.integration</code>, while tests calling a single service API layer should be annotated with 'api' pytest marker <code>@pytest.mark.api</code>.</p> <p>Running the e2e-tests locally can be done in two ways:</p> <ol> <li> <p>Using cli, from e2e-tests directory, run:</p> <p>On Mac / Linux / WSL:</p> <pre><code># Create a virtualenv named presidio-e2e (needs to be done only on the first run)\npython -m venv presidio-e2e\n# Activate the virtualenv\nsource presidio-e2e/bin/activate\n# Install e2e-tests requirements using pip\npip install -r requirements.txt\n# Run pytest\npytest\n# Deactivate the virtualenv\ndeactivate\n</code></pre> <p>On Windows CMD / Powershell:</p> <pre><code># Create a virtualenv named presidio-e2e (needs to be done only on the first run)\npy -m venv presidio-e2e\n# Activate the virtualenv\npresidio-e2e\\Scripts\\activate\n# Install e2e-tests requirements using pip\npip install -r requirements.txt\n# Run pytest\npytest\n# Deactivate the virtualenv\ndeactivate\n</code></pre> </li> <li> <p>Using your IDE</p> <p>See references in the section above.</p> </li> </ol> <p>Note</p> <p>The e2e tests require a Presidio cluster to be up, for example using the containerized cluster with docker-compose.</p>"},{"location":"development/#linting","title":"Linting","text":"<p>Presidio services are PEP8 compliant and continuously enforced on style guide issues during the build process using <code>ruff</code>, in turn running <code>flake8</code> and other linters.</p> <p>Running ruff locally, using <code>poetry run ruff check</code>, you can check for those issues prior to committing a change.</p> <p>Ruff runs linters in addition to the basic <code>flake8</code> functionality, Presidio uses linters as part as ruff such as:</p> <ul> <li>pep8-naming: To check that variable names are PEP8 compliant.</li> <li>flake8-docstrings: To check that docstrings are compliant.</li> </ul>"},{"location":"development/#automatically-format-code-and-check-for-code-styling","title":"Automatically format code and check for code styling","text":"<p>To make the linting process easier, you can use pre-commit hooks to verify and automatically format code upon a git commit, using <code>ruff-format</code>:</p> <ol> <li> <p>Install pre-commit package manager locally.</p> </li> <li> <p>From the project's root, enable pre-commit, installing git hooks in the <code>.git/</code> directory by running: <code>pre-commit install</code>.</p> </li> <li> <p>Commit non PEP8 compliant code will cause commit failure and automatically     format your code using, as well as checking code formatting using <code>ruff</code></p> </li> </ol> <pre><code>[INFO] Initializing environment for https://github.com/astral-sh/ruff-pre-commit.\n[INFO] Installing environment for https://github.com/astral-sh/ruff-pre-commit.\n[INFO] Once installed this environment will be reused.\n[INFO] This may take a few minutes...\nruff.....................................................................Passed\nruff-format..............................................................Failed\n- hook id: ruff-format\n- files were modified by this hook\n  5 files reformatted, 4 files left unchanged\n</code></pre> <ol> <li>Committing again will finish successfully, with a well-formatted code.</li> </ol>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<ul> <li>General</li> <li>What is Presidio?</li> <li>Why did Microsoft create Presidio?</li> <li>Is Microsoft Presidio an official Microsoft product?</li> <li>What is the difference between Presidio and different PII detection services like Azure AI Language, Azure Health Data Services, and Amazon Comprehend?</li> <li>Using Presidio</li> <li>How can I start using Presidio?</li> <li>What are the main building blocks in Presidio?</li> <li>Customizing Presidio</li> <li>How can Presidio be customized to my needs?</li> <li>What NLP frameworks does Presidio support?</li> <li>Can Presidio be used for Pseudonymization?</li> <li>Does Presidio work on structured/tabular data?</li> <li>Improving detection accuracy</li> <li>What can I do if Presidio does not detect some of the PII entities in my data (False Negatives)?</li> <li>What can I do if Presidio falsely detects text as PII entities (False Positives)?</li> <li>How can I evaluate the performance of my Presidio instance?</li> <li>Deployment</li> <li>How can I deploy Presidio into my environment?</li> <li>Contributing</li> <li>How can I contribute to Presidio?</li> <li>How can I report security vulnerabilities?</li> </ul>"},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#what-is-presidio","title":"What is Presidio?","text":"<p>Presidio (Origin from Latin praesidium \u2018protection, garrison\u2019) helps to ensure sensitive data is properly managed and governed. It provides fast identification and anonymization modules for private entities in text and images. It is fully customizable and pluggable, can be adapted to your needs and be deployed into various environments.</p> <p>Note</p> <p>Presidio is a library or SDK rather than a service. It is meant to be customized to the user's or organization's specific needs.</p> <p>Warning</p> <p>Presidio can help identify sensitive/PII data in un/structured text. However, because it is using automated detection mechanisms, there is no guarantee that Presidio will find all sensitive information. Consequently, additional systems and protections should be employed.</p>"},{"location":"faq/#why-did-microsoft-create-presidio","title":"Why did Microsoft create Presidio?","text":"<p>By developing Presidio, our goals are:</p> <ol> <li>Allow organizations to preserve privacy in a simpler way by democratizing de-identification technologies and introducing transparency in decisions.</li> <li>Embrace extensibility and customizability to a specific business need.</li> <li>Facilitate both fully automated and semi-automated PII de-identification flows on multiple platforms.</li> </ol>"},{"location":"faq/#is-microsoft-presidio-an-official-microsoft-product","title":"Is Microsoft Presidio an official Microsoft product?","text":"<p>The authors and maintainers of Presidio come from the Industry Solutions Engineering team. We work with customers on various engineering problems, and have found the proper handling of private and sensitive data a recurring challenge across many customers and industries.</p> <p>Note</p> <p>Microsoft Presidio is not an official Microsoft product. Usage terms are defined in the repository's license.</p>"},{"location":"faq/#what-is-the-difference-between-presidio-and-different-pii-detection-services-like-azure-ai-language-azure-health-data-services-and-amazon-comprehend","title":"What is the difference between Presidio and different PII detection services like Azure AI Language, Azure Health Data Services, and Amazon Comprehend?","text":"<p>In a nutshell, Presidio is a library which is meant to be customized, whereas different SaaS tools for PII detection have less customization capabilities. Most of these SaaS offerings use dedicated ML models and other logic for PII detection and often have better entity coverage or accuracy than Presidio.</p> <p>Based on our internal research, leveraging Presidio in parallel to 3rd party PII detection services like Azure AI Language can bring optimal results mainly when the data in hand has entity types or values not supported by the 3rd party service. (see example here).</p>"},{"location":"faq/#using-presidio","title":"Using Presidio","text":""},{"location":"faq/#how-can-i-start-using-presidio","title":"How can I start using Presidio?","text":"<ol> <li>Check out the installation docs.</li> <li>Take a look at the different samples.</li> <li>Try the demo website.</li> </ol>"},{"location":"faq/#what-are-the-main-building-blocks-in-presidio","title":"What are the main building blocks in Presidio?","text":"<p>Presidio is a suite built of several packages and building blocks:</p> <ol> <li>Presidio Analyzer: a package for detecting PII entities in natural language.</li> <li>Presidio Anonymizer: a package for manipulating PII entities in text (e.g. remove, redact, hash, encrypt).</li> <li>Presidio Image Redactor: A package for detecting PII entities in image using OCR.</li> <li>Presidio Structured: A package for detecting PII entities in structured/semi-structured data.</li> <li>A set of sample deployments as Python packages or Docker containers for Kubernetes, Azure Data Factory, Spark and more.</li> </ol>"},{"location":"faq/#customizing-presidio","title":"Customizing Presidio","text":""},{"location":"faq/#how-can-presidio-be-customized-to-my-needs","title":"How can Presidio be customized to my needs?","text":"<p>Users can customize Presidio in multiple ways:</p> <ol> <li>Create new or updated PII recognizers (docs).</li> <li>Adapt Presidio to new languages (docs).</li> <li>Leverage state of the art Named Entity Recognition models (docs).</li> <li>Add new types of anonymizers (docs).</li> <li>Create PII analysis and anonymization pipelines on different environments using Docker or Python (samples).</li> </ol> <p>And more.</p>"},{"location":"faq/#what-nlp-frameworks-does-presidio-support","title":"What NLP frameworks does Presidio support?","text":"<p>Presidio supports spaCy version 3+ for Named Entity Recognition, tokenization, lemmatization and more. We also support Stanza using the spacy-stanza package, and it is further possible to create PII recognizers leveraging other frameworks like transformers or Flair.</p> <p>For more information, see the docs.</p>"},{"location":"faq/#can-presidio-be-used-for-pseudonymization","title":"Can Presidio be used for Pseudonymization?","text":"<p>Pseudonymization is a de-identification technique in which the real data is replaced with fake data in a reversible way. Since there are various ways and approaches for this, we provide a simple sample which can be extended for more sophisticated usage. If you have a question or a request on this topic, please open an issue on the repo.</p>"},{"location":"faq/#does-presidio-work-on-structuredtabular-data","title":"Does Presidio work on structured/tabular data?","text":"<p>Presidio-structured is a capability in Presidio for detecting PII entities in structured/semi-structured data. It scans datasets for PII using Presidio Analyzer, and supports the redaction of text, cells, or columns in a tabular dataset.</p>"},{"location":"faq/#improving-detection-accuracy","title":"Improving detection accuracy","text":""},{"location":"faq/#what-can-i-do-if-presidio-does-not-detect-some-of-the-pii-entities-in-my-data-false-negatives","title":"What can I do if Presidio does not detect some of the PII entities in my data (False Negatives)?","text":"<p>Presidio comes loaded with several PII recognizers (see list here), however its main strength lies in its customization capabilities to new entities, specific datasets, file types, languages or use cases.</p>"},{"location":"faq/#what-can-i-do-if-presidio-falsely-detects-text-as-pii-entities-false-positives","title":"What can I do if Presidio falsely detects text as PII entities (False Positives)?","text":"<p>Some PII recognizers are less specific than others. A driver's license number, for example, could be any 9-digit number. While Presidio leverages context words and other logic to improve the detection quality, it could still falsely detect non-entity values as PII entities.</p> <p>In order to avoid false positives, one could try to:</p> <ol> <li>Change the acceptance threshold, which defines what is the minimum confidence value for a detected entity to be returned.</li> <li>Remove unnecessary PII recognizers, if the dataset does not contain these entities.</li> <li>Update/replace the logic of specific recognizers to better suit a specific dataset or use case.</li> <li>Replace PII recognizers with those coming from 3rd party services.</li> </ol> <p>Every PII identification logic would have its errors, and there is a trade-off between false positives (falsely detected text) and false negatives (PII entities which are not detected).</p>"},{"location":"faq/#how-can-i-evaluate-the-performance-of-my-presidio-instance","title":"How can I evaluate the performance of my Presidio instance?","text":"<p>In addition to Presidio, we maintain a repo focused on evaluation of models and PII recognizers here. It also features a simple PII data generator.</p>"},{"location":"faq/#deployment","title":"Deployment","text":""},{"location":"faq/#how-can-i-deploy-presidio-into-my-environment","title":"How can I deploy Presidio into my environment?","text":"<p>The main Presidio modules (analyzer, anonymizer, image-redactor) can be used both as a Python package and as a dockerized REST API. See the different deployment samples for example deployments.</p>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#how-can-i-contribute-to-presidio","title":"How can I contribute to Presidio?","text":"<p>First, review the contribution guidelines, and feel free to reach out by opening an issue, posting a discussion or emailing us at presidio@microsoft.com</p>"},{"location":"faq/#how-can-i-report-security-vulnerabilities","title":"How can I report security vulnerabilities?","text":"<p>Please see the security information.</p>"},{"location":"getting_started/","title":"Getting started with Microsoft Presidio","text":"<p>The core functionality in Presidio is to detect PII in text. Presidio further contains a set of tools that build on top of text PII detection, for example in images, structured data, JSON and more.</p> <ul> <li>For a quickstart for PII detection and de-identification in text click here.</li> <li>For a quickstart for PII detection and de-identification in images click here.</li> <li>For a quickstart for PII detection and de-identification in structured and semi-structured data click here.</li> </ul>"},{"location":"installation/","title":"Installing Presidio","text":""},{"location":"installation/#description","title":"Description","text":"<p>This document describes the installation of the entire Presidio suite using <code>pip</code> (as Python packages) or using <code>Docker</code> (As containerized services).</p>"},{"location":"installation/#using-pip","title":"Using pip","text":"<p>Note</p> <p>Consider installing the Presidio python packages in a virtual environment like venv or conda.</p>"},{"location":"installation/#supported-python-versions","title":"Supported Python Versions","text":"<p>Presidio is supported for the following python versions:</p> <ul> <li>3.9</li> <li>3.10</li> <li>3.11</li> <li>3.12</li> </ul>"},{"location":"installation/#pii-anonymization-on-text","title":"PII anonymization on text","text":"<p>For PII anonymization on text, install the <code>presidio-analyzer</code> and <code>presidio-anonymizer</code> packages with at least one NLP engine (<code>spaCy</code>, <code>transformers</code> or <code>stanza</code>):</p> spaCy (default)TransformersStanza <pre><code>pip install presidio_analyzer\npip install presidio_anonymizer\npython -m spacy download en_core_web_lg\n</code></pre> <pre><code>pip install \"presidio_analyzer[transformers]\"\npip install presidio_anonymizer\npython -m spacy download en_core_web_sm\n</code></pre> <p>Note</p> <p>When using a transformers NLP engine, Presidio would still use spaCy for other capabilities, therefore a small spaCy model (such as en_core_web_sm) is required.  Transformers models would be loaded lazily. To pre-load them, see: Downloading a pre-trained model</p> <pre><code>pip install \"presidio_analyzer[stanza]\"\npip install presidio_anonymizer\n</code></pre> <p>Note</p> <p>Stanza models would be loaded lazily. To pre-load them, see: Downloading a pre-trained model.</p>"},{"location":"installation/#pii-redaction-in-images","title":"PII redaction in images","text":"<p>For PII redaction in images</p> <ol> <li> <p>Install the <code>presidio-image-redactor</code> package:</p> <pre><code>pip install presidio_image_redactor\n\n# Presidio image redactor uses the presidio-analyzer\n# which requires a spaCy language model:\npython -m spacy download en_core_web_lg\n</code></pre> </li> <li> <p>Install an OCR engine. The default version uses the Tesseract OCR Engine. More information on installation can be found here.</p> </li> </ol>"},{"location":"installation/#using-docker","title":"Using Docker","text":"<p>Presidio can expose REST endpoints for each service using Flask and Docker. To download the Presidio Docker containers, run the following command:</p> <p>Note</p> <p>This requires Docker to be installed. Download Docker.</p>"},{"location":"installation/#for-pii-anonymization-in-text","title":"For PII anonymization in text","text":"<p>For PII detection and anonymization in text, the <code>presidio-analyzer</code> and <code>presidio-anonymizer</code> modules are required.</p> <pre><code># Download Docker images\ndocker pull mcr.microsoft.com/presidio-analyzer\ndocker pull mcr.microsoft.com/presidio-anonymizer\n\n# Run containers with default ports\ndocker run -d -p 5002:3000 mcr.microsoft.com/presidio-analyzer:latest\n\ndocker run -d -p 5001:3000 mcr.microsoft.com/presidio-anonymizer:latest\n</code></pre>"},{"location":"installation/#for-pii-redaction-in-images","title":"For PII redaction in images","text":"<p>For PII detection in images, the <code>presidio-image-redactor</code> is required.</p> <pre><code># Download Docker image\ndocker pull mcr.microsoft.com/presidio-image-redactor\n\n# Run container with the default port\ndocker run -d -p 5003:3000 mcr.microsoft.com/presidio-image-redactor:latest\n</code></pre> <p>Once the services are running, their APIs are available. API reference and example calls can be found here.</p>"},{"location":"installation/#install-from-source","title":"Install from source","text":"<p>To install Presidio from source, first clone the repo:</p> <ul> <li>using HTTPS</li> </ul> <pre><code>git clone https://github.com/microsoft/presidio.git\n</code></pre> <ul> <li>Using SSH</li> </ul> <pre><code>git clone git@github.com:microsoft/presidio.git\n</code></pre> <p>Then, build the containers locally.</p> <p>Note</p> <p>Presidio uses docker-compose to manage the different Presidio containers.</p> <p>From the root folder of the repo:</p> <pre><code>docker-compose up --build\n</code></pre> <p>Alternatively, you can build and run individual services. For example, for the <code>presidio-anonymizer</code> service:</p> <pre><code>docker build ./presidio-anonymizer -t presidio/presidio-anonymizer\n</code></pre> <p>And run:</p> <pre><code>docker run -d -p 5001:5001 presidio/presidio-anonymizer\n</code></pre> <p>For more information on developing locally, refer to the setting up a development environment section.</p>"},{"location":"presidio_V2/","title":"Presidio Revamp (aka V2)","text":"<p>As of March 2021, Presidio underwent a revamp to a new version referred to as V2.</p> <p>The main changes introduced in V2 are:</p> <ol> <li>gRPC replaced with HTTP to allow more customizable APIs and easier debugging</li> <li> <p>Focus on the Analyzer and Anonymizer services.</p> <ol> <li>Presidio Anonymizer is now Python-based and pip-installable.</li> <li>Presidio Analyzer does not use templates and external recognizer store.</li> <li>Image Redactor (formerly presidio-image-anonymizer) is in early beta and is Python based and pip installable.</li> <li>Other services are deprecated and may be migrated over time to V2 with the help of the community.</li> </ol> </li> <li> <p>Improved documentation, sample code, and build workflows.</p> </li> <li> <p>Format-Preserving Encryption replaced with Advanced Encryption Standard (AES) </p> </li> </ol>"},{"location":"presidio_V2/#v1-availability","title":"V1 Availability","text":"<p>Version V1 (legacy) is still available for download. To continue using the previous version: -   For docker containers, use tag=v1  -   For python packages, download version &lt; 2 (e.g. pip install presidio-analyzer==0.95)</p> <p>Note</p> <p>The legacy V1 code base will continue to be available under branch V1 but will no longer be officially supported.</p>"},{"location":"presidio_V2/#api-changes","title":"API Changes","text":"<p>The move from gRPC to HTTP-based APIs included changes to the API requests.</p> <ol> <li> <p>Changed payload format \u2013 moving from structured objects to JSON.</p> </li> <li> <p>Removed templates from the API, including flattening the JSON structure.</p> </li> <li> <p>Using snake_case instead of camelCase .</p> </li> </ol> <p>Below is a detailed outline of all changes made to the Analyzer and Anonymizer.</p>"},{"location":"presidio_V2/#analyzer-api-changes","title":"Analyzer API Changes","text":""},{"location":"presidio_V2/#legacy-json-request-grpc","title":"Legacy json request (gRPC)","text":"<pre><code>{\n    \"text\": \"My phone number is 212-555-5555\",\n    \"AnalyzeTemplateId\": \"1234\",\n    \"AnalyzeTemplate\": {\n        \"Fields\": [\n            {\n                \"Name\": \"PHONE_NUMBER\",\n                \"MinScore\": \"0.5\"\n            }\n        ],\n        \"AllFields\": true,\n        \"Description\": \"template description\",\n        \"CreateTime\": \"template creation time\",\n        \"ModifiedTime\": \"template modification time\",\n        \"Language\": \"fr\",\n        \"ResultsScoreThreshold\": 0.5\n    }\n}\n</code></pre>"},{"location":"presidio_V2/#v2-json-request-http","title":"V2 json request (HTTP)","text":"<pre><code>{\n    \"text\": \"My phone number is 212-555-5555\",\n    \"entities\": [\"PHONE_NUMBER\"],\n    \"language\": \"en\",\n    \"correlation_id\": \"213\",\n    \"score_threshold\": 0.5,\n    \"trace\": true,\n    \"return_decision_process\": true\n}\n</code></pre>"},{"location":"presidio_V2/#anonymizer-api-changes","title":"Anonymizer API Changes","text":""},{"location":"presidio_V2/#legacy-json-request-grpc_1","title":"Legacy json request (gRPC)","text":"<pre><code>{\n  \"text\": \"hello world, my name is Jane Doe. My number is: 034453334\",\n  \"template\": {\n    \"description\": \"DEPRECATED\",\n    \"create_time\": \"DEPRECATED\",\n    \"modified_time\": \"DEPRECATED\",\n    \"default_transformation\": {\n      \"replace_value\": {...},\n      \"redact_value\": {...},\n      \"hash_value\": {...},\n      \"mask_value\": {...},\n      \"fpe_value\": {...}\n    },\n    \"field_type_transformations\": [\n      {\n        \"fields\": [\n          {\n            \"name\": \"FIRST_NAME\",\n            \"min_score\": \"0.2\"\n          }\n        ],\n        \"transformation\": {\n          \"replace_value\": {...},\n          \"redact_value\": {...},\n          \"hash_value\": {...},\n          \"mask_value\": {...},\n          \"fpe_value\": {...}\n        }\n      }\n    ],\n    \"analyze_results\": [\n      {\n        \"text\": \"Jane\",\n        \"field\": {\n          \"name\": \"FIRST_NAME\",\n          \"min_score\": \"0.5\"\n        },\n        \"location\": {\n          \"start\": 24,\n          \"end\": 32,\n          \"length\": 6\n        },\n        \"score\": 0.8\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"presidio_V2/#v2-json-request-http_1","title":"V2 json request (HTTP)","text":"<pre><code>{\n    \"text\": \"hello world, my name is Jane Doe. My number is: 034453334\",\n    \"anonymizers\": {\n        \"DEFAULT\": {\n            \"type\": \"replace\",\n            \"new_value\": \"val\"\n        },\n        \"PHONE_NUMBER\": {\n            \"type\": \"mask\",\n            \"masking_char\": \"*\",\n            \"chars_to_mask\": 4,\n            \"from_end\": true\n        }\n    },\n    \"analyzer_results\": [\n        {\n            \"start\": 24,\n            \"end\": 32,\n            \"score\": 0.8,\n            \"entity_type\": \"NAME\"\n        },\n        {\n            \"start\": 24,\n            \"end\": 28,\n            \"score\": 0.8,\n            \"entity_type\": \"FIRST_NAME\"\n        },\n        {\n            \"start\": 29,\n            \"end\": 32,\n            \"score\": 0.6,\n            \"entity_type\": \"LAST_NAME\"\n        },\n        {\n            \"start\": 48,\n            \"end\": 57,\n            \"score\": 0.95,\n            \"entity_type\": \"PHONE_NUMBER\"\n        }\n    ]\n}\n</code></pre> <p>Specific for each anonymization type:</p> Anonymization name Legacy format (V1) New json format (V2) Replace <pre>string newValue = 1;</pre> <pre>{ \"new_value\": \"VALUE\" }</pre> Redact NONE NONE Mask <pre>string maskingCharacter = 1;int32 charsToMask = 2; bool fromEnd = 3;</pre> <pre>{ \"chars_to_mask\": 10, \"from_end\": true, \"masking_char\": \"*\" }</pre> Hash NONE <pre>{\"hash_type\": \"VALUE\"}</pre> FPE (now Encrypt) <pre>string key = 3t6w9z$C&amp;F)J@NcR;int32 tweak = D8E7920AFA330A73</pre> <pre>{\"key\": \"3t6w9z$C&amp;F)J@NcR\"}</pre> <p>Note</p> <p>The V2 API is continuously evolving. please follow the change log for updates.</p>"},{"location":"supported_entities/","title":"PII entities supported by Presidio","text":"<p>Presidio contains predefined recognizers for PII entities. This page describes the different entities Presidio can detect and the method Presidio employs to detect those.</p> <p>In addition, Presidio allows you to add custom entity recognizers. For more information, refer to the adding new recognizers documentation.</p>"},{"location":"supported_entities/#list-of-supported-entities","title":"List of supported entities","text":""},{"location":"supported_entities/#global","title":"Global","text":"Entity Type Description Detection Method CREDIT_CARD A credit card number is between 12 to 19 digits. https://en.wikipedia.org/wiki/Payment_card_number Pattern match and checksum CRYPTO A Crypto wallet number. Currently only Bitcoin address is supported Pattern match, context and checksum DATE_TIME Absolute or relative dates or periods or times smaller than a day. Pattern match and context EMAIL_ADDRESS An email address identifies an email box to which email messages are delivered Pattern match, context and RFC-822 validation IBAN_CODE The International Bank Account Number (IBAN) is an internationally agreed system of identifying bank accounts across national borders to facilitate the communication and processing of cross border transactions with a reduced risk of transcription errors. Pattern match, context and checksum IP_ADDRESS An Internet Protocol (IP) address (either IPv4 or IPv6). Pattern match, context and checksum NRP A person\u2019s Nationality, religious or political group. Custom logic and context LOCATION Name of politically or geographically defined location (cities, provinces, countries, international regions, bodies of water, mountains Custom logic and context PERSON A full person name, which can include first names, middle names or initials, and last names. Custom logic and context PHONE_NUMBER A telephone number Custom logic, pattern match and context MEDICAL_LICENSE Common medical license numbers. Pattern match, context and checksum URL A URL (Uniform Resource Locator), unique identifier used to locate a resource on the Internet Pattern match, context and top level url validation"},{"location":"supported_entities/#usa","title":"USA","text":"Entity Type Description Detection Method US_BANK_NUMBER A US bank account number is between 8 to 17 digits. Pattern match and context US_DRIVER_LICENSE A US driver license according to https://ntsi.com/drivers-license-format/ Pattern match and context US_ITIN US Individual Taxpayer Identification Number (ITIN). Nine digits that start with a \"9\" and contain a \"7\" or \"8\" as the 4 digit. Pattern match and context US_PASSPORT A US passport number with 9 digits. Pattern match and context US_SSN A US Social Security Number (SSN) with 9 digits. Pattern match and context"},{"location":"supported_entities/#uk","title":"UK","text":"Entity Type Description Detection Method UK_NHS A UK NHS number is 10 digits. Pattern match, context and checksum UK_NINO UK National Insurance Number is a unique identifier used in the administration of National Insurance and tax. Pattern match and context"},{"location":"supported_entities/#spain","title":"Spain","text":"Entity Type Description Detection Method ES_NIF A spanish NIF number (Personal tax ID) . Pattern match, context and checksum ES_NIE A spanish NIE number (Foreigners ID card) . Pattern match, context and checksum"},{"location":"supported_entities/#italy","title":"Italy","text":"Entity Type Description Detection Method IT_FISCAL_CODE An Italian personal identification code. https://en.wikipedia.org/wiki/Italian_fiscal_code Pattern match, context and checksum IT_DRIVER_LICENSE An Italian driver license number. Pattern match and context IT_VAT_CODE An Italian VAT code number Pattern match, context and checksum IT_PASSPORT An Italian passport number. Pattern match and context IT_IDENTITY_CARD An Italian identity card number. https://en.wikipedia.org/wiki/Italian_electronic_identity_card Pattern match and context"},{"location":"supported_entities/#poland","title":"Poland","text":"Entity Type Description Detection Method PL_PESEL Polish PESEL number Pattern match, context and checksum"},{"location":"supported_entities/#singapore","title":"Singapore","text":"FieldType Description Detection Method SG_NRIC_FIN A National Registration Identification Card Pattern match and context SG_UEN A Unique Entity Number (UEN) is a standard identification number for entities registered in Singapore. Pattern match, context, and checksum"},{"location":"supported_entities/#australia","title":"Australia","text":"FieldType Description Detection Method AU_ABN The Australian Business Number (ABN) is a unique 11 digit identifier issued to all entities registered in the Australian Business Register (ABR). Pattern match, context, and checksum AU_ACN An Australian Company Number is a unique nine-digit number issued by the Australian Securities and Investments Commission to every company registered under the Commonwealth Corporations Act 2001 as an identifier. Pattern match, context, and checksum AU_TFN The tax file number (TFN) is a unique identifier issued by the Australian Taxation Office to each taxpaying entity Pattern match, context, and checksum AU_MEDICARE Medicare number is a unique identifier issued by Australian Government that enables the cardholder to receive a rebates of medical expenses under Australia's Medicare system Pattern match, context, and checksum"},{"location":"supported_entities/#india","title":"India","text":"FieldType Description Detection Method IN_PAN The Indian Permanent Account Number (PAN) is a unique 12 character alphanumeric identifier issued to all business and individual entities registered as Tax Payers. Pattern match, context IN_AADHAAR Indian government issued unique 12 digit individual identity number Pattern match, context, and checksum IN_VEHICLE_REGISTRATION Indian government issued transport (govt, personal, diplomatic, defence)  vehicle registration number Pattern match, context, and checksum IN_VOTER Indian Election Commission issued 10 digit alpha numeric voter id for all indian citizens (age 18 or above) Pattern match, context IN_PASSPORT Indian Passport Number Pattern match, Context"},{"location":"supported_entities/#finland","title":"Finland","text":"FieldType Description Detection Method FI_PERSONAL_IDENTITY_CODE The Finnish Personal Identity Code (Henkil\u00f6tunnus) is a unique 11 character individual identity number. Pattern match, context and custom logic."},{"location":"supported_entities/#korea","title":"Korea","text":"FieldType Description Detection Method KR_RRN The Korean Resident Registration Number (RRN) is a 13-digit number issued to all Korean residents. Pattern match, context and custom logic."},{"location":"supported_entities/#thai","title":"Thai","text":"FieldType Description Detection Method TH_TNIN The Thai National ID Number (TNIN) is a unique 13-digit number issued to all Thai residents. Pattern match, context and custom logic."},{"location":"supported_entities/#adding-a-custom-pii-entity","title":"Adding a custom PII entity","text":"<p>See this documentation for instructions on how to add a new Recognizer for a new type of PII entity.</p>"},{"location":"supported_entities/#complementing-presidio-with-azure-ai-language-pii","title":"Complementing Presidio with Azure AI Language PII","text":"<p>Azure AI Language PII  is a cloud-based service that provides Natural Language Processing (NLP) features for detecting PII in text.</p> <p>A list of supported entities by Azure AI Language PII can be found here.</p> <p>To add Azure AI language into Presidio, see this sample.</p>"},{"location":"supported_entities/#complementing-presidio-with-azure-health-data-services-phi","title":"Complementing Presidio with Azure Health Data Services PHI","text":"<p>Azure Health Data Services PHI is a cloud-based service that provides Natural Language Processing (NLP) features for detecting PHI in text.</p> <p>A list of supported entities by Azure Health Data Services PHI can be found here.</p> <p>To add Azure AI language into Presidio, see this sample.</p>"},{"location":"supported_entities/#connecting-to-3rd-party-pii-detectors","title":"Connecting to 3rd party PII detectors","text":"<p>See this documentation for instructions on how to implement an external PII detector for a new or existing type of PII entity.</p>"},{"location":"text_anonymization/","title":"Text anonymization","text":"<p>Presidio's features two main modules for anonymization PII in text:</p> <ul> <li>Presidio analyzer: Identification of PII in text</li> <li>Presidio anonymizer: De-identify detected PII entities using different operators</li> </ul> <p>In most cases, we would run the Presidio analyzer to detect where PII entities exist, and then the Presidio anonymizer to remove those using specific operators (such as redact, replace, hash or encrypt)</p> <p>This figure presents the overall flow in high level:</p> <p></p> <ul> <li>The Presidio Analyzer holds multiple recognizers, each one capable of detecting specific PII entities. These recognizers leverage regular expressions, deny lists, checksum, rule based logic, Named Entity Recognition ML models and context from surrounding words.</li> <li>The Presidio Anonymizer holds multiple operators, each one can be used to anonymize the PII entity in a different way. Additionally, it can be used to de-anonymize an already anonymized entity (For example, decrypt an encrypted entity)</li> </ul>"},{"location":"analyzer/","title":"Presidio Analyzer","text":"<p>The Presidio analyzer is a Python based service for detecting PII entities in text.</p> <p>During analysis, it runs a set of different PII Recognizers, each one in charge of detecting one or more PII entities using different mechanisms.</p> <p>Presidio analyzer comes with a set of predefined recognizers, but can easily be extended with other types of custom recognizers. Predefined and custom recognizers leverage regex, Named Entity Recognition and other types of logic to detect PII in unstructured text.</p> <p></p>"},{"location":"analyzer/#installation","title":"Installation","text":"<p>see Installing Presidio.</p>"},{"location":"analyzer/#getting-started","title":"Getting started","text":"PythonAs an HTTP server <p>Once the Presidio-analyzer package is installed, run this simple analysis script:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text=\"My phone number is 212-555-5555\",\n                           entities=[\"PHONE_NUMBER\"],\n                           language='en')\nprint(results)\n</code></pre> <p>You can run presidio analyzer as an http server using either python runtime or using a docker container.</p>"},{"location":"analyzer/#using-docker-container","title":"Using docker container","text":"<pre><code>cd presidio-analyzer\ndocker run -p 5002:3000 presidio-analyzer\n</code></pre>"},{"location":"analyzer/#using-python-runtime","title":"Using python runtime","text":"<p>Note</p> <p>This requires the Presidio Github repository to be cloned.</p> <pre><code>cd presidio-analyzer\npython app.py\ncurl -d '{\"text\":\"John Smith drivers license is AC432223\", \"language\":\"en\"}' -H \"Content-Type: application/json\" -X POST http://localhost:3000/analyze\n</code></pre>"},{"location":"analyzer/#main-concepts","title":"Main concepts","text":"<p>Presidio analyzer is a set of tools that are used to detect entities in text. The main object in Presidio Analyzer is the <code>AnalyzerEngine</code>. In the following section we'll describe the main concepts in Presidio Analyzer.</p> <p>This simplified class diagram shows the main classes in Presidio Analyzer:</p> <pre><code>classDiagram\n    direction LR\n    class RecognizerResult {\n        +str entity_type\n        +float score\n        +int start\n        +int end\n    }\n\n    class EntityRecognizer {\n        +str name\n        +int version\n        +List[str] supported_entities\n        +analyze(text, entities) List[RecognizerResult]\n    }\n\n\n    class RecognizerRegistry {\n        +add_recognizer(recognizer) None\n        +remove_recognizer(recognizer) None\n        +load_predefined_recognizers() None\n        +get_recognizers() List[EntityRecognizer]\n\n\n    }\n\n    class NlpEngine {\n        +process_text(text, language) NlpArtifacts\n        +process_batch(texts, language) Iterator[NlpArtifacts]\n    }\n\n    class ContextAwareEnhancer {\n        +enhance_using_context(text, recognizer_results) List[RecognizerResult]\n    }\n\n\n    class AnalyzerEngine {\n        +NlpEngine nlp_engine\n        +RecognizerRegistry registry\n        +ContextAwareEnhancer context_aware_enhancer\n        +analyze(text: str, language) List[RecognizerResult]\n\n    }\n\n    NlpEngine &lt;|-- SpacyNlpEngine\n    NlpEngine &lt;|-- TransformersNlpEngine\n    NlpEngine &lt;|-- StanzaNlpEngine\n    AnalyzerEngine *-- RecognizerRegistry\n    AnalyzerEngine *-- NlpEngine\n    AnalyzerEngine *-- ContextAwareEnhancer\n    RecognizerRegistry o-- \"0..*\" EntityRecognizer\n    ContextAwareEnhancer &lt;|-- LemmaContextAwareEnhancer\n\n    %% Defining styles\n    style RecognizerRegistry fill:#E6F7FF,stroke:#005BAC,stroke-width:2px\n    style NlpEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style SpacyNlpEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style YourNlpEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style TransformersNlpEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style StanzaNlpEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style ContextAwareEnhancer fill:#E6FFE6,stroke:#008000,stroke-width:2px\n    style LemmaContextAwareEnhancer fill:#E6FFE6,stroke:#008000,stroke-width:2px\n    style EntityRecognizer fill:#F5F5DC,stroke:#8B4513,stroke-width:2px\n    style YourEntityRecognizer fill:#F5F5DC,stroke:#8B4513,stroke-width:2px\n    style RecognizerResult fill:#FFF0F5,stroke:#FF69B4,stroke-width:2px</code></pre>"},{"location":"analyzer/#recognizerresult","title":"<code>RecognizerResult</code>","text":"<p>A <code>RecognizerResult</code> holds the type and span of a PII entity.</p>"},{"location":"analyzer/#entityrecognizer","title":"<code>EntityRecognizer</code>","text":"<p>An entity recognizer is an object in Presidio that is responsible for detecting entities in text. An entity recognizer can be a rule-based recognizer, a machine learning model, or a combination of both.</p>"},{"location":"analyzer/#patternrecognizer","title":"<code>PatternRecognizer</code>","text":"<p>A <code>PatternRecognizer</code> is a type of entity recognizer that uses regular expressions to detect entities in text. One can create new <code>PatternRecognizer</code> objects by providing a list of regular expressions, context words, validation and invalidation logic and additional parameters that facilitate the detection of entities.</p>"},{"location":"analyzer/#analyzerengine","title":"<code>AnalyzerEngine</code>","text":"<p>The <code>AnalyzerEngine</code> is the main object in Presidio Analyzer that is responsible for detecting entities in text. The <code>AnalyzerEngine</code> can be configured in various ways to fit the specific needs of the user.</p>"},{"location":"analyzer/#recognizerregistry","title":"<code>RecognizerRegistry</code>","text":"<p>The <code>RecognizerRegistry</code> is a registry that contains all the entity recognizers that are available in Presidio. The <code>AnalyzerEngine</code> uses the <code>RecognizerRegistry</code> to detect entities in text.</p>"},{"location":"analyzer/#nlpengine","title":"<code>NlpEngine</code>","text":"<p>An NLP Engine is an object that holds the NLP model that is used by the <code>AnalyzerEngine</code> to parse the input text and extract different features from it, such as tokens, lemmas, entities, and more. Note that Named Entity Recognition (NER) models can be added in two ways to Presidio: One is through the <code>NlpEngine</code> object, and the other is through a new <code>EntityRecognizer</code> object. By creating a Named Entity Recognition model through the <code>NlpEngine</code>, the named entities will be available to the different modules in Presidio. Furthermore, the <code>NlpEngine</code> object supports a batch mode (i.e., processing multiple texts at once) which allows for faster processing of large amounts of text. It is possible to mix multiple NER models in Presidio, for instance, one model as the <code>NlpEngine</code> and others as additional <code>EntityRecognizer</code> objects.</p> <p>Presidio has an off-the-shelf support for multiple NLP packages, such as spaCy, stanza, and huggingface. The simplest way to integrate a model from these packages is through the <code>NlpEngine</code>. More information on this can be found in the NlpEngine documentation. The samples gallery has several examples of leveraging NER models as new <code>EntityRecognizer</code> objects. For example, flair and spanmarker. For a detailed flow of Named Entities within presidio, see the diagram in this document.</p>"},{"location":"analyzer/#context-aware-enhancer","title":"<code>Context Aware Enhancer</code>","text":"<p>The <code>ContextAwareEnhancer</code> is a module that enhances the detection of entities by using the context of the text. The <code>ContextAwareEnhancer</code> can be used to improve the detection of entities that are dependent on the context of the text, such as dates, locations, and more. The default implementation is the <code>LemmaContextAwareEnhancer</code> which uses the lemmas of the tokens in the text to enhance the detection of entities. Note that it's possible (and sometimes recommended) to create custom <code>ContextAwareEnhancer</code> objects to fit the specific needs of the user, for example if the context should support more than one word, which is currently not supported by the default Lemma based enhancer. More information on this can be found in this sample.</p>"},{"location":"analyzer/#creating-pii-recognizers","title":"Creating PII recognizers","text":"<p>Presidio analyzer can be easily extended to support additional PII entities. See this tutorial on adding new PII recognizers for more information.</p>"},{"location":"analyzer/#multi-language-support","title":"Multi-language support","text":"<p>Presidio can be used to detect PII entities in multiple languages. Refer to the multi-language support for more information.</p>"},{"location":"analyzer/#outputting-the-analyzer-decision-process","title":"Outputting the analyzer decision process","text":"<p>Presidio analyzer has a built in mechanism for tracing each decision made. This can be useful when attempting to understand a specific PII detection. For more info, see the decision process documentation.</p>"},{"location":"analyzer/#supported-entities","title":"Supported entities","text":"<p>For a list of the current supported entities: Supported entities.</p>"},{"location":"analyzer/#api-reference","title":"API reference","text":"<p>Follow the API Spec for the Analyzer REST API reference details and Analyzer Python API for Python API reference</p>"},{"location":"analyzer/#samples","title":"Samples","text":"<p>Samples illustrating the usage of the Presidio Analyzer can be found in the Python samples.</p>"},{"location":"analyzer/adding_recognizers/","title":"Supporting detection of new types of PII entities","text":"<p>Presidio can be extended to support detection of new types of PII entities, and to support additional languages. These PII recognizers could be added via code or ad-hoc as part of the request.</p>"},{"location":"analyzer/adding_recognizers/#introduction-to-recognizer-development","title":"Introduction to recognizer development","text":"<p>Entity recognizers are Python objects capable of detecting one or more entities in a specific language. In order to extend Presidio's detection capabilities to new types of PII entities, these <code>EntityRecognizer</code> objects should be added to the existing list of recognizers.</p>"},{"location":"analyzer/adding_recognizers/#types-of-recognizer-classes-in-presidio","title":"Types of recognizer classes in Presidio","text":"<p>The following class diagram shows the different types of recognizer families Presidio contains.</p> <p></p> <ul> <li>The <code>EntityRecognizer</code> is an abstract class for all recognizers.</li> <li>The <code>RemoteRecognizer</code> is an abstract class for calling external PII detectors. See more info here.</li> <li>The abstract class <code>LocalRecognizer</code> is implemented by all recognizers running within the Presidio-analyzer process.</li> <li>The <code>PatternRecognizer</code> is an class for supporting regex and deny-list based recognition logic, including validation (e.g., with checksum) and context support. See an example here.</li> </ul>"},{"location":"analyzer/adding_recognizers/#extending-the-analyzer-for-additional-pii-entities","title":"Extending the analyzer for additional PII entities","text":"<ol> <li>Create a new class based on <code>EntityRecognizer</code>.</li> <li>Add the new recognizer to the recognizer registry so that the <code>AnalyzerEngine</code> can use the new recognizer during analysis.</li> </ol>"},{"location":"analyzer/adding_recognizers/#simple-example","title":"Simple example","text":"<p>For simple recognizers based on regular expressions or deny-lists, we can leverage the provided <code>PatternRecognizer</code>:</p> <pre><code>from presidio_analyzer import PatternRecognizer\ntitles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n                                      deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n</code></pre> <p>Calling the recognizer itself:</p> <pre><code>titles_recognizer.analyze(text=\"Mr. Schmidt\", entities=\"TITLE\")\n</code></pre> <p>Adding it to the list of recognizers:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nregistry = RecognizerRegistry()\nregistry.load_predefined_recognizers()\n\n# Add the recognizer to the existing list of recognizers\nregistry.add_recognizer(titles_recognizer)\n\n# Set up analyzer with our updated recognizer registry\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Run with input text\ntext=\"His name is Mr. Jones\"\nresults = analyzer.analyze(text=text, language=\"en\")\nprint(results)\n</code></pre> <p>Alternatively, we can add the recognizer directly to the existing registry:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\nanalyzer = AnalyzerEngine()\n\nanalyzer.registry.add_recognizer(titles_recognizer)\n\nresults = analyzer.analyze(text=text, language=\"en\")\nprint(results)\n</code></pre> <p>For pattern based recognizers, it is possible to change the regex flags, either for one recognizer or for all. For one recognizer, use the <code>global_regex_flags</code> parameter in the <code>PatternRecognizer</code> constructor. For all recognizers, use the <code>global_regex_flags</code> parameter in the <code>RecognizerRegistry</code> constructor:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nimport regex as re\n\nregistry = RecognizerRegistry(global_regex_flags=re.DOTALL | re.MULTILINE | re.IGNORECASE)\nengine = AnalyzerEngine(registry=registry)\nengine.analyze(...)\n</code></pre>"},{"location":"analyzer/adding_recognizers/#creating-a-new-entityrecognizer-in-code","title":"Creating a new <code>EntityRecognizer</code> in code","text":"<p>To create a new recognizer via code:</p> <ol> <li> <p>Create a new Python class which implements LocalRecognizer. (<code>LocalRecognizer</code> implements the base EntityRecognizer class)</p> <p>This class has the following functions:</p> <p>i. load: load a model / resource to be used during recognition</p> <p> <pre><code>def load(self)\n</code></pre></p> <p>ii. analyze: The main function to be called for getting entities out of the new recognizer:</p> <p> <pre><code>def analyze(self, text, entities, nlp_artifacts)\n</code></pre></p> <p>Notes: 1. Each recognizer has access to different NLP assets such as tokens, lemmas, and more. These are given through the <code>nlp_artifacts</code> parameter. Refer to the source code for more information.</p> <ol> <li>The <code>analyze</code> method should return a list of RecognizerResult.</li> </ol> </li> <li> <p>Add it to the recognizer registry using <code>registry.add_recognizer(my_recognizer)</code>.</p> </li> </ol> <p>For more examples, see the Customizing Presidio Analyzer jupyter notebook.</p>"},{"location":"analyzer/adding_recognizers/#creating-a-remote-recognizer","title":"Creating a remote recognizer","text":"<p>A remote recognizer is an <code>EntityRecognizer</code> object interacting with an external service. The external service could be a 3rd party PII detection service or a custom service deployed in parallel to Presidio.</p> <p>Sample implementation of a <code>RemoteRecognizer</code>. In this example, an external PII detection service exposes two APIs: <code>detect</code> and <code>supported_entities</code>. The class implemented here, <code>ExampleRemoteRecognizer</code>, uses the <code>requests</code> package to call the external service via HTTP.</p> <p>In this code snippet, we simulate the external PII detector by using the Presidio analyzer. In reality, we would adapt this code to fit the external PII detector we have in hand.</p> <p>For an example of integrating a <code>RemoteRecognizer</code> with Presidio-Analyzer, see this example.</p>"},{"location":"analyzer/adding_recognizers/#creating-pre-defined-recognizers","title":"Creating pre-defined recognizers","text":"<p>Once a recognizer is created, it can either be added to the <code>RecognizerRegistry</code> via the <code>add_recognizer</code> method, or it could be added into the list of predefined recognizers. To add a recognizer to the list of pre-defined recognizers:</p> <ol> <li>Clone the repo.</li> <li>Create a file containing the new recognizer Python class.</li> <li>Add the recognizer to the <code>recognizers</code> in the <code>default_recognizers</code> config. Details of recognizer parameters are given Here.</li> <li>Optional: Update documentation (e.g., the supported entities list).</li> </ol>"},{"location":"analyzer/adding_recognizers/#azure-ai-language-recognizer","title":"Azure AI Language recognizer","text":"<p>On how to integrate Presidio with Azure AI Language PII detection service, and a sample for a Text Analytics Remote Recognizer, refer to the Azure Text Analytics Integration document.</p>"},{"location":"analyzer/adding_recognizers/#azure-health-data-services-ahds-de-identification-recognizer","title":"Azure Health Data Services (AHDS) de-identification recognizer","text":"<p>On how to integrate Presidio with AHDS De-Identification Protected Health Information (PHI) detection service, and a sample for a ADHS Remote Recognizer, refer to the AHDS de-Identification Integration document.</p>"},{"location":"analyzer/adding_recognizers/#creating-ad-hoc-recognizers","title":"Creating ad-hoc recognizers","text":"<p>In addition to recognizers in code, it is possible to create ad-hoc recognizers via the Presidio Analyzer API for regex and deny-list based logic. These recognizers, in JSON form, are added to the <code>/analyze</code> request and are only used in the context of this request.</p> <ul> <li> <p>The json structure for a regex ad-hoc recognizer is the following:</p> <pre><code>{\n    \"text\": \"John Smith drivers license is AC432223. Zip code: 10023\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Zip code Recognizer\",\n        \"supported_language\": \"en\",\n        \"patterns\": [\n            {\n            \"name\": \"zip code (weak)\", \n            \"regex\": \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\", \n            \"score\": 0.01\n            }\n        ],\n        \"context\": [\"zip\", \"code\"],\n        \"supported_entity\":\"ZIP\"\n        }\n    ]\n}\n</code></pre> </li> <li> <p>The json structure for a deny-list based recognizers is the following:</p> <pre><code>{\n    \"text\": \"Mr. John Smith's drivers license is AC432223\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Mr. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Mr\", \"Mr.\", \"Mister\"],\n        \"supported_entity\":\"MR_TITLE\"\n        },\n        {\n        \"name\": \"Ms. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Ms\", \"Ms.\", \"Miss\", \"Mrs\", \"Mrs.\"],\n        \"supported_entity\":\"MS_TITLE\"\n        }\n    ]\n}\n</code></pre> </li> </ul> <p>In both examples, the <code>/analyze</code> request is extended with a list of <code>ad_hoc_recognizers</code>, which could be either <code>patterns</code>, <code>deny_list</code> or both.</p> <p>Additional examples can be found in the OpenAPI spec.</p>"},{"location":"analyzer/adding_recognizers/#reading-pattern-recognizers-from-yaml","title":"Reading pattern recognizers from YAML","text":"<p>Recognizers can be loaded from a YAML file, which allows users to add recognition logic without writing code. An example YAML file can be found here.</p> <p>Once the YAML file is created, it can be loaded into the <code>RecognizerRegistry</code> instance.</p> <p>This example creates a <code>RecognizerRegistry</code> holding only the recognizers in the YAML file:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.recognizer_registry import RecognizerRegistryProvider\n\nrecognizer_registry_conf_file = \"./analyzer/recognizers-config.yml\"\n\nprovider = RecognizerRegistryProvider(\n                conf_file=recognizer_registry_conf_file\n            )\nregistry = provider.create_recognizer_registry()\nanalyzer = AnalyzerEngine(registry=registry)\n\nresults = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results)\n</code></pre> <p>This example adds the new recognizers to the predefined recognizers in Presidio:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nyaml_file = \"recognizers.yaml\"\nregistry = RecognizerRegistry()\nregistry.load_predefined_recognizers()\n\nregistry.add_recognizers_from_yaml(yaml_file)\n\nanalyzer = AnalyzerEngine()\nanalyzer.analyze(text=\"Mr. and Mrs. Smith\", language=\"en\")\n</code></pre> <p>Further reading:</p> <ol> <li>PII detection in different languages.</li> <li>Customizing the NLP model.</li> <li>Best practices for developing PII recognizers.</li> <li>Code samples for customizing Presidio Analyzer with new recognizers.</li> </ol>"},{"location":"analyzer/analyzer_engine_provider/","title":"Configuring the Analyzer Engine from file","text":"<p>Presidio uses <code>AnalyzerEngineProvider</code> to load <code>AnalyzerEngine</code> configuration from file.  Configuration can be loaded in three different ways:</p>"},{"location":"analyzer/analyzer_engine_provider/#using-a-single-file","title":"Using a single file","text":"<p>Create an <code>AnalyzerEngineProvider</code> using a single configuration file and set its path to <code>analyzer_engine_conf_file</code>, then create <code>AnalyzerEngine</code> based on it:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, AnalyzerEngineProvider\n\nanalyzer_conf_file = \"./analyzer/analyzer-config-all.yml\"\n\nprovider = AnalyzerEngineProvider(\n    analyzer_engine_conf_file=analyzer_conf_file\n    )\nanalyzer = provider.create_engine()\n\nresults = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results)\n</code></pre> <p>An example configuration file:</p> <pre><code>supported_languages: \n- en\ndefault_score_threshold: 0\n\nnlp_configuration:\n  nlp_engine_name: spacy\n  models:\n  -\n    lang_code: en\n    model_name: en_core_web_lg\n  -\n    lang_code: es\n    model_name: es_core_news_md\n  ner_model_configuration:\n    model_to_presidio_entity_mapping:\n      PER: PERSON\n      PERSON: PERSON\n      LOC: LOCATION\n      LOCATION: LOCATION\n      GPE: LOCATION\n      ORG: ORGANIZATION\n      DATE: DATE_TIME\n      TIME: DATE_TIME\n      NORP: NRP\n\n    low_confidence_score_multiplier: 0.4\n    low_score_entity_names:\n    - ORGANIZATION\n    - ORG\n    default_score: 0.85\n\nrecognizer_registry:\n  global_regex_flags: 26\n  recognizers: \n  - name: CreditCardRecognizer\n    supported_languages: \n      - en\n    supported_entity: IT_FISCAL_CODE\n    type: predefined\n\n  - name: ItFiscalCodeRecognizer\n    type: predefined\n</code></pre> <p>The configuration file contains the following parameters:</p> <ul> <li><code>supported_languages</code>: A list of supported languages that the analyzer will support.</li> <li><code>default_score_threshold</code>: A score that determines the minimal threshold for detection.</li> <li><code>nlp_configuration</code>: Configuration given to the NLP engine which will detect the PIIs and extract features for the downstream logic.</li> <li><code>recognizer_registry</code>: All the recognizers that will be used by the analyzer. </li> </ul> <p>Note</p> <p><code>supported_languages</code> must be identical to the same field in recognizer_registry</p>"},{"location":"analyzer/analyzer_engine_provider/#using-multiple-files","title":"Using multiple files","text":"<p>Create an <code>AnalyzerEngineProvider</code> using three different configuration files for each of the following components:</p> <ul> <li>Analyzer</li> <li>NLP Engine</li> <li>Recognizer Registry</li> </ul> <p>Note</p> <p>Each of these parameters is optional and in case it's not set, the default configuration will be used. </p> <pre><code>from presidio_analyzer import AnalyzerEngine, AnalyzerEngineProvider\n\nanalyzer_conf_file = \"./analyzer/analyzer-config.yml\"\nnlp_engine_conf_file = \"./analyzer/nlp-config.yml\"\nrecognizer_registry_conf_file = \"./analyzer/recognizers-config.yml\"\n\nprovider = AnalyzerEngineProvider(\n    analyzer_engine_conf_file=analyzer_conf_file,\n    nlp_engine_conf_file=nlp_engine_conf_file,\n    recognizer_registry_conf_file=recognizer_registry_conf_file,\n    )\nanalyzer = provider.create_engine()\n\nresults = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results)\n</code></pre> <p>The structure of the configuration files is as follows:</p> <ul> <li> <p>Analyzer engine configuration file:</p> <pre><code>supported_languages: \n- en\ndefault_score_threshold: 0\n</code></pre> </li> <li> <p>NLP engine configuration file structure is examined thoroughly in the Customizing the NLP model section.</p> </li> <li> <p>Recognizer registry configuration file structure is examined thoroughly in the Customizing recognizer registry from file section.</p> </li> </ul>"},{"location":"analyzer/analyzer_engine_provider/#using-the-default-configuration","title":"Using the default configuration","text":"<p>Create an <code>AnalyzerEngineProvider</code> without any parameters. This will load the default configuration:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, AnalyzerEngineProvider\n\nprovider = AnalyzerEngineProvider().create_engine()\n\nresults = provider.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results)\n</code></pre> <p>The default configuration of <code>AnalyzerEngine</code> is defined in the following files: </p> <ul> <li>Analyzer Engine</li> <li>NLP Engine</li> <li>Recognizer Registry</li> </ul>"},{"location":"analyzer/analyzer_engine_provider/#enabling-and-disabling-recognizers","title":"Enabling and disabling recognizers","text":"<p>In general, recognizers that are not added to the configuration would not be created, with one exception.</p>"},{"location":"analyzer/analyzer_engine_provider/#enablingdisabling-the-nlp-recognizer","title":"Enabling/Disabling the NLP recognizer","text":"<p>One exception to this is the recognizer which extracts the <code>NlpEngine</code> entities (e.g. <code>SpacyRecognizer</code> when the <code>NlpEngine</code> is <code>SpacyNlpEngine</code>; <code>TransformersRecognizer</code> when the engine is <code>TransformersNlpEngine</code> and <code>StanzaRecognizer</code> when the engine is <code>StanzaNlpEngine</code>). </p> <p>Recognizers (including the NLP recognizer) could be disabled by defining <code>enabled=false</code> in the YAML configuration. For example: <pre><code>recognizer_registry:\n  global_regex_flags: 26\n  recognizers:\n    - name: SpacyRecognizer\n      type: predefined\n      enabled: false\n    - name: CreditCardRecognizer\n      type: predefined\n      enabled: true\n\nsupported_languages:\n  - en\ndefault_score_threshold: 0.7\n\nnlp_configuration:\n  nlp_engine_name: spacy\n  models:\n    -\n      lang_code: en\n      model_name: en_core_web_lg\n</code></pre></p> <p>In this example, the <code>SpacyRecognizer</code> is disabled, and the <code>CreditCardRecognizer</code> is enabled, resulting in only the <code>CREDIT_CARD</code> PII entity to be returned if detected.</p>"},{"location":"analyzer/analyzer_engine_provider/#adding-context-words-in-yaml-recognizers","title":"Adding context words in YAML recognizers","text":"<p>Recognizers defined in YAML can also include a <code>context</code> field. When used with <code>AnalyzerEngine</code> and a context enhancer, these words boost the score if they appear near the detected entity.</p> <p>Example:</p> <p><pre><code>recognizers:\n  - name: \"Date of Birth Recognizer\"\n    supported_entity: \"DATE_TIME\"\n    supported_language: \"en\"\n    patterns:\n      - name: \"DOB without slashes\"\n        regex: \"((19|20)\\\\d{2}(0[1-9]|1[0-2])(0[1-9]|[12]\\\\d|3[01]))\"\n        score: 0.8\n    context:\n      - DOB\n</code></pre> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nfrom presidio_analyzer.context_aware_enhancers.lemma_context_aware_enhancer import LemmaContextAwareEnhancer\n\n# Save the DOB recognizer YAML to disk\ndob_yaml = \"\"\"\nrecognizers:\n  - name: \"Date of Birth Recognizer\"\n    supported_entity: \"DATE_TIME\"\n    supported_language: \"en\"\n    patterns:\n      - name: \"DOB without slashes\"\n        regex: \"((19|20)\\\\d{2}(0[1-9]|1[0-2])(0[1-9]|[12]\\\\d|3[01]))\"\n        score: 0.8\n    context:\n      - DOB\n\"\"\"\nwith open(\"dob_recognizer.yml\", \"w\") as f:\n    f.write(dob_yaml)\n\n# Configure NLP engine\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n}\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine = provider.create_engine()\n\n# Load recognizer from YAML\nregistry = RecognizerRegistry()\nregistry.add_recognizers_from_yaml(\"dob_recognizer.yml\")\n\n# Analyzer with custom registry\nanalyzer = AnalyzerEngine(\n    registry=registry,\n    nlp_engine=nlp_engine,\n    supported_languages=[\"en\"]\n)\n\ntext = \"DOB: 19571012\"\n\n# Run base analysis\nresults = analyzer.analyze(text=text, language=\"en\")\nprint(\"Base results:\", results)\n\n# Apply context enhancer\nenhancer = LemmaContextAwareEnhancer()\nnlp_artifacts = analyzer.nlp_engine.process_text(text, language=\"en\")\n\nboosted = enhancer.enhance_using_context(\n    text=text,\n    raw_results=results,\n    nlp_artifacts=nlp_artifacts,\n    recognizers=registry.recognizers,\n    context=[\"DOB\"]\n)\nprint(\"Boosted results:\", boosted)\n</code></pre></p>"},{"location":"analyzer/customizing_nlp_models/","title":"Customizing the NLP engine in Presidio Analyzer","text":"<p>Presidio uses NLP engines for two main tasks: NER based PII identification, and feature extraction for downstream rule based logic (such as leveraging context words for improved detection). While Presidio comes with an open-source model (the <code>en_core_web_lg</code> model from spaCy), additional NLP models and frameworks could be plugged in, either public or proprietary. These models can be trained or downloaded from existing NLP frameworks like spaCy, Stanza and transformers.</p> <p>In addition, other types of NLP frameworks can be integrated into Presidio.</p>"},{"location":"analyzer/customizing_nlp_models/#setting-up-a-custom-nlp-model","title":"Setting up a custom NLP model","text":"<ul> <li>spaCy or stanza</li> <li>transformers</li> </ul>"},{"location":"analyzer/customizing_nlp_models/#configure-presidio-to-use-the-new-model","title":"Configure Presidio to use the new model","text":"<p>Configuration can be done in two ways:</p> <ul> <li> <p>Via code: Create an <code>NlpEngine</code> using the <code>NlpEnginerProvider</code> class, and pass it to the <code>AnalyzerEngine</code> as input:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\n# Create configuration containing engine name and models\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n                {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n}\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, \n    supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es Morris\", language=\"es\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results_english)\n</code></pre> </li> <li> <p>Via configuration: Set up the models which should be used in the default <code>conf</code> file.</p> <p>An example Conf file:</p> <pre><code>nlp_engine_name: spacy\nmodels:\n    -\n    lang_code: en\n    model_name: en_core_web_lg\n    -\n    lang_code: es\n    model_name: es_core_news_md \nner_model_configuration:\nlabels_to_ignore:\n- O\nmodel_to_presidio_entity_mapping:\n    PER: PERSON\n    LOC: LOCATION\n    ORG: ORGANIZATION\n    AGE: AGE\n    ID: ID\n    DATE: DATE_TIME\nlow_confidence_score_multiplier: 0.4\nlow_score_entity_names:\n- ID\n- ORG\n</code></pre> <p>The <code>ner_model_configuration</code> section contains the following parameters:</p> </li> <li> <p><code>labels_to_ignore</code>: A list of labels to ignore. For example, <code>O</code> (no entity) or entities you are not interested in returning.</p> </li> <li><code>model_to_presidio_entity_mapping</code>: A mapping between the transformers model labels and the Presidio entity types.</li> <li><code>low_confidence_score_multiplier</code>: A multiplier to apply to the score of entities with low confidence.</li> <li> <p><code>low_score_entity_names</code>: A list of entity types to apply the low confidence score multiplier to.</p> <p>The default conf file is read during the default initialization of the <code>AnalyzerEngine</code>. Alternatively, the path to a custom configuration file can be passed to the <code>NlpEngineProvider</code>:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nLANGUAGES_CONFIG_FILE = \"./docs/analyzer/languages-config.yml\"\n\n# Create NLP engine based on configuration file\nprovider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, \n    supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es David\", language=\"es\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is David\", language=\"en\")\nprint(results_english)\n</code></pre> <p>In this examples we:     a. create an <code>NlpEngine</code> holding two spaCy models (one in English: <code>en_core_web_lg</code> and one in Spanish: <code>es_core_news_md</code>).     b. define the <code>supported_languages</code> parameter accordingly.     c. pass requests in each of these languages.</p> <p>Note</p> <p>Presidio can currently use one NER model per language via the <code>NlpEngine</code>. If multiple are required, consider wrapping NER models as additional recognizers (see sample here).</p> </li> </ul>"},{"location":"analyzer/customizing_nlp_models/#leverage-frameworks-other-than-spacy-stanza-and-transformers-for-ml-based-pii-detection","title":"Leverage frameworks other than spaCy, Stanza and transformers for ML based PII detection","text":"<p>In addition to the built-in spaCy/Stanza/transformers capabilities, it is possible to create new recognizers which serve as interfaces to other models. For more information:</p> <ul> <li>Remote recognizer documentation and samples.</li> <li>Flair recognizer example</li> </ul> <p>For considerations for creating such recognizers, see the best practices for adding ML recognizers documentation.</p>"},{"location":"analyzer/decision_process/","title":"The Presidio-analyzer decision process","text":""},{"location":"analyzer/decision_process/#background","title":"Background","text":"<p>Presidio-analyzer's decision process exposes information on why a specific PII was detected. Such information could contain:</p> <ul> <li>Which recognizer detected the entity</li> <li>Which regex pattern was used</li> <li>Interpretability mechanisms in ML models</li> <li>Which context words improved the score</li> <li>Confidence scores before and after each step</li> </ul> <p>And more.</p>"},{"location":"analyzer/decision_process/#usage","title":"Usage","text":"<p>The decision process can be leveraged in two ways:</p> <ol> <li>Presidio-analyzer can log its decision process into a designated logger, which allows you to investigate a specific api request, by exposing a <code>correlation-id</code> as part of the api response headers.</li> <li>The decision process can be returned as part of the <code>/analyze</code>  response.</li> </ol>"},{"location":"analyzer/decision_process/#getting-the-decision-process-as-part-of-the-response","title":"Getting the decision process as part of the response","text":"<p>The decision process result can be added to the response. To enable it, call the <code>analyze</code> method with <code>return_decision_process</code> set as True.</p> <p>For example:</p> HTTPPython <pre><code>curl -d '{\n    \"text\": \"John Smith drivers license is AC432223\", \n    \"language\": \"en\", \n    \"return_decision_process\": true}' -H \"Content-Type: application/json\" -X POST http://localhost:3000/analyze\n</code></pre> <pre><code>from presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default)\n# and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text='My phone number is 212-555-5555', \n                        entities=['PHONE_NUMBER'], \n                        language='en', \n                        return_decision_process=True)\n\n# Get the decision process results for the first result\nprint(results[0].analysis_explanation)\n</code></pre>"},{"location":"analyzer/decision_process/#logging-the-decision-process","title":"Logging the decision process","text":"<p>Logging of the decision process is turned off by default. To turn it on, create the <code>AnalyzerEngine</code> object with <code>log_decision_process=True</code>.</p> <p>For example:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default)\n# and other PII recognizers\nanalyzer = AnalyzerEngine(log_decision_process=True)\n\n# Call analyzer to get results\nresults = analyzer.analyze(text='My phone number is 212-555-5555', \n                           entities=['PHONE_NUMBER'], \n                           language='en', \n                           correlation_id=\"xyz\")\n</code></pre> <p>The decision process logs will be written to standard output. Note that it is possible to define a <code>correlation-id</code> which is the trace identification. It will help you to query the stdout logs. The id can be retrieved from each API response header: <code>x-correlation-id</code>.</p> <p>By having the traces written into the <code>stdout</code> it's very easy to configure a monitoring solution to ease the process of reading processing the tracing logs in a distributed system.</p>"},{"location":"analyzer/decision_process/#examples","title":"Examples","text":"<p>For the a request with the following text:</p> <pre><code>My name is Bart Simpson, my Credit card is: 4095-2609-9393-4932,  my phone is 425 8829090 \n</code></pre> <p>The following traces will be written to log, with this format:</p> <p><code>[Date Time][decision_process][Log Level][Unique Correlation ID][Trace Message]</code></p> <pre><code>[2019-07-14 14:22:32,409][decision_process][INFO][00000000-0000-0000-0000-000000000000][nlp artifacts:{'entities': (Bart Simpson, 4095, 425), 'tokens': ['My', 'name', 'is', 'Bart', 'Simpson', ',', 'my', 'Credit', 'card', 'is', ':', '4095', '-', '2609', '-', '9393', '-', '4932', ',', ' ', 'my', 'phone', 'is', '425', '8829090'], 'lemmas': ['My', 'name', 'be', 'Bart', 'Simpson', ',', 'my', 'Credit', 'card', 'be', ':', '4095', '-', '2609', '-', '9393', '-', '4932', ',', ' ', 'my', 'phone', 'be', '425', '8829090'], 'tokens_indices': [0, 3, 8, 11, 16, 23, 25, 28, 35, 40, 42, 44, 48, 49, 53, 54, 58, 59, 63, 65, 66, 69, 75, 78, 82], 'keywords': ['bart', 'simpson', 'credit', 'card', '4095', '2609', '9393', '4932', ' ', 'phone', '425', '8829090']}]\n\n[2019-07-14 14:22:32,417][decision_process][INFO][00000000-0000-0000-0000-000000000000][[\"{'entity_type': 'CREDIT_CARD', 'start': 44, 'end': 63, 'score': 1.0, 'analysis_explanation': {'recognizer': 'CreditCardRecognizer', 'pattern_name': 'All Credit Cards (weak)', 'pattern': '\\\\\\\\b((4\\\\\\\\d{3})|(5[0-5]\\\\\\\\d{2})|(6\\\\\\\\d{3})|(1\\\\\\\\d{3})|(3\\\\\\\\d{3}))[- ]?(\\\\\\\\d{3,4})[- ]?(\\\\\\\\d{3,4})[- ]?(\\\\\\\\d{3,5})\\\\\\\\b', 'original_score': 0.3, 'score': 1.0, 'textual_explanation': None, 'score_context_improvement': 0.7, 'supportive_context_word': 'credit', 'validation_result': True}}\", \"{'entity_type': 'PERSON', 'start': 11, 'end': 23, 'score': 0.85, 'analysis_explanation': {'recognizer': 'SpacyRecognizer', 'pattern_name': None, 'pattern': None, 'original_score': 0.85, 'score': 0.85, 'textual_explanation': \\\"Identified as PERSON by Spacy's Named Entity Recognition\\\", 'score_context_improvement': 0, 'supportive_context_word': '', 'validation_result': None}}\", \"{'entity_type': 'PHONE_NUMBER', 'start': 78, 'end': 89, 'score': 0.85, 'analysis_explanation': {'recognizer': 'UsPhoneRecognizer', 'pattern_name': 'Phone (medium)', 'pattern': '\\\\\\\\b(\\\\\\\\d{3}[-\\\\\\\\.\\\\\\\\s]\\\\\\\\d{3}[-\\\\\\\\.\\\\\\\\s]??\\\\\\\\d{4})\\\\\\\\b', 'original_score': 0.5, 'score': 0.85, 'textual_explanation': None, 'score_context_improvement': 0.35, 'supportive_context_word': 'phone', 'validation_result': None}}\"]]\n</code></pre>"},{"location":"analyzer/decision_process/#writing-custom-decision-process-for-a-recognizer","title":"Writing custom decision process for a recognizer","text":"<p>When creating new PII recognizers, it is possible to add information about the recognizer's decision process. This information will be traced or returned to the user, depending on the configuration.</p> <p>For example, the spacy_recognizer.py implements a custom trace as follows:</p> <pre><code>SPACY_DEFAULT_EXPLANATION = \"Identified as {} by Spacy's Named Entity Recognition\"\n\ndef build_spacy_explanation(recognizer_name, original_score, entity):\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        textual_explanation=SPACY_DEFAULT_EXPLANATION.format(entity))\n    return explanation\n</code></pre> <p>The <code>textual_explanation</code> field in <code>AnalysisExplanation</code> class allows you to add your own custom text into the final trace which will be written.</p> <p>Note</p> <p>These traces leverage the Python <code>logging</code> mechanisms. In the default configuration, A <code>StreamHandler</code> is used to write these logs to <code>sys.stdout</code>.</p> <p>Warning</p> <p>Decision-process traces explain why PIIs were detected, but not why they were not detected!</p>"},{"location":"analyzer/developing_recognizers/","title":"Recognizers Development - Best Practices and Considerations","text":"<p>Recognizers are the main building blocks in Presidio. Each recognizer is in charge of detecting one or more entities in one or more languages. Recognizers define the logic for detection, as well as the confidence a prediction receives and a list of words to be used when context is leveraged.</p>"},{"location":"analyzer/developing_recognizers/#implementation-considerations","title":"Implementation Considerations","text":""},{"location":"analyzer/developing_recognizers/#accuracy","title":"Accuracy","text":"<p>Each recognizer, regardless of its complexity, could have false positives and false negatives. When adding new recognizers, we try to balance the effect of each recognizer on the entire system. A recognizer with many false positives would affect the system's usability, while a recognizer with many false negatives might require more work before it can be integrated. For reproducibility purposes, it is be best to note how the recognizer's accuracy was tested, and on which datasets. For tools and documentation on evaluating and analyzing recognizers, refer to the presidio-research Github repository.</p> <p>Note</p> <p>When contributing recognizers to the Presidio OSS, new predefined recognizers should be added to the supported entities list, and follow the contribution guidelines.</p>"},{"location":"analyzer/developing_recognizers/#performance","title":"Performance","text":"<p>Make sure your recognizer doesn't take too long to process text. Anything above 100ms per request with 100 tokens is probably not good enough.</p>"},{"location":"analyzer/developing_recognizers/#environment","title":"Environment","text":"<p>When adding new recognizers that have 3rd party dependencies, make sure that the new dependencies don't interfere with Presidio's dependencies. In the case of a conflict, one can create an isolated model environment (outside the main presidio-analyzer process) and implement a <code>RemoteRecognizer</code> on the presidio-analyzer side to interact with the model's endpoint.</p>"},{"location":"analyzer/developing_recognizers/#recognizer-types","title":"Recognizer Types","text":"<p>Generally speaking, there are three types of recognizers:</p>"},{"location":"analyzer/developing_recognizers/#deny-lists","title":"Deny Lists","text":"<p>A deny list is a list of words that should be removed during text analysis. For example, it can include a list of titles (<code>[\"Mr.\", \"Mrs.\", \"Ms.\", \"Dr.\"]</code> to detect a \"Title\" entity.)</p> <p>See this documentation on adding a new recognizer. The <code>PatternRecognizer</code> class has built-in support for a deny-list input.</p>"},{"location":"analyzer/developing_recognizers/#pattern-based","title":"Pattern Based","text":"<p>Pattern based recognizers use regular expressions to identify entities in text. See this documentation on adding a new recognizer via code. The <code>PatternRecognizer</code> class should be extended. See some examples here:</p> <p>Examples</p> <p>Examples of pattern based recognizers are the <code>CreditCardRecognizer</code> and <code>EmailRecognizer</code>.</p>"},{"location":"analyzer/developing_recognizers/#machine-learning-ml-based-or-rule-based","title":"Machine Learning (ML) Based or Rule-Based","text":"<p>Many PII entities are undetectable using naive approaches like deny-lists or regular expressions. In these cases, we would wish to utilize a Machine Learning model capable of identifying entities in free text, or a rule-based recognizer.</p>"},{"location":"analyzer/developing_recognizers/#ml-utilize-spacy-stanza-or-transformers","title":"ML: Utilize SpaCy, Stanza or Transformers","text":"<p>Presidio currently uses spaCy as a framework for text analysis and Named Entity Recognition (NER), and stanza and huggingface transformers as an alternative. To avoid introducing new tools, it is recommended to first try to use <code>spaCy</code>, <code>stanza</code> or <code>transformers</code> over other tools if possible. <code>spaCy</code> provides descent results compared to state-of-the-art NER models, but with much better computational performance. <code>spaCy</code>, <code>stanza</code> and <code>transformers</code> models could be trained from scratch, used in combination with pre-trained embeddings, or be fine-tuned.</p> <p>In addition to those, it is also possible to use other ML models. In that case, a new <code>EntityRecognizer</code> should be created. See an example using Flair here.</p>"},{"location":"analyzer/developing_recognizers/#apply-custom-logic","title":"Apply Custom Logic","text":"<p>In some cases, rule-based logic provides reasonable ways for detecting entities. The Presidio <code>EntityRecognizer</code> API allows you to use <code>spaCy</code> extracted features like lemmas, part of speech, dependencies and more to create your logic. When integrating such logic into Presidio, a class inheriting from the <code>EntityRecognizer</code> should be created.</p> <p>Considerations for selecting one option over another</p> <ul> <li>Accuracy.</li> <li>Ease of integration.</li> <li>Runtime considerations (For example if the new model requires a GPU).</li> <li>3rd party dependencies of the new model vs. the existing <code>presidio-analyzer</code> package.</li> </ul>"},{"location":"analyzer/languages/","title":"PII detection in different languages","text":"<p>Presidio supports PII detection in multiple languages. In its default configuration, it contains recognizers and models for English.</p> <p>To extend Presidio to detect PII in an additional language, these modules require modification:</p> <ol> <li>The <code>NlpEngine</code> containing the NLP model which performs tokenization, lemmatization, Named Entity Recognition and other NLP tasks.</li> <li>PII recognizers (different <code>EntityRecognizer</code> objects) should be adapted or created.</li> </ol> <p>Note</p> <p>While different detection mechanisms such as regular expressions are language agnostic, the context words used to increase the PII detection confidence aren't. Consider updating the list of context words for each recognizer to leverage context words in additional languages.</p>"},{"location":"analyzer/languages/#table-of-contents","title":"Table of contents","text":"<ul> <li>Configuring the NLP Engine</li> <li>Set up language specific recognizers</li> <li>Automatically install NLP models into the Docker container</li> </ul>"},{"location":"analyzer/languages/#configuring-the-nlp-engine","title":"Configuring the NLP Engine","text":"<p>Presidio's NLP engine can be adapted to support multiple languages and frameworks (such as spaCy, Stanza and transformers). Configuring the NLP engine for a new language or NLP framework is done by downloading or using a model trained on a different language, and providing a configuration. See the NLP model customization documentation for details on how to configure models for new languages.</p>"},{"location":"analyzer/languages/#set-up-language-specific-recognizers","title":"Set up language specific recognizers","text":"<p>Recognizers are language dependent either by their logic or by the context words used while scanning the surrounding of a detected entity. As these context words are used to increase score, they should be in the expected input language.</p> <p>Consider updating the context words of existing recognizers or add new recognizers to support new languages. Each recognizer can support one language. For example:</p> <p><pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.predefined_recognizers import EmailRecognizer\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nLANGUAGES_CONFIG_FILE = \"./docs/analyzer/languages-config.yml\"\n\n# Create NLP engine based on configuration file\nprovider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Setting up an English Email recognizer:\nemail_recognizer_en = EmailRecognizer(supported_language=\"en\", context=[\"email\", \"mail\"])\n\n# Setting up a Spanish Email recognizer\nemail_recognizer_es = EmailRecognizer(supported_language=\"es\", context=[\"correo\", \"electr\u00f3nico\"])\n\nregistry = RecognizerRegistry()\n\n# Add recognizers to registry\nregistry.add_recognizer(email_recognizer_en)\nregistry.add_recognizer(email_recognizer_es)\n\n# Set up analyzer with our updated recognizer registry\nanalyzer = AnalyzerEngine(\n    registry=registry,\n    supported_languages=[\"en\",\"es\"],\n    nlp_engine=nlp_engine_with_spanish)\n\nanalyzer.analyze(text=\"My name is David\", language=\"en\")\n</code></pre> Link to LANGUAGES_CONFIG_FILE=languages-config.yml</p>"},{"location":"analyzer/languages/#automatically-install-nlp-models-into-the-docker-container","title":"Automatically install NLP models into the Docker container","text":"<p>When packaging the code into a Docker container, NLP models are automatically installed. To define which models should be installed, update the conf/default.yaml file. This file is read during the <code>docker build</code> phase and the models defined in it are installed automatically.</p> <p>For <code>transformers</code> based models, the configuration can be found here.  A docker file supporting transformers models can be found here.</p>"},{"location":"analyzer/recognizer_registry_provider/","title":"Customizing recognizer registry from file","text":"<p>To load recognizers from file, use <code>RecognizerRegistryProvider</code> to instantiate the recognizer registry and then pass it through to the analyzer engine:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.recognizer_registry import RecognizerRegistryProvider\n\nrecognizer_registry_conf_file = \"./analyzer/recognizers-config.yml\"\n\nprovider = RecognizerRegistryProvider(\n                conf_file=recognizer_registry_conf_file\n            )\nregistry = provider.create_recognizer_registry()\nanalyzer = AnalyzerEngine(registry=registry)\n\nresults = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results)\n</code></pre>"},{"location":"analyzer/recognizer_registry_provider/#configuration-file-structure","title":"Configuration file structure","text":"<pre><code>global_regex_flags: 26\n\nsupported_languages: \n  - en\n\nrecognizers: \n...\n</code></pre> <p>The configuration file consists of two parts:</p> <ul> <li><code>global_regex_flags</code>: regex flags to be used in regex matching (see regex flags).</li> <li><code>supported_languages</code>: A list of supported languages that the registry will support.</li> <li><code>recognizers</code>: a list of recognizers to be loaded by the recognizer registry. This list consists of two different types of recognizers: <ul> <li>Predefined: A set of already defined recognizer classes in presidio. This includes all recognizers defined in the codebase (along with user defined recognizers) that inherit from EntityRecognizer.</li> <li>Custom: custom created pattern recognizers that are created based on the fields provided in the configuration file.</li> </ul> </li> </ul> <p>Note</p> <p>supported_languages must be identical to the same field in analyzer_engine</p>"},{"location":"analyzer/recognizer_registry_provider/#recognizer-list","title":"Recognizer list","text":"<p>The recognizer list comprises of both the predefined and custom recognizers, for example: </p> <pre><code>...\n  - name: CreditCardRecognizer\n    supported_languages:\n    - language: en\n      context: [credit, card, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    - language: es\n      context: [tarjeta, credito, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    - language: it\n    - language: pl\n    type: predefined\n\n  - name: UsBankRecognizer\n    supported_languages: \n    - en\n    type: predefined\n\n  - name: MedicalLicenseRecognizer\n    type: predefined\n\n  - name: ExampleCustomRecognizer\n    patterns:\n    - name: \"zip code (weak)\"\n      regex: \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\"\n      score: 0.01\n    - name: \"zip code (weak)\"\n      regex: \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\"\n      score: 0.01\n    supported_languages:\n    - language: en\n      context: [zip, code]\n    - language: es\n      context: [c\u00f3digo, postal]\n    supported_entity: \"ZIP\"\n    type: custom\n    enabled: true\n\n  - name: \"TitlesRecognizer\"\n    supported_language: \"en\"\n    supported_entity: \"TITLE\"\n    deny_list: [Mr., Mrs., Ms., Miss, Dr., Prof.]\n    deny_list_score: 1\n</code></pre>"},{"location":"analyzer/recognizer_registry_provider/#the-recognizer-parameters","title":"The recognizer parameters","text":"<ul> <li><code>supported_languages</code>: A list of supported languages that the analyzer will support. In case this field is missing, a recognizer will be created for each supported language provided to the <code>AnalyzerEngine</code>.    In addition to the language code, this field also contains a list of context words, which increases confidence in the detection in case it is found in the surroundings of a detected entity (as seen in the credit card example above).</li> <li><code>type</code>: this could be either predefined or custom. As this is optional, if not stated otherwise, the default type is custom.</li> <li><code>name</code>: Different per the type of the recognizer. For predefined recognizers, this is the class name as defined in presidio, while for custom recognizers, it will be set as the name of the recognizer.</li> <li><code>patterns</code>: a list of objects of type <code>Pattern</code> that contains a name, score and regex that define matching patterns.</li> <li><code>enabled</code>: enables or disables the recognizer.</li> <li><code>supported_entity</code>: the detected entity associated by the recognizer.</li> <li><code>deny_list</code>: A list of words to detect, in case the recognizer uses a predefined list of words.</li> <li><code>deny_list_score</code>: confidence score for a term identified using a deny-list.</li> </ul>"},{"location":"analyzer/nlp_engines/spacy_stanza/","title":"spaCy/Stanza NLP engine","text":"<p>Presidio can be loaded with pre-trained or custom models coming from spaCy or Stanza.</p>"},{"location":"analyzer/nlp_engines/spacy_stanza/#using-a-public-pre-trained-spacystanza-model","title":"Using a public pre-trained spaCy/Stanza model","text":""},{"location":"analyzer/nlp_engines/spacy_stanza/#download-the-pre-trained-model","title":"Download the pre-trained model","text":"<p>To replace the default model with a different public model, first download the desired spaCy/Stanza NER models.</p> <ul> <li> <p>To download a new model with spaCy:</p> <pre><code>python -m spacy download es_core_news_md\n</code></pre> <p>In this example we download the medium size model for Spanish.</p> </li> <li> <p>To download a new model with Stanza:</p> <p> <pre><code>import stanza\nstanza.download(\"en\") # where en is the language code of the model.\n</code></pre></p> </li> </ul> <p>For the available models, follow these links: spaCy, stanza.</p> <p>Tip</p> <p>For Person, Location and Organization detection, it could be useful to try out the transformers based models (e.g. <code>en_core_web_trf</code>) which uses a more modern deep-learning architecture, but is generally slower than the default <code>en_core_web_lg</code> model.</p>"},{"location":"analyzer/nlp_engines/spacy_stanza/#configure-presidio-to-use-the-pre-trained-model","title":"Configure Presidio to use the pre-trained model","text":"<p>Once created, see the NLP configuration documentation for more information.</p>"},{"location":"analyzer/nlp_engines/spacy_stanza/#how-ner-results-flow-within-presidio","title":"How NER results flow within Presidio","text":"<p>This diagram describes the flow of NER results within Presidio, and the relationship between the <code>SpacyNlpEngine</code> component and the <code>SpacyRecognizer</code> component: <pre><code>sequenceDiagram\n    AnalyzerEngine-&gt;&gt;SpacyNlpEngine: Call engine.process_text(text) &lt;br&gt;to get model results\n    SpacyNlpEngine-&gt;&gt;spaCy: Call spaCy pipeline\n    spaCy-&gt;&gt;SpacyNlpEngine: return entities and other attributes\n    Note over SpacyNlpEngine: Map entity names to Presidio's, &lt;BR&gt;update scores, &lt;BR&gt;remove unwanted entities &lt;BR&gt; based on NerModelConfiguration\n    SpacyNlpEngine-&gt;&gt;AnalyzerEngine: Pass NlpArtifacts&lt;BR&gt;(Entities, lemmas, tokens, scores etc.)\n    Note over AnalyzerEngine: Call all recognizers\n    AnalyzerEngine-&gt;&gt;SpacyRecognizer: Pass NlpArtifacts\n    Note over SpacyRecognizer: Extract PII entities out of NlpArtifacts\n    SpacyRecognizer-&gt;&gt;AnalyzerEngine: Return List[RecognizerResult]\n</code></pre></p>"},{"location":"analyzer/nlp_engines/spacy_stanza/#training-your-own-model","title":"Training your own model","text":"<p>Note</p> <p>A labeled dataset containing text and labeled PII entities is required for training a new model.</p> <p>For more information on model training and evaluation for Presidio, see the Presidio-Research Github repository.</p> <p>To train your own model, see these links on spaCy and Stanza:</p> <ul> <li>Train your own spaCy model.</li> <li>Train your own Stanza model.</li> </ul> <p>Once models are trained, they should be installed locally in the same environment as Presidio Analyzer.</p>"},{"location":"analyzer/nlp_engines/spacy_stanza/#using-a-previously-loaded-spacy-pipeline","title":"Using a previously loaded spaCy pipeline","text":"<p>If the app is already loading an existing spaCy NLP pipeline, it can be re-used to prevent presidio from loading it again by extending the relevant engine.</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import SpacyNlpEngine\nimport spacy\n\n# Create a class inheriting from SpacyNlpEngine\nclass LoadedSpacyNlpEngine(SpacyNlpEngine):\n    def __init__(self, loaded_spacy_model):\n        super().__init__()\n        self.nlp = {\"en\": loaded_spacy_model}\n\n# Load a model a-priori\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Pass the loaded model to the new LoadedSpacyNlpEngine\nloaded_nlp_engine = LoadedSpacyNlpEngine(loaded_spacy_model = nlp)\n\n# Pass the engine to the analyzer\nanalyzer = AnalyzerEngine(nlp_engine = loaded_nlp_engine)\n\n# Analyze text\nanalyzer.analyze(text=\"My name is Bob\", language=\"en\")\n</code></pre>"},{"location":"analyzer/nlp_engines/transformers/","title":"Transformers based Named Entity Recognition models","text":"<p>Presidio's <code>TransformersNlpEngine</code> consists of a spaCy pipeline which encapsulates a Huggingface Transformers model instead of the spaCy NER component:</p> <p></p> <p>Presidio leverages other types of information from spaCy such as tokens, lemmas and part-of-speech. Therefore the pipeline returns both the NER model results as well as results from other pipeline components.</p>"},{"location":"analyzer/nlp_engines/transformers/#how-ner-results-flow-within-presidio","title":"How NER results flow within Presidio","text":"<p>This diagram describes the flow of NER results within Presidio, and the relationship between the <code>TransformersNlpEngine</code> component and the <code>TransformersRecognizer</code> component:</p> <pre><code>sequenceDiagram\n    AnalyzerEngine-&gt;&gt;TransformersNlpEngine: Call engine.process_text(text) &lt;br&gt;to get model results\n    TransformersNlpEngine-&gt;&gt;spaCy: Call spaCy pipeline\n    spaCy-&gt;&gt;transformers: call NER model\n    transformers-&gt;&gt;spaCy: get entities\n    spaCy-&gt;&gt;TransformersNlpEngine: return transformers entities &lt;BR&gt;+ spaCy attributes\n    Note over TransformersNlpEngine: Map entity names to Presidio's, &lt;BR&gt;update scores, &lt;BR&gt;remove unwanted entities &lt;BR&gt; based on NerModelConfiguration\n    TransformersNlpEngine-&gt;&gt;AnalyzerEngine: Pass NlpArtifacts&lt;BR&gt;(Entities, lemmas, tokens, scores etc.)\n    Note over AnalyzerEngine: Call all recognizers\n    AnalyzerEngine-&gt;&gt;TransformersRecognizer: Pass NlpArtifacts\n    Note over TransformersRecognizer: Extract PII entities out of NlpArtifacts\n    TransformersRecognizer-&gt;&gt;AnalyzerEngine: Return List[RecognizerResult]\n</code></pre>"},{"location":"analyzer/nlp_engines/transformers/#adding-a-new-model","title":"Adding a new model","text":"<p>As the underlying transformers model, you can choose from either a public pretrained model or a custom model.</p>"},{"location":"analyzer/nlp_engines/transformers/#using-a-public-pre-trained-transformers-model","title":"Using a public pre-trained transformers model","text":""},{"location":"analyzer/nlp_engines/transformers/#downloading-a-pre-trained-model","title":"Downloading a pre-trained model","text":"<p>To download the desired NER model from HuggingFace:</p> <pre><code>import transformers\nfrom huggingface_hub import snapshot_download\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntransformers_model = &lt;PATH_TO_MODEL&gt; # e.g. \"obi/deid_roberta_i2b2\"\n\nsnapshot_download(repo_id=transformers_model)\n\n# Instantiate to make sure it's downloaded during installation and not runtime\nAutoTokenizer.from_pretrained(transformers_model)\nAutoModelForTokenClassification.from_pretrained(transformers_model)\n</code></pre> <p>Then, also download a spaCy pipeline/model:</p> <pre><code>python -m spacy download en_core_web_sm\n</code></pre>"},{"location":"analyzer/nlp_engines/transformers/#configuring-the-ner-pipeline","title":"Configuring the NER pipeline","text":"<p>Once the models are downloaded, one option to configure them is to create a YAML configuration file. Note that the configuration needs to contain both a <code>spaCy</code> pipeline name and a transformers model name. In addition, different configurations for parsing the results of the transformers model can be added.</p> <p>The NER model configuration can be done in a YAML file or in Python:</p>"},{"location":"analyzer/nlp_engines/transformers/#configuring-the-ner-pipeline-via-code","title":"Configuring the NER pipeline via code","text":"<p>Example configuration in Python:</p> <pre><code># Transformer model config\nmodel_config = [\n    {\"lang_code\": \"en\",\n     \"model_name\": {\n         \"spacy\": \"en_core_web_sm\", # for tokenization, lemmatization\n         \"transformers\": \"StanfordAIMI/stanford-deidentifier-base\" # for NER\n    }\n}]\n\n# Entity mappings between the model's and Presidio's\nmapping = dict(\n    PER=\"PERSON\",\n    LOC=\"LOCATION\",\n    ORG=\"ORGANIZATION\",\n    AGE=\"AGE\",\n    ID=\"ID\",\n    EMAIL=\"EMAIL\",\n    DATE=\"DATE_TIME\",\n    PHONE=\"PHONE_NUMBER\",\n    PERSON=\"PERSON\",\n    LOCATION=\"LOCATION\",\n    GPE=\"LOCATION\",\n    ORGANIZATION=\"ORGANIZATION\",\n    NORP=\"NRP\",\n    PATIENT=\"PERSON\",\n    STAFF=\"PERSON\",\n    HOSP=\"LOCATION\",\n    PATORG=\"ORGANIZATION\",\n    TIME=\"DATE_TIME\",\n    HCW=\"PERSON\",\n    HOSPITAL=\"LOCATION\",\n    FACILITY=\"LOCATION\",\n    VENDOR=\"ORGANIZATION\",\n)\n\nlabels_to_ignore = [\"O\"]\n\nner_model_configuration = NerModelConfiguration(\n    model_to_presidio_entity_mapping=mapping,\n    alignment_mode=\"expand\", # \"strict\", \"contract\", \"expand\"\n    aggregation_strategy=\"max\", # \"simple\", \"first\", \"average\", \"max\"\n    labels_to_ignore = labels_to_ignore)\n\ntransformers_nlp_engine = TransformersNlpEngine(\n    models=model_config,\n    ner_model_configuration=ner_model_configuration)\n\n# Transformer-based analyzer\nanalyzer = AnalyzerEngine(\n    nlp_engine=transformers_nlp_engine, \n    supported_languages=[\"en\"]\n)\n</code></pre>"},{"location":"analyzer/nlp_engines/transformers/#creating-a-yaml-configuration-file","title":"Creating a YAML configuration file","text":"<p>Once the models are downloaded, one option to configure them is to create a YAML configuration file. Note that the configuration needs to contain both a <code>spaCy</code> pipeline name and a transformers model name. In addition, different configurations for parsing the results of the transformers model can be added.</p> <p>Example configuration (in YAML):</p> <pre><code>nlp_engine_name: transformers\nmodels:\n  -\n    lang_code: en\n    model_name:\n      spacy: en_core_web_sm\n      transformers: StanfordAIMI/stanford-deidentifier-base\n\nner_model_configuration:\n  labels_to_ignore:\n  - O\n  aggregation_strategy: max # \"simple\", \"first\", \"average\", \"max\"\n  stride: 16\n  alignment_mode: expand # \"strict\", \"contract\", \"expand\"\n  model_to_presidio_entity_mapping:\n    PER: PERSON\n    LOC: LOCATION\n    ORG: ORGANIZATION\n    AGE: AGE\n    ID: ID\n    EMAIL: EMAIL\n    PATIENT: PERSON\n    STAFF: PERSON\n    HOSP: ORGANIZATION\n    PATORG: ORGANIZATION\n    DATE: DATE_TIME\n    PHONE: PHONE_NUMBER\n    HCW: PERSON\n    HOSPITAL: LOCATION\n    VENDOR: ORGANIZATION\n\n  low_confidence_score_multiplier: 0.4\n  low_score_entity_names:\n  - ID\n</code></pre>"},{"location":"analyzer/nlp_engines/transformers/#calling-the-new-model","title":"Calling the new model","text":"<p>Once the configuration file is created, it can be used to create a new <code>TransformersNlpEngine</code>:</p> <pre><code>    from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n    from presidio_analyzer.nlp_engine import NlpEngineProvider\n\n    # Create configuration containing engine name and models\n    conf_file = PATH_TO_CONF_FILE\n\n    # Create NLP engine based on configuration\n    provider = NlpEngineProvider(conf_file=conf_file)\n    nlp_engine = provider.create_engine()\n\n    # Pass the created NLP engine and supported_languages to the AnalyzerEngine\n    analyzer = AnalyzerEngine(\n        nlp_engine=nlp_engine, \n        supported_languages=[\"en\"]\n    )\n\n    results_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\n    print(results_english)\n</code></pre>"},{"location":"analyzer/nlp_engines/transformers/#explaining-the-configuration-options","title":"Explaining the configuration options","text":"<ul> <li><code>model_name.spacy</code> is a name of a spaCy model/pipeline, which would wrap the transformers NER model. For example, <code>en_core_web_sm</code>.</li> <li>The <code>model_name.transformers</code> is the full path for a huggingface model. Models can be found on HuggingFace Models Hub. For example, <code>obi/deid_roberta_i2b2</code></li> </ul> <p>The <code>ner_model_configuration</code> section contains the following parameters:</p> <ul> <li><code>labels_to_ignore</code>: A list of labels to ignore. For example, <code>O</code> (no entity) or entities you are not interested in returning.</li> <li><code>aggregation_strategy</code>: The strategy to use when aggregating the results of the transformers model.</li> <li><code>stride</code>: The value is the length of the window overlap in transformer tokenizer tokens.</li> <li><code>alignment_mode</code>: The strategy to use when aligning the results of the transformers model to the original text.</li> <li><code>model_to_presidio_entity_mapping</code>: A mapping between the transformers model labels and the Presidio entity types.</li> <li><code>low_confidence_score_multiplier</code>: A multiplier to apply to the score of entities with low confidence.</li> <li><code>low_score_entity_names</code>: A list of entity types to apply the low confidence score multiplier to.</li> </ul> <p>Defining the entity mapping</p> <p>To be able to create the <code>model_to_presidio_entity_mapping</code> dictionary, it is advised to check which classes the model is able to predict. This can be found on the huggingface hub site for the model in some cases. In other, one can check the model's <code>config.json</code> under <code>id2label</code>. For example, for <code>bert-base-NER-uncased</code>, it can be found here: https://huggingface.co/dslim/bert-base-NER-uncased/blob/main/config.json. Note that most NER models add a prefix to the class (e.g. <code>B-PER</code> for class <code>PER</code>). When creating the mapping, do not add the prefix.</p> <p>See more information on parameters on the spacy-huggingface-pipelines Github repo.</p> <p>Once created, see the NLP configuration documentation for more information.</p>"},{"location":"analyzer/nlp_engines/transformers/#training-your-own-model","title":"Training your own model","text":"<p>Note</p> <p>A labeled dataset containing text and labeled PII entities is required for training a new model.</p> <p>For more information on model training and evaluation for Presidio, see the Presidio-Research Github repository.</p> <p>To train your own model, see this tutorial: Train your own transformers model.</p>"},{"location":"analyzer/nlp_engines/transformers/#using-a-transformers-model-as-an-entityrecognizer","title":"Using a transformers model as an <code>EntityRecognizer</code>","text":"<p>In addition to the approach described in this document, one can decide to integrate a transformers model as a recognizer. We allow these two options, as a user might want to have multiple NER models running in parallel. In this case, one can create multiple <code>EntityRecognizer</code> instances, each serving a different model, instead of one model used in an <code>NlpEngine</code>. See this sample for more info on integrating a transformers model as a Presidio recognizer and not as a Presidio <code>NLPEngine</code>.</p>"},{"location":"anonymizer/","title":"Presidio Anonymizer","text":"<p>The Presidio anonymizer is a Python based module for anonymizing detected PII text entities with desired values. Presidio anonymizer supports both anonymization and deanonymization by applying different operators. Operators are built-in text manipulation classes which can be easily extended.</p> <p></p> <p>The Presidio-Anonymizer package contains both <code>Anonymizers</code> and <code>Deanonymizers</code>.</p> <ul> <li>Anonymizers are used to replace a PII entity text with some other value by applying a certain operator (e.g. replace, mask, redact, encrypt)</li> <li>Deanonymizers are used to revert the anonymization operation.   (e.g. to decrypt an encrypted text).</li> </ul>"},{"location":"anonymizer/#installation","title":"Installation","text":"<p>see Installing Presidio.</p>"},{"location":"anonymizer/#getting-started","title":"Getting started","text":"PythonAs an HTTP server <p>Simple example:</p> <pre><code>from presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n\n# Initialize the engine:\nengine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text, \n# analyzer results (potentially coming from presidio-analyzer) and\n# Operators to get the anonymization output:\nresult = engine.anonymize(\n    text=\"My name is Bond, James Bond\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=15, score=0.8),\n        RecognizerResult(entity_type=\"PERSON\", start=17, end=27, score=0.8),\n    ],\n    operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"BIP\"})},\n)\n\nprint(result)\n</code></pre> <p>This example takes the output of the <code>AnonymizerEngine</code>  containing an encrypted PII entity, and decrypts it back to the original text:</p> <pre><code>from presidio_anonymizer import DeanonymizeEngine\nfrom presidio_anonymizer.entities import OperatorResult, OperatorConfig\n\n# Initialize the engine:\nengine = DeanonymizeEngine()\n\n# Invoke the deanonymize function with the text, anonymizer results and\n# Operators to define the deanonymization type.\nresult = engine.deanonymize(\n    text=\"My name is S184CMt9Drj7QaKQ21JTrpYzghnboTF9pn/neN8JME0=\",\n    entities=[\n        OperatorResult(start=11, end=55, entity_type=\"PERSON\"),\n    ],\n    operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": \"WmZq4t7w!z%C&amp;F)J\"})},\n)\n\nprint(result)\n</code></pre> <p>You can run presidio anonymizer as an http server using either python runtime or using a docker container.</p>"},{"location":"anonymizer/#using-docker-container","title":"Using docker container","text":"<pre><code>cd presidio-anonymizer\ndocker run -p 5001:3000 presidio-anonymizer \n</code></pre>"},{"location":"anonymizer/#using-python-runtime","title":"Using python runtime","text":"<p>Note</p> <p>This requires the Presidio Github repository to be cloned.</p> <pre><code>cd presidio-anonymizer\npython app.py\n\nAnonymize:\n\ncurl -XPOST http://localhost:3000/anonymize -H \"Content-Type: application/json\" -d @payload\n\npayload example:\n{\n\"text\": \"hello world, my name is Jane Doe. My number is: 034453334\",\n\"anonymizers\": {\n    \"PHONE_NUMBER\": {\n        \"type\": \"mask\",\n        \"masking_char\": \"*\",\n        \"chars_to_mask\": 4,\n        \"from_end\": true\n    }\n},\n\"analyzer_results\": [\n    {\n        \"start\": 24,\n        \"end\": 32,\n        \"score\": 0.8,\n        \"entity_type\": \"NAME\"\n    },\n    {\n        \"start\": 24,\n        \"end\": 28,\n        \"score\": 0.8,\n        \"entity_type\": \"FIRST_NAME\"\n    },\n    {\n        \"start\": 29,\n        \"end\": 32,\n        \"score\": 0.6,\n        \"entity_type\": \"LAST_NAME\"\n    },\n    {\n        \"start\": 48,\n        \"end\": 57,\n        \"score\": 0.95,\n        \"entity_type\": \"PHONE_NUMBER\"\n    }\n]}\n\nDeanonymize:\n\ncurl -XPOST http://localhost:3000/deanonymize -H \"Content-Type: application/json\" -d @payload\n\npayload example:\n{\n\"text\": \"My name is S184CMt9Drj7QaKQ21JTrpYzghnboTF9pn/neN8JME0=\",\n\"deanonymizers\": {\n    \"PERSON\": {\n        \"type\": \"decrypt\",\n        \"key\": \"WmZq4t7w!z%C&amp;F)J\"\n    }\n},\n\"anonymizer_results\": [\n    {\n        \"start\": 11,\n        \"end\": 55,\n        \"entity_type\": \"PERSON\"\n    }\n]}\n</code></pre>"},{"location":"anonymizer/#main-concepts","title":"Main concepts","text":"<p>The following class diagram shows a simplified view of the main classes in Presidio Anonymizer:</p> <pre><code>classDiagram\n    direction LR\n\n    class RecognizerResult {\n        +entity_type: str\n        +start: int\n        +end: int\n        +score: float\n    }\n\n    class AnonymizerEngine {\n        +anonymize(text: str, analyzer_results: List[RecognizerResult], operators: Dict[str, OperatorConfig], ...) EngineResult\n        +add_anonymizer(anonymizer_cls: Type[Operator]) None\n        +remove_anonymizer(deanonymizer_cls: Type[Operator]) None\n    }\n    class DeanonymizeEngine {\n        +deanonymize(text: str, entities: List[OperatorResult], operators: Dict[str, OperatorConfig]) EngineResult\n        +get_deanonymizers() List[str]\n        +add_deanonymizer(deanonymizer_cls: Type[Operator]) None\n        +remove_deanonymizer(deanonymizer_cls: Type[Operator]) None\n    }\n    class Operator {\n        +operate(text: str, params: Dict) str\n    }\n    class OperatorConfig {\n        +operator_name: str\n        +params: Dict\n    }\n\n    class EngineResult {\n        +text: str\n        +items: List[OperatorResult]\n    }\n\n    class OperatorResult {\n        +start: int\n        +end: int\n        +entity_type: str\n        +text: str\n        +operator: str\n    }\n\n\n    RecognizerResult &lt;-- AnonymizerEngine\n    RecognizerResult &lt;-- DeanonymizeEngine\n    AnonymizerEngine o-- \"1..*\"  Operator\n    AnonymizerEngine --o OperatorConfig\n    DeanonymizeEngine o-- \"1..*\"  Operator\n    DeanonymizeEngine --o OperatorConfig\n    EngineBase --|&gt; DeanonymizeEngine\n    EngineBase --|&gt; AnonymizerEngine\n    EngineResult --o OperatorResult\n\n\n %% Defining styles\n    style Operator fill:#E6F7FF,stroke:#005BAC,stroke-width:2px\n    style AnonymizerEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style DeanonymizeEngine fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style EngineBase fill:#FFF5E6,stroke:#FFA500,stroke-width:2px\n    style OperatorConfig fill:#E6FFE6,stroke:#008000,stroke-width:2px\n    style EngineResult fill:#FFF0F5,stroke:#FF69B4,stroke-width:2px\n    style OperatorResult fill:#FFF0F5,stroke:#FF69B4,stroke-width:2px\n\nnote for RecognizerResult \"RecognizerResults \nare the output \nof the AnalyzerEngine\"\n</code></pre> <ul> <li>The AnonymizerEngine is the main class in Presidio that is responsible for anonymizing PII entities in text. It uses the results from the AnalyzerEngine to perform the anonymization.</li> <li>The DeanonymizerEngine is a class in Presidio that is responsible for deanonymizing text that has been anonymized by the AnonymizerEngine, given that the operation is reversible (e.g. encryption).</li> <li>An Operator is an object in Presidio that is responsible for performing the anonymization operation on a PII entity. Presidio provides several built-in operators, such as Replace, Redact, and Encrypt, and allows users to create custom operators.</li> <li>The BatchAnonymizerEngine is a class in Presidio that is responsible for anonymizing PII entities in a batch of texts. It uses the AnonymizerEngine to perform the anonymization on each text in the batch. (see more here).</li> </ul>"},{"location":"anonymizer/#built-in-operators","title":"Built-in operators","text":"Operator type Operator name Description Parameters Anonymize replace Replace the PII with desired value <code>new_value</code>: replaces existing text with the given value. If <code>new_value</code> is not supplied or empty, default behavior will be: &lt;entity_type&gt; e.g: &lt;PHONE_NUMBER&gt; Anonymize redact Remove the PII completely from text None Anonymize hash Hashes the PII text <code>hash_type</code>: sets the type of hashing. Can be either <code>sha256</code> or <code>sha512</code> The default hash type is <code>sha256</code>. Anonymize mask Replace the PII with a given character <code>chars_to_mask</code>: the amount of characters out of the PII that should be replaced.  <code>masking_char</code>: the character to be replaced with.  <code>from_end</code>: Whether to mask the PII from it's end. Anonymize encrypt Encrypt the PII using a given key <code>key</code>: a cryptographic key used for the encryption. Anonymize custom Replace the PII with the result of the function executed on the PII <code>lambda</code>: lambda to execute on the PII data. The lambda return type must be a string. Anonymize surrogate_ahds Generate realistic, medically-appropriate surrogates using Azure Health Data Services de-identification service surrogation <code>endpoint</code>: AHDS endpoint (optional, uses AHDS_ENDPOINT env var)<code>entities</code>: List of entities detected by analyzer<code>input_locale</code>: Input locale (default: \"en-US\")<code>surrogate_locale</code>: Surrogate locale (default: \"en-US\")Requires: <code>pip install presidio-anonymizer[ahds]</code> Anonymize keep Preserver the PII unmodified None Deanonymize decrypt Decrypt the encrypted PII in the text using the encryption key <code>key</code>: a cryptographic key used for the encryption is also used for the decryption. <p>Note</p> <p>When performing anonymization, if anonymizers map is empty or \"DEFAULT\" key is not stated, the default anonymization operator is \"replace\" for all entities. The replacing value will be the entity type e.g.: &lt;PHONE_NUMBER&gt;</p>"},{"location":"anonymizer/#handling-overlaps-between-entities","title":"Handling overlaps between entities","text":"<p>As the input text could potentially have overlapping PII entities, there are different anonymization scenarios:</p> <ul> <li>No overlap (single PII): When there is no overlap in spans of entities,     Presidio Anonymizer uses a given or default anonymization operator to anonymize     and replace the PII text entity.</li> <li>Full overlap of PII entity spans: When entities have overlapping substrings,     the PII with the higher score will be taken.     Between PIIs with identical scores, the selection is arbitrary.</li> <li>One PII is contained in another: Presidio Anonymizer will use the PII with the larger text even if it's score is lower.</li> <li> <p>Partial intersection: Presidio Anonymizer will anonymize each individually and will return a concatenation of the anonymized text.     For example:     For the text</p> <pre><code>I'm George Washington Square Park.\n</code></pre> <p>Assuming one entity is <code>George Washington</code> and the other is <code>Washington State Park</code> and assuming we're using the default anonymizer, the result would be:</p> <pre><code>I'm &lt;PERSON&gt;&lt;LOCATION&gt;.\n</code></pre> </li> </ul>"},{"location":"anonymizer/#additional-examples-for-overlapping-pii-scenarios","title":"Additional examples for overlapping PII scenarios","text":"<p>Text:</p> <pre><code>My name is Inigo Montoya. You Killed my Father. Prepare to die. BTW my number is:\n03-232323.\n</code></pre> <p>Results:</p> <ul> <li> <p>No overlaps: Assuming only <code>Inigo</code> is recognized as NAME:</p> <pre><code>My name is &lt;NAME&gt; Montoya. You Killed my Father. Prepare to die. BTW my number is:\n03-232323.\n</code></pre> </li> <li> <p>Full overlap: Assuming the number is recognized as PHONE_NUMBER with score of 0.7 and as SSN     with score of 0.6, the higher score would count:</p> <pre><code>My name is Inigo Montoya. You Killed my Father. Prepare to die. BTW my number is: &lt;\nPHONE_NUMBER&gt;.\n</code></pre> </li> <li> <p>One PII is contained is another: Assuming Inigo is recognized as FIRST_NAME and Inigo Montoya     was recognized as NAME, the larger one will be used:</p> <pre><code>My name is &lt;NAME&gt;. You Killed my Father. Prepare to die. BTW my number is: 03-232323.\n</code></pre> </li> <li> <p>Partial intersection: Assuming the number 03-2323 is recognized as a PHONE_NUMBER but 232323     is recognized as SSN:</p> <pre><code>My name is Inigo Montoya. You Killed my Father. Prepare to die. BTW my number is: &lt;\nPHONE_NUMBER&gt;&lt;SSN&gt;.\n</code></pre> </li> </ul>"},{"location":"anonymizer/#creating-a-new-operator","title":"Creating a new <code>operator</code>","text":"<p>Presidio anonymizer can be easily extended to support additional operators. See this tutorial on adding new operators for more information.</p>"},{"location":"anonymizer/#api-reference","title":"API reference","text":"<p>Follow the API Spec for the Anonymizer REST API reference details and Anonymizer Python API for Python API reference.</p>"},{"location":"anonymizer/adding_operators/","title":"Supporting new types of PII operators","text":"<p>Operators are the presidio-anonymizer actions over the text.</p> <p>There are two types of operators:</p> <ul> <li>Anonymize (e.g., hash, replace, redact, encrypt, mask)</li> <li>Deanonymize (e.g., decrypt)</li> </ul> <p>Presidio anonymizer can be easily extended to support additional anonymization and deanonymization methods (called Operators).</p>"},{"location":"anonymizer/adding_operators/#extending-presidio-anonymizer-for-additional-pii-operators","title":"Extending presidio-anonymizer for additional PII operators","text":"<ol> <li>Create new python class implementing the abstract Operator class.</li> <li>Implement the methods:<ul> <li><code>operate</code> - gets the data and returns a new text expected to replace the old one.</li> <li><code>validate</code> - validate the parameters entered for the anonymizer exists and valid.</li> <li><code>operator_name</code> - this method helps to automatically load the existing anonymizers.</li> <li><code>operator_type</code> - either Anonymize or Deanonymize. Will be mapped to the proper engine.</li> </ul> </li> <li>Call the <code>AnonymizerEngine.add_anonymizer</code> method to add a new  operator to the anonymizer. Alternatively, call the <code>DeanonymizeEngine.add_deanonymizer</code> method to add a new deanonymizer.</li> </ol> <p>See a detailed example here.</p>"},{"location":"api/analyzer_python/","title":"Presidio Analyzer API Reference","text":""},{"location":"api/analyzer_python/#objects-at-the-top-of-the-presidio-analyzer-package","title":"Objects at the top of the presidio-analyzer package","text":""},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine","title":"presidio_analyzer.AnalyzerEngine","text":"<p>Entry point for Presidio Analyzer.</p> <p>Orchestrating the detection of PII entities and all related logic.</p> PARAMETER DESCRIPTION <code>registry</code> <p>instance of type RecognizerRegistry</p> <p> TYPE: <code>RecognizerRegistry</code> DEFAULT: <code>None</code> </p> <code>nlp_engine</code> <p>instance of type NlpEngine (for example SpacyNlpEngine)</p> <p> TYPE: <code>NlpEngine</code> DEFAULT: <code>None</code> </p> <code>app_tracer</code> <p>instance of type AppTracer, used to trace the logic used during each request for interpretability reasons.</p> <p> TYPE: <code>AppTracer</code> DEFAULT: <code>None</code> </p> <code>log_decision_process</code> <p>bool, defines whether the decision process within the analyzer should be logged or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_score_threshold</code> <p>Minimum confidence value for detected entities to be returned</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>supported_languages</code> <p>List of possible languages this engine could be run on. Used for loading the right NLP models and recognizers for these languages.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>context_aware_enhancer</code> <p>instance of type ContextAwareEnhancer for enhancing confidence score based on context words, (LemmaContextAwareEnhancer will be created by default if None passed)</p> <p> TYPE: <code>Optional[ContextAwareEnhancer]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>get_recognizers</code> <p>Return a list of PII recognizers currently loaded.</p> <code>get_supported_entities</code> <p>Return a list of the entities that can be detected.</p> <code>analyze</code> <p>Find PII entities in text using different PII recognizers for a given language.</p> Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>class AnalyzerEngine:\n    \"\"\"\n    Entry point for Presidio Analyzer.\n\n    Orchestrating the detection of PII entities and all related logic.\n\n    :param registry: instance of type RecognizerRegistry\n    :param nlp_engine: instance of type NlpEngine\n    (for example SpacyNlpEngine)\n    :param app_tracer: instance of type AppTracer, used to trace the logic\n    used during each request for interpretability reasons.\n    :param log_decision_process: bool,\n    defines whether the decision process within the analyzer should be logged or not.\n    :param default_score_threshold: Minimum confidence value\n    for detected entities to be returned\n    :param supported_languages: List of possible languages this engine could be run on.\n    Used for loading the right NLP models and recognizers for these languages.\n    :param context_aware_enhancer: instance of type ContextAwareEnhancer for enhancing\n    confidence score based on context words, (LemmaContextAwareEnhancer will be created\n    by default if None passed)\n    \"\"\"\n\n    def __init__(\n        self,\n        registry: RecognizerRegistry = None,\n        nlp_engine: NlpEngine = None,\n        app_tracer: AppTracer = None,\n        log_decision_process: bool = False,\n        default_score_threshold: float = 0,\n        supported_languages: List[str] = None,\n        context_aware_enhancer: Optional[ContextAwareEnhancer] = None,\n    ):\n        if not supported_languages:\n            supported_languages = [\"en\"]\n\n        if not nlp_engine:\n            logger.info(\"nlp_engine not provided, creating default.\")\n            provider = NlpEngineProvider()\n            nlp_engine = provider.create_engine()\n\n        if not app_tracer:\n            app_tracer = AppTracer()\n        self.app_tracer = app_tracer\n\n        self.supported_languages = supported_languages\n\n        self.nlp_engine = nlp_engine\n        if not self.nlp_engine.is_loaded():\n            self.nlp_engine.load()\n\n        if not registry:\n            logger.info(\"registry not provided, creating default.\")\n            provider = RecognizerRegistryProvider(\n                registry_configuration={\"supported_languages\": self.supported_languages}\n            )\n            registry = provider.create_recognizer_registry()\n            registry.add_nlp_recognizer(nlp_engine=self.nlp_engine)\n        else:\n            if Counter(registry.supported_languages) != Counter(\n                self.supported_languages\n            ):\n                raise ValueError(\n                    f\"Misconfigured engine, supported languages have to be consistent\"\n                    f\"registry.supported_languages: {registry.supported_languages}, \"\n                    f\"analyzer_engine.supported_languages: {self.supported_languages}\"\n                )\n\n        # added to support the previous interface\n        if not registry.recognizers:\n            registry.load_predefined_recognizers(\n                nlp_engine=self.nlp_engine, languages=self.supported_languages\n            )\n\n        self.registry = registry\n\n        self.log_decision_process = log_decision_process\n        self.default_score_threshold = default_score_threshold\n\n        if not context_aware_enhancer:\n            logger.debug(\n                \"context aware enhancer not provided, creating default\"\n                + \" lemma based enhancer.\"\n            )\n            context_aware_enhancer = LemmaContextAwareEnhancer()\n\n        self.context_aware_enhancer = context_aware_enhancer\n\n    def get_recognizers(self, language: Optional[str] = None) -&gt; List[EntityRecognizer]:\n        \"\"\"\n        Return a list of PII recognizers currently loaded.\n\n        :param language: Return the recognizers supporting a given language.\n        :return: List of [Recognizer] as a RecognizersAllResponse\n        \"\"\"\n        if not language:\n            languages = self.supported_languages\n        else:\n            languages = [language]\n\n        recognizers = []\n        for language in languages:\n            logger.info(f\"Fetching all recognizers for language {language}\")\n            recognizers.extend(\n                self.registry.get_recognizers(language=language, all_fields=True)\n            )\n\n        return list(set(recognizers))\n\n    def get_supported_entities(self, language: Optional[str] = None) -&gt; List[str]:\n        \"\"\"\n        Return a list of the entities that can be detected.\n\n        :param language: Return only entities supported in a specific language.\n        :return: List of entity names\n        \"\"\"\n        recognizers = self.get_recognizers(language=language)\n        supported_entities = []\n        for recognizer in recognizers:\n            supported_entities.extend(recognizer.get_supported_entities())\n\n        return list(set(supported_entities))\n\n    def analyze(\n        self,\n        text: str,\n        language: str,\n        entities: Optional[List[str]] = None,\n        correlation_id: Optional[str] = None,\n        score_threshold: Optional[float] = None,\n        return_decision_process: Optional[bool] = False,\n        ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n        context: Optional[List[str]] = None,\n        allow_list: Optional[List[str]] = None,\n        allow_list_match: Optional[str] = \"exact\",\n        regex_flags: Optional[int] = re.DOTALL | re.MULTILINE | re.IGNORECASE,\n        nlp_artifacts: Optional[NlpArtifacts] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Find PII entities in text using different PII recognizers for a given language.\n\n        :param text: the text to analyze\n        :param language: the language of the text\n        :param entities: List of PII entities that should be looked for in the text.\n        If entities=None then all entities are looked for.\n        :param correlation_id: cross call ID for this request\n        :param score_threshold: A minimum value for which\n        to return an identified entity\n        :param return_decision_process: Whether the analysis decision process steps\n        returned in the response.\n        :param ad_hoc_recognizers: List of recognizers which will be used only\n        for this specific request.\n        :param context: List of context words to enhance confidence score if matched\n        with the recognized entity's recognizer context\n        :param allow_list: List of words that the user defines as being allowed to keep\n        in the text\n        :param allow_list_match: How the allow_list should be interpreted; either as \"exact\" or as \"regex\".\n        - If `regex`, results which match with any regex condition in the allow_list would be allowed and not be returned as potential PII.\n        - if `exact`, results which exactly match any value in the allow_list would be allowed and not be returned as potential PII.\n        :param regex_flags: regex flags to be used for when allow_list_match is \"regex\"\n        :param nlp_artifacts: precomputed NlpArtifacts\n        :return: an array of the found entities in the text\n\n        :Example:\n\n        ```python\n        from presidio_analyzer import AnalyzerEngine\n\n        # Set up the engine, loads the NLP module (spaCy model by default)\n        # and other PII recognizers\n        analyzer = AnalyzerEngine()\n\n        # Call analyzer to get results\n        results = analyzer.analyze(text='My phone number is 212-555-5555', entities=['PHONE_NUMBER'], language='en')\n        print(results)\n        ```\n\n        \"\"\"  # noqa: E501\n\n        all_fields = not entities\n\n        recognizers = self.registry.get_recognizers(\n            language=language,\n            entities=entities,\n            all_fields=all_fields,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n        )\n\n        if all_fields:\n            # Since all_fields=True, list all entities by iterating\n            # over all recognizers\n            entities = self.get_supported_entities(language=language)\n\n        # run the nlp pipeline over the given text, store the results in\n        # a NlpArtifacts instance\n        if not nlp_artifacts:\n            nlp_artifacts = self.nlp_engine.process_text(text, language)\n\n        if self.log_decision_process:\n            self.app_tracer.trace(\n                correlation_id, \"nlp artifacts:\" + nlp_artifacts.to_json()\n            )\n\n        results = []\n        for recognizer in recognizers:\n            # Lazy loading of the relevant recognizers\n            if not recognizer.is_loaded:\n                recognizer.load()\n                recognizer.is_loaded = True\n\n            # analyze using the current recognizer and append the results\n            current_results = recognizer.analyze(\n                text=text, entities=entities, nlp_artifacts=nlp_artifacts\n            )\n            if current_results:\n                # add recognizer name to recognition metadata inside results\n                # if not exists\n                self.__add_recognizer_id_if_not_exists(current_results, recognizer)\n                results.extend(current_results)\n\n        results = self._enhance_using_context(\n            text, results, nlp_artifacts, recognizers, context\n        )\n\n        if self.log_decision_process:\n            self.app_tracer.trace(\n                correlation_id,\n                json.dumps([str(result.to_dict()) for result in results]),\n            )\n\n        # Remove duplicates or low score results\n        results = EntityRecognizer.remove_duplicates(results)\n        results = self.__remove_low_scores(results, score_threshold)\n\n        if allow_list:\n            results = self._remove_allow_list(\n                results, allow_list, text, regex_flags, allow_list_match\n            )\n\n        if not return_decision_process:\n            results = self.__remove_decision_process(results)\n\n        return results\n\n    def _enhance_using_context(\n        self,\n        text: str,\n        raw_results: List[RecognizerResult],\n        nlp_artifacts: NlpArtifacts,\n        recognizers: List[EntityRecognizer],\n        context: Optional[List[str]] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Enhance confidence score using context words.\n\n        :param text: The actual text that was analyzed\n        :param raw_results: Recognizer results which didn't take\n                            context into consideration\n        :param nlp_artifacts: The nlp artifacts contains elements\n                              such as lemmatized tokens for better\n                              accuracy of the context enhancement process\n        :param recognizers: the list of recognizers\n        :param context: list of context words\n        \"\"\"\n        results = []\n\n        for recognizer in recognizers:\n            recognizer_results = [\n                r\n                for r in raw_results\n                if r.recognition_metadata[RecognizerResult.RECOGNIZER_IDENTIFIER_KEY]\n                == recognizer.id\n            ]\n            other_recognizer_results = [\n                r\n                for r in raw_results\n                if r.recognition_metadata[RecognizerResult.RECOGNIZER_IDENTIFIER_KEY]\n                != recognizer.id\n            ]\n\n            # enhance score using context in recognizer level if implemented\n            recognizer_results = recognizer.enhance_using_context(\n                text=text,\n                # each recognizer will get access to all recognizer results\n                # to allow related entities contex enhancement\n                raw_recognizer_results=recognizer_results,\n                other_raw_recognizer_results=other_recognizer_results,\n                nlp_artifacts=nlp_artifacts,\n                context=context,\n            )\n\n            results.extend(recognizer_results)\n\n        # Update results in case surrounding words or external context are relevant to\n        # the context words.\n        results = self.context_aware_enhancer.enhance_using_context(\n            text=text,\n            raw_results=results,\n            nlp_artifacts=nlp_artifacts,\n            recognizers=recognizers,\n            context=context,\n        )\n\n        return results\n\n    def __remove_low_scores(\n        self, results: List[RecognizerResult], score_threshold: float = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Remove results for which the confidence is lower than the threshold.\n\n        :param results: List of RecognizerResult\n        :param score_threshold: float value for minimum possible confidence\n        :return: List[RecognizerResult]\n        \"\"\"\n        if score_threshold is None:\n            score_threshold = self.default_score_threshold\n\n        new_results = [result for result in results if result.score &gt;= score_threshold]\n        return new_results\n\n    @staticmethod\n    def _remove_allow_list(\n        results: List[RecognizerResult],\n        allow_list: List[str],\n        text: str,\n        regex_flags: Optional[int],\n        allow_list_match: str,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Remove results which are part of the allow list.\n\n        :param results: List of RecognizerResult\n        :param allow_list: list of allowed terms\n        :param text: the text to analyze\n        :param regex_flags: regex flags to be used for when allow_list_match is \"regex\"\n        :param allow_list_match: How the allow_list\n        should be interpreted; either as \"exact\" or as \"regex\"\n        :return: List[RecognizerResult]\n        \"\"\"\n        new_results = []\n        if allow_list_match == \"regex\":\n            pattern = \"|\".join(allow_list)\n            re_compiled = re.compile(pattern, flags=regex_flags)\n\n            for result in results:\n                word = text[result.start : result.end]\n\n                # if the word is not specified to be allowed, keep in the PII entities\n                if not re_compiled.search(word):\n                    new_results.append(result)\n        elif allow_list_match == \"exact\":\n            for result in results:\n                word = text[result.start : result.end]\n\n                # if the word is not specified to be allowed, keep in the PII entities\n                if word not in allow_list:\n                    new_results.append(result)\n        else:\n            raise ValueError(\n                \"allow_list_match must either be set to 'exact' or 'regex'.\"\n            )\n\n        return new_results\n\n    @staticmethod\n    def __add_recognizer_id_if_not_exists(\n        results: List[RecognizerResult], recognizer: EntityRecognizer\n    ) -&gt; None:\n        \"\"\"Ensure recognition metadata with recognizer id existence.\n\n        Ensure recognizer result list contains recognizer id inside recognition\n        metadata dictionary, and if not create it. recognizer_id is needed\n        for context aware enhancement.\n\n        :param results: List of RecognizerResult\n        :param recognizer: Entity recognizer\n        \"\"\"\n        for result in results:\n            if not result.recognition_metadata:\n                result.recognition_metadata = dict()\n            if (\n                RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n                not in result.recognition_metadata\n            ):\n                result.recognition_metadata[\n                    RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n                ] = recognizer.id\n            if RecognizerResult.RECOGNIZER_NAME_KEY not in result.recognition_metadata:\n                result.recognition_metadata[RecognizerResult.RECOGNIZER_NAME_KEY] = (\n                    recognizer.name\n                )\n\n    @staticmethod\n    def __remove_decision_process(\n        results: List[RecognizerResult],\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Remove decision process / analysis explanation from response.\"\"\"\n\n        for result in results:\n            result.analysis_explanation = None\n\n        return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.get_recognizers","title":"get_recognizers","text":"<pre><code>get_recognizers(language: Optional[str] = None) -&gt; List[EntityRecognizer]\n</code></pre> <p>Return a list of PII recognizers currently loaded.</p> PARAMETER DESCRIPTION <code>language</code> <p>Return the recognizers supporting a given language.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[EntityRecognizer]</code> <p>List of [Recognizer] as a RecognizersAllResponse</p> Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>def get_recognizers(self, language: Optional[str] = None) -&gt; List[EntityRecognizer]:\n    \"\"\"\n    Return a list of PII recognizers currently loaded.\n\n    :param language: Return the recognizers supporting a given language.\n    :return: List of [Recognizer] as a RecognizersAllResponse\n    \"\"\"\n    if not language:\n        languages = self.supported_languages\n    else:\n        languages = [language]\n\n    recognizers = []\n    for language in languages:\n        logger.info(f\"Fetching all recognizers for language {language}\")\n        recognizers.extend(\n            self.registry.get_recognizers(language=language, all_fields=True)\n        )\n\n    return list(set(recognizers))\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities(language: Optional[str] = None) -&gt; List[str]\n</code></pre> <p>Return a list of the entities that can be detected.</p> PARAMETER DESCRIPTION <code>language</code> <p>Return only entities supported in a specific language.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of entity names</p> Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>def get_supported_entities(self, language: Optional[str] = None) -&gt; List[str]:\n    \"\"\"\n    Return a list of the entities that can be detected.\n\n    :param language: Return only entities supported in a specific language.\n    :return: List of entity names\n    \"\"\"\n    recognizers = self.get_recognizers(language=language)\n    supported_entities = []\n    for recognizer in recognizers:\n        supported_entities.extend(recognizer.get_supported_entities())\n\n    return list(set(supported_entities))\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    language: str,\n    entities: Optional[List[str]] = None,\n    correlation_id: Optional[str] = None,\n    score_threshold: Optional[float] = None,\n    return_decision_process: Optional[bool] = False,\n    ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n    context: Optional[List[str]] = None,\n    allow_list: Optional[List[str]] = None,\n    allow_list_match: Optional[str] = \"exact\",\n    regex_flags: Optional[int] = re.DOTALL | re.MULTILINE | re.IGNORECASE,\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Find PII entities in text using different PII recognizers for a given language.</p> <p>:Example:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default)\n# and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text='My phone number is 212-555-5555', entities=['PHONE_NUMBER'], language='en')\nprint(results)\n</code></pre> PARAMETER DESCRIPTION <code>text</code> <p>the text to analyze</p> <p> TYPE: <code>str</code> </p> <code>language</code> <p>the language of the text</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>List of PII entities that should be looked for in the text. If entities=None then all entities are looked for.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>correlation_id</code> <p>cross call ID for this request</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>score_threshold</code> <p>A minimum value for which to return an identified entity</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>return_decision_process</code> <p>Whether the analysis decision process steps returned in the response.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>ad_hoc_recognizers</code> <p>List of recognizers which will be used only for this specific request.</p> <p> TYPE: <code>Optional[List[EntityRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to enhance confidence score if matched with the recognized entity's recognizer context</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>allow_list</code> <p>List of words that the user defines as being allowed to keep in the text</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>allow_list_match</code> <p>How the allow_list should be interpreted; either as \"exact\" or as \"regex\". - If <code>regex</code>, results which match with any regex condition in the allow_list would be allowed and not be returned as potential PII. - if <code>exact</code>, results which exactly match any value in the allow_list would be allowed and not be returned as potential PII.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'exact'</code> </p> <code>regex_flags</code> <p>regex flags to be used for when allow_list_match is \"regex\"</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>DOTALL | MULTILINE | IGNORECASE</code> </p> <code>nlp_artifacts</code> <p>precomputed NlpArtifacts</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>an array of the found entities in the text</p> Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    language: str,\n    entities: Optional[List[str]] = None,\n    correlation_id: Optional[str] = None,\n    score_threshold: Optional[float] = None,\n    return_decision_process: Optional[bool] = False,\n    ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n    context: Optional[List[str]] = None,\n    allow_list: Optional[List[str]] = None,\n    allow_list_match: Optional[str] = \"exact\",\n    regex_flags: Optional[int] = re.DOTALL | re.MULTILINE | re.IGNORECASE,\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Find PII entities in text using different PII recognizers for a given language.\n\n    :param text: the text to analyze\n    :param language: the language of the text\n    :param entities: List of PII entities that should be looked for in the text.\n    If entities=None then all entities are looked for.\n    :param correlation_id: cross call ID for this request\n    :param score_threshold: A minimum value for which\n    to return an identified entity\n    :param return_decision_process: Whether the analysis decision process steps\n    returned in the response.\n    :param ad_hoc_recognizers: List of recognizers which will be used only\n    for this specific request.\n    :param context: List of context words to enhance confidence score if matched\n    with the recognized entity's recognizer context\n    :param allow_list: List of words that the user defines as being allowed to keep\n    in the text\n    :param allow_list_match: How the allow_list should be interpreted; either as \"exact\" or as \"regex\".\n    - If `regex`, results which match with any regex condition in the allow_list would be allowed and not be returned as potential PII.\n    - if `exact`, results which exactly match any value in the allow_list would be allowed and not be returned as potential PII.\n    :param regex_flags: regex flags to be used for when allow_list_match is \"regex\"\n    :param nlp_artifacts: precomputed NlpArtifacts\n    :return: an array of the found entities in the text\n\n    :Example:\n\n    ```python\n    from presidio_analyzer import AnalyzerEngine\n\n    # Set up the engine, loads the NLP module (spaCy model by default)\n    # and other PII recognizers\n    analyzer = AnalyzerEngine()\n\n    # Call analyzer to get results\n    results = analyzer.analyze(text='My phone number is 212-555-5555', entities=['PHONE_NUMBER'], language='en')\n    print(results)\n    ```\n\n    \"\"\"  # noqa: E501\n\n    all_fields = not entities\n\n    recognizers = self.registry.get_recognizers(\n        language=language,\n        entities=entities,\n        all_fields=all_fields,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n    )\n\n    if all_fields:\n        # Since all_fields=True, list all entities by iterating\n        # over all recognizers\n        entities = self.get_supported_entities(language=language)\n\n    # run the nlp pipeline over the given text, store the results in\n    # a NlpArtifacts instance\n    if not nlp_artifacts:\n        nlp_artifacts = self.nlp_engine.process_text(text, language)\n\n    if self.log_decision_process:\n        self.app_tracer.trace(\n            correlation_id, \"nlp artifacts:\" + nlp_artifacts.to_json()\n        )\n\n    results = []\n    for recognizer in recognizers:\n        # Lazy loading of the relevant recognizers\n        if not recognizer.is_loaded:\n            recognizer.load()\n            recognizer.is_loaded = True\n\n        # analyze using the current recognizer and append the results\n        current_results = recognizer.analyze(\n            text=text, entities=entities, nlp_artifacts=nlp_artifacts\n        )\n        if current_results:\n            # add recognizer name to recognition metadata inside results\n            # if not exists\n            self.__add_recognizer_id_if_not_exists(current_results, recognizer)\n            results.extend(current_results)\n\n    results = self._enhance_using_context(\n        text, results, nlp_artifacts, recognizers, context\n    )\n\n    if self.log_decision_process:\n        self.app_tracer.trace(\n            correlation_id,\n            json.dumps([str(result.to_dict()) for result in results]),\n        )\n\n    # Remove duplicates or low score results\n    results = EntityRecognizer.remove_duplicates(results)\n    results = self.__remove_low_scores(results, score_threshold)\n\n    if allow_list:\n        results = self._remove_allow_list(\n            results, allow_list, text, regex_flags, allow_list_match\n        )\n\n    if not return_decision_process:\n        results = self.__remove_decision_process(results)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.analyzer_engine_provider.AnalyzerEngineProvider","title":"presidio_analyzer.analyzer_engine_provider.AnalyzerEngineProvider","text":"<p>Utility function for loading Presidio Analyzer.</p> <p>Use this class to load presidio analyzer engine from a yaml file</p> PARAMETER DESCRIPTION <code>analyzer_engine_conf_file</code> <p>the path to the analyzer configuration file</p> <p> TYPE: <code>Optional[Union[Path, str]]</code> DEFAULT: <code>None</code> </p> <code>nlp_engine_conf_file</code> <p>the path to the nlp engine configuration file</p> <p> TYPE: <code>Optional[Union[Path, str]]</code> DEFAULT: <code>None</code> </p> <code>recognizer_registry_conf_file</code> <p>the path to the recognizer registry configuration file</p> <p> TYPE: <code>Optional[Union[Path, str]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>get_configuration</code> <p>Retrieve the analyzer engine configuration from the provided file.</p> <code>create_engine</code> <p>Load Presidio Analyzer from yaml configuration file.</p> Source code in <code>presidio_analyzer/analyzer_engine_provider.py</code> <pre><code>class AnalyzerEngineProvider:\n    \"\"\"\n    Utility function for loading Presidio Analyzer.\n\n    Use this class to load presidio analyzer engine from a yaml file\n\n    :param analyzer_engine_conf_file: the path to the analyzer configuration file\n    :param nlp_engine_conf_file: the path to the nlp engine configuration file\n    :param recognizer_registry_conf_file: the path to the recognizer\n    registry configuration file\n    \"\"\"\n\n    def __init__(\n        self,\n        analyzer_engine_conf_file: Optional[Union[Path, str]] = None,\n        nlp_engine_conf_file: Optional[Union[Path, str]] = None,\n        recognizer_registry_conf_file: Optional[Union[Path, str]] = None,\n    ):\n        self.configuration = self.get_configuration(conf_file=analyzer_engine_conf_file)\n        self.nlp_engine_conf_file = nlp_engine_conf_file\n        self.recognizer_registry_conf_file = recognizer_registry_conf_file\n\n    def get_configuration(\n        self, conf_file: Optional[Union[Path, str]]\n    ) -&gt; Union[Dict[str, Any]]:\n        \"\"\"Retrieve the analyzer engine configuration from the provided file.\"\"\"\n\n        if not conf_file:\n            default_conf_file = self._get_full_conf_path()\n            with open(default_conf_file) as file:\n                configuration = yaml.safe_load(file)\n            logger.info(\n                f\"Analyzer Engine configuration file \"\n                f\"not provided. Using {default_conf_file}.\"\n            )\n        else:\n            try:\n                logger.info(f\"Reading analyzer configuration from {conf_file}\")\n                with open(conf_file) as file:\n                    configuration = yaml.safe_load(file)\n            except OSError:\n                logger.warning(\n                    f\"configuration file {conf_file} not found.  \"\n                    f\"Using default config.\"\n                )\n                with open(self._get_full_conf_path()) as file:\n                    configuration = yaml.safe_load(file)\n            except Exception:\n                print(f\"Failed to parse file {conf_file}, resorting to default\")\n                with open(self._get_full_conf_path()) as file:\n                    configuration = yaml.safe_load(file)\n\n        return configuration\n\n    def create_engine(self) -&gt; AnalyzerEngine:\n        \"\"\"\n        Load Presidio Analyzer from yaml configuration file.\n\n        :return: analyzer engine initialized with yaml configuration\n        \"\"\"\n\n        nlp_engine = self._load_nlp_engine()\n        supported_languages = self.configuration.get(\"supported_languages\", [\"en\"])\n        default_score_threshold = self.configuration.get(\"default_score_threshold\", 0)\n\n        registry = self._load_recognizer_registry(\n            supported_languages=supported_languages, nlp_engine=nlp_engine\n        )\n\n        analyzer = AnalyzerEngine(\n            nlp_engine=nlp_engine,\n            registry=registry,\n            supported_languages=supported_languages,\n            default_score_threshold=default_score_threshold,\n        )\n\n        return analyzer\n\n    def _load_recognizer_registry(\n        self,\n        supported_languages: List[str],\n        nlp_engine: NlpEngine,\n    ) -&gt; RecognizerRegistry:\n        if self.recognizer_registry_conf_file:\n            logger.info(\n                f\"Reading recognizer registry \"\n                f\"configuration from {self.recognizer_registry_conf_file}\"\n            )\n            provider = RecognizerRegistryProvider(\n                conf_file=self.recognizer_registry_conf_file, nlp_engine=nlp_engine\n            )\n        elif \"recognizer_registry\" in self.configuration:\n            registry_configuration = self.configuration[\"recognizer_registry\"]\n            provider = RecognizerRegistryProvider(\n                registry_configuration={\n                    **registry_configuration,\n                    \"supported_languages\": supported_languages,\n                },\n                nlp_engine=nlp_engine,\n            )\n        else:\n            logger.warning(\n                \"configuration file is missing for 'recognizer_registry'. \"\n                \"Using default configuration for recognizer registry\"\n            )\n            registry_configuration = self.configuration.get(\"recognizer_registry\", {})\n            provider = RecognizerRegistryProvider(\n                registry_configuration={\n                    **registry_configuration,\n                    \"supported_languages\": supported_languages,\n                },\n                nlp_engine=nlp_engine,\n            )\n        registry = provider.create_recognizer_registry()\n\n        return registry\n\n    def _load_nlp_engine(self) -&gt; NlpEngine:\n        if self.nlp_engine_conf_file:\n            logger.info(f\"Reading nlp configuration from {self.nlp_engine_conf_file}\")\n            provider = NlpEngineProvider(conf_file=self.nlp_engine_conf_file)\n        elif \"nlp_configuration\" in self.configuration:\n            nlp_configuration = self.configuration[\"nlp_configuration\"]\n            provider = NlpEngineProvider(nlp_configuration=nlp_configuration)\n        else:\n            logger.warning(\n                \"configuration file is missing for 'nlp_configuration'.\"\n                \"Using default configuration for nlp engine\"\n            )\n            provider = NlpEngineProvider()\n\n        return provider.create_engine()\n\n    @staticmethod\n    def _get_full_conf_path(\n        default_conf_file: Union[Path, str] = \"default_analyzer.yaml\",\n    ) -&gt; Path:\n        \"\"\"Return a Path to the default conf file.\"\"\"\n        return Path(Path(__file__).parent, \"conf\", default_conf_file)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.analyzer_engine_provider.AnalyzerEngineProvider.get_configuration","title":"get_configuration","text":"<pre><code>get_configuration(\n    conf_file: Optional[Union[Path, str]],\n) -&gt; Union[Dict[str, Any]]\n</code></pre> <p>Retrieve the analyzer engine configuration from the provided file.</p> Source code in <code>presidio_analyzer/analyzer_engine_provider.py</code> <pre><code>def get_configuration(\n    self, conf_file: Optional[Union[Path, str]]\n) -&gt; Union[Dict[str, Any]]:\n    \"\"\"Retrieve the analyzer engine configuration from the provided file.\"\"\"\n\n    if not conf_file:\n        default_conf_file = self._get_full_conf_path()\n        with open(default_conf_file) as file:\n            configuration = yaml.safe_load(file)\n        logger.info(\n            f\"Analyzer Engine configuration file \"\n            f\"not provided. Using {default_conf_file}.\"\n        )\n    else:\n        try:\n            logger.info(f\"Reading analyzer configuration from {conf_file}\")\n            with open(conf_file) as file:\n                configuration = yaml.safe_load(file)\n        except OSError:\n            logger.warning(\n                f\"configuration file {conf_file} not found.  \"\n                f\"Using default config.\"\n            )\n            with open(self._get_full_conf_path()) as file:\n                configuration = yaml.safe_load(file)\n        except Exception:\n            print(f\"Failed to parse file {conf_file}, resorting to default\")\n            with open(self._get_full_conf_path()) as file:\n                configuration = yaml.safe_load(file)\n\n    return configuration\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.analyzer_engine_provider.AnalyzerEngineProvider.create_engine","title":"create_engine","text":"<pre><code>create_engine() -&gt; AnalyzerEngine\n</code></pre> <p>Load Presidio Analyzer from yaml configuration file.</p> RETURNS DESCRIPTION <code>AnalyzerEngine</code> <p>analyzer engine initialized with yaml configuration</p> Source code in <code>presidio_analyzer/analyzer_engine_provider.py</code> <pre><code>def create_engine(self) -&gt; AnalyzerEngine:\n    \"\"\"\n    Load Presidio Analyzer from yaml configuration file.\n\n    :return: analyzer engine initialized with yaml configuration\n    \"\"\"\n\n    nlp_engine = self._load_nlp_engine()\n    supported_languages = self.configuration.get(\"supported_languages\", [\"en\"])\n    default_score_threshold = self.configuration.get(\"default_score_threshold\", 0)\n\n    registry = self._load_recognizer_registry(\n        supported_languages=supported_languages, nlp_engine=nlp_engine\n    )\n\n    analyzer = AnalyzerEngine(\n        nlp_engine=nlp_engine,\n        registry=registry,\n        supported_languages=supported_languages,\n        default_score_threshold=default_score_threshold,\n    )\n\n    return analyzer\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.analysis_explanation.AnalysisExplanation","title":"presidio_analyzer.analysis_explanation.AnalysisExplanation","text":"<p>Hold tracing information to explain why PII entities were identified as such.</p> PARAMETER DESCRIPTION <code>recognizer</code> <p>name of recognizer that made the decision</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>recognizer's confidence in result</p> <p> TYPE: <code>float</code> </p> <code>pattern_name</code> <p>name of pattern (if decision was made by a PatternRecognizer)</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>pattern</code> <p>regex pattern that was applied (if PatternRecognizer)</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>validation_result</code> <p>result of a validation (e.g. checksum)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>textual_explanation</code> <p>Free text for describing a decision of a logic or model</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>set_improved_score</code> <p>Update the score and calculate the difference from the original score.</p> <code>set_supportive_context_word</code> <p>Set the context word which helped increase the score.</p> <code>append_textual_explanation_line</code> <p>Append a new line to textual_explanation field.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>class AnalysisExplanation:\n    \"\"\"\n    Hold tracing information to explain why PII entities were identified as such.\n\n    :param recognizer: name of recognizer that made the decision\n    :param original_score: recognizer's confidence in result\n    :param pattern_name: name of pattern\n            (if decision was made by a PatternRecognizer)\n    :param pattern: regex pattern that was applied (if PatternRecognizer)\n    :param validation_result: result of a validation (e.g. checksum)\n    :param textual_explanation: Free text for describing\n            a decision of a logic or model\n    \"\"\"\n\n    def __init__(\n        self,\n        recognizer: str,\n        original_score: float,\n        pattern_name: str = None,\n        pattern: str = None,\n        validation_result: float = None,\n        textual_explanation: str = None,\n        regex_flags: int = None,\n    ):\n        self.recognizer = recognizer\n        self.pattern_name = pattern_name\n        self.pattern = pattern\n        self.original_score = original_score\n        self.score = original_score\n        self.textual_explanation = textual_explanation\n        self.score_context_improvement = 0\n        self.supportive_context_word = \"\"\n        self.validation_result = validation_result\n        self.regex_flags = regex_flags\n\n    def __repr__(self):\n        \"\"\"Create string representation of the object.\"\"\"\n        return str(self.__dict__)\n\n    def set_improved_score(self, score: float) -&gt; None:\n        \"\"\"Update the score and calculate the difference from the original score.\"\"\"\n        self.score = score\n        self.score_context_improvement = self.score - self.original_score\n\n    def set_supportive_context_word(self, word: str) -&gt; None:\n        \"\"\"Set the context word which helped increase the score.\"\"\"\n        self.supportive_context_word = word\n\n    def append_textual_explanation_line(self, text: str) -&gt; None:\n        \"\"\"Append a new line to textual_explanation field.\"\"\"\n        if self.textual_explanation is None:\n            self.textual_explanation = text\n        else:\n            self.textual_explanation = f\"{self.textual_explanation}\\n{text}\"\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"\n        Serialize self to dictionary.\n\n        :return: a dictionary\n        \"\"\"\n        return self.__dict__\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.analysis_explanation.AnalysisExplanation.set_improved_score","title":"set_improved_score","text":"<pre><code>set_improved_score(score: float) -&gt; None\n</code></pre> <p>Update the score and calculate the difference from the original score.</p> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>def set_improved_score(self, score: float) -&gt; None:\n    \"\"\"Update the score and calculate the difference from the original score.\"\"\"\n    self.score = score\n    self.score_context_improvement = self.score - self.original_score\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.analysis_explanation.AnalysisExplanation.set_supportive_context_word","title":"set_supportive_context_word","text":"<pre><code>set_supportive_context_word(word: str) -&gt; None\n</code></pre> <p>Set the context word which helped increase the score.</p> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>def set_supportive_context_word(self, word: str) -&gt; None:\n    \"\"\"Set the context word which helped increase the score.\"\"\"\n    self.supportive_context_word = word\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.analysis_explanation.AnalysisExplanation.append_textual_explanation_line","title":"append_textual_explanation_line","text":"<pre><code>append_textual_explanation_line(text: str) -&gt; None\n</code></pre> <p>Append a new line to textual_explanation field.</p> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>def append_textual_explanation_line(self, text: str) -&gt; None:\n    \"\"\"Append a new line to textual_explanation field.\"\"\"\n    if self.textual_explanation is None:\n        self.textual_explanation = text\n    else:\n        self.textual_explanation = f\"{self.textual_explanation}\\n{text}\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.analysis_explanation.AnalysisExplanation.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return self.__dict__\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_result.RecognizerResult","title":"presidio_analyzer.recognizer_result.RecognizerResult","text":"<p>Recognizer Result represents the findings of the detected entity.</p> <p>Result of a recognizer analyzing the text.</p> PARAMETER DESCRIPTION <code>entity_type</code> <p>the type of the entity</p> <p> TYPE: <code>str</code> </p> <code>start</code> <p>the start location of the detected entity</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>the end location of the detected entity</p> <p> TYPE: <code>int</code> </p> <code>score</code> <p>the score of the detection</p> <p> TYPE: <code>float</code> </p> <code>analysis_explanation</code> <p>contains the explanation of why this entity was identified</p> <p> TYPE: <code>AnalysisExplanation</code> DEFAULT: <code>None</code> </p> <code>recognition_metadata</code> <p>a dictionary of metadata to be used in recognizer specific cases, for example specific recognized context words and recognizer name</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>append_analysis_explanation_text</code> <p>Add text to the analysis explanation.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_json</code> <p>Create RecognizerResult from json.</p> <code>intersects</code> <p>Check if self intersects with a different RecognizerResult.</p> <code>contained_in</code> <p>Check if self is contained in a different RecognizerResult.</p> <code>contains</code> <p>Check if one result is contained or equal to another result.</p> <code>equal_indices</code> <p>Check if the indices are equal between two results.</p> <code>has_conflict</code> <p>Check if two recognizer results are conflicted or not.</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>class RecognizerResult:\n    \"\"\"\n    Recognizer Result represents the findings of the detected entity.\n\n    Result of a recognizer analyzing the text.\n\n    :param entity_type: the type of the entity\n    :param start: the start location of the detected entity\n    :param end: the end location of the detected entity\n    :param score: the score of the detection\n    :param analysis_explanation: contains the explanation of why this\n                                 entity was identified\n    :param recognition_metadata: a dictionary of metadata to be used in\n    recognizer specific cases, for example specific recognized context words\n    and recognizer name\n    \"\"\"\n\n    # Keys for recognizer metadata\n    RECOGNIZER_NAME_KEY = \"recognizer_name\"\n    RECOGNIZER_IDENTIFIER_KEY = \"recognizer_identifier\"\n\n    # Key of a flag inside recognition_metadata dictionary\n    # which is set to true if the result enhanced by context\n    IS_SCORE_ENHANCED_BY_CONTEXT_KEY = \"is_score_enhanced_by_context\"\n\n    logger = logging.getLogger(\"presidio-analyzer\")\n\n    def __init__(\n        self,\n        entity_type: str,\n        start: int,\n        end: int,\n        score: float,\n        analysis_explanation: AnalysisExplanation = None,\n        recognition_metadata: Dict = None,\n    ):\n        self.entity_type = entity_type\n        self.start = start\n        self.end = end\n        self.score = score\n        self.analysis_explanation = analysis_explanation\n\n        if not recognition_metadata:\n            self.logger.debug(\n                \"recognition_metadata should be passed, \"\n                \"containing a recognizer_name value\"\n            )\n\n        self.recognition_metadata = recognition_metadata\n\n    def append_analysis_explanation_text(self, text: str) -&gt; None:\n        \"\"\"Add text to the analysis explanation.\"\"\"\n        if self.analysis_explanation:\n            self.analysis_explanation.append_textual_explanation_line(text)\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"\n        Serialize self to dictionary.\n\n        :return: a dictionary\n        \"\"\"\n        return self.__dict__\n\n    @classmethod\n    def from_json(cls, data: Dict) -&gt; \"RecognizerResult\":\n        \"\"\"\n        Create RecognizerResult from json.\n\n        :param data: e.g. {\n            \"start\": 24,\n            \"end\": 32,\n            \"score\": 0.8,\n            \"entity_type\": \"NAME\"\n        }\n        :return: RecognizerResult\n        \"\"\"\n        score = data.get(\"score\")\n        entity_type = data.get(\"entity_type\")\n        start = data.get(\"start\")\n        end = data.get(\"end\")\n        return cls(entity_type, start, end, score)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the instance.\"\"\"\n        return self.__str__()\n\n    def intersects(self, other: \"RecognizerResult\") -&gt; int:\n        \"\"\"\n        Check if self intersects with a different RecognizerResult.\n\n        :return: If intersecting, returns the number of\n        intersecting characters.\n        If not, returns 0\n        \"\"\"\n        # if they do not overlap the intersection is 0\n        if self.end &lt; other.start or other.end &lt; self.start:\n            return 0\n\n        # otherwise the intersection is min(end) - max(start)\n        return min(self.end, other.end) - max(self.start, other.start)\n\n    def contained_in(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check if self is contained in a different RecognizerResult.\n\n        :return: true if contained\n        \"\"\"\n        return self.start &gt;= other.start and self.end &lt;= other.end\n\n    def contains(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check if one result is contained or equal to another result.\n\n        :param other: another RecognizerResult\n        :return: bool\n        \"\"\"\n        return self.start &lt;= other.start and self.end &gt;= other.end\n\n    def equal_indices(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check if the indices are equal between two results.\n\n        :param other: another RecognizerResult\n        :return:\n        \"\"\"\n        return self.start == other.start and self.end == other.end\n\n    def __gt__(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check if one result is greater by using the results indices in the text.\n\n        :param other: another RecognizerResult\n        :return: bool\n        \"\"\"\n        if self.start == other.start:\n            return self.end &gt; other.end\n        return self.start &gt; other.start\n\n    def __eq__(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check two results are equal by using all class fields.\n\n        :param other: another RecognizerResult\n        :return: bool\n        \"\"\"\n        equal_type = self.entity_type == other.entity_type\n        equal_score = self.score == other.score\n        return self.equal_indices(other) and equal_type and equal_score\n\n    def __hash__(self):\n        \"\"\"\n        Hash the result data by using all class fields.\n\n        :return: int\n        \"\"\"\n        return hash(\n            f\"{str(self.start)} {str(self.end)} {str(self.score)} {self.entity_type}\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the instance.\"\"\"\n        return (\n            f\"type: {self.entity_type}, \"\n            f\"start: {self.start}, \"\n            f\"end: {self.end}, \"\n            f\"score: {self.score}\"\n        )\n\n    def has_conflict(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check if two recognizer results are conflicted or not.\n\n        I have a conflict if:\n        1. My indices are the same as the other and my score is lower.\n        2. If my indices are contained in another.\n\n        :param other: RecognizerResult\n        :return:\n        \"\"\"\n        if self.equal_indices(other):\n            return self.score &lt;= other.score\n        return other.contains(self)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_result.RecognizerResult.append_analysis_explanation_text","title":"append_analysis_explanation_text","text":"<pre><code>append_analysis_explanation_text(text: str) -&gt; None\n</code></pre> <p>Add text to the analysis explanation.</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def append_analysis_explanation_text(self, text: str) -&gt; None:\n    \"\"\"Add text to the analysis explanation.\"\"\"\n    if self.analysis_explanation:\n        self.analysis_explanation.append_textual_explanation_line(text)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_result.RecognizerResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return self.__dict__\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_result.RecognizerResult.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(data: Dict) -&gt; RecognizerResult\n</code></pre> <p>Create RecognizerResult from json.</p> PARAMETER DESCRIPTION <code>data</code> <p>e.g. { \"start\": 24, \"end\": 32, \"score\": 0.8, \"entity_type\": \"NAME\" }</p> <p> TYPE: <code>Dict</code> </p> RETURNS DESCRIPTION <code>RecognizerResult</code> <p>RecognizerResult</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>@classmethod\ndef from_json(cls, data: Dict) -&gt; \"RecognizerResult\":\n    \"\"\"\n    Create RecognizerResult from json.\n\n    :param data: e.g. {\n        \"start\": 24,\n        \"end\": 32,\n        \"score\": 0.8,\n        \"entity_type\": \"NAME\"\n    }\n    :return: RecognizerResult\n    \"\"\"\n    score = data.get(\"score\")\n    entity_type = data.get(\"entity_type\")\n    start = data.get(\"start\")\n    end = data.get(\"end\")\n    return cls(entity_type, start, end, score)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_result.RecognizerResult.intersects","title":"intersects","text":"<pre><code>intersects(other: RecognizerResult) -&gt; int\n</code></pre> <p>Check if self intersects with a different RecognizerResult.</p> RETURNS DESCRIPTION <code>int</code> <p>If intersecting, returns the number of intersecting characters. If not, returns 0</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def intersects(self, other: \"RecognizerResult\") -&gt; int:\n    \"\"\"\n    Check if self intersects with a different RecognizerResult.\n\n    :return: If intersecting, returns the number of\n    intersecting characters.\n    If not, returns 0\n    \"\"\"\n    # if they do not overlap the intersection is 0\n    if self.end &lt; other.start or other.end &lt; self.start:\n        return 0\n\n    # otherwise the intersection is min(end) - max(start)\n    return min(self.end, other.end) - max(self.start, other.start)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_result.RecognizerResult.contained_in","title":"contained_in","text":"<pre><code>contained_in(other: RecognizerResult) -&gt; bool\n</code></pre> <p>Check if self is contained in a different RecognizerResult.</p> RETURNS DESCRIPTION <code>bool</code> <p>true if contained</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def contained_in(self, other: \"RecognizerResult\") -&gt; bool:\n    \"\"\"\n    Check if self is contained in a different RecognizerResult.\n\n    :return: true if contained\n    \"\"\"\n    return self.start &gt;= other.start and self.end &lt;= other.end\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_result.RecognizerResult.contains","title":"contains","text":"<pre><code>contains(other: RecognizerResult) -&gt; bool\n</code></pre> <p>Check if one result is contained or equal to another result.</p> PARAMETER DESCRIPTION <code>other</code> <p>another RecognizerResult</p> <p> TYPE: <code>RecognizerResult</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>bool</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def contains(self, other: \"RecognizerResult\") -&gt; bool:\n    \"\"\"\n    Check if one result is contained or equal to another result.\n\n    :param other: another RecognizerResult\n    :return: bool\n    \"\"\"\n    return self.start &lt;= other.start and self.end &gt;= other.end\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_result.RecognizerResult.equal_indices","title":"equal_indices","text":"<pre><code>equal_indices(other: RecognizerResult) -&gt; bool\n</code></pre> <p>Check if the indices are equal between two results.</p> PARAMETER DESCRIPTION <code>other</code> <p>another RecognizerResult</p> <p> TYPE: <code>RecognizerResult</code> </p> RETURNS DESCRIPTION <code>bool</code> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def equal_indices(self, other: \"RecognizerResult\") -&gt; bool:\n    \"\"\"\n    Check if the indices are equal between two results.\n\n    :param other: another RecognizerResult\n    :return:\n    \"\"\"\n    return self.start == other.start and self.end == other.end\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_result.RecognizerResult.has_conflict","title":"has_conflict","text":"<pre><code>has_conflict(other: RecognizerResult) -&gt; bool\n</code></pre> <p>Check if two recognizer results are conflicted or not.</p> <p>I have a conflict if: 1. My indices are the same as the other and my score is lower. 2. If my indices are contained in another.</p> PARAMETER DESCRIPTION <code>other</code> <p>RecognizerResult</p> <p> TYPE: <code>RecognizerResult</code> </p> RETURNS DESCRIPTION <code>bool</code> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def has_conflict(self, other: \"RecognizerResult\") -&gt; bool:\n    \"\"\"\n    Check if two recognizer results are conflicted or not.\n\n    I have a conflict if:\n    1. My indices are the same as the other and my score is lower.\n    2. If my indices are contained in another.\n\n    :param other: RecognizerResult\n    :return:\n    \"\"\"\n    if self.equal_indices(other):\n        return self.score &lt;= other.score\n    return other.contains(self)\n</code></pre>"},{"location":"api/analyzer_python/#batch-modules","title":"Batch modules","text":""},{"location":"api/analyzer_python/#presidio_analyzer.batch_analyzer_engine.BatchAnalyzerEngine","title":"presidio_analyzer.batch_analyzer_engine.BatchAnalyzerEngine","text":"<p>Batch analysis of documents (tables, lists, dicts).</p> <p>Wrapper class to run Presidio Analyzer Engine on multiple values, either lists/iterators of strings, or dictionaries.</p> PARAMETER DESCRIPTION <code>analyzer_engine</code> <p>AnalyzerEngine instance to use for handling the values in those collections.</p> <p> TYPE: <code>Optional[AnalyzerEngine]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze_iterator</code> <p>Analyze an iterable of strings.</p> <code>analyze_dict</code> <p>Analyze a dictionary of keys (strings) and values/iterable of values.</p> Source code in <code>presidio_analyzer/batch_analyzer_engine.py</code> <pre><code>class BatchAnalyzerEngine:\n    \"\"\"\n    Batch analysis of documents (tables, lists, dicts).\n\n    Wrapper class to run Presidio Analyzer Engine on multiple values,\n    either lists/iterators of strings, or dictionaries.\n\n    :param analyzer_engine: AnalyzerEngine instance to use\n    for handling the values in those collections.\n    \"\"\"\n\n    def __init__(self, analyzer_engine: Optional[AnalyzerEngine] = None):\n        self.analyzer_engine = analyzer_engine\n        if not analyzer_engine:\n            self.analyzer_engine = AnalyzerEngine()\n\n    def analyze_iterator(\n        self,\n        texts: Iterable[Union[str, bool, float, int]],\n        language: str,\n        batch_size: int = 1,\n        n_process: int = 1,\n        **kwargs,\n    ) -&gt; List[List[RecognizerResult]]:\n        \"\"\"\n        Analyze an iterable of strings.\n\n        :param texts: An list containing strings to be analyzed.\n        :param language: Input language\n        :param batch_size: Batch size to process in a single iteration\n        :param n_process: Number of processors to use. Defaults to `1`\n        :param kwargs: Additional parameters for the `AnalyzerEngine.analyze` method.\n        (default value depends on the nlp engine implementation)\n        \"\"\"\n\n        # validate types\n        texts = self._validate_types(texts)\n\n        # Process the texts as batch for improved performance\n        nlp_artifacts_batch: Iterator[Tuple[str, NlpArtifacts]] = (\n            self.analyzer_engine.nlp_engine.process_batch(\n                texts=texts,\n                language=language,\n                batch_size=batch_size,\n                n_process=n_process,\n            )\n        )\n\n        list_results = []\n        for text, nlp_artifacts in nlp_artifacts_batch:\n            results = self.analyzer_engine.analyze(\n                text=str(text), nlp_artifacts=nlp_artifacts, language=language, **kwargs\n            )\n\n            list_results.append(results)\n\n        return list_results\n\n    def analyze_dict(\n        self,\n        input_dict: Dict[str, Union[Any, Iterable[Any]]],\n        language: str,\n        keys_to_skip: Optional[List[str]] = None,\n        batch_size: int = 1,\n        n_process: int = 1,\n        **kwargs,\n    ) -&gt; Iterator[DictAnalyzerResult]:\n        \"\"\"\n        Analyze a dictionary of keys (strings) and values/iterable of values.\n\n        Non-string values are returned as is.\n\n        :param input_dict: The input dictionary for analysis\n        :param language: Input language\n        :param keys_to_skip: Keys to ignore during analysis\n        :param batch_size: Batch size to process in a single iteration\n        :param n_process: Number of processors to use. Defaults to `1`\n\n        :param kwargs: Additional keyword arguments\n        for the `AnalyzerEngine.analyze` method.\n        Use this to pass arguments to the analyze method,\n        such as `ad_hoc_recognizers`, `context`, `return_decision_process`.\n        See `AnalyzerEngine.analyze` for the full list.\n        \"\"\"\n\n        context = []\n        if \"context\" in kwargs:\n            context = kwargs[\"context\"]\n            del kwargs[\"context\"]\n\n        if not keys_to_skip:\n            keys_to_skip = []\n\n        for key, value in input_dict.items():\n            if not value or key in keys_to_skip:\n                yield DictAnalyzerResult(key=key, value=value, recognizer_results=[])\n                continue  # skip this key as requested\n\n            # Add the key as an additional context\n            specific_context = context[:]\n            specific_context.append(key)\n\n            if type(value) in (str, int, bool, float):\n                results: List[RecognizerResult] = self.analyzer_engine.analyze(\n                    text=str(value), language=language, context=[key], **kwargs\n                )\n            elif isinstance(value, dict):\n                new_keys_to_skip = self._get_nested_keys_to_skip(key, keys_to_skip)\n                results = self.analyze_dict(\n                    input_dict=value,\n                    language=language,\n                    context=specific_context,\n                    keys_to_skip=new_keys_to_skip,\n                    **kwargs,\n                )\n            elif isinstance(value, Iterable):\n                # Recursively iterate nested dicts\n\n                results: List[List[RecognizerResult]] = self.analyze_iterator(\n                    texts=value,\n                    language=language,\n                    context=specific_context,\n                    n_process=n_process,\n                    batch_size=batch_size,\n                    **kwargs,\n                )\n            else:\n                raise ValueError(f\"type {type(value)} is unsupported.\")\n\n            yield DictAnalyzerResult(key=key, value=value, recognizer_results=results)\n\n    @staticmethod\n    def _validate_types(value_iterator: Iterable[Any]) -&gt; Iterator[Any]:\n        for val in value_iterator:\n            if val and type(val) not in (int, float, bool, str):\n                err_msg = (\n                    \"Analyzer.analyze_iterator only works \"\n                    \"on primitive types (int, float, bool, str). \"\n                    \"Lists of objects are not yet supported.\"\n                )\n                logger.error(err_msg)\n                raise ValueError(err_msg)\n            yield val\n\n    @staticmethod\n    def _get_nested_keys_to_skip(key, keys_to_skip):\n        new_keys_to_skip = [\n            k.replace(f\"{key}.\", \"\") for k in keys_to_skip if k.startswith(key)\n        ]\n        return new_keys_to_skip\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.batch_analyzer_engine.BatchAnalyzerEngine.analyze_iterator","title":"analyze_iterator","text":"<pre><code>analyze_iterator(\n    texts: Iterable[Union[str, bool, float, int]],\n    language: str,\n    batch_size: int = 1,\n    n_process: int = 1,\n    **kwargs\n) -&gt; List[List[RecognizerResult]]\n</code></pre> <p>Analyze an iterable of strings.</p> PARAMETER DESCRIPTION <code>texts</code> <p>An list containing strings to be analyzed.</p> <p> TYPE: <code>Iterable[Union[str, bool, float, int]]</code> </p> <code>language</code> <p>Input language</p> <p> TYPE: <code>str</code> </p> <code>batch_size</code> <p>Batch size to process in a single iteration</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_process</code> <p>Number of processors to use. Defaults to <code>1</code></p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>kwargs</code> <p>Additional parameters for the <code>AnalyzerEngine.analyze</code> method. (default value depends on the nlp engine implementation)</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>presidio_analyzer/batch_analyzer_engine.py</code> <pre><code>def analyze_iterator(\n    self,\n    texts: Iterable[Union[str, bool, float, int]],\n    language: str,\n    batch_size: int = 1,\n    n_process: int = 1,\n    **kwargs,\n) -&gt; List[List[RecognizerResult]]:\n    \"\"\"\n    Analyze an iterable of strings.\n\n    :param texts: An list containing strings to be analyzed.\n    :param language: Input language\n    :param batch_size: Batch size to process in a single iteration\n    :param n_process: Number of processors to use. Defaults to `1`\n    :param kwargs: Additional parameters for the `AnalyzerEngine.analyze` method.\n    (default value depends on the nlp engine implementation)\n    \"\"\"\n\n    # validate types\n    texts = self._validate_types(texts)\n\n    # Process the texts as batch for improved performance\n    nlp_artifacts_batch: Iterator[Tuple[str, NlpArtifacts]] = (\n        self.analyzer_engine.nlp_engine.process_batch(\n            texts=texts,\n            language=language,\n            batch_size=batch_size,\n            n_process=n_process,\n        )\n    )\n\n    list_results = []\n    for text, nlp_artifacts in nlp_artifacts_batch:\n        results = self.analyzer_engine.analyze(\n            text=str(text), nlp_artifacts=nlp_artifacts, language=language, **kwargs\n        )\n\n        list_results.append(results)\n\n    return list_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.batch_analyzer_engine.BatchAnalyzerEngine.analyze_dict","title":"analyze_dict","text":"<pre><code>analyze_dict(\n    input_dict: Dict[str, Union[Any, Iterable[Any]]],\n    language: str,\n    keys_to_skip: Optional[List[str]] = None,\n    batch_size: int = 1,\n    n_process: int = 1,\n    **kwargs\n) -&gt; Iterator[DictAnalyzerResult]\n</code></pre> <p>Analyze a dictionary of keys (strings) and values/iterable of values.</p> <p>Non-string values are returned as is.</p> PARAMETER DESCRIPTION <code>input_dict</code> <p>The input dictionary for analysis</p> <p> TYPE: <code>Dict[str, Union[Any, Iterable[Any]]]</code> </p> <code>language</code> <p>Input language</p> <p> TYPE: <code>str</code> </p> <code>keys_to_skip</code> <p>Keys to ignore during analysis</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Batch size to process in a single iteration</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_process</code> <p>Number of processors to use. Defaults to <code>1</code></p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>kwargs</code> <p>Additional keyword arguments for the <code>AnalyzerEngine.analyze</code> method. Use this to pass arguments to the analyze method, such as <code>ad_hoc_recognizers</code>, <code>context</code>, <code>return_decision_process</code>. See <code>AnalyzerEngine.analyze</code> for the full list.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>presidio_analyzer/batch_analyzer_engine.py</code> <pre><code>def analyze_dict(\n    self,\n    input_dict: Dict[str, Union[Any, Iterable[Any]]],\n    language: str,\n    keys_to_skip: Optional[List[str]] = None,\n    batch_size: int = 1,\n    n_process: int = 1,\n    **kwargs,\n) -&gt; Iterator[DictAnalyzerResult]:\n    \"\"\"\n    Analyze a dictionary of keys (strings) and values/iterable of values.\n\n    Non-string values are returned as is.\n\n    :param input_dict: The input dictionary for analysis\n    :param language: Input language\n    :param keys_to_skip: Keys to ignore during analysis\n    :param batch_size: Batch size to process in a single iteration\n    :param n_process: Number of processors to use. Defaults to `1`\n\n    :param kwargs: Additional keyword arguments\n    for the `AnalyzerEngine.analyze` method.\n    Use this to pass arguments to the analyze method,\n    such as `ad_hoc_recognizers`, `context`, `return_decision_process`.\n    See `AnalyzerEngine.analyze` for the full list.\n    \"\"\"\n\n    context = []\n    if \"context\" in kwargs:\n        context = kwargs[\"context\"]\n        del kwargs[\"context\"]\n\n    if not keys_to_skip:\n        keys_to_skip = []\n\n    for key, value in input_dict.items():\n        if not value or key in keys_to_skip:\n            yield DictAnalyzerResult(key=key, value=value, recognizer_results=[])\n            continue  # skip this key as requested\n\n        # Add the key as an additional context\n        specific_context = context[:]\n        specific_context.append(key)\n\n        if type(value) in (str, int, bool, float):\n            results: List[RecognizerResult] = self.analyzer_engine.analyze(\n                text=str(value), language=language, context=[key], **kwargs\n            )\n        elif isinstance(value, dict):\n            new_keys_to_skip = self._get_nested_keys_to_skip(key, keys_to_skip)\n            results = self.analyze_dict(\n                input_dict=value,\n                language=language,\n                context=specific_context,\n                keys_to_skip=new_keys_to_skip,\n                **kwargs,\n            )\n        elif isinstance(value, Iterable):\n            # Recursively iterate nested dicts\n\n            results: List[List[RecognizerResult]] = self.analyze_iterator(\n                texts=value,\n                language=language,\n                context=specific_context,\n                n_process=n_process,\n                batch_size=batch_size,\n                **kwargs,\n            )\n        else:\n            raise ValueError(f\"type {type(value)} is unsupported.\")\n\n        yield DictAnalyzerResult(key=key, value=value, recognizer_results=results)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.dict_analyzer_result.DictAnalyzerResult","title":"presidio_analyzer.dict_analyzer_result.DictAnalyzerResult  <code>dataclass</code>","text":"<p>Data class for holding the output of the Presidio Analyzer on dictionaries.</p> PARAMETER DESCRIPTION <code>key</code> <p>key in dictionary</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>value to run analysis on (either string or list of strings)</p> <p> TYPE: <code>Union[str, List[str], dict]</code> </p> <code>recognizer_results</code> <p>Analyzer output for one value. Could be either: - A list of recognizer results if the input is one string - A list of lists of recognizer results, if the input is a list of strings. - An iterator of a DictAnalyzerResult, if the input is a dictionary. In this case the recognizer_results would be the iterator of the DictAnalyzerResults next level in the dictionary.</p> <p> TYPE: <code>Union[List[RecognizerResult], List[List[RecognizerResult]], Iterator[DictAnalyzerResult]]</code> </p> Source code in <code>presidio_analyzer/dict_analyzer_result.py</code> <pre><code>@dataclass\nclass DictAnalyzerResult:\n    \"\"\"\n    Data class for holding the output of the Presidio Analyzer on dictionaries.\n\n    :param key: key in dictionary\n    :param value: value to run analysis on (either string or list of strings)\n    :param recognizer_results: Analyzer output for one value.\n    Could be either:\n     - A list of recognizer results if the input is one string\n     - A list of lists of recognizer results, if the input is a list of strings.\n     - An iterator of a DictAnalyzerResult, if the input is a dictionary.\n     In this case the recognizer_results would be the iterator\n     of the DictAnalyzerResults next level in the dictionary.\n    \"\"\"\n\n    key: str\n    value: Union[str, List[str], dict]\n    recognizer_results: Union[\n        List[RecognizerResult],\n        List[List[RecognizerResult]],\n        Iterator[\"DictAnalyzerResult\"],\n    ]\n</code></pre>"},{"location":"api/analyzer_python/#recognizers-and-patterns","title":"Recognizers and patterns","text":""},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer","title":"presidio_analyzer.entity_recognizer.EntityRecognizer","text":"<p>A class representing an abstract PII entity recognizer.</p> <p>EntityRecognizer is an abstract class to be inherited by Recognizers which hold the logic for recognizing specific PII entities.</p> <p>EntityRecognizer exposes a method called enhance_using_context which can be overridden in case a custom context aware enhancement is needed in derived class of a recognizer.</p> PARAMETER DESCRIPTION <code>supported_entities</code> <p>the entities supported by this recognizer (for example, phone number, address, etc.)</p> <p> TYPE: <code>List[str]</code> </p> <code>supported_language</code> <p>the language supported by this recognizer. The supported language code is iso6391Name</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>name</code> <p>the name of this recognizer (optional)</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>version</code> <p>the recognizer current version</p> <p> TYPE: <code>str</code> DEFAULT: <code>'0.0.1'</code> </p> <code>context</code> <p>a list of words which can help boost confidence score when they appear in context of the matched entity</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>load</code> <p>Initialize the recognizer assets if needed.</p> <code>analyze</code> <p>Analyze text to identify entities.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_dict</code> <p>Create EntityRecognizer from a dict input.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>class EntityRecognizer:\n    \"\"\"\n    A class representing an abstract PII entity recognizer.\n\n    EntityRecognizer is an abstract class to be inherited by\n    Recognizers which hold the logic for recognizing specific PII entities.\n\n    EntityRecognizer exposes a method called enhance_using_context which\n    can be overridden in case a custom context aware enhancement is needed\n    in derived class of a recognizer.\n\n    :param supported_entities: the entities supported by this recognizer\n    (for example, phone number, address, etc.)\n    :param supported_language: the language supported by this recognizer.\n    The supported language code is iso6391Name\n    :param name: the name of this recognizer (optional)\n    :param version: the recognizer current version\n    :param context: a list of words which can help boost confidence score\n    when they appear in context of the matched entity\n    \"\"\"\n\n    MIN_SCORE = 0\n    MAX_SCORE = 1.0\n\n    def __init__(\n        self,\n        supported_entities: List[str],\n        name: str = None,\n        supported_language: str = \"en\",\n        version: str = \"0.0.1\",\n        context: Optional[List[str]] = None,\n    ):\n        self.supported_entities = supported_entities\n\n        if name is None:\n            self.name = self.__class__.__name__  # assign class name as name\n        else:\n            self.name = name\n\n        self._id = f\"{self.name}_{id(self)}\"\n\n        self.supported_language = supported_language\n        self.version = version\n        self.is_loaded = False\n        self.context = context if context else []\n\n        self.load()\n        logger.info(\"Loaded recognizer: %s\", self.name)\n        self.is_loaded = True\n\n    @property\n    def id(self):\n        \"\"\"Return a unique identifier of this recognizer.\"\"\"\n\n        return self._id\n\n    @abstractmethod\n    def load(self) -&gt; None:\n        \"\"\"\n        Initialize the recognizer assets if needed.\n\n        (e.g. machine learning models)\n        \"\"\"\n\n    @abstractmethod\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyze text to identify entities.\n\n        :param text: The text to be analyzed\n        :param entities: The list of entities this recognizer is able to detect\n        :param nlp_artifacts: A group of attributes which are the result of\n        an NLP process over the input text.\n        :return: List of results detected by this recognizer.\n        \"\"\"\n        return None\n\n    def enhance_using_context(\n        self,\n        text: str,\n        raw_recognizer_results: List[RecognizerResult],\n        other_raw_recognizer_results: List[RecognizerResult],\n        nlp_artifacts: NlpArtifacts,\n        context: Optional[List[str]] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Enhance confidence score using context of the entity.\n\n        Override this method in derived class in case a custom logic\n        is needed, otherwise return value will be equal to\n        raw_results.\n\n        in case a result score is boosted, derived class need to update\n        result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n        :param text: The actual text that was analyzed\n        :param raw_recognizer_results: This recognizer's results, to be updated\n        based on recognizer specific context.\n        :param other_raw_recognizer_results: Other recognizer results matched in\n        the given text to allow related entity context enhancement\n        :param nlp_artifacts: The nlp artifacts contains elements\n                              such as lemmatized tokens for better\n                              accuracy of the context enhancement process\n        :param context: list of context words\n        \"\"\"\n        return raw_recognizer_results\n\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"\n        Return the list of entities this recognizer can identify.\n\n        :return: A list of the supported entities by this recognizer\n        \"\"\"\n        return self.supported_entities\n\n    def get_supported_language(self) -&gt; str:\n        \"\"\"\n        Return the language this recognizer can support.\n\n        :return: A list of the supported language by this recognizer\n        \"\"\"\n        return self.supported_language\n\n    def get_version(self) -&gt; str:\n        \"\"\"\n        Return the version of this recognizer.\n\n        :return: The current version of this recognizer\n        \"\"\"\n        return self.version\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"\n        Serialize self to dictionary.\n\n        :return: a dictionary\n        \"\"\"\n        return_dict = {\n            \"supported_entities\": self.supported_entities,\n            \"supported_language\": self.supported_language,\n            \"name\": self.name,\n            \"version\": self.version,\n        }\n        return return_dict\n\n    @classmethod\n    def from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n        \"\"\"\n        Create EntityRecognizer from a dict input.\n\n        :param entity_recognizer_dict: Dict containing keys and values for instantiation\n        \"\"\"\n        return cls(**entity_recognizer_dict)\n\n    @staticmethod\n    def remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Remove duplicate results.\n\n        Remove duplicates in case the two results\n        have identical start and ends and types.\n        :param results: List[RecognizerResult]\n        :return: List[RecognizerResult]\n        \"\"\"\n        results = list(set(results))\n        results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n        filtered_results = []\n\n        for result in results:\n            if result.score == 0:\n                continue\n\n            to_keep = result not in filtered_results  # equals based comparison\n            if to_keep:\n                for filtered in filtered_results:\n                    # If result is contained in one of the other results\n                    if (\n                        result.contained_in(filtered)\n                        and result.entity_type == filtered.entity_type\n                    ):\n                        to_keep = False\n                        break\n\n            if to_keep:\n                filtered_results.append(result)\n\n        return filtered_results\n\n    @staticmethod\n    def sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n        \"\"\"\n        Cleanse the input string of the replacement pairs specified as argument.\n\n        :param text: input string\n        :param replacement_pairs: pairs of what has to be replaced with which value\n        :return: cleansed string\n        \"\"\"\n        for search_string, replacement_string in replacement_pairs:\n            text = text.replace(search_string, replacement_string)\n        return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load() -&gt; None\n</code></pre> <p>Initialize the recognizer assets if needed.</p> <p>(e.g. machine learning models)</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@abstractmethod\ndef load(self) -&gt; None:\n    \"\"\"\n    Initialize the recognizer assets if needed.\n\n    (e.g. machine learning models)\n    \"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.analyze","title":"analyze  <code>abstractmethod</code>","text":"<pre><code>analyze(\n    text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyze text to identify entities.</p> PARAMETER DESCRIPTION <code>text</code> <p>The text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>The list of entities this recognizer is able to detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>A group of attributes which are the result of an NLP process over the input text.</p> <p> TYPE: <code>NlpArtifacts</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List of results detected by this recognizer.</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@abstractmethod\ndef analyze(\n    self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyze text to identify entities.\n\n    :param text: The text to be analyzed\n    :param entities: The list of entities this recognizer is able to detect\n    :param nlp_artifacts: A group of attributes which are the result of\n    an NLP process over the input text.\n    :return: List of results detected by this recognizer.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; EntityRecognizer\n</code></pre> <p>Create EntityRecognizer from a dict input.</p> PARAMETER DESCRIPTION <code>entity_recognizer_dict</code> <p>Dict containing keys and values for instantiation</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.entity_recognizer.EntityRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer","title":"presidio_analyzer.local_recognizer.LocalRecognizer","text":"<p>               Bases: <code>ABC</code>, <code>EntityRecognizer</code></p> <p>PII entity recognizer which runs on the same process as the AnalyzerEngine.</p> METHOD DESCRIPTION <code>load</code> <p>Initialize the recognizer assets if needed.</p> <code>analyze</code> <p>Analyze text to identify entities.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_dict</code> <p>Create EntityRecognizer from a dict input.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> Source code in <code>presidio_analyzer/local_recognizer.py</code> <pre><code>class LocalRecognizer(ABC, EntityRecognizer):\n    \"\"\"PII entity recognizer which runs on the same process as the AnalyzerEngine.\"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load() -&gt; None\n</code></pre> <p>Initialize the recognizer assets if needed.</p> <p>(e.g. machine learning models)</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@abstractmethod\ndef load(self) -&gt; None:\n    \"\"\"\n    Initialize the recognizer assets if needed.\n\n    (e.g. machine learning models)\n    \"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.analyze","title":"analyze  <code>abstractmethod</code>","text":"<pre><code>analyze(\n    text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyze text to identify entities.</p> PARAMETER DESCRIPTION <code>text</code> <p>The text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>The list of entities this recognizer is able to detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>A group of attributes which are the result of an NLP process over the input text.</p> <p> TYPE: <code>NlpArtifacts</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List of results detected by this recognizer.</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@abstractmethod\ndef analyze(\n    self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyze text to identify entities.\n\n    :param text: The text to be analyzed\n    :param entities: The list of entities this recognizer is able to detect\n    :param nlp_artifacts: A group of attributes which are the result of\n    an NLP process over the input text.\n    :return: List of results detected by this recognizer.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; EntityRecognizer\n</code></pre> <p>Create EntityRecognizer from a dict input.</p> PARAMETER DESCRIPTION <code>entity_recognizer_dict</code> <p>Dict containing keys and values for instantiation</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.local_recognizer.LocalRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern.Pattern","title":"presidio_analyzer.pattern.Pattern","text":"<p>A class that represents a regex pattern.</p> PARAMETER DESCRIPTION <code>name</code> <p>the name of the pattern</p> <p> TYPE: <code>str</code> </p> <code>regex</code> <p>the regex pattern to detect</p> <p> TYPE: <code>str</code> </p> <code>score</code> <p>the pattern's strength (values varies 0-1)</p> <p> TYPE: <code>float</code> </p> METHOD DESCRIPTION <code>to_dict</code> <p>Turn this instance into a dictionary.</p> <code>from_dict</code> <p>Load an instance from a dictionary.</p> Source code in <code>presidio_analyzer/pattern.py</code> <pre><code>class Pattern:\n    \"\"\"\n    A class that represents a regex pattern.\n\n    :param name: the name of the pattern\n    :param regex: the regex pattern to detect\n    :param score: the pattern's strength (values varies 0-1)\n    \"\"\"\n\n    def __init__(self, name: str, regex: str, score: float):\n        self.name = name\n        self.regex = regex\n        self.score = score\n        self.compiled_regex = None\n        self.compiled_with_flags = None\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"\n        Turn this instance into a dictionary.\n\n        :return: a dictionary\n        \"\"\"\n        return_dict = {\"name\": self.name, \"score\": self.score, \"regex\": self.regex}\n        return return_dict\n\n    @classmethod\n    def from_dict(cls, pattern_dict: Dict) -&gt; \"Pattern\":\n        \"\"\"\n        Load an instance from a dictionary.\n\n        :param pattern_dict: a dictionary holding the pattern's parameters\n        :return: a Pattern instance\n        \"\"\"\n        return cls(**pattern_dict)\n\n    def __repr__(self):\n        \"\"\"Return string representation of instance.\"\"\"\n        return json.dumps(self.to_dict())\n\n    def __str__(self):\n        \"\"\"Return string representation of instance.\"\"\"\n        return json.dumps(self.to_dict())\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern.Pattern.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Turn this instance into a dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/pattern.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Turn this instance into a dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\"name\": self.name, \"score\": self.score, \"regex\": self.regex}\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern.Pattern.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(pattern_dict: Dict) -&gt; Pattern\n</code></pre> <p>Load an instance from a dictionary.</p> PARAMETER DESCRIPTION <code>pattern_dict</code> <p>a dictionary holding the pattern's parameters</p> <p> TYPE: <code>Dict</code> </p> RETURNS DESCRIPTION <code>Pattern</code> <p>a Pattern instance</p> Source code in <code>presidio_analyzer/pattern.py</code> <pre><code>@classmethod\ndef from_dict(cls, pattern_dict: Dict) -&gt; \"Pattern\":\n    \"\"\"\n    Load an instance from a dictionary.\n\n    :param pattern_dict: a dictionary holding the pattern's parameters\n    :return: a Pattern instance\n    \"\"\"\n    return cls(**pattern_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer","title":"presidio_analyzer.pattern_recognizer.PatternRecognizer","text":"<p>               Bases: <code>LocalRecognizer</code></p> <p>PII entity recognizer using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>A list of patterns to detect</p> <p> TYPE: <code>List[Pattern]</code> DEFAULT: <code>None</code> </p> <code>deny_list</code> <p>A list of words to detect, in case our recognizer uses a predefined list of words (deny list)</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>deny_list_score</code> <p>confidence score for a term identified using a deny-list</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>global_regex_flags</code> <p>regex flags to be used in regex matching, including deny-lists.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>DOTALL | MULTILINE | IGNORECASE</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>class PatternRecognizer(LocalRecognizer):\n    \"\"\"\n    PII entity recognizer using regular expressions or deny-lists.\n\n    :param patterns: A list of patterns to detect\n    :param deny_list: A list of words to detect,\n    in case our recognizer uses a predefined list of words (deny list)\n    :param context: list of context words\n    :param deny_list_score: confidence score for a term\n    identified using a deny-list\n    :param global_regex_flags: regex flags to be used in regex matching,\n    including deny-lists.\n    \"\"\"\n\n    def __init__(\n        self,\n        supported_entity: str,\n        name: str = None,\n        supported_language: str = \"en\",\n        patterns: List[Pattern] = None,\n        deny_list: List[str] = None,\n        context: List[str] = None,\n        deny_list_score: float = 1.0,\n        global_regex_flags: Optional[int] = re.DOTALL | re.MULTILINE | re.IGNORECASE,\n        version: str = \"0.0.1\",\n    ):\n        if not supported_entity:\n            raise ValueError(\"Pattern recognizer should be initialized with entity\")\n\n        if not patterns and not deny_list:\n            raise ValueError(\n                \"Pattern recognizer should be initialized with patterns\"\n                \" or with deny list\"\n            )\n\n        super().__init__(\n            supported_entities=[supported_entity],\n            supported_language=supported_language,\n            name=name,\n            version=version,\n        )\n        if patterns is None:\n            self.patterns = []\n        else:\n            self.patterns = patterns\n        self.context = context\n        self.deny_list_score = deny_list_score\n        self.global_regex_flags = global_regex_flags\n\n        if deny_list:\n            deny_list_pattern = self._deny_list_to_regex(deny_list)\n            self.patterns.append(deny_list_pattern)\n            self.deny_list = deny_list\n        else:\n            self.deny_list = []\n\n    def load(self):  # noqa D102\n        pass\n\n    def analyze(\n        self,\n        text: str,\n        entities: List[str],\n        nlp_artifacts: Optional[NlpArtifacts] = None,\n        regex_flags: Optional[int] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyzes text to detect PII using regular expressions or deny-lists.\n\n        :param text: Text to be analyzed\n        :param entities: Entities this recognizer can detect\n        :param nlp_artifacts: Output values from the NLP engine\n        :param regex_flags: regex flags to be used in regex matching\n        :return:\n        \"\"\"\n        results = []\n\n        if self.patterns:\n            pattern_result = self.__analyze_patterns(text, regex_flags)\n            results.extend(pattern_result)\n\n        return results\n\n    def _deny_list_to_regex(self, deny_list: List[str]) -&gt; Pattern:\n        \"\"\"\n        Convert a list of words to a matching regex.\n\n        To be analyzed by the analyze method as any other regex patterns.\n\n        :param deny_list: the list of words to detect\n        :return:the regex of the words for detection\n        \"\"\"\n\n        # Escape deny list elements as preparation for regex\n        escaped_deny_list = [re.escape(element) for element in deny_list]\n        regex = r\"(?:^|(?&lt;=\\W))(\" + \"|\".join(escaped_deny_list) + r\")(?:(?=\\W)|$)\"\n        return Pattern(name=\"deny_list\", regex=regex, score=self.deny_list_score)\n\n    def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the validation was successful.\n        \"\"\"\n        return None\n\n    def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n        \"\"\"\n        Logic to check for result invalidation by running pruning logic.\n\n        For example, each SSN number group should not consist of all the same digits.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the result is invalidated\n        \"\"\"\n        return None\n\n    @staticmethod\n    def build_regex_explanation(\n        recognizer_name: str,\n        pattern_name: str,\n        pattern: str,\n        original_score: float,\n        validation_result: bool,\n        regex_flags: int,\n    ) -&gt; AnalysisExplanation:\n        \"\"\"\n        Construct an explanation for why this entity was detected.\n\n        :param recognizer_name: Name of recognizer detecting the entity\n        :param pattern_name: Regex pattern name which detected the entity\n        :param pattern: Regex pattern logic\n        :param original_score: Score given by the recognizer\n        :param validation_result: Whether validation was used and its result\n        :param regex_flags: Regex flags used in the regex matching\n        :return: Analysis explanation\n        \"\"\"\n        textual_explanation = (\n            f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n        )\n\n        explanation = AnalysisExplanation(\n            recognizer=recognizer_name,\n            original_score=original_score,\n            pattern_name=pattern_name,\n            pattern=pattern,\n            validation_result=validation_result,\n            regex_flags=regex_flags,\n            textual_explanation=textual_explanation,\n        )\n        return explanation\n\n    def __analyze_patterns(\n        self, text: str, flags: int = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Evaluate all patterns in the provided text.\n\n        Including words in the provided deny-list\n\n        :param text: text to analyze\n        :param flags: regex flags\n        :return: A list of RecognizerResult\n        \"\"\"\n        flags = flags if flags else self.global_regex_flags\n        results = []\n        for pattern in self.patterns:\n            match_start_time = datetime.datetime.now()\n\n            # Compile regex if flags differ from flags the regex was compiled with\n            if not pattern.compiled_regex or pattern.compiled_with_flags != flags:\n                pattern.compiled_with_flags = flags\n                pattern.compiled_regex = re.compile(pattern.regex, flags=flags)\n\n            matches = pattern.compiled_regex.finditer(text)\n            match_time = datetime.datetime.now() - match_start_time\n            logger.debug(\n                \"--- match_time[%s]: %.6f seconds\",\n                pattern.name,\n                match_time.total_seconds()\n            )\n\n            for match in matches:\n                start, end = match.span()\n                current_match = text[start:end]\n\n                # Skip empty results\n                if current_match == \"\":\n                    continue\n\n                score = pattern.score\n\n                validation_result = self.validate_result(current_match)\n                description = self.build_regex_explanation(\n                    self.name,\n                    pattern.name,\n                    pattern.regex,\n                    score,\n                    validation_result,\n                    flags,\n                )\n                pattern_result = RecognizerResult(\n                    entity_type=self.supported_entities[0],\n                    start=start,\n                    end=end,\n                    score=score,\n                    analysis_explanation=description,\n                    recognition_metadata={\n                        RecognizerResult.RECOGNIZER_NAME_KEY: self.name,\n                        RecognizerResult.RECOGNIZER_IDENTIFIER_KEY: self.id,\n                    },\n                )\n\n                if validation_result is not None:\n                    if validation_result:\n                        pattern_result.score = EntityRecognizer.MAX_SCORE\n                    else:\n                        pattern_result.score = EntityRecognizer.MIN_SCORE\n\n                invalidation_result = self.invalidate_result(current_match)\n                if invalidation_result is not None and invalidation_result:\n                    pattern_result.score = EntityRecognizer.MIN_SCORE\n\n                if pattern_result.score &gt; EntityRecognizer.MIN_SCORE:\n                    results.append(pattern_result)\n\n                # Update analysis explanation score following validation or invalidation\n                description.score = pattern_result.score\n\n        results = EntityRecognizer.remove_duplicates(results)\n        return results\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"Serialize instance into a dictionary.\"\"\"\n        return_dict = super().to_dict()\n\n        return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n        return_dict[\"deny_list\"] = self.deny_list\n        return_dict[\"context\"] = self.context\n        return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n        del return_dict[\"supported_entities\"]\n\n        return return_dict\n\n    @classmethod\n    def from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n        \"\"\"Create instance from a serialized dict.\"\"\"\n        patterns = entity_recognizer_dict.get(\"patterns\")\n        if patterns:\n            patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n            entity_recognizer_dict[\"patterns\"] = patterns_list\n\n        return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.pattern_recognizer.PatternRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.remote_recognizer.RemoteRecognizer","title":"presidio_analyzer.remote_recognizer.RemoteRecognizer","text":"<p>               Bases: <code>ABC</code>, <code>EntityRecognizer</code></p> <p>A configuration for a recognizer that runs on a different process / remote machine.</p> PARAMETER DESCRIPTION <code>supported_entities</code> <p>A list of entities this recognizer can identify</p> <p> TYPE: <code>List[str]</code> </p> <code>name</code> <p>name of recognizer</p> <p> TYPE: <code>Optional[str]</code> </p> <code>supported_language</code> <p>The language this recognizer can detect entities in</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of this recognizer</p> <p> TYPE: <code>str</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Call an external service for PII detection.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_dict</code> <p>Create EntityRecognizer from a dict input.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> Source code in <code>presidio_analyzer/remote_recognizer.py</code> <pre><code>class RemoteRecognizer(ABC, EntityRecognizer):\n    \"\"\"\n    A configuration for a recognizer that runs on a different process / remote machine.\n\n    :param supported_entities: A list of entities this recognizer can identify\n    :param name: name of recognizer\n    :param supported_language: The language this recognizer can detect entities in\n    :param version: Version of this recognizer\n    \"\"\"\n\n    def __init__(\n        self,\n        supported_entities: List[str],\n        name: Optional[str],\n        supported_language: str,\n        version: str,\n        context: Optional[List[str]] = None,\n    ):\n        super().__init__(\n            supported_entities=supported_entities,\n            name=name,\n            supported_language=supported_language,\n            version=version,\n            context=context,\n        )\n\n    def load(self):  # noqa D102\n        pass\n\n    @abstractmethod\n    def analyze(self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts):  # noqa ANN201\n        \"\"\"\n        Call an external service for PII detection.\n\n        :param text: text to be analyzed\n        :param entities: Entities that should be looked for\n        :param nlp_artifacts: Additional metadata from the NLP engine\n        :return: List of identified PII entities\n        \"\"\"\n\n        # 1. Call the external service.\n        # 2. Translate results into List[RecognizerResult]\n        pass\n\n    @abstractmethod\n    def get_supported_entities(self) -&gt; List[str]:  # noqa D102\n        pass\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.remote_recognizer.RemoteRecognizer.analyze","title":"analyze  <code>abstractmethod</code>","text":"<pre><code>analyze(text: str, entities: List[str], nlp_artifacts: NlpArtifacts)\n</code></pre> <p>Call an external service for PII detection.</p> PARAMETER DESCRIPTION <code>text</code> <p>text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities that should be looked for</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Additional metadata from the NLP engine</p> <p> TYPE: <code>NlpArtifacts</code> </p> RETURNS DESCRIPTION <p>List of identified PII entities</p> Source code in <code>presidio_analyzer/remote_recognizer.py</code> <pre><code>@abstractmethod\ndef analyze(self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts):  # noqa ANN201\n    \"\"\"\n    Call an external service for PII detection.\n\n    :param text: text to be analyzed\n    :param entities: Entities that should be looked for\n    :param nlp_artifacts: Additional metadata from the NLP engine\n    :return: List of identified PII entities\n    \"\"\"\n\n    # 1. Call the external service.\n    # 2. Translate results into List[RecognizerResult]\n    pass\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.remote_recognizer.RemoteRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.remote_recognizer.RemoteRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.remote_recognizer.RemoteRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.remote_recognizer.RemoteRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.remote_recognizer.RemoteRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.remote_recognizer.RemoteRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; EntityRecognizer\n</code></pre> <p>Create EntityRecognizer from a dict input.</p> PARAMETER DESCRIPTION <code>entity_recognizer_dict</code> <p>Dict containing keys and values for instantiation</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.remote_recognizer.RemoteRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.remote_recognizer.RemoteRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#recognizer-registry-modules","title":"Recognizer registry modules","text":""},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistry","title":"presidio_analyzer.recognizer_registry.RecognizerRegistry","text":"<p>Detect, register and hold all recognizers to be used by the analyzer.</p> PARAMETER DESCRIPTION <code>recognizers</code> <p>An optional list of recognizers, that will be available instead of the predefined recognizers</p> <p> TYPE: <code>Optional[Iterable[EntityRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>global_regex_flags</code> <p>regex flags to be used in regex matching, including deny-lists</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>DOTALL | MULTILINE | IGNORECASE</code> </p> <code>supported_languages</code> <p>List of languages supported by this registry.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>add_nlp_recognizer</code> <p>Adding NLP recognizer in accordance with the nlp engine.</p> <code>load_predefined_recognizers</code> <p>Load the existing recognizers into memory.</p> <code>get_nlp_recognizer</code> <p>Return the recognizer leveraging the selected NLP Engine.</p> <code>get_recognizers</code> <p>Return a list of recognizers which supports the specified name and language.</p> <code>add_recognizer</code> <p>Add a new recognizer to the list of recognizers.</p> <code>remove_recognizer</code> <p>Remove a recognizer based on its name.</p> <code>add_pattern_recognizer_from_dict</code> <p>Load a pattern recognizer from a Dict into the recognizer registry.</p> <code>add_recognizers_from_yaml</code> <p>Read YAML file and load recognizers into the recognizer registry.</p> <code>get_supported_entities</code> <p>Return the supported entities by the set of recognizers loaded.</p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry.py</code> <pre><code>class RecognizerRegistry:\n    \"\"\"\n    Detect, register and hold all recognizers to be used by the analyzer.\n\n    :param recognizers: An optional list of recognizers,\n    that will be available instead of the predefined recognizers\n    :param global_regex_flags: regex flags to be used in regex matching,\n    including deny-lists\n    :param supported_languages: List of languages supported by this registry.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        recognizers: Optional[Iterable[EntityRecognizer]] = None,\n        global_regex_flags: Optional[int] = re.DOTALL | re.MULTILINE | re.IGNORECASE,\n        supported_languages: Optional[List[str]] = None,\n    ):\n        if recognizers:\n            self.recognizers = recognizers\n        else:\n            self.recognizers = []\n        self.global_regex_flags = global_regex_flags\n        self.supported_languages = (\n            supported_languages if supported_languages else [\"en\"]\n        )\n\n    def _create_nlp_recognizer(\n        self,\n        nlp_engine: Optional[NlpEngine] = None,\n        supported_language: Optional[str] = None\n    ) -&gt; SpacyRecognizer:\n        nlp_recognizer = self.get_nlp_recognizer(nlp_engine)\n\n        if nlp_engine:\n            return nlp_recognizer(\n                supported_language=supported_language,\n                supported_entities=nlp_engine.get_supported_entities(),\n            )\n\n        return nlp_recognizer(supported_language=supported_language)\n\n    def add_nlp_recognizer(self, nlp_engine: NlpEngine) -&gt; None:\n        \"\"\"\n        Adding NLP recognizer in accordance with the nlp engine.\n\n        :param nlp_engine: The NLP engine.\n        :return: None\n        \"\"\"\n\n        if not nlp_engine:\n            supported_languages = self.supported_languages\n        else:\n            supported_languages = nlp_engine.get_supported_languages()\n\n        self.recognizers.extend(\n            [\n                self._create_nlp_recognizer(\n                    nlp_engine=nlp_engine, supported_language=supported_language\n                )\n                for supported_language in supported_languages\n            ]\n        )\n\n    def load_predefined_recognizers(\n        self, languages: Optional[List[str]] = None, nlp_engine: NlpEngine = None\n    ) -&gt; None:\n        \"\"\"\n        Load the existing recognizers into memory.\n\n        :param languages: List of languages for which to load recognizers\n        :param nlp_engine: The NLP engine to use.\n        :return: None\n        \"\"\"\n\n        registry_configuration = {\"global_regex_flags\": self.global_regex_flags}\n        if languages is not None:\n            registry_configuration[\"supported_languages\"] = languages\n\n        configuration = RecognizerConfigurationLoader.get(\n            registry_configuration=registry_configuration\n        )\n        recognizers = RecognizerListLoader.get(**configuration)\n\n        self.recognizers.extend(recognizers)\n        self.add_nlp_recognizer(nlp_engine=nlp_engine)\n\n    @staticmethod\n    def get_nlp_recognizer(\n        nlp_engine: NlpEngine,\n    ) -&gt; Type[SpacyRecognizer]:\n        \"\"\"Return the recognizer leveraging the selected NLP Engine.\"\"\"\n\n        if isinstance(nlp_engine, StanzaNlpEngine):\n            return StanzaRecognizer\n        if isinstance(nlp_engine, TransformersNlpEngine):\n            return TransformersRecognizer\n        if not nlp_engine or isinstance(nlp_engine, SpacyNlpEngine):\n            return SpacyRecognizer\n        else:\n            logger.warning(\n                \"nlp engine should be either SpacyNlpEngine,\"\n                \"StanzaNlpEngine or TransformersNlpEngine\"\n            )\n            # Returning default\n            return SpacyRecognizer\n\n    def get_recognizers(\n        self,\n        language: str,\n        entities: Optional[List[str]] = None,\n        all_fields: bool = False,\n        ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n    ) -&gt; List[EntityRecognizer]:\n        \"\"\"\n        Return a list of recognizers which supports the specified name and language.\n\n        :param entities: the requested entities\n        :param language: the requested language\n        :param all_fields: a flag to return all fields of a requested language.\n        :param ad_hoc_recognizers: Additional recognizers provided by the user\n        as part of the request\n        :return: A list of the recognizers which supports the supplied entities\n        and language\n        \"\"\"\n        if language is None:\n            raise ValueError(\"No language provided\")\n\n        if entities is None and all_fields is False:\n            raise ValueError(\"No entities provided\")\n\n        all_possible_recognizers = copy.copy(self.recognizers)\n        if ad_hoc_recognizers:\n            all_possible_recognizers.extend(ad_hoc_recognizers)\n\n        # filter out unwanted recognizers\n        to_return = set()\n        if all_fields:\n            to_return = [\n                rec\n                for rec in all_possible_recognizers\n                if language == rec.supported_language\n            ]\n        else:\n            for entity in entities:\n                subset = [\n                    rec\n                    for rec in all_possible_recognizers\n                    if entity in rec.supported_entities\n                    and language == rec.supported_language\n                ]\n\n                if not subset:\n                    logger.warning(\n                        \"Entity %s doesn't have the corresponding\"\n                        \" recognizer in language : %s\",\n                        entity,\n                        language,\n                    )\n                else:\n                    to_return.update(set(subset))\n\n        logger.debug(\n            \"Returning a total of %s recognizers\",\n            str(len(to_return)),\n        )\n\n        if not to_return:\n            raise ValueError(\"No matching recognizers were found to serve the request.\")\n\n        return list(to_return)\n\n    def add_recognizer(self, recognizer: EntityRecognizer) -&gt; None:\n        \"\"\"\n        Add a new recognizer to the list of recognizers.\n\n        :param recognizer: Recognizer to add\n        \"\"\"\n        if not isinstance(recognizer, EntityRecognizer):\n            raise ValueError(\"Input is not of type EntityRecognizer\")\n\n        self.recognizers.append(recognizer)\n\n    def remove_recognizer(\n        self, recognizer_name: str, language: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"\n        Remove a recognizer based on its name.\n\n        :param recognizer_name: Name of recognizer to remove\n        :param language: The supported language of the recognizer to be removed,\n        in case multiple recognizers with the same name are present,\n        and only one should be removed.\n        \"\"\"\n\n        if not language:\n            new_recognizers = [\n                rec for rec in self.recognizers if rec.name != recognizer_name\n            ]\n\n            logger.info(\n                \"Removed %s recognizers which had the name %s\",\n                str(len(self.recognizers) - len(new_recognizers)),\n                recognizer_name,\n            )\n\n        else:\n            new_recognizers = [\n                rec\n                for rec in self.recognizers\n                if rec.name != recognizer_name or rec.supported_language != language\n            ]\n\n            logger.info(\n                \"Removed %s recognizers which had the name %s and language %s\",\n                str(len(self.recognizers) - len(new_recognizers)),\n                recognizer_name,\n                language,\n            )\n\n        self.recognizers = new_recognizers\n\n    def add_pattern_recognizer_from_dict(self, recognizer_dict: Dict) -&gt; None:\n        \"\"\"\n        Load a pattern recognizer from a Dict into the recognizer registry.\n\n        :param recognizer_dict: Dict holding a serialization of an PatternRecognizer\n\n        :example:\n        &gt;&gt;&gt; registry = RecognizerRegistry()\n        &gt;&gt;&gt; recognizer = { \"name\": \"Titles Recognizer\", \"supported_language\": \"de\",\"supported_entity\": \"TITLE\", \"deny_list\": [\"Mr.\",\"Mrs.\"]}\n        &gt;&gt;&gt; registry.add_pattern_recognizer_from_dict(recognizer)\n        \"\"\"  # noqa: E501\n\n        recognizer = PatternRecognizer.from_dict(recognizer_dict)\n        self.add_recognizer(recognizer)\n\n    def add_recognizers_from_yaml(self, yml_path: Union[str, Path]) -&gt; None:\n        r\"\"\"\n        Read YAML file and load recognizers into the recognizer registry.\n\n        See example yaml file here:\n        https://github.com/microsoft/presidio/blob/main/presidio-analyzer/presidio_analyzer/conf/example_recognizers.yaml\n\n        :example:\n        &gt;&gt;&gt; yaml_file = \"recognizers.yaml\"\n        &gt;&gt;&gt; registry = RecognizerRegistry()\n        &gt;&gt;&gt; registry.add_recognizers_from_yaml(yaml_file)\n\n        \"\"\"\n\n        try:\n            with open(yml_path) as stream:\n                yaml_recognizers = yaml.safe_load(stream)\n\n            for yaml_recognizer in yaml_recognizers[\"recognizers\"]:\n                self.add_pattern_recognizer_from_dict(yaml_recognizer)\n        except OSError as io_error:\n            print(f\"Error reading file {yml_path}\")\n            raise io_error\n        except yaml.YAMLError as yaml_error:\n            print(f\"Failed to parse file {yml_path}\")\n            raise yaml_error\n        except TypeError as yaml_error:\n            print(f\"Failed to parse file {yml_path}\")\n            raise yaml_error\n\n    def __instantiate_recognizer(\n        self, recognizer_class: Type[EntityRecognizer], supported_language: str\n    ):\n        \"\"\"\n        Instantiate a recognizer class given type and input.\n\n        :param recognizer_class: Class object of the recognizer\n        :param supported_language: Language this recognizer should support\n        \"\"\"\n\n        inst = recognizer_class(supported_language=supported_language)\n        if isinstance(inst, PatternRecognizer):\n            inst.global_regex_flags = self.global_regex_flags\n        return inst\n\n    def _get_supported_languages(self) -&gt; List[str]:\n        languages = []\n        for rec in self.recognizers:\n            languages.append(rec.supported_language)\n\n        return list(set(languages))\n\n    def get_supported_entities(\n        self, languages: Optional[List[str]] = None\n    ) -&gt; List[str]:\n        \"\"\"\n        Return the supported entities by the set of recognizers loaded.\n\n        :param languages: The languages to get the supported entities for.\n        If languages=None, returns all entities for all languages.\n        \"\"\"\n        if not languages:\n            languages = self._get_supported_languages()\n\n        supported_entities = []\n        for language in languages:\n            recognizers = self.get_recognizers(language=language, all_fields=True)\n\n            for recognizer in recognizers:\n                supported_entities.extend(recognizer.get_supported_entities())\n\n        return list(set(supported_entities))\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistry.add_nlp_recognizer","title":"add_nlp_recognizer","text":"<pre><code>add_nlp_recognizer(nlp_engine: NlpEngine) -&gt; None\n</code></pre> <p>Adding NLP recognizer in accordance with the nlp engine.</p> PARAMETER DESCRIPTION <code>nlp_engine</code> <p>The NLP engine.</p> <p> TYPE: <code>NlpEngine</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry.py</code> <pre><code>def add_nlp_recognizer(self, nlp_engine: NlpEngine) -&gt; None:\n    \"\"\"\n    Adding NLP recognizer in accordance with the nlp engine.\n\n    :param nlp_engine: The NLP engine.\n    :return: None\n    \"\"\"\n\n    if not nlp_engine:\n        supported_languages = self.supported_languages\n    else:\n        supported_languages = nlp_engine.get_supported_languages()\n\n    self.recognizers.extend(\n        [\n            self._create_nlp_recognizer(\n                nlp_engine=nlp_engine, supported_language=supported_language\n            )\n            for supported_language in supported_languages\n        ]\n    )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistry.load_predefined_recognizers","title":"load_predefined_recognizers","text":"<pre><code>load_predefined_recognizers(\n    languages: Optional[List[str]] = None, nlp_engine: NlpEngine = None\n) -&gt; None\n</code></pre> <p>Load the existing recognizers into memory.</p> PARAMETER DESCRIPTION <code>languages</code> <p>List of languages for which to load recognizers</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>nlp_engine</code> <p>The NLP engine to use.</p> <p> TYPE: <code>NlpEngine</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry.py</code> <pre><code>def load_predefined_recognizers(\n    self, languages: Optional[List[str]] = None, nlp_engine: NlpEngine = None\n) -&gt; None:\n    \"\"\"\n    Load the existing recognizers into memory.\n\n    :param languages: List of languages for which to load recognizers\n    :param nlp_engine: The NLP engine to use.\n    :return: None\n    \"\"\"\n\n    registry_configuration = {\"global_regex_flags\": self.global_regex_flags}\n    if languages is not None:\n        registry_configuration[\"supported_languages\"] = languages\n\n    configuration = RecognizerConfigurationLoader.get(\n        registry_configuration=registry_configuration\n    )\n    recognizers = RecognizerListLoader.get(**configuration)\n\n    self.recognizers.extend(recognizers)\n    self.add_nlp_recognizer(nlp_engine=nlp_engine)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistry.get_nlp_recognizer","title":"get_nlp_recognizer  <code>staticmethod</code>","text":"<pre><code>get_nlp_recognizer(nlp_engine: NlpEngine) -&gt; Type[SpacyRecognizer]\n</code></pre> <p>Return the recognizer leveraging the selected NLP Engine.</p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry.py</code> <pre><code>@staticmethod\ndef get_nlp_recognizer(\n    nlp_engine: NlpEngine,\n) -&gt; Type[SpacyRecognizer]:\n    \"\"\"Return the recognizer leveraging the selected NLP Engine.\"\"\"\n\n    if isinstance(nlp_engine, StanzaNlpEngine):\n        return StanzaRecognizer\n    if isinstance(nlp_engine, TransformersNlpEngine):\n        return TransformersRecognizer\n    if not nlp_engine or isinstance(nlp_engine, SpacyNlpEngine):\n        return SpacyRecognizer\n    else:\n        logger.warning(\n            \"nlp engine should be either SpacyNlpEngine,\"\n            \"StanzaNlpEngine or TransformersNlpEngine\"\n        )\n        # Returning default\n        return SpacyRecognizer\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistry.get_recognizers","title":"get_recognizers","text":"<pre><code>get_recognizers(\n    language: str,\n    entities: Optional[List[str]] = None,\n    all_fields: bool = False,\n    ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n) -&gt; List[EntityRecognizer]\n</code></pre> <p>Return a list of recognizers which supports the specified name and language.</p> PARAMETER DESCRIPTION <code>entities</code> <p>the requested entities</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>the requested language</p> <p> TYPE: <code>str</code> </p> <code>all_fields</code> <p>a flag to return all fields of a requested language.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ad_hoc_recognizers</code> <p>Additional recognizers provided by the user as part of the request</p> <p> TYPE: <code>Optional[List[EntityRecognizer]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[EntityRecognizer]</code> <p>A list of the recognizers which supports the supplied entities and language</p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry.py</code> <pre><code>def get_recognizers(\n    self,\n    language: str,\n    entities: Optional[List[str]] = None,\n    all_fields: bool = False,\n    ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n) -&gt; List[EntityRecognizer]:\n    \"\"\"\n    Return a list of recognizers which supports the specified name and language.\n\n    :param entities: the requested entities\n    :param language: the requested language\n    :param all_fields: a flag to return all fields of a requested language.\n    :param ad_hoc_recognizers: Additional recognizers provided by the user\n    as part of the request\n    :return: A list of the recognizers which supports the supplied entities\n    and language\n    \"\"\"\n    if language is None:\n        raise ValueError(\"No language provided\")\n\n    if entities is None and all_fields is False:\n        raise ValueError(\"No entities provided\")\n\n    all_possible_recognizers = copy.copy(self.recognizers)\n    if ad_hoc_recognizers:\n        all_possible_recognizers.extend(ad_hoc_recognizers)\n\n    # filter out unwanted recognizers\n    to_return = set()\n    if all_fields:\n        to_return = [\n            rec\n            for rec in all_possible_recognizers\n            if language == rec.supported_language\n        ]\n    else:\n        for entity in entities:\n            subset = [\n                rec\n                for rec in all_possible_recognizers\n                if entity in rec.supported_entities\n                and language == rec.supported_language\n            ]\n\n            if not subset:\n                logger.warning(\n                    \"Entity %s doesn't have the corresponding\"\n                    \" recognizer in language : %s\",\n                    entity,\n                    language,\n                )\n            else:\n                to_return.update(set(subset))\n\n    logger.debug(\n        \"Returning a total of %s recognizers\",\n        str(len(to_return)),\n    )\n\n    if not to_return:\n        raise ValueError(\"No matching recognizers were found to serve the request.\")\n\n    return list(to_return)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistry.add_recognizer","title":"add_recognizer","text":"<pre><code>add_recognizer(recognizer: EntityRecognizer) -&gt; None\n</code></pre> <p>Add a new recognizer to the list of recognizers.</p> PARAMETER DESCRIPTION <code>recognizer</code> <p>Recognizer to add</p> <p> TYPE: <code>EntityRecognizer</code> </p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry.py</code> <pre><code>def add_recognizer(self, recognizer: EntityRecognizer) -&gt; None:\n    \"\"\"\n    Add a new recognizer to the list of recognizers.\n\n    :param recognizer: Recognizer to add\n    \"\"\"\n    if not isinstance(recognizer, EntityRecognizer):\n        raise ValueError(\"Input is not of type EntityRecognizer\")\n\n    self.recognizers.append(recognizer)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistry.remove_recognizer","title":"remove_recognizer","text":"<pre><code>remove_recognizer(recognizer_name: str, language: Optional[str] = None) -&gt; None\n</code></pre> <p>Remove a recognizer based on its name.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer to remove</p> <p> TYPE: <code>str</code> </p> <code>language</code> <p>The supported language of the recognizer to be removed, in case multiple recognizers with the same name are present, and only one should be removed.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry.py</code> <pre><code>def remove_recognizer(\n    self, recognizer_name: str, language: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Remove a recognizer based on its name.\n\n    :param recognizer_name: Name of recognizer to remove\n    :param language: The supported language of the recognizer to be removed,\n    in case multiple recognizers with the same name are present,\n    and only one should be removed.\n    \"\"\"\n\n    if not language:\n        new_recognizers = [\n            rec for rec in self.recognizers if rec.name != recognizer_name\n        ]\n\n        logger.info(\n            \"Removed %s recognizers which had the name %s\",\n            str(len(self.recognizers) - len(new_recognizers)),\n            recognizer_name,\n        )\n\n    else:\n        new_recognizers = [\n            rec\n            for rec in self.recognizers\n            if rec.name != recognizer_name or rec.supported_language != language\n        ]\n\n        logger.info(\n            \"Removed %s recognizers which had the name %s and language %s\",\n            str(len(self.recognizers) - len(new_recognizers)),\n            recognizer_name,\n            language,\n        )\n\n    self.recognizers = new_recognizers\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistry.add_pattern_recognizer_from_dict","title":"add_pattern_recognizer_from_dict","text":"<pre><code>add_pattern_recognizer_from_dict(recognizer_dict: Dict) -&gt; None\n</code></pre> <p>Load a pattern recognizer from a Dict into the recognizer registry.</p> <p>:example:</p> <p>registry = RecognizerRegistry() recognizer = { \"name\": \"Titles Recognizer\", \"supported_language\": \"de\",\"supported_entity\": \"TITLE\", \"deny_list\": [\"Mr.\",\"Mrs.\"]} registry.add_pattern_recognizer_from_dict(recognizer)</p> PARAMETER DESCRIPTION <code>recognizer_dict</code> <p>Dict holding a serialization of an PatternRecognizer</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry.py</code> <pre><code>def add_pattern_recognizer_from_dict(self, recognizer_dict: Dict) -&gt; None:\n    \"\"\"\n    Load a pattern recognizer from a Dict into the recognizer registry.\n\n    :param recognizer_dict: Dict holding a serialization of an PatternRecognizer\n\n    :example:\n    &gt;&gt;&gt; registry = RecognizerRegistry()\n    &gt;&gt;&gt; recognizer = { \"name\": \"Titles Recognizer\", \"supported_language\": \"de\",\"supported_entity\": \"TITLE\", \"deny_list\": [\"Mr.\",\"Mrs.\"]}\n    &gt;&gt;&gt; registry.add_pattern_recognizer_from_dict(recognizer)\n    \"\"\"  # noqa: E501\n\n    recognizer = PatternRecognizer.from_dict(recognizer_dict)\n    self.add_recognizer(recognizer)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistry.add_recognizers_from_yaml","title":"add_recognizers_from_yaml","text":"<pre><code>add_recognizers_from_yaml(yml_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Read YAML file and load recognizers into the recognizer registry.</p> <p>See example yaml file here: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/presidio_analyzer/conf/example_recognizers.yaml</p> <p>:example:</p> <p>yaml_file = \"recognizers.yaml\" registry = RecognizerRegistry() registry.add_recognizers_from_yaml(yaml_file)</p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry.py</code> <pre><code>def add_recognizers_from_yaml(self, yml_path: Union[str, Path]) -&gt; None:\n    r\"\"\"\n    Read YAML file and load recognizers into the recognizer registry.\n\n    See example yaml file here:\n    https://github.com/microsoft/presidio/blob/main/presidio-analyzer/presidio_analyzer/conf/example_recognizers.yaml\n\n    :example:\n    &gt;&gt;&gt; yaml_file = \"recognizers.yaml\"\n    &gt;&gt;&gt; registry = RecognizerRegistry()\n    &gt;&gt;&gt; registry.add_recognizers_from_yaml(yaml_file)\n\n    \"\"\"\n\n    try:\n        with open(yml_path) as stream:\n            yaml_recognizers = yaml.safe_load(stream)\n\n        for yaml_recognizer in yaml_recognizers[\"recognizers\"]:\n            self.add_pattern_recognizer_from_dict(yaml_recognizer)\n    except OSError as io_error:\n        print(f\"Error reading file {yml_path}\")\n        raise io_error\n    except yaml.YAMLError as yaml_error:\n        print(f\"Failed to parse file {yml_path}\")\n        raise yaml_error\n    except TypeError as yaml_error:\n        print(f\"Failed to parse file {yml_path}\")\n        raise yaml_error\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistry.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities(languages: Optional[List[str]] = None) -&gt; List[str]\n</code></pre> <p>Return the supported entities by the set of recognizers loaded.</p> PARAMETER DESCRIPTION <code>languages</code> <p>The languages to get the supported entities for. If languages=None, returns all entities for all languages.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry.py</code> <pre><code>def get_supported_entities(\n    self, languages: Optional[List[str]] = None\n) -&gt; List[str]:\n    \"\"\"\n    Return the supported entities by the set of recognizers loaded.\n\n    :param languages: The languages to get the supported entities for.\n    If languages=None, returns all entities for all languages.\n    \"\"\"\n    if not languages:\n        languages = self._get_supported_languages()\n\n    supported_entities = []\n    for language in languages:\n        recognizers = self.get_recognizers(language=language, all_fields=True)\n\n        for recognizer in recognizers:\n            supported_entities.extend(recognizer.get_supported_entities())\n\n    return list(set(supported_entities))\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistryProvider","title":"presidio_analyzer.recognizer_registry.RecognizerRegistryProvider","text":"<p>Utility class for loading Recognizer Registry.</p> <p>Use this class to load recognizer registry from a yaml file</p> <p>:example:     {         \"supported_languages\": [\"de\", \"es\"],         \"recognizers\": [             {                 \"name\": \"Zip code Recognizer\",                 \"supported_language\": \"en\",                 \"patterns\": [                     {                         \"name\": \"zip code (weak)\",                         \"regex\": \"(\\b\\d{5}(?:\\-\\d{4})?\\b)\",                         \"score\": 0.01,                     }                 ],                 \"context\": [\"zip\", \"code\"],                 \"supported_entity\": \"ZIP\",             }         ]     }</p> PARAMETER DESCRIPTION <code>conf_file</code> <p>Path to yaml file containing registry configuration</p> <p> TYPE: <code>Optional[Union[Path, str]]</code> DEFAULT: <code>None</code> </p> <code>registry_configuration</code> <p>Dict containing registry configuration</p> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>create_recognizer_registry</code> <p>Create a recognizer registry according to configuration loaded previously.</p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry_provider.py</code> <pre><code>class RecognizerRegistryProvider:\n    r\"\"\"\n    Utility class for loading Recognizer Registry.\n\n    Use this class to load recognizer registry from a yaml file\n\n    :param conf_file: Path to yaml file containing registry configuration\n    :param registry_configuration: Dict containing registry configuration\n    :example:\n        {\n            \"supported_languages\": [\"de\", \"es\"],\n            \"recognizers\": [\n                {\n                    \"name\": \"Zip code Recognizer\",\n                    \"supported_language\": \"en\",\n                    \"patterns\": [\n                        {\n                            \"name\": \"zip code (weak)\",\n                            \"regex\": \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\",\n                            \"score\": 0.01,\n                        }\n                    ],\n                    \"context\": [\"zip\", \"code\"],\n                    \"supported_entity\": \"ZIP\",\n                }\n            ]\n        }\n    \"\"\"\n\n    def __init__(\n        self,\n        conf_file: Optional[Union[Path, str]] = None,\n        registry_configuration: Optional[Dict] = None,\n        nlp_engine: Optional[NlpEngine] = None,\n    ):\n        self.configuration = RecognizerConfigurationLoader.get(\n            conf_file=conf_file, registry_configuration=registry_configuration\n        )\n        self.nlp_engine = nlp_engine\n\n    def create_recognizer_registry(self) -&gt; RecognizerRegistry:\n        \"\"\"Create a recognizer registry according to configuration loaded previously.\"\"\"\n        supported_languages = self.configuration.get(\"supported_languages\")\n        global_regex_flags = self.configuration.get(\"global_regex_flags\")\n        recognizers_conf = self.configuration.get(\"recognizers\")\n        recognizers = RecognizerListLoader.get(\n            recognizers_conf,\n            supported_languages,\n            global_regex_flags,\n        )\n\n        recognizers = list(recognizers)\n\n        self.__update_based_on_nlp_recognizer_conf(\n            recognizers, recognizers_conf, supported_languages\n        )\n\n        registry = RecognizerRegistry(\n            recognizers=recognizers,\n            supported_languages=supported_languages,\n            global_regex_flags=global_regex_flags,\n        )\n\n        return registry\n\n    def __update_based_on_nlp_recognizer_conf(\n        self,\n        recognizers: List[EntityRecognizer],\n        recognizers_conf: Optional[Dict],\n        supported_languages: List[str],\n    ) -&gt; None:\n        \"\"\"Update the list of recognizers based on the NLP recognizer configuration.\n\n        The method adds the NLP recognizer to the list of recognizers\n        if it is not already present,\n        or removes it if it is not enabled in the configuration.\n        Furthermore, it checks if there are\n        any inconsistencies in configuration. For example:\n        - Multiple enabled NLP recognizers in the configuration for one language.\n        - The NLP recognizer in the configuration does not match the Nlp Engine.\n\n        :param recognizers: List of recognizers to update.\n        :param recognizers_conf: Configuration of the recognizers from the YAML file\n        :param supported_languages: List of supported languages.\n\n        :raises ValueError: If there are multiple enabled NLP recognizers\n        in the configuration.\n        :raises ValueError: If the NLP recognizer\n        in the configuration does not match the Nlp Engine.\n        \"\"\"\n        nlp_engine = self.nlp_engine\n\n        if not nlp_engine:\n            return\n\n        for language in nlp_engine.get_supported_languages():\n            self.__update_based_on_nlp_recognizer_conf_and_lang(\n                recognizers=recognizers, nlp_engine=nlp_engine, language=language\n            )\n            self.__remove_disabled_nlp_recognizers(\n                recognizers=recognizers,\n                recognizers_conf=recognizers_conf,\n                language=language,\n            )\n\n    @staticmethod\n    def __update_based_on_nlp_recognizer_conf_and_lang(\n        recognizers: List[EntityRecognizer],\n        nlp_engine: NlpEngine,\n        language: str,\n    ):\n        \"\"\"\n        Update the list of recognizers with nlp recognizers.\n\n        Update based on the NLP recognizer configuration for a specific language.\n        \"\"\"\n\n        nlp_recognizers = [\n            rec\n            for rec in recognizers\n            if isinstance(rec, SpacyRecognizer) and rec.supported_language == language\n        ]\n\n        # Case 1: NLP recognizer is not in the list of recognizers\n        if not nlp_recognizers:\n            warning_text = (\n                f\"NLP recognizer (e.g. SpacyRecognizer, StanzaRecognizer) \"\n                f\"is not in the list of recognizers \"\n                f\"for language {language}. \"\n                f\"Adding the default recognizer to the list.\"\n                f\"If you wish to remove the NLP recognizer, \"\n                f\"define it as `enabled=false`.\"\n            )\n            logger.warning(warning_text)\n            warnings.warn(warning_text)\n            if nlp_engine:\n                nlp_recognizer_cls = RecognizerRegistry.get_nlp_recognizer(\n                    nlp_engine=nlp_engine\n                )\n                recognizers.append(\n                    nlp_recognizer_cls(\n                        supported_language=language,\n                        supported_entities=nlp_engine.get_supported_entities(),\n                    )\n                )\n            else:\n                recognizers.append(SpacyRecognizer(supported_language=language))\n            return\n\n        # Case 2: There are multiple NLP recognizers for this language, throw error\n        if len(nlp_recognizers) &gt; 1:\n            raise ValueError(\n                f\"Multiple NLP recognizers for language {language} \"\n                f\"found in the configuration. \"\n                f\"Please remove the duplicates.\"\n            )\n\n        # Case 3: There is a mismatch between the NLP Engine and the NLP Recognizer\n        nlp_recognizer = nlp_recognizers[0]\n        expected_nlp_recognizer_cls = RecognizerRegistry.get_nlp_recognizer(nlp_engine)\n        if nlp_recognizer.__class__ != expected_nlp_recognizer_cls:\n            raise ValueError(\n                f\"There is a mismatch between the NLP Engine defined \"\n                f\"({nlp_engine.__class__.__name__}),\"\n                f\"and the configured NLP recognizer \"\n                f\"({nlp_recognizer.__class__.__name__}).\"\n                f\"Make sure the NLP recognizer is aligned with the \"\n                f\"NLP engine and that all others are removed/disabled.\"\n            )\n\n    @staticmethod\n    def __remove_disabled_nlp_recognizers(\n        recognizers: List[EntityRecognizer],\n        recognizers_conf: Dict[str, Any],\n        language: str,\n    ):\n        \"\"\"\n        Remove recognizers that are disabled in the configuration.\n\n        Goes through the recognizer conf provided by the user,\n        and checks if a recognizer for a given language is disabled.\n        If yes, it removes it from the recognizers list\n        (as some are not removed in the previous step).\n        \"\"\"\n\n        disabled = [\n            rec_conf\n            for rec_conf in recognizers_conf\n            if not RecognizerListLoader.is_recognizer_enabled(rec_conf)\n        ]\n\n        disabled_rec_names = [\n            RecognizerListLoader.get_recognizer_name(rec) for rec in disabled\n        ]\n\n        if not disabled:\n            return\n\n        disabled_rec_classes = [\n            cls\n            for cls in RecognizerListLoader.get_all_existing_recognizers()\n            if cls.__name__ in disabled_rec_names\n        ]\n\n        lang_recognizers = [\n            rec for rec in recognizers if rec.supported_language == language\n        ]\n\n        for recognizer in lang_recognizers:\n            if type(recognizer) in disabled_rec_classes:\n                recognizers.remove(recognizer)\n                logger.info(\n                    f\"Disabled {recognizer.__class__.__name__} \"\n                    f\"recognizer for language {language}.\"\n                )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.recognizer_registry.RecognizerRegistryProvider.create_recognizer_registry","title":"create_recognizer_registry","text":"<pre><code>create_recognizer_registry() -&gt; RecognizerRegistry\n</code></pre> <p>Create a recognizer registry according to configuration loaded previously.</p> Source code in <code>presidio_analyzer/recognizer_registry/recognizer_registry_provider.py</code> <pre><code>def create_recognizer_registry(self) -&gt; RecognizerRegistry:\n    \"\"\"Create a recognizer registry according to configuration loaded previously.\"\"\"\n    supported_languages = self.configuration.get(\"supported_languages\")\n    global_regex_flags = self.configuration.get(\"global_regex_flags\")\n    recognizers_conf = self.configuration.get(\"recognizers\")\n    recognizers = RecognizerListLoader.get(\n        recognizers_conf,\n        supported_languages,\n        global_regex_flags,\n    )\n\n    recognizers = list(recognizers)\n\n    self.__update_based_on_nlp_recognizer_conf(\n        recognizers, recognizers_conf, supported_languages\n    )\n\n    registry = RecognizerRegistry(\n        recognizers=recognizers,\n        supported_languages=supported_languages,\n        global_regex_flags=global_regex_flags,\n    )\n\n    return registry\n</code></pre>"},{"location":"api/analyzer_python/#context-awareness-modules","title":"Context awareness modules","text":""},{"location":"api/analyzer_python/#presidio_analyzer.context_aware_enhancers","title":"presidio_analyzer.context_aware_enhancers","text":"<p>Context awareness modules.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.context_aware_enhancers.ContextAwareEnhancer","title":"ContextAwareEnhancer","text":"<p>A class representing an abstract context aware enhancer.</p> <p>Context words might enhance confidence score of a recognized entity, ContextAwareEnhancer is an abstract class to be inherited by a context aware enhancer logic.</p> PARAMETER DESCRIPTION <code>context_similarity_factor</code> <p>How much to enhance confidence of match entity</p> <p> TYPE: <code>float</code> </p> <code>min_score_with_context_similarity</code> <p>Minimum confidence score</p> <p> TYPE: <code>float</code> </p> <code>context_prefix_count</code> <p>how many words before the entity to match context</p> <p> TYPE: <code>int</code> </p> <code>context_suffix_count</code> <p>how many words after the entity to match context</p> <p> TYPE: <code>int</code> </p> METHOD DESCRIPTION <code>enhance_using_context</code> <p>Update results in case surrounding words are relevant to the context words.</p> Source code in <code>presidio_analyzer/context_aware_enhancers/context_aware_enhancer.py</code> <pre><code>class ContextAwareEnhancer:\n    \"\"\"\n    A class representing an abstract context aware enhancer.\n\n    Context words might enhance confidence score of a recognized entity,\n    ContextAwareEnhancer is an abstract class to be inherited by a context aware\n    enhancer logic.\n\n    :param context_similarity_factor: How much to enhance confidence of match entity\n    :param min_score_with_context_similarity: Minimum confidence score\n    :param context_prefix_count: how many words before the entity to match context\n    :param context_suffix_count: how many words after the entity to match context\n    \"\"\"\n\n    MIN_SCORE = 0\n    MAX_SCORE = 1.0\n\n    def __init__(\n        self,\n        context_similarity_factor: float,\n        min_score_with_context_similarity: float,\n        context_prefix_count: int,\n        context_suffix_count: int,\n    ):\n        self.context_similarity_factor = context_similarity_factor\n        self.min_score_with_context_similarity = min_score_with_context_similarity\n        self.context_prefix_count = context_prefix_count\n        self.context_suffix_count = context_suffix_count\n\n    @abstractmethod\n    def enhance_using_context(\n        self,\n        text: str,\n        raw_results: List[RecognizerResult],\n        nlp_artifacts: NlpArtifacts,\n        recognizers: List[EntityRecognizer],\n        context: Optional[List[str]] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Update results in case surrounding words are relevant to the context words.\n\n        Using the surrounding words of the actual word matches, look\n        for specific strings that if found contribute to the score\n        of the result, improving the confidence that the match is\n        indeed of that PII entity type\n\n        :param text: The actual text that was analyzed\n        :param raw_results: Recognizer results which didn't take\n                            context into consideration\n        :param nlp_artifacts: The nlp artifacts contains elements\n                              such as lemmatized tokens for better\n                              accuracy of the context enhancement process\n        :param recognizers: the list of recognizers\n        :param context: list of context words\n        \"\"\"\n        return raw_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.context_aware_enhancers.ContextAwareEnhancer.enhance_using_context","title":"enhance_using_context  <code>abstractmethod</code>","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    recognizers: List[EntityRecognizer],\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Update results in case surrounding words are relevant to the context words.</p> <p>Using the surrounding words of the actual word matches, look for specific strings that if found contribute to the score of the result, improving the confidence that the match is indeed of that PII entity type</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_results</code> <p>Recognizer results which didn't take context into consideration</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>recognizers</code> <p>the list of recognizers</p> <p> TYPE: <code>List[EntityRecognizer]</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/context_aware_enhancers/context_aware_enhancer.py</code> <pre><code>@abstractmethod\ndef enhance_using_context(\n    self,\n    text: str,\n    raw_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    recognizers: List[EntityRecognizer],\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Update results in case surrounding words are relevant to the context words.\n\n    Using the surrounding words of the actual word matches, look\n    for specific strings that if found contribute to the score\n    of the result, improving the confidence that the match is\n    indeed of that PII entity type\n\n    :param text: The actual text that was analyzed\n    :param raw_results: Recognizer results which didn't take\n                        context into consideration\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param recognizers: the list of recognizers\n    :param context: list of context words\n    \"\"\"\n    return raw_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.context_aware_enhancers.LemmaContextAwareEnhancer","title":"LemmaContextAwareEnhancer","text":"<p>               Bases: <code>ContextAwareEnhancer</code></p> <p>A class representing a lemma based context aware enhancer logic.</p> <p>Context words might enhance confidence score of a recognized entity, LemmaContextAwareEnhancer is an implementation of Lemma based context aware logic, it compares spacy lemmas of each word in context of the matched entity to given context and the recognizer context words, if matched it enhance the recognized entity confidence score by a given factor.</p> PARAMETER DESCRIPTION <code>context_similarity_factor</code> <p>How much to enhance confidence of match entity</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.35</code> </p> <code>min_score_with_context_similarity</code> <p>Minimum confidence score</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>context_prefix_count</code> <p>how many words before the entity to match context</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>context_suffix_count</code> <p>how many words after the entity to match context</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> METHOD DESCRIPTION <code>enhance_using_context</code> <p>Update results in case the lemmas of surrounding words or input context</p> Source code in <code>presidio_analyzer/context_aware_enhancers/lemma_context_aware_enhancer.py</code> <pre><code>class LemmaContextAwareEnhancer(ContextAwareEnhancer):\n    \"\"\"\n    A class representing a lemma based context aware enhancer logic.\n\n    Context words might enhance confidence score of a recognized entity,\n    LemmaContextAwareEnhancer is an implementation of Lemma based context aware logic,\n    it compares spacy lemmas of each word in context of the matched entity to given\n    context and the recognizer context words,\n    if matched it enhance the recognized entity confidence score by a given factor.\n\n    :param context_similarity_factor: How much to enhance confidence of match entity\n    :param min_score_with_context_similarity: Minimum confidence score\n    :param context_prefix_count: how many words before the entity to match context\n    :param context_suffix_count: how many words after the entity to match context\n    \"\"\"\n\n    def __init__(\n        self,\n        context_similarity_factor: float = 0.35,\n        min_score_with_context_similarity: float = 0.4,\n        context_prefix_count: int = 5,\n        context_suffix_count: int = 0,\n    ):\n        super().__init__(\n            context_similarity_factor=context_similarity_factor,\n            min_score_with_context_similarity=min_score_with_context_similarity,\n            context_prefix_count=context_prefix_count,\n            context_suffix_count=context_suffix_count,\n        )\n\n    def enhance_using_context(\n        self,\n        text: str,\n        raw_results: List[RecognizerResult],\n        nlp_artifacts: NlpArtifacts,\n        recognizers: List[EntityRecognizer],\n        context: Optional[List[str]] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Update results in case the lemmas of surrounding words or input context\n        words are identical to the context words.\n\n        Using the surrounding words of the actual word matches, look\n        for specific strings that if found contribute to the score\n        of the result, improving the confidence that the match is\n        indeed of that PII entity type\n\n        :param text: The actual text that was analyzed\n        :param raw_results: Recognizer results which didn't take\n                            context into consideration\n        :param nlp_artifacts: The nlp artifacts contains elements\n                              such as lemmatized tokens for better\n                              accuracy of the context enhancement process\n        :param recognizers: the list of recognizers\n        :param context: list of context words\n        \"\"\"  # noqa D205 D400\n\n        # create a deep copy of the results object, so we can manipulate it\n        results = copy.deepcopy(raw_results)\n\n        # create recognizer context dictionary\n        recognizers_dict = {recognizer.id: recognizer for recognizer in recognizers}\n\n        # Create empty list in None or lowercase all context words in the list\n        if not context:\n            context = []\n        else:\n            context = [word.lower() for word in context]\n\n        # Sanity\n        if nlp_artifacts is None:\n            logger.warning(\"NLP artifacts were not provided\")\n            return results\n\n        for result in results:\n            recognizer = None\n            # get recognizer matching the result, if found.\n            if (\n                result.recognition_metadata\n                and RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n                in result.recognition_metadata.keys()\n            ):\n                recognizer = recognizers_dict.get(\n                    result.recognition_metadata[\n                        RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n                    ]\n                )\n\n            if not recognizer:\n                logger.debug(\n                    \"Recognizer name not found as part of the \"\n                    \"recognition_metadata dict in the RecognizerResult. \"\n                )\n                continue\n\n            # skip recognizer result if the recognizer doesn't support\n            # context enhancement\n            if not recognizer.context:\n                logger.debug(\n                    \"recognizer '%s' does not support context enhancement\",\n                    recognizer.name,\n                )\n                continue\n\n            # skip context enhancement if already boosted by recognizer level\n            if result.recognition_metadata.get(\n                RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY\n            ):\n                logger.debug(\"result score already boosted, skipping\")\n                continue\n\n            # extract lemmatized context from the surrounding of the match\n            word = text[result.start : result.end]\n\n            surrounding_words = self._extract_surrounding_words(\n                nlp_artifacts=nlp_artifacts, word=word, start=result.start\n            )\n\n            # combine other sources of context with surrounding words\n            surrounding_words.extend(context)\n\n            supportive_context_word = self._find_supportive_word_in_context(\n                surrounding_words, recognizer.context\n            )\n            if supportive_context_word != \"\":\n                result.score += self.context_similarity_factor\n                result.score = max(result.score, self.min_score_with_context_similarity)\n                result.score = min(result.score, ContextAwareEnhancer.MAX_SCORE)\n\n                # Update the explainability object with context information\n                # helped to improve the score\n                result.analysis_explanation.set_supportive_context_word(\n                    supportive_context_word\n                )\n                result.analysis_explanation.set_improved_score(result.score)\n        return results\n\n    @staticmethod\n    def _find_supportive_word_in_context(\n        context_list: List[str], recognizer_context_list: List[str]\n    ) -&gt; str:\n        \"\"\"\n        Find words in the text which are relevant for context evaluation.\n\n        A word is considered a supportive context word if there's exact match\n        between a keyword in context_text and any keyword in context_list.\n\n        :param context_list words before and after the matched entity within\n               a specified window size\n        :param recognizer_context_list a list of words considered as\n                context keywords manually specified by the recognizer's author\n        \"\"\"\n        word = \"\"\n        # If the context list is empty, no need to continue\n        if context_list is None or recognizer_context_list is None:\n            return word\n\n        for predefined_context_word in recognizer_context_list:\n            # result == true only if any of the predefined context words\n            # is found exactly or as a substring in any of the collected\n            # context words\n            result = next(\n                (\n                    True\n                    for keyword in context_list\n                    if predefined_context_word in keyword\n                ),\n                False,\n            )\n            if result:\n                logger.debug(\"Found context keyword '%s'\", predefined_context_word)\n                word = predefined_context_word\n                break\n\n        return word\n\n    def _extract_surrounding_words(\n        self, nlp_artifacts: NlpArtifacts, word: str, start: int\n    ) -&gt; List[str]:\n        \"\"\"Extract words surrounding another given word.\n\n        The text from which the context is extracted is given in the nlp\n        doc.\n\n        :param nlp_artifacts: An abstraction layer which holds different\n                              items which are the result of a NLP pipeline\n                              execution on a given text\n        :param word: The word to look for context around\n        :param start: The start index of the word in the original text\n        \"\"\"\n        if not nlp_artifacts.tokens:\n            logger.info(\"Skipping context extraction due to lack of NLP artifacts\")\n            # if there are no nlp artifacts, this is ok, we can\n            # extract context and we return a valid, yet empty\n            # context\n            return [\"\"]\n\n        # Get the already prepared words in the given text, in their\n        # LEMMATIZED version\n        lemmatized_keywords = nlp_artifacts.keywords\n\n        # since the list of tokens is not necessarily aligned\n        # with the actual index of the match, we look for the\n        # token index which corresponds to the match\n        token_index = self._find_index_of_match_token(\n            word, start, nlp_artifacts.tokens, nlp_artifacts.tokens_indices\n        )\n\n        # index i belongs to the PII entity, take the preceding n words\n        # and the successing m words into a context list\n\n        backward_context = self._add_n_words_backward(\n            token_index,\n            self.context_prefix_count,\n            nlp_artifacts.lemmas,\n            lemmatized_keywords,\n        )\n        forward_context = self._add_n_words_forward(\n            token_index,\n            self.context_suffix_count,\n            nlp_artifacts.lemmas,\n            lemmatized_keywords,\n        )\n\n        context_list = []\n        context_list.extend(backward_context)\n        context_list.extend(forward_context)\n        context_list = list(set(context_list))\n        logger.debug(\"Context list is: %s\", \" \".join(context_list))\n        return context_list\n\n    @staticmethod\n    def _find_index_of_match_token(\n        word: str,\n        start: int,\n        tokens,\n        tokens_indices: List[int],  # noqa ANN001\n    ) -&gt; int:\n        found = False\n        # we use the known start index of the original word to find the actual\n        # token at that index, we are not checking for equivalence since the\n        # token might be just a substring of that word (e.g. for phone number\n        # 555-124564 the first token might be just '555' or for a match like '\n        # rocket' the actual token will just be 'rocket' hence the misalignment\n        # of indices)\n        # Note: we are iterating over the original tokens (not the lemmatized)\n        i = -1\n        for i, token in enumerate(tokens, 0):\n            # Either we found a token with the exact location, or\n            # we take a token which its characters indices covers\n            # the index we are looking for.\n            if (tokens_indices[i] == start) or (start &lt; tokens_indices[i] + len(token)):\n                # found the interesting token, the one that around it\n                # we take n words, we save the matching lemma\n                found = True\n                break\n\n        if not found:\n            raise ValueError(\n                \"Did not find word '\" + word + \"' \"\n                \"in the list of tokens although it \"\n                \"is expected to be found\"\n            )\n        return i\n\n    @staticmethod\n    def _add_n_words(\n        index: int,\n        n_words: int,\n        lemmas: List[str],\n        lemmatized_filtered_keywords: List[str],\n        is_backward: bool,\n    ) -&gt; List[str]:\n        \"\"\"\n        Prepare a string of context words.\n\n        Return a list of words which surrounds a lemma at a given index.\n        The words will be collected only if exist in the filtered array\n\n        :param index: index of the lemma that its surrounding words we want\n        :param n_words: number of words to take\n        :param lemmas: array of lemmas\n        :param lemmatized_filtered_keywords: the array of filtered\n               lemmas from the original sentence,\n        :param is_backward: if true take the preceeding words, if false,\n                            take the successing words\n        \"\"\"\n        i = index\n        context_words = []\n        # The entity itself is no interest to us...however we want to\n        # consider it anyway for cases were it is attached with no spaces\n        # to an interesting context word, so we allow it and add 1 to\n        # the number of collected words\n\n        # collect at most n words (in lower case)\n        remaining = n_words + 1\n        while 0 &lt;= i &lt; len(lemmas) and remaining &gt; 0:\n            lower_lemma = lemmas[i].lower()\n            if lower_lemma in lemmatized_filtered_keywords:\n                context_words.append(lower_lemma)\n                remaining -= 1\n            i = i - 1 if is_backward else i + 1\n        return context_words\n\n    def _add_n_words_forward(\n        self,\n        index: int,\n        n_words: int,\n        lemmas: List[str],\n        lemmatized_filtered_keywords: List[str],\n    ) -&gt; List[str]:\n        return self._add_n_words(\n            index, n_words, lemmas, lemmatized_filtered_keywords, False\n        )\n\n    def _add_n_words_backward(\n        self,\n        index: int,\n        n_words: int,\n        lemmas: List[str],\n        lemmatized_filtered_keywords: List[str],\n    ) -&gt; List[str]:\n        return self._add_n_words(\n            index, n_words, lemmas, lemmatized_filtered_keywords, True\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.context_aware_enhancers.LemmaContextAwareEnhancer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    recognizers: List[EntityRecognizer],\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Update results in case the lemmas of surrounding words or input context words are identical to the context words.</p> <p>Using the surrounding words of the actual word matches, look for specific strings that if found contribute to the score of the result, improving the confidence that the match is indeed of that PII entity type</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_results</code> <p>Recognizer results which didn't take context into consideration</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>recognizers</code> <p>the list of recognizers</p> <p> TYPE: <code>List[EntityRecognizer]</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/context_aware_enhancers/lemma_context_aware_enhancer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    recognizers: List[EntityRecognizer],\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Update results in case the lemmas of surrounding words or input context\n    words are identical to the context words.\n\n    Using the surrounding words of the actual word matches, look\n    for specific strings that if found contribute to the score\n    of the result, improving the confidence that the match is\n    indeed of that PII entity type\n\n    :param text: The actual text that was analyzed\n    :param raw_results: Recognizer results which didn't take\n                        context into consideration\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param recognizers: the list of recognizers\n    :param context: list of context words\n    \"\"\"  # noqa D205 D400\n\n    # create a deep copy of the results object, so we can manipulate it\n    results = copy.deepcopy(raw_results)\n\n    # create recognizer context dictionary\n    recognizers_dict = {recognizer.id: recognizer for recognizer in recognizers}\n\n    # Create empty list in None or lowercase all context words in the list\n    if not context:\n        context = []\n    else:\n        context = [word.lower() for word in context]\n\n    # Sanity\n    if nlp_artifacts is None:\n        logger.warning(\"NLP artifacts were not provided\")\n        return results\n\n    for result in results:\n        recognizer = None\n        # get recognizer matching the result, if found.\n        if (\n            result.recognition_metadata\n            and RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n            in result.recognition_metadata.keys()\n        ):\n            recognizer = recognizers_dict.get(\n                result.recognition_metadata[\n                    RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n                ]\n            )\n\n        if not recognizer:\n            logger.debug(\n                \"Recognizer name not found as part of the \"\n                \"recognition_metadata dict in the RecognizerResult. \"\n            )\n            continue\n\n        # skip recognizer result if the recognizer doesn't support\n        # context enhancement\n        if not recognizer.context:\n            logger.debug(\n                \"recognizer '%s' does not support context enhancement\",\n                recognizer.name,\n            )\n            continue\n\n        # skip context enhancement if already boosted by recognizer level\n        if result.recognition_metadata.get(\n            RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY\n        ):\n            logger.debug(\"result score already boosted, skipping\")\n            continue\n\n        # extract lemmatized context from the surrounding of the match\n        word = text[result.start : result.end]\n\n        surrounding_words = self._extract_surrounding_words(\n            nlp_artifacts=nlp_artifacts, word=word, start=result.start\n        )\n\n        # combine other sources of context with surrounding words\n        surrounding_words.extend(context)\n\n        supportive_context_word = self._find_supportive_word_in_context(\n            surrounding_words, recognizer.context\n        )\n        if supportive_context_word != \"\":\n            result.score += self.context_similarity_factor\n            result.score = max(result.score, self.min_score_with_context_similarity)\n            result.score = min(result.score, ContextAwareEnhancer.MAX_SCORE)\n\n            # Update the explainability object with context information\n            # helped to improve the score\n            result.analysis_explanation.set_supportive_context_word(\n                supportive_context_word\n            )\n            result.analysis_explanation.set_improved_score(result.score)\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#nlp-engine-modules","title":"NLP Engine modules","text":""},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine","title":"presidio_analyzer.nlp_engine","text":"<p>NLP engine package. Performs text pre-processing.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NerModelConfiguration","title":"NerModelConfiguration  <code>dataclass</code>","text":"<p>NER model configuration.</p> PARAMETER DESCRIPTION <code>labels_to_ignore</code> <p>List of labels to not return predictions for.</p> <p> TYPE: <code>Optional[Collection[str]]</code> DEFAULT: <code>None</code> </p> <code>aggregation_strategy</code> <p>See https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TokenClassificationPipeline.aggregation_strategy</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'max'</code> </p> <code>stride</code> <p>See https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TokenClassificationPipeline.stride</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>14</code> </p> <code>alignment_mode</code> <p>See https://spacy.io/api/doc#char_span</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'expand'</code> </p> <code>default_score</code> <p>Default confidence score if the model does not provide one.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>0.85</code> </p> <code>model_to_presidio_entity_mapping</code> <p>Mapping between the NER model entities and Presidio entities.</p> <p> TYPE: <code>Optional[Dict[str, str]]</code> DEFAULT: <code>None</code> </p> <code>low_score_entity_names</code> <p>Set of entity names that are likely to have low detection accuracy that should be adjusted.</p> <p> TYPE: <code>Optional[Collection]</code> DEFAULT: <code>None</code> </p> <code>low_confidence_score_multiplier</code> <p>A multiplier for the score given for low_score_entity_names. Multiplier to the score given for low_score_entity_names.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>0.4</code> </p> METHOD DESCRIPTION <code>from_dict</code> <p>Load NLP engine configuration from dict.</p> <code>to_dict</code> <p>Return the configuration as a dict.</p> Source code in <code>presidio_analyzer/nlp_engine/ner_model_configuration.py</code> <pre><code>@dataclass\nclass NerModelConfiguration:\n    \"\"\"NER model configuration.\n\n    :param labels_to_ignore: List of labels to not return predictions for.\n    :param aggregation_strategy:\n    See https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TokenClassificationPipeline.aggregation_strategy\n    :param stride:\n    See https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TokenClassificationPipeline.stride\n    :param alignment_mode: See https://spacy.io/api/doc#char_span\n    :param default_score: Default confidence score if the model does not provide one.\n    :param model_to_presidio_entity_mapping:\n    Mapping between the NER model entities and Presidio entities.\n    :param low_score_entity_names:\n    Set of entity names that are likely to have low detection accuracy that should be adjusted.\n    :param low_confidence_score_multiplier: A multiplier for the score given for low_score_entity_names.\n    Multiplier to the score given for low_score_entity_names.\n    \"\"\"  # noqa E501\n\n    labels_to_ignore: Optional[Collection[str]] = None\n    aggregation_strategy: Optional[str] = \"max\"\n    stride: Optional[int] = 14\n    alignment_mode: Optional[str] = \"expand\"\n    default_score: Optional[float] = 0.85\n    model_to_presidio_entity_mapping: Optional[Dict[str, str]] = None\n    low_score_entity_names: Optional[Collection] = None\n    low_confidence_score_multiplier: Optional[float] = 0.4\n\n    def __post_init__(self):\n        \"\"\"Validate the configuration and set defaults.\"\"\"\n        if self.model_to_presidio_entity_mapping is None:\n            logger.warning(\n                \"model_to_presidio_entity_mapping is missing from configuration, \"\n                \"using default\"\n            )\n            self.model_to_presidio_entity_mapping = MODEL_TO_PRESIDIO_ENTITY_MAPPING\n        if self.low_score_entity_names is None:\n            logger.warning(\n                \"low_score_entity_names is missing from configuration, \" \"using default\"\n            )\n            self.low_score_entity_names = LOW_SCORE_ENTITY_NAMES\n        if self.labels_to_ignore is None:\n            logger.warning(\n                \"labels_to_ignore is missing from configuration, \" \"using default\"\n            )\n            self.labels_to_ignore = {}\n\n    @classmethod\n    def _validate_input(cls, ner_model_configuration_dict: Dict) -&gt; None:\n        key_to_type = {\n            \"labels_to_ignore\": Collection,\n            \"aggregation_strategy\": str,\n            \"alignment_mode\": str,\n            \"model_to_presidio_entity_mapping\": dict,\n            \"low_confidence_score_multiplier\": float,\n            \"low_score_entity_names\": Collection,\n            \"stride\": int,\n        }\n\n        for key, field_type in key_to_type.items():\n            cls.__validate_type(\n                config_dict=ner_model_configuration_dict, key=key, field_type=field_type\n            )\n\n    @staticmethod\n    def __validate_type(config_dict: Dict, key: str, field_type: Type) -&gt; None:\n        if key in config_dict:\n            if not isinstance(config_dict[key], field_type):\n                raise ValueError(f\"{key} must be of type {field_type}\")\n\n    @classmethod\n    def from_dict(cls, nlp_engine_configuration: Dict) -&gt; \"NerModelConfiguration\":\n        \"\"\"Load NLP engine configuration from dict.\n\n        :param nlp_engine_configuration: Dict with the configuration to load.\n        \"\"\"\n        cls._validate_input(nlp_engine_configuration)\n\n        return cls(**nlp_engine_configuration)\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"Return the configuration as a dict.\"\"\"\n        return self.__dict__\n\n    def __str__(self) -&gt; str:  # noqa D105\n        return str(self.to_dict())\n\n    def __repr__(self) -&gt; str:  # noqa D105\n        return str(self)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NerModelConfiguration.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(nlp_engine_configuration: Dict) -&gt; NerModelConfiguration\n</code></pre> <p>Load NLP engine configuration from dict.</p> PARAMETER DESCRIPTION <code>nlp_engine_configuration</code> <p>Dict with the configuration to load.</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/nlp_engine/ner_model_configuration.py</code> <pre><code>@classmethod\ndef from_dict(cls, nlp_engine_configuration: Dict) -&gt; \"NerModelConfiguration\":\n    \"\"\"Load NLP engine configuration from dict.\n\n    :param nlp_engine_configuration: Dict with the configuration to load.\n    \"\"\"\n    cls._validate_input(nlp_engine_configuration)\n\n    return cls(**nlp_engine_configuration)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NerModelConfiguration.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Return the configuration as a dict.</p> Source code in <code>presidio_analyzer/nlp_engine/ner_model_configuration.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Return the configuration as a dict.\"\"\"\n    return self.__dict__\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpArtifacts","title":"NlpArtifacts","text":"<p>NlpArtifacts is an abstraction layer over the results of an NLP pipeline.</p> <p>processing over a given text, it holds attributes such as entities, tokens and lemmas which can be used by any recognizer</p> PARAMETER DESCRIPTION <code>entities</code> <p>Identified entities</p> <p> TYPE: <code>List[Span]</code> </p> <code>tokens</code> <p>Tokenized text</p> <p> TYPE: <code>Doc</code> </p> <code>tokens_indices</code> <p>Indices of tokens</p> <p> TYPE: <code>List[int]</code> </p> <code>lemmas</code> <p>List of lemmas in text</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_engine</code> <p>NlpEngine object</p> <p> TYPE: <code>NlpEngine</code> </p> <code>language</code> <p>Text language</p> <p> TYPE: <code>str</code> </p> <code>scores</code> <p>Entity confidence scores</p> <p> TYPE: <code>Optional[List[float]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>set_keywords</code> <p>Return keywords fpr text.</p> <code>to_json</code> <p>Convert nlp artifacts to json.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_artifacts.py</code> <pre><code>class NlpArtifacts:\n    \"\"\"\n    NlpArtifacts is an abstraction layer over the results of an NLP pipeline.\n\n    processing over a given text, it holds attributes such as entities,\n    tokens and lemmas which can be used by any recognizer\n\n    :param entities: Identified entities\n    :param tokens: Tokenized text\n    :param tokens_indices: Indices of tokens\n    :param lemmas: List of lemmas in text\n    :param nlp_engine: NlpEngine object\n    :param language: Text language\n    :param scores: Entity confidence scores\n    \"\"\"\n\n    def __init__(\n        self,\n        entities: List[Span],\n        tokens: Doc,\n        tokens_indices: List[int],\n        lemmas: List[str],\n        nlp_engine: \"NlpEngine\",  # noqa F821\n        language: str,\n        scores: Optional[List[float]] = None,\n    ):\n        self.entities = entities\n        self.tokens = tokens\n        self.lemmas = lemmas\n        self.tokens_indices = tokens_indices\n        self.keywords = self.set_keywords(nlp_engine, lemmas, language)\n        self.nlp_engine = nlp_engine\n        self.scores = scores if scores else [0.85] * len(entities)\n\n    @staticmethod\n    def set_keywords(\n        nlp_engine,\n        lemmas: List[str],\n        language: str,  # noqa ANN001\n    ) -&gt; List[str]:\n        \"\"\"\n        Return keywords fpr text.\n\n        Extracts lemmas with certain conditions as keywords.\n        \"\"\"\n        if not nlp_engine:\n            return []\n        keywords = [\n            k.lower()\n            for k in lemmas\n            if not nlp_engine.is_stopword(k, language)\n            and not nlp_engine.is_punct(k, language)\n            and k != \"-PRON-\"\n            and k != \"be\"\n        ]\n\n        # best effort, try even further to break tokens into sub tokens,\n        # this can result in reducing false negatives\n        keywords = [i.split(\":\") for i in keywords]\n\n        # splitting the list can, if happened, will result in list of lists,\n        # we flatten the list\n        keywords = [item for sublist in keywords for item in sublist]\n        return keywords\n\n    def to_json(self) -&gt; str:\n        \"\"\"Convert nlp artifacts to json.\"\"\"\n\n        return_dict = self.__dict__.copy()\n\n        # Ignore NLP engine as it's not serializable currently\n        del return_dict[\"nlp_engine\"]\n\n        # Converting spaCy tokens and spans to string as they are not serializable\n        if \"tokens\" in return_dict:\n            return_dict[\"tokens\"] = [token.text for token in self.tokens]\n        if \"entities\" in return_dict:\n            return_dict[\"entities\"] = [entity.text for entity in self.entities]\n        if \"scores\" in return_dict:\n            return_dict[\"scores\"] = [float(score) for score in self.scores]\n\n        return json.dumps(return_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpArtifacts.set_keywords","title":"set_keywords  <code>staticmethod</code>","text":"<pre><code>set_keywords(nlp_engine, lemmas: List[str], language: str) -&gt; List[str]\n</code></pre> <p>Return keywords fpr text.</p> <p>Extracts lemmas with certain conditions as keywords.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_artifacts.py</code> <pre><code>@staticmethod\ndef set_keywords(\n    nlp_engine,\n    lemmas: List[str],\n    language: str,  # noqa ANN001\n) -&gt; List[str]:\n    \"\"\"\n    Return keywords fpr text.\n\n    Extracts lemmas with certain conditions as keywords.\n    \"\"\"\n    if not nlp_engine:\n        return []\n    keywords = [\n        k.lower()\n        for k in lemmas\n        if not nlp_engine.is_stopword(k, language)\n        and not nlp_engine.is_punct(k, language)\n        and k != \"-PRON-\"\n        and k != \"be\"\n    ]\n\n    # best effort, try even further to break tokens into sub tokens,\n    # this can result in reducing false negatives\n    keywords = [i.split(\":\") for i in keywords]\n\n    # splitting the list can, if happened, will result in list of lists,\n    # we flatten the list\n    keywords = [item for sublist in keywords for item in sublist]\n    return keywords\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpArtifacts.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Convert nlp artifacts to json.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_artifacts.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Convert nlp artifacts to json.\"\"\"\n\n    return_dict = self.__dict__.copy()\n\n    # Ignore NLP engine as it's not serializable currently\n    del return_dict[\"nlp_engine\"]\n\n    # Converting spaCy tokens and spans to string as they are not serializable\n    if \"tokens\" in return_dict:\n        return_dict[\"tokens\"] = [token.text for token in self.tokens]\n    if \"entities\" in return_dict:\n        return_dict[\"entities\"] = [entity.text for entity in self.entities]\n    if \"scores\" in return_dict:\n        return_dict[\"scores\"] = [float(score) for score in self.scores]\n\n    return json.dumps(return_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngine","title":"NlpEngine","text":"<p>               Bases: <code>ABC</code></p> <p>NlpEngine is an abstraction layer over the nlp module.</p> <p>It provides NLP preprocessing functionality as well as other queries on tokens.</p> METHOD DESCRIPTION <code>load</code> <p>Load the NLP model.</p> <code>is_loaded</code> <p>Return True if the model is already loaded.</p> <code>process_text</code> <p>Execute the NLP pipeline on the given text and language.</p> <code>process_batch</code> <p>Execute the NLP pipeline on a batch of texts.</p> <code>is_stopword</code> <p>Return true if the given word is a stop word.</p> <code>is_punct</code> <p>Return true if the given word is a punctuation word.</p> <code>get_supported_entities</code> <p>Return the supported entities for this NLP engine.</p> <code>get_supported_languages</code> <p>Return the supported languages for this NLP engine.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine.py</code> <pre><code>class NlpEngine(ABC):\n    \"\"\"\n    NlpEngine is an abstraction layer over the nlp module.\n\n    It provides NLP preprocessing functionality as well as other queries\n    on tokens.\n    \"\"\"\n\n    @abstractmethod\n    def load(self) -&gt; None:\n        \"\"\"Load the NLP model.\"\"\"\n\n    @abstractmethod\n    def is_loaded(self) -&gt; bool:\n        \"\"\"Return True if the model is already loaded.\"\"\"\n\n    @abstractmethod\n    def process_text(self, text: str, language: str) -&gt; NlpArtifacts:\n        \"\"\"Execute the NLP pipeline on the given text and language.\"\"\"\n\n    @abstractmethod\n    def process_batch(\n        self,\n        texts: Iterable[str],\n        language: str,\n        batch_size: int = 1,\n        n_process: int = 1,\n        **kwargs,  # noqa ANN003\n    ) -&gt; Iterator[Tuple[str, NlpArtifacts]]:\n        \"\"\"Execute the NLP pipeline on a batch of texts.\n\n        Returns a tuple of (text, NlpArtifacts)\n        \"\"\"\n\n    @abstractmethod\n    def is_stopword(self, word: str, language: str) -&gt; bool:\n        \"\"\"\n        Return true if the given word is a stop word.\n\n        (within the given language)\n        \"\"\"\n\n    @abstractmethod\n    def is_punct(self, word: str, language: str) -&gt; bool:\n        \"\"\"\n        Return true if the given word is a punctuation word.\n\n        (within the given language)\n        \"\"\"\n\n    @abstractmethod\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"Return the supported entities for this NLP engine.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_supported_languages(self) -&gt; List[str]:\n        \"\"\"Return the supported languages for this NLP engine.\"\"\"\n        pass\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngine.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load() -&gt; None\n</code></pre> <p>Load the NLP model.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine.py</code> <pre><code>@abstractmethod\ndef load(self) -&gt; None:\n    \"\"\"Load the NLP model.\"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngine.is_loaded","title":"is_loaded  <code>abstractmethod</code>","text":"<pre><code>is_loaded() -&gt; bool\n</code></pre> <p>Return True if the model is already loaded.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine.py</code> <pre><code>@abstractmethod\ndef is_loaded(self) -&gt; bool:\n    \"\"\"Return True if the model is already loaded.\"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngine.process_text","title":"process_text  <code>abstractmethod</code>","text":"<pre><code>process_text(text: str, language: str) -&gt; NlpArtifacts\n</code></pre> <p>Execute the NLP pipeline on the given text and language.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine.py</code> <pre><code>@abstractmethod\ndef process_text(self, text: str, language: str) -&gt; NlpArtifacts:\n    \"\"\"Execute the NLP pipeline on the given text and language.\"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngine.process_batch","title":"process_batch  <code>abstractmethod</code>","text":"<pre><code>process_batch(\n    texts: Iterable[str],\n    language: str,\n    batch_size: int = 1,\n    n_process: int = 1,\n    **kwargs\n) -&gt; Iterator[Tuple[str, NlpArtifacts]]\n</code></pre> <p>Execute the NLP pipeline on a batch of texts.</p> <p>Returns a tuple of (text, NlpArtifacts)</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine.py</code> <pre><code>@abstractmethod\ndef process_batch(\n    self,\n    texts: Iterable[str],\n    language: str,\n    batch_size: int = 1,\n    n_process: int = 1,\n    **kwargs,  # noqa ANN003\n) -&gt; Iterator[Tuple[str, NlpArtifacts]]:\n    \"\"\"Execute the NLP pipeline on a batch of texts.\n\n    Returns a tuple of (text, NlpArtifacts)\n    \"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngine.is_stopword","title":"is_stopword  <code>abstractmethod</code>","text":"<pre><code>is_stopword(word: str, language: str) -&gt; bool\n</code></pre> <p>Return true if the given word is a stop word.</p> <p>(within the given language)</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine.py</code> <pre><code>@abstractmethod\ndef is_stopword(self, word: str, language: str) -&gt; bool:\n    \"\"\"\n    Return true if the given word is a stop word.\n\n    (within the given language)\n    \"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngine.is_punct","title":"is_punct  <code>abstractmethod</code>","text":"<pre><code>is_punct(word: str, language: str) -&gt; bool\n</code></pre> <p>Return true if the given word is a punctuation word.</p> <p>(within the given language)</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine.py</code> <pre><code>@abstractmethod\ndef is_punct(self, word: str, language: str) -&gt; bool:\n    \"\"\"\n    Return true if the given word is a punctuation word.\n\n    (within the given language)\n    \"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngine.get_supported_entities","title":"get_supported_entities  <code>abstractmethod</code>","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the supported entities for this NLP engine.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine.py</code> <pre><code>@abstractmethod\ndef get_supported_entities(self) -&gt; List[str]:\n    \"\"\"Return the supported entities for this NLP engine.\"\"\"\n    pass\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngine.get_supported_languages","title":"get_supported_languages  <code>abstractmethod</code>","text":"<pre><code>get_supported_languages() -&gt; List[str]\n</code></pre> <p>Return the supported languages for this NLP engine.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine.py</code> <pre><code>@abstractmethod\ndef get_supported_languages(self) -&gt; List[str]:\n    \"\"\"Return the supported languages for this NLP engine.\"\"\"\n    pass\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.SpacyNlpEngine","title":"SpacyNlpEngine","text":"<p>               Bases: <code>NlpEngine</code></p> <p>SpacyNlpEngine is an abstraction layer over the nlp module.</p> <p>It provides processing functionality as well as other queries on tokens. The SpacyNlpEngine uses SpaCy as its NLP module</p> METHOD DESCRIPTION <code>load</code> <p>Load the spaCy NLP model.</p> <code>get_supported_entities</code> <p>Return the supported entities for this NLP engine.</p> <code>get_supported_languages</code> <p>Return the supported languages for this NLP engine.</p> <code>is_loaded</code> <p>Return True if the model is already loaded.</p> <code>process_text</code> <p>Execute the SpaCy NLP pipeline on the given text and language.</p> <code>process_batch</code> <p>Execute the NLP pipeline on a batch of texts using spacy pipe.</p> <code>is_stopword</code> <p>Return true if the given word is a stop word.</p> <code>is_punct</code> <p>Return true if the given word is a punctuation word.</p> <code>get_nlp</code> <p>Return the language model loaded for a language.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>class SpacyNlpEngine(NlpEngine):\n    \"\"\"\n    SpacyNlpEngine is an abstraction layer over the nlp module.\n\n    It provides processing functionality as well as other queries\n    on tokens.\n    The SpacyNlpEngine uses SpaCy as its NLP module\n    \"\"\"\n\n    engine_name = \"spacy\"\n    is_available = bool(spacy)\n\n    def __init__(\n        self,\n        models: Optional[List[Dict[str, str]]] = None,\n        ner_model_configuration: Optional[NerModelConfiguration] = None,\n    ):\n        \"\"\"\n        Initialize a wrapper on spaCy functionality.\n\n        :param models: Dictionary with the name of the spaCy model per language.\n        For example: models = [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}]\n        :param ner_model_configuration: Parameters for the NER model.\n        See conf/spacy.yaml for an example\n        \"\"\"\n        if not models:\n            models = [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}]\n        self.models = models\n\n        if not ner_model_configuration:\n            ner_model_configuration = NerModelConfiguration()\n        self.ner_model_configuration = ner_model_configuration\n\n        self.nlp = None\n\n    def load(self) -&gt; None:\n        \"\"\"Load the spaCy NLP model.\"\"\"\n        logger.debug(f\"Loading SpaCy models: {self.models}\")\n\n        self.nlp = {}\n        # Download spaCy model if missing\n        for model in self.models:\n            self._validate_model_params(model)\n            self._download_spacy_model_if_needed(model[\"model_name\"])\n            self.nlp[model[\"lang_code\"]] = spacy.load(model[\"model_name\"])\n\n    @staticmethod\n    def _download_spacy_model_if_needed(model_name: str) -&gt; None:\n        if not (spacy.util.is_package(model_name) or Path(model_name).exists()):\n            logger.warning(f\"Model {model_name} is not installed. Downloading...\")\n            spacy.cli.download(model_name)\n            logger.info(f\"Finished downloading model {model_name}\")\n\n    @staticmethod\n    def _validate_model_params(model: Dict) -&gt; None:\n        if \"lang_code\" not in model:\n            raise ValueError(\"lang_code is missing from model configuration\")\n        if \"model_name\" not in model:\n            raise ValueError(\"model_name is missing from model configuration\")\n        if not isinstance(model[\"model_name\"], str):\n            raise ValueError(\"model_name must be a string\")\n\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"Return the supported entities for this NLP engine.\"\"\"\n        if not self.ner_model_configuration.model_to_presidio_entity_mapping:\n            raise ValueError(\n                \"model_to_presidio_entity_mapping is missing from model configuration\"\n            )\n        entities_from_mapping = list(\n            set(self.ner_model_configuration.model_to_presidio_entity_mapping.values())\n        )\n        entities = [\n            ent\n            for ent in entities_from_mapping\n            if ent not in self.ner_model_configuration.labels_to_ignore\n        ]\n        return entities\n\n    def get_supported_languages(self) -&gt; List[str]:\n        \"\"\"Return the supported languages for this NLP engine.\"\"\"\n        if not self.nlp:\n            raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n        return list(self.nlp.keys())\n\n    def is_loaded(self) -&gt; bool:\n        \"\"\"Return True if the model is already loaded.\"\"\"\n        return self.nlp is not None\n\n    def process_text(self, text: str, language: str) -&gt; NlpArtifacts:\n        \"\"\"Execute the SpaCy NLP pipeline on the given text and language.\"\"\"\n        if not self.nlp:\n            raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n\n        doc = self.nlp[language](text)\n        return self._doc_to_nlp_artifact(doc, language)\n\n    def process_batch(\n        self,\n        texts: Union[List[str], List[Tuple[str, object]]],\n        language: str,\n        batch_size: int = 1,\n        n_process: int = 1,\n        as_tuples: bool = False,\n    ) -&gt; Generator[\n        Union[Tuple[Any, NlpArtifacts, Any], Tuple[Any, NlpArtifacts]], Any, None\n    ]:\n        \"\"\"Execute the NLP pipeline on a batch of texts using spacy pipe.\n\n        :param texts: A list of texts to process. if as_tuples is set to True,\n            texts should be a list of tuples (text, context).\n        :param language: The language of the texts.\n        :param batch_size: Default batch size for pipe and evaluate.\n        :param n_process: Number of processors to process texts.\n        :param as_tuples: If set to True, inputs should be a sequence of\n            (text, context) tuples. Output will then be a sequence of\n            (doc, context) tuples. Defaults to False.\n\n        :return: A generator of tuples (text, NlpArtifacts, context) or\n            (text, NlpArtifacts) depending on the value of as_tuples.\n        \"\"\"\n\n        if not self.nlp:\n            raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n\n        if as_tuples:\n            if not all(isinstance(item, tuple) and len(item) == 2 for item in texts):\n                raise ValueError(\n                    \"When 'as_tuples' is True, \"\n                    \"'texts' must be a list of tuples (text, context).\"\n                )\n            texts = ((str(text), context) for text, context in texts)\n        else:\n            texts = (str(text) for text in texts)\n        batch_output = self.nlp[language].pipe(\n            texts, as_tuples=as_tuples, batch_size=batch_size, n_process=n_process\n        )\n        for output in batch_output:\n            if as_tuples:\n                doc, context = output\n                yield doc.text, self._doc_to_nlp_artifact(doc, language), context\n            else:\n                doc = output\n                yield doc.text, self._doc_to_nlp_artifact(doc, language)\n\n    def is_stopword(self, word: str, language: str) -&gt; bool:\n        \"\"\"\n        Return true if the given word is a stop word.\n\n        (within the given language)\n        \"\"\"\n        return self.nlp[language].vocab[word].is_stop\n\n    def is_punct(self, word: str, language: str) -&gt; bool:\n        \"\"\"\n        Return true if the given word is a punctuation word.\n\n        (within the given language).\n        \"\"\"\n        return self.nlp[language].vocab[word].is_punct\n\n    def get_nlp(self, language: str) -&gt; Language:\n        \"\"\"\n        Return the language model loaded for a language.\n\n        :param language: Language\n        :return: Model from spaCy\n        \"\"\"\n        return self.nlp[language]\n\n    def _doc_to_nlp_artifact(self, doc: Doc, language: str) -&gt; NlpArtifacts:\n        lemmas = [token.lemma_ for token in doc]\n        tokens_indices = [token.idx for token in doc]\n\n        entities = self._get_entities(doc)\n        scores = self._get_scores_for_entities(doc)\n\n        entities, scores = self._get_updated_entities(entities, scores)\n\n        return NlpArtifacts(\n            entities=entities,\n            tokens=doc,\n            tokens_indices=tokens_indices,\n            lemmas=lemmas,\n            nlp_engine=self,\n            language=language,\n            scores=scores,\n        )\n\n    def _get_entities(self, doc: Doc) -&gt; List[Span]:\n        \"\"\"\n        Extract entities out of a spaCy pipeline, depending on the type of pipeline.\n\n        For normal spaCy, this would be doc.ents\n        :param doc: the output spaCy doc.\n        :return: List of entities\n        \"\"\"\n\n        return doc.ents\n\n    def _get_scores_for_entities(self, doc: Doc) -&gt; List[float]:\n        \"\"\"Extract scores for entities from the doc.\n\n        Since spaCy does not provide confidence scores for entities by default,\n        we use the default score from the ner model configuration.\n        :param doc: SpaCy doc\n        \"\"\"\n\n        entities = doc.ents\n        scores = [self.ner_model_configuration.default_score] * len(entities)\n        return scores\n\n    def _get_updated_entities(\n        self, entities: List[Span], scores: List[float]\n    ) -&gt; Tuple[List[Span], List[float]]:\n        \"\"\"\n        Get an updated list of entities based on the ner model configuration.\n\n        Remove entities that are in labels_to_ignore,\n        update entity names based on model_to_presidio_entity_mapping\n\n        :param entities: Entities that were extracted from a spaCy pipeline\n        :param scores: Original confidence scores for the entities extracted\n        :return: Tuple holding the entities and confidence scores\n        \"\"\"\n        if len(entities) != len(scores):\n            raise ValueError(\"Entities and scores must be the same length\")\n\n        new_entities = []\n        new_scores = []\n\n        mapping = self.ner_model_configuration.model_to_presidio_entity_mapping\n        to_ignore = self.ner_model_configuration.labels_to_ignore\n        for ent, score in zip(entities, scores):\n            # Remove model labels in the ignore list\n            if ent.label_ in to_ignore:\n                continue\n\n            # Update entity label based on mapping\n            if ent.label_ in mapping:\n                ent.label_ = mapping[ent.label_]\n            else:\n                logger.warning(\n                    f\"Entity {ent.label_} is not mapped to a Presidio entity, \"\n                    f\"but keeping anyway. \"\n                    f\"Add to `NerModelConfiguration.labels_to_ignore` to remove.\"\n                )\n\n            # Remove presidio entities in the ignore list\n            if ent.label_ in to_ignore:\n                continue\n\n            new_entities.append(ent)\n\n            # Update score if entity is in low score entity names\n            if ent.label_ in self.ner_model_configuration.low_score_entity_names:\n                score *= self.ner_model_configuration.low_confidence_score_multiplier\n\n            new_scores.append(score)\n\n        return new_entities, new_scores\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.SpacyNlpEngine.load","title":"load","text":"<pre><code>load() -&gt; None\n</code></pre> <p>Load the spaCy NLP model.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the spaCy NLP model.\"\"\"\n    logger.debug(f\"Loading SpaCy models: {self.models}\")\n\n    self.nlp = {}\n    # Download spaCy model if missing\n    for model in self.models:\n        self._validate_model_params(model)\n        self._download_spacy_model_if_needed(model[\"model_name\"])\n        self.nlp[model[\"lang_code\"]] = spacy.load(model[\"model_name\"])\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.SpacyNlpEngine.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the supported entities for this NLP engine.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"Return the supported entities for this NLP engine.\"\"\"\n    if not self.ner_model_configuration.model_to_presidio_entity_mapping:\n        raise ValueError(\n            \"model_to_presidio_entity_mapping is missing from model configuration\"\n        )\n    entities_from_mapping = list(\n        set(self.ner_model_configuration.model_to_presidio_entity_mapping.values())\n    )\n    entities = [\n        ent\n        for ent in entities_from_mapping\n        if ent not in self.ner_model_configuration.labels_to_ignore\n    ]\n    return entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.SpacyNlpEngine.get_supported_languages","title":"get_supported_languages","text":"<pre><code>get_supported_languages() -&gt; List[str]\n</code></pre> <p>Return the supported languages for this NLP engine.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def get_supported_languages(self) -&gt; List[str]:\n    \"\"\"Return the supported languages for this NLP engine.\"\"\"\n    if not self.nlp:\n        raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n    return list(self.nlp.keys())\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.SpacyNlpEngine.is_loaded","title":"is_loaded","text":"<pre><code>is_loaded() -&gt; bool\n</code></pre> <p>Return True if the model is already loaded.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def is_loaded(self) -&gt; bool:\n    \"\"\"Return True if the model is already loaded.\"\"\"\n    return self.nlp is not None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.SpacyNlpEngine.process_text","title":"process_text","text":"<pre><code>process_text(text: str, language: str) -&gt; NlpArtifacts\n</code></pre> <p>Execute the SpaCy NLP pipeline on the given text and language.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def process_text(self, text: str, language: str) -&gt; NlpArtifacts:\n    \"\"\"Execute the SpaCy NLP pipeline on the given text and language.\"\"\"\n    if not self.nlp:\n        raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n\n    doc = self.nlp[language](text)\n    return self._doc_to_nlp_artifact(doc, language)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.SpacyNlpEngine.process_batch","title":"process_batch","text":"<pre><code>process_batch(\n    texts: Union[List[str], List[Tuple[str, object]]],\n    language: str,\n    batch_size: int = 1,\n    n_process: int = 1,\n    as_tuples: bool = False,\n) -&gt; Generator[\n    Union[Tuple[Any, NlpArtifacts, Any], Tuple[Any, NlpArtifacts]], Any, None\n]\n</code></pre> <p>Execute the NLP pipeline on a batch of texts using spacy pipe.</p> PARAMETER DESCRIPTION <code>texts</code> <p>A list of texts to process. if as_tuples is set to True, texts should be a list of tuples (text, context).</p> <p> TYPE: <code>Union[List[str], List[Tuple[str, object]]]</code> </p> <code>language</code> <p>The language of the texts.</p> <p> TYPE: <code>str</code> </p> <code>batch_size</code> <p>Default batch size for pipe and evaluate.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_process</code> <p>Number of processors to process texts.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>as_tuples</code> <p>If set to True, inputs should be a sequence of (text, context) tuples. Output will then be a sequence of (doc, context) tuples. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Generator[Union[Tuple[Any, NlpArtifacts, Any], Tuple[Any, NlpArtifacts]], Any, None]</code> <p>A generator of tuples (text, NlpArtifacts, context) or (text, NlpArtifacts) depending on the value of as_tuples.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def process_batch(\n    self,\n    texts: Union[List[str], List[Tuple[str, object]]],\n    language: str,\n    batch_size: int = 1,\n    n_process: int = 1,\n    as_tuples: bool = False,\n) -&gt; Generator[\n    Union[Tuple[Any, NlpArtifacts, Any], Tuple[Any, NlpArtifacts]], Any, None\n]:\n    \"\"\"Execute the NLP pipeline on a batch of texts using spacy pipe.\n\n    :param texts: A list of texts to process. if as_tuples is set to True,\n        texts should be a list of tuples (text, context).\n    :param language: The language of the texts.\n    :param batch_size: Default batch size for pipe and evaluate.\n    :param n_process: Number of processors to process texts.\n    :param as_tuples: If set to True, inputs should be a sequence of\n        (text, context) tuples. Output will then be a sequence of\n        (doc, context) tuples. Defaults to False.\n\n    :return: A generator of tuples (text, NlpArtifacts, context) or\n        (text, NlpArtifacts) depending on the value of as_tuples.\n    \"\"\"\n\n    if not self.nlp:\n        raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n\n    if as_tuples:\n        if not all(isinstance(item, tuple) and len(item) == 2 for item in texts):\n            raise ValueError(\n                \"When 'as_tuples' is True, \"\n                \"'texts' must be a list of tuples (text, context).\"\n            )\n        texts = ((str(text), context) for text, context in texts)\n    else:\n        texts = (str(text) for text in texts)\n    batch_output = self.nlp[language].pipe(\n        texts, as_tuples=as_tuples, batch_size=batch_size, n_process=n_process\n    )\n    for output in batch_output:\n        if as_tuples:\n            doc, context = output\n            yield doc.text, self._doc_to_nlp_artifact(doc, language), context\n        else:\n            doc = output\n            yield doc.text, self._doc_to_nlp_artifact(doc, language)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.SpacyNlpEngine.is_stopword","title":"is_stopword","text":"<pre><code>is_stopword(word: str, language: str) -&gt; bool\n</code></pre> <p>Return true if the given word is a stop word.</p> <p>(within the given language)</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def is_stopword(self, word: str, language: str) -&gt; bool:\n    \"\"\"\n    Return true if the given word is a stop word.\n\n    (within the given language)\n    \"\"\"\n    return self.nlp[language].vocab[word].is_stop\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.SpacyNlpEngine.is_punct","title":"is_punct","text":"<pre><code>is_punct(word: str, language: str) -&gt; bool\n</code></pre> <p>Return true if the given word is a punctuation word.</p> <p>(within the given language).</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def is_punct(self, word: str, language: str) -&gt; bool:\n    \"\"\"\n    Return true if the given word is a punctuation word.\n\n    (within the given language).\n    \"\"\"\n    return self.nlp[language].vocab[word].is_punct\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.SpacyNlpEngine.get_nlp","title":"get_nlp","text":"<pre><code>get_nlp(language: str) -&gt; Language\n</code></pre> <p>Return the language model loaded for a language.</p> PARAMETER DESCRIPTION <code>language</code> <p>Language</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Language</code> <p>Model from spaCy</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def get_nlp(self, language: str) -&gt; Language:\n    \"\"\"\n    Return the language model loaded for a language.\n\n    :param language: Language\n    :return: Model from spaCy\n    \"\"\"\n    return self.nlp[language]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.StanzaNlpEngine","title":"StanzaNlpEngine","text":"<p>               Bases: <code>SpacyNlpEngine</code></p> <p>StanzaNlpEngine is an abstraction layer over the nlp module.</p> <p>It provides processing functionality as well as other queries on tokens. The StanzaNlpEngine uses spacy-stanza and stanza as its NLP module</p> PARAMETER DESCRIPTION <code>models</code> <p>Dictionary with the name of the spaCy model per language. For example: models = [{\"lang_code\": \"en\", \"model_name\": \"en\"}]</p> <p> TYPE: <code>Optional[List[Dict[str, str]]]</code> DEFAULT: <code>None</code> </p> <code>ner_model_configuration</code> <p>Parameters for the NER model. See conf/stanza.yaml for an example</p> <p> TYPE: <code>Optional[NerModelConfiguration]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>is_loaded</code> <p>Return True if the model is already loaded.</p> <code>process_text</code> <p>Execute the SpaCy NLP pipeline on the given text and language.</p> <code>process_batch</code> <p>Execute the NLP pipeline on a batch of texts using spacy pipe.</p> <code>is_stopword</code> <p>Return true if the given word is a stop word.</p> <code>is_punct</code> <p>Return true if the given word is a punctuation word.</p> <code>get_supported_entities</code> <p>Return the supported entities for this NLP engine.</p> <code>get_supported_languages</code> <p>Return the supported languages for this NLP engine.</p> <code>get_nlp</code> <p>Return the language model loaded for a language.</p> <code>load</code> <p>Load the NLP model.</p> Source code in <code>presidio_analyzer/nlp_engine/stanza_nlp_engine.py</code> <pre><code>class StanzaNlpEngine(SpacyNlpEngine):\n    \"\"\"\n    StanzaNlpEngine is an abstraction layer over the nlp module.\n\n    It provides processing functionality as well as other queries\n    on tokens.\n    The StanzaNlpEngine uses spacy-stanza and stanza as its NLP module\n\n    :param models: Dictionary with the name of the spaCy model per language.\n    For example: models = [{\"lang_code\": \"en\", \"model_name\": \"en\"}]\n    :param ner_model_configuration: Parameters for the NER model.\n    See conf/stanza.yaml for an example\n\n    \"\"\"\n\n    engine_name = \"stanza\"\n    is_available = bool(stanza)\n\n    def __init__(\n        self,\n        models: Optional[List[Dict[str, str]]] = None,\n        ner_model_configuration: Optional[NerModelConfiguration] = None,\n        download_if_missing: bool = True,\n    ):\n        super().__init__(models, ner_model_configuration)\n        self.download_if_missing = download_if_missing\n\n    def load(self) -&gt; None:\n        \"\"\"Load the NLP model.\"\"\"\n\n        logger.debug(f\"Loading Stanza models: {self.models}\")\n\n        self.nlp = {}\n        for model in self.models:\n            self._validate_model_params(model)\n            self.nlp[model[\"lang_code\"]] = load_pipeline(\n                model[\"model_name\"],\n                processors=\"tokenize,pos,lemma,ner\",\n                download_method=\"DOWNLOAD_RESOURCES\"\n                if self.download_if_missing\n                else None,\n            )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.StanzaNlpEngine.is_loaded","title":"is_loaded","text":"<pre><code>is_loaded() -&gt; bool\n</code></pre> <p>Return True if the model is already loaded.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def is_loaded(self) -&gt; bool:\n    \"\"\"Return True if the model is already loaded.\"\"\"\n    return self.nlp is not None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.StanzaNlpEngine.process_text","title":"process_text","text":"<pre><code>process_text(text: str, language: str) -&gt; NlpArtifacts\n</code></pre> <p>Execute the SpaCy NLP pipeline on the given text and language.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def process_text(self, text: str, language: str) -&gt; NlpArtifacts:\n    \"\"\"Execute the SpaCy NLP pipeline on the given text and language.\"\"\"\n    if not self.nlp:\n        raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n\n    doc = self.nlp[language](text)\n    return self._doc_to_nlp_artifact(doc, language)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.StanzaNlpEngine.process_batch","title":"process_batch","text":"<pre><code>process_batch(\n    texts: Union[List[str], List[Tuple[str, object]]],\n    language: str,\n    batch_size: int = 1,\n    n_process: int = 1,\n    as_tuples: bool = False,\n) -&gt; Generator[\n    Union[Tuple[Any, NlpArtifacts, Any], Tuple[Any, NlpArtifacts]], Any, None\n]\n</code></pre> <p>Execute the NLP pipeline on a batch of texts using spacy pipe.</p> PARAMETER DESCRIPTION <code>texts</code> <p>A list of texts to process. if as_tuples is set to True, texts should be a list of tuples (text, context).</p> <p> TYPE: <code>Union[List[str], List[Tuple[str, object]]]</code> </p> <code>language</code> <p>The language of the texts.</p> <p> TYPE: <code>str</code> </p> <code>batch_size</code> <p>Default batch size for pipe and evaluate.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_process</code> <p>Number of processors to process texts.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>as_tuples</code> <p>If set to True, inputs should be a sequence of (text, context) tuples. Output will then be a sequence of (doc, context) tuples. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Generator[Union[Tuple[Any, NlpArtifacts, Any], Tuple[Any, NlpArtifacts]], Any, None]</code> <p>A generator of tuples (text, NlpArtifacts, context) or (text, NlpArtifacts) depending on the value of as_tuples.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def process_batch(\n    self,\n    texts: Union[List[str], List[Tuple[str, object]]],\n    language: str,\n    batch_size: int = 1,\n    n_process: int = 1,\n    as_tuples: bool = False,\n) -&gt; Generator[\n    Union[Tuple[Any, NlpArtifacts, Any], Tuple[Any, NlpArtifacts]], Any, None\n]:\n    \"\"\"Execute the NLP pipeline on a batch of texts using spacy pipe.\n\n    :param texts: A list of texts to process. if as_tuples is set to True,\n        texts should be a list of tuples (text, context).\n    :param language: The language of the texts.\n    :param batch_size: Default batch size for pipe and evaluate.\n    :param n_process: Number of processors to process texts.\n    :param as_tuples: If set to True, inputs should be a sequence of\n        (text, context) tuples. Output will then be a sequence of\n        (doc, context) tuples. Defaults to False.\n\n    :return: A generator of tuples (text, NlpArtifacts, context) or\n        (text, NlpArtifacts) depending on the value of as_tuples.\n    \"\"\"\n\n    if not self.nlp:\n        raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n\n    if as_tuples:\n        if not all(isinstance(item, tuple) and len(item) == 2 for item in texts):\n            raise ValueError(\n                \"When 'as_tuples' is True, \"\n                \"'texts' must be a list of tuples (text, context).\"\n            )\n        texts = ((str(text), context) for text, context in texts)\n    else:\n        texts = (str(text) for text in texts)\n    batch_output = self.nlp[language].pipe(\n        texts, as_tuples=as_tuples, batch_size=batch_size, n_process=n_process\n    )\n    for output in batch_output:\n        if as_tuples:\n            doc, context = output\n            yield doc.text, self._doc_to_nlp_artifact(doc, language), context\n        else:\n            doc = output\n            yield doc.text, self._doc_to_nlp_artifact(doc, language)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.StanzaNlpEngine.is_stopword","title":"is_stopword","text":"<pre><code>is_stopword(word: str, language: str) -&gt; bool\n</code></pre> <p>Return true if the given word is a stop word.</p> <p>(within the given language)</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def is_stopword(self, word: str, language: str) -&gt; bool:\n    \"\"\"\n    Return true if the given word is a stop word.\n\n    (within the given language)\n    \"\"\"\n    return self.nlp[language].vocab[word].is_stop\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.StanzaNlpEngine.is_punct","title":"is_punct","text":"<pre><code>is_punct(word: str, language: str) -&gt; bool\n</code></pre> <p>Return true if the given word is a punctuation word.</p> <p>(within the given language).</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def is_punct(self, word: str, language: str) -&gt; bool:\n    \"\"\"\n    Return true if the given word is a punctuation word.\n\n    (within the given language).\n    \"\"\"\n    return self.nlp[language].vocab[word].is_punct\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.StanzaNlpEngine.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the supported entities for this NLP engine.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"Return the supported entities for this NLP engine.\"\"\"\n    if not self.ner_model_configuration.model_to_presidio_entity_mapping:\n        raise ValueError(\n            \"model_to_presidio_entity_mapping is missing from model configuration\"\n        )\n    entities_from_mapping = list(\n        set(self.ner_model_configuration.model_to_presidio_entity_mapping.values())\n    )\n    entities = [\n        ent\n        for ent in entities_from_mapping\n        if ent not in self.ner_model_configuration.labels_to_ignore\n    ]\n    return entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.StanzaNlpEngine.get_supported_languages","title":"get_supported_languages","text":"<pre><code>get_supported_languages() -&gt; List[str]\n</code></pre> <p>Return the supported languages for this NLP engine.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def get_supported_languages(self) -&gt; List[str]:\n    \"\"\"Return the supported languages for this NLP engine.\"\"\"\n    if not self.nlp:\n        raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n    return list(self.nlp.keys())\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.StanzaNlpEngine.get_nlp","title":"get_nlp","text":"<pre><code>get_nlp(language: str) -&gt; Language\n</code></pre> <p>Return the language model loaded for a language.</p> PARAMETER DESCRIPTION <code>language</code> <p>Language</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Language</code> <p>Model from spaCy</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def get_nlp(self, language: str) -&gt; Language:\n    \"\"\"\n    Return the language model loaded for a language.\n\n    :param language: Language\n    :return: Model from spaCy\n    \"\"\"\n    return self.nlp[language]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.StanzaNlpEngine.load","title":"load","text":"<pre><code>load() -&gt; None\n</code></pre> <p>Load the NLP model.</p> Source code in <code>presidio_analyzer/nlp_engine/stanza_nlp_engine.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the NLP model.\"\"\"\n\n    logger.debug(f\"Loading Stanza models: {self.models}\")\n\n    self.nlp = {}\n    for model in self.models:\n        self._validate_model_params(model)\n        self.nlp[model[\"lang_code\"]] = load_pipeline(\n            model[\"model_name\"],\n            processors=\"tokenize,pos,lemma,ner\",\n            download_method=\"DOWNLOAD_RESOURCES\"\n            if self.download_if_missing\n            else None,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.TransformersNlpEngine","title":"TransformersNlpEngine","text":"<p>               Bases: <code>SpacyNlpEngine</code></p> <p>TransformersNlpEngine is a transformers based NlpEngine.</p> <p>It comprises a spacy pipeline used for tokenization, lemmatization, pos, and a transformers component for NER.</p> <p>Both the underlying spacy pipeline and the transformers engine could be configured by the user. :example: [{\"lang_code\": \"en\", \"model_name\": {         \"spacy\": \"en_core_web_sm\",         \"transformers\": \"dslim/bert-base-NER\"         } }]</p> PARAMETER DESCRIPTION <code>models</code> <p>A dict holding the model's configuration.</p> <p> TYPE: <code>Optional[List[Dict]]</code> DEFAULT: <code>None</code> </p> <code>ner_model_configuration</code> <p>Parameters for the NER model. See conf/transformers.yaml for an example   Note that since the spaCy model is not used for NER, we recommend using a simple model, such as en_core_web_sm for English. For potential Transformers models, see a list of models here: https://huggingface.co/models?pipeline_tag=token-classification It is further recommended to fine-tune these models to the specific scenario in hand.</p> <p> TYPE: <code>Optional[NerModelConfiguration]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>is_loaded</code> <p>Return True if the model is already loaded.</p> <code>process_text</code> <p>Execute the SpaCy NLP pipeline on the given text and language.</p> <code>process_batch</code> <p>Execute the NLP pipeline on a batch of texts using spacy pipe.</p> <code>is_stopword</code> <p>Return true if the given word is a stop word.</p> <code>is_punct</code> <p>Return true if the given word is a punctuation word.</p> <code>get_supported_entities</code> <p>Return the supported entities for this NLP engine.</p> <code>get_supported_languages</code> <p>Return the supported languages for this NLP engine.</p> <code>get_nlp</code> <p>Return the language model loaded for a language.</p> <code>load</code> <p>Load the spaCy and transformers models.</p> Source code in <code>presidio_analyzer/nlp_engine/transformers_nlp_engine.py</code> <pre><code>class TransformersNlpEngine(SpacyNlpEngine):\n    \"\"\"\n\n    TransformersNlpEngine is a transformers based NlpEngine.\n\n    It comprises a spacy pipeline used for tokenization,\n    lemmatization, pos, and a transformers component for NER.\n\n    Both the underlying spacy pipeline and the transformers engine could be\n    configured by the user.\n    :param models: A dict holding the model's configuration.\n    :example:\n    [{\"lang_code\": \"en\", \"model_name\": {\n            \"spacy\": \"en_core_web_sm\",\n            \"transformers\": \"dslim/bert-base-NER\"\n            }\n    }]\n    :param ner_model_configuration: Parameters for the NER model.\n    See conf/transformers.yaml for an example\n\n\n    Note that since the spaCy model is not used for NER,\n    we recommend using a simple model, such as en_core_web_sm for English.\n    For potential Transformers models, see a list of models here:\n    https://huggingface.co/models?pipeline_tag=token-classification\n    It is further recommended to fine-tune these models\n    to the specific scenario in hand.\n\n    \"\"\"\n\n    engine_name = \"transformers\"\n    is_available = bool(spacy_huggingface_pipelines)\n\n    def __init__(\n        self,\n        models: Optional[List[Dict]] = None,\n        ner_model_configuration: Optional[NerModelConfiguration] = None,\n    ):\n        if not models:\n            models = [\n                {\n                    \"lang_code\": \"en\",\n                    \"model_name\": {\n                        \"spacy\": \"en_core_web_sm\",\n                        \"transformers\": \"obi/deid_roberta_i2b2\",\n                    },\n                }\n            ]\n        super().__init__(models=models, ner_model_configuration=ner_model_configuration)\n        self.entity_key = \"bert-base-ner\"\n\n    def load(self) -&gt; None:\n        \"\"\"Load the spaCy and transformers models.\"\"\"\n\n        logger.debug(f\"Loading SpaCy and transformers models: {self.models}\")\n        self.nlp = {}\n\n        for model in self.models:\n            self._validate_model_params(model)\n            spacy_model = model[\"model_name\"][\"spacy\"]\n            transformers_model = model[\"model_name\"][\"transformers\"]\n            self._download_spacy_model_if_needed(spacy_model)\n\n            nlp = spacy.load(spacy_model, disable=[\"parser\", \"ner\"])\n            nlp.add_pipe(\n                \"hf_token_pipe\",\n                config={\n                    \"model\": transformers_model,\n                    \"annotate\": \"spans\",\n                    \"stride\": self.ner_model_configuration.stride,\n                    \"alignment_mode\": self.ner_model_configuration.alignment_mode,\n                    \"aggregation_strategy\": self.ner_model_configuration.aggregation_strategy,  # noqa E501\n                    \"annotate_spans_key\": self.entity_key,\n                },\n            )\n            self.nlp[model[\"lang_code\"]] = nlp\n\n    @staticmethod\n    def _validate_model_params(model: Dict) -&gt; None:\n        if \"lang_code\" not in model:\n            raise ValueError(\"lang_code is missing from model configuration\")\n        if \"model_name\" not in model:\n            raise ValueError(\"model_name is missing from model configuration\")\n        if not isinstance(model[\"model_name\"], dict):\n            raise ValueError(\"model_name must be a dictionary\")\n        if \"spacy\" not in model[\"model_name\"]:\n            raise ValueError(\"spacy model name is missing from model configuration\")\n        if \"transformers\" not in model[\"model_name\"]:\n            raise ValueError(\n                \"transformers model name is missing from model configuration\"\n            )\n\n    def _get_entities(self, doc: Doc) -&gt; List[Span]:\n        \"\"\"\n        Extract entities out of a spaCy pipeline, depending on the type of pipeline.\n\n        For spacy-huggingface-pipeline, this would be doc.spans[key]\n        :param doc: the output spaCy doc.\n        :return: List of entities\n        \"\"\"\n\n        return doc.spans[self.entity_key]\n\n    def _get_scores_for_entities(self, doc: Doc) -&gt; List[float]:\n        \"\"\"Extract scores for entities from the doc.\n\n        While spaCy does not provide confidence scores,\n        the spacy-huggingface-pipeline flow adds confidence scores\n        as SpanGroup attributes.\n        :param doc: SpaCy doc\n        \"\"\"\n\n        return doc.spans[self.entity_key].attrs[\"scores\"]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.TransformersNlpEngine.is_loaded","title":"is_loaded","text":"<pre><code>is_loaded() -&gt; bool\n</code></pre> <p>Return True if the model is already loaded.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def is_loaded(self) -&gt; bool:\n    \"\"\"Return True if the model is already loaded.\"\"\"\n    return self.nlp is not None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.TransformersNlpEngine.process_text","title":"process_text","text":"<pre><code>process_text(text: str, language: str) -&gt; NlpArtifacts\n</code></pre> <p>Execute the SpaCy NLP pipeline on the given text and language.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def process_text(self, text: str, language: str) -&gt; NlpArtifacts:\n    \"\"\"Execute the SpaCy NLP pipeline on the given text and language.\"\"\"\n    if not self.nlp:\n        raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n\n    doc = self.nlp[language](text)\n    return self._doc_to_nlp_artifact(doc, language)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.TransformersNlpEngine.process_batch","title":"process_batch","text":"<pre><code>process_batch(\n    texts: Union[List[str], List[Tuple[str, object]]],\n    language: str,\n    batch_size: int = 1,\n    n_process: int = 1,\n    as_tuples: bool = False,\n) -&gt; Generator[\n    Union[Tuple[Any, NlpArtifacts, Any], Tuple[Any, NlpArtifacts]], Any, None\n]\n</code></pre> <p>Execute the NLP pipeline on a batch of texts using spacy pipe.</p> PARAMETER DESCRIPTION <code>texts</code> <p>A list of texts to process. if as_tuples is set to True, texts should be a list of tuples (text, context).</p> <p> TYPE: <code>Union[List[str], List[Tuple[str, object]]]</code> </p> <code>language</code> <p>The language of the texts.</p> <p> TYPE: <code>str</code> </p> <code>batch_size</code> <p>Default batch size for pipe and evaluate.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_process</code> <p>Number of processors to process texts.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>as_tuples</code> <p>If set to True, inputs should be a sequence of (text, context) tuples. Output will then be a sequence of (doc, context) tuples. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Generator[Union[Tuple[Any, NlpArtifacts, Any], Tuple[Any, NlpArtifacts]], Any, None]</code> <p>A generator of tuples (text, NlpArtifacts, context) or (text, NlpArtifacts) depending on the value of as_tuples.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def process_batch(\n    self,\n    texts: Union[List[str], List[Tuple[str, object]]],\n    language: str,\n    batch_size: int = 1,\n    n_process: int = 1,\n    as_tuples: bool = False,\n) -&gt; Generator[\n    Union[Tuple[Any, NlpArtifacts, Any], Tuple[Any, NlpArtifacts]], Any, None\n]:\n    \"\"\"Execute the NLP pipeline on a batch of texts using spacy pipe.\n\n    :param texts: A list of texts to process. if as_tuples is set to True,\n        texts should be a list of tuples (text, context).\n    :param language: The language of the texts.\n    :param batch_size: Default batch size for pipe and evaluate.\n    :param n_process: Number of processors to process texts.\n    :param as_tuples: If set to True, inputs should be a sequence of\n        (text, context) tuples. Output will then be a sequence of\n        (doc, context) tuples. Defaults to False.\n\n    :return: A generator of tuples (text, NlpArtifacts, context) or\n        (text, NlpArtifacts) depending on the value of as_tuples.\n    \"\"\"\n\n    if not self.nlp:\n        raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n\n    if as_tuples:\n        if not all(isinstance(item, tuple) and len(item) == 2 for item in texts):\n            raise ValueError(\n                \"When 'as_tuples' is True, \"\n                \"'texts' must be a list of tuples (text, context).\"\n            )\n        texts = ((str(text), context) for text, context in texts)\n    else:\n        texts = (str(text) for text in texts)\n    batch_output = self.nlp[language].pipe(\n        texts, as_tuples=as_tuples, batch_size=batch_size, n_process=n_process\n    )\n    for output in batch_output:\n        if as_tuples:\n            doc, context = output\n            yield doc.text, self._doc_to_nlp_artifact(doc, language), context\n        else:\n            doc = output\n            yield doc.text, self._doc_to_nlp_artifact(doc, language)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.TransformersNlpEngine.is_stopword","title":"is_stopword","text":"<pre><code>is_stopword(word: str, language: str) -&gt; bool\n</code></pre> <p>Return true if the given word is a stop word.</p> <p>(within the given language)</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def is_stopword(self, word: str, language: str) -&gt; bool:\n    \"\"\"\n    Return true if the given word is a stop word.\n\n    (within the given language)\n    \"\"\"\n    return self.nlp[language].vocab[word].is_stop\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.TransformersNlpEngine.is_punct","title":"is_punct","text":"<pre><code>is_punct(word: str, language: str) -&gt; bool\n</code></pre> <p>Return true if the given word is a punctuation word.</p> <p>(within the given language).</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def is_punct(self, word: str, language: str) -&gt; bool:\n    \"\"\"\n    Return true if the given word is a punctuation word.\n\n    (within the given language).\n    \"\"\"\n    return self.nlp[language].vocab[word].is_punct\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.TransformersNlpEngine.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the supported entities for this NLP engine.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"Return the supported entities for this NLP engine.\"\"\"\n    if not self.ner_model_configuration.model_to_presidio_entity_mapping:\n        raise ValueError(\n            \"model_to_presidio_entity_mapping is missing from model configuration\"\n        )\n    entities_from_mapping = list(\n        set(self.ner_model_configuration.model_to_presidio_entity_mapping.values())\n    )\n    entities = [\n        ent\n        for ent in entities_from_mapping\n        if ent not in self.ner_model_configuration.labels_to_ignore\n    ]\n    return entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.TransformersNlpEngine.get_supported_languages","title":"get_supported_languages","text":"<pre><code>get_supported_languages() -&gt; List[str]\n</code></pre> <p>Return the supported languages for this NLP engine.</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def get_supported_languages(self) -&gt; List[str]:\n    \"\"\"Return the supported languages for this NLP engine.\"\"\"\n    if not self.nlp:\n        raise ValueError(\"NLP engine is not loaded. Consider calling .load()\")\n    return list(self.nlp.keys())\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.TransformersNlpEngine.get_nlp","title":"get_nlp","text":"<pre><code>get_nlp(language: str) -&gt; Language\n</code></pre> <p>Return the language model loaded for a language.</p> PARAMETER DESCRIPTION <code>language</code> <p>Language</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Language</code> <p>Model from spaCy</p> Source code in <code>presidio_analyzer/nlp_engine/spacy_nlp_engine.py</code> <pre><code>def get_nlp(self, language: str) -&gt; Language:\n    \"\"\"\n    Return the language model loaded for a language.\n\n    :param language: Language\n    :return: Model from spaCy\n    \"\"\"\n    return self.nlp[language]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.TransformersNlpEngine.load","title":"load","text":"<pre><code>load() -&gt; None\n</code></pre> <p>Load the spaCy and transformers models.</p> Source code in <code>presidio_analyzer/nlp_engine/transformers_nlp_engine.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the spaCy and transformers models.\"\"\"\n\n    logger.debug(f\"Loading SpaCy and transformers models: {self.models}\")\n    self.nlp = {}\n\n    for model in self.models:\n        self._validate_model_params(model)\n        spacy_model = model[\"model_name\"][\"spacy\"]\n        transformers_model = model[\"model_name\"][\"transformers\"]\n        self._download_spacy_model_if_needed(spacy_model)\n\n        nlp = spacy.load(spacy_model, disable=[\"parser\", \"ner\"])\n        nlp.add_pipe(\n            \"hf_token_pipe\",\n            config={\n                \"model\": transformers_model,\n                \"annotate\": \"spans\",\n                \"stride\": self.ner_model_configuration.stride,\n                \"alignment_mode\": self.ner_model_configuration.alignment_mode,\n                \"aggregation_strategy\": self.ner_model_configuration.aggregation_strategy,  # noqa E501\n                \"annotate_spans_key\": self.entity_key,\n            },\n        )\n        self.nlp[model[\"lang_code\"]] = nlp\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngineProvider","title":"NlpEngineProvider","text":"<p>Create different NLP engines from configuration.</p> <p>:example: configuration:         {             \"nlp_engine_name\": \"spacy\",             \"models\": [{\"lang_code\": \"en\",                         \"model_name\": \"en_core_web_lg\"                       }]         } Nlp engine names available by default: spacy, stanza.</p> PARAMETER DESCRIPTION <code>nlp_engines</code> <p>List of available NLP engines. Default: (SpacyNlpEngine, StanzaNlpEngine)</p> <p> TYPE: <code>Optional[Tuple]</code> DEFAULT: <code>None</code> </p> <code>nlp_configuration</code> <p>Dict containing nlp configuration</p> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>None</code> </p> <code>conf_file</code> <p>Path to yaml file containing nlp engine configuration.</p> <p> TYPE: <code>Optional[Union[Path, str]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>create_engine</code> <p>Create an NLP engine instance.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine_provider.py</code> <pre><code>class NlpEngineProvider:\n    \"\"\"Create different NLP engines from configuration.\n\n    :param nlp_engines: List of available NLP engines.\n    Default: (SpacyNlpEngine, StanzaNlpEngine)\n    :param nlp_configuration: Dict containing nlp configuration\n    :example: configuration:\n            {\n                \"nlp_engine_name\": \"spacy\",\n                \"models\": [{\"lang_code\": \"en\",\n                            \"model_name\": \"en_core_web_lg\"\n                          }]\n            }\n    Nlp engine names available by default: spacy, stanza.\n    :param conf_file: Path to yaml file containing nlp engine configuration.\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp_engines: Optional[Tuple] = None,\n        conf_file: Optional[Union[Path, str]] = None,\n        nlp_configuration: Optional[Dict] = None,\n    ):\n        if nlp_engines:\n            self._validate_nlp_engines(nlp_engines)\n        else:\n            nlp_engines = (SpacyNlpEngine, StanzaNlpEngine, TransformersNlpEngine)\n\n        self.nlp_engines = {\n            engine.engine_name: engine for engine in nlp_engines if engine.is_available\n        }\n        logger.debug(\n            f\"Loaded these available nlp engines: {list(self.nlp_engines.keys())}\"\n        )\n\n        if conf_file and nlp_configuration:\n            raise ValueError(\n                \"Either conf_file or nlp_configuration should be provided, not both.\"\n            )\n\n        if nlp_configuration:\n            self._validate_nlp_configuration(nlp_configuration)\n            self.nlp_configuration = nlp_configuration\n\n        if conf_file or conf_file == '':\n            self._validate_conf_file_path(conf_file)\n            self.nlp_configuration = self._read_nlp_conf(conf_file)\n\n        if conf_file is None and nlp_configuration is None:\n            conf_file = self._get_full_conf_path()\n            logger.debug(f\"Reading default conf file from {conf_file}\")\n            self.nlp_configuration = self._read_nlp_conf(conf_file)\n\n    @staticmethod\n    def _validate_nlp_engines(nlp_engines: Tuple) -&gt; None:\n        \"\"\"\n        Validate that all NLP engine classes have the required attributes.\n\n        :param nlp_engines: Tuple of NLP engine classes to validate.\n        \"\"\"\n\n        if not isinstance(nlp_engines, tuple):\n            raise ValueError(f\"nlp_engines must be a tuple, got {type(nlp_engines)}\")\n\n        required_attributes = ['engine_name', 'is_available']\n\n        for engine_class in nlp_engines:\n            missing_attributes = []\n\n            for attr in required_attributes:\n                if not hasattr(engine_class, attr):\n                    missing_attributes.append(attr)\n\n            if missing_attributes:\n                raise ValueError(\n                    f\"NLP engine class {engine_class} is missing required \"\n                    f\"class attributes: {missing_attributes}. \"\n                    \"All NLP engine classes must have 'engine_name' and 'is_available' \"\n                    \"as class attributes.\"\n                )\n\n            if not isinstance(engine_class.engine_name, str):\n                raise ValueError(\n                    f\"NLP engine class {engine_class} has invalid \"\n                    f\"'engine_name' attribute. Expected string, \"\n                    f\"got {type(engine_class.engine_name)}.\"\n                )\n\n            if not isinstance(engine_class.is_available, bool):\n                raise ValueError(\n                    f\"NLP engine class {engine_class} has invalid \"\n                    f\"'is_available' attribute. Expected boolean, \"\n                    f\"got {type(engine_class.is_available)}.\"\n                )\n\n    @staticmethod\n    def _validate_nlp_configuration(nlp_configuration: Dict) -&gt; None:\n        \"\"\"\n        Validate the NLP configuration structure and content.\n\n        :param nlp_configuration: The configuration dictionary to validate\n        \"\"\"\n        if not isinstance(nlp_configuration, Dict):\n            raise ValueError(f\"nlp_configuration must be a dictionary, \"\n                             f\"got {type(nlp_configuration)}\")\n\n        required_fields = ['nlp_engine_name', 'models']\n        missing_fields = []\n\n        for field in required_fields:\n            if field not in nlp_configuration.keys():\n                missing_fields.append(field)\n\n        if missing_fields:\n            raise ValueError(\n                f\"nlp_configuration is missing required fields: {missing_fields}. \"\n                f\"Required fields are: {required_fields}\"\n            )\n\n    @staticmethod\n    def _validate_conf_file_path(conf_file: Union[Path, str]) -&gt; None:\n        \"\"\"\n        Validate the conf file path.\n\n        :param conf_file: The conf file path to validate\n        \"\"\"\n\n        if conf_file == '':\n            raise ValueError(\"conf_file is empty\")\n\n        if not isinstance(conf_file, (Path, str)):\n            raise ValueError(f\"conf_file must be a string or Path, \"\n                             f\"got {type(conf_file)}\")\n\n        if not Path(conf_file).exists():\n            raise ValueError(f\"conf_file {conf_file} does not exist\")\n\n        if Path(conf_file).is_dir():\n            raise ValueError(f\"conf_file {conf_file} is a directory, not a file\")\n\n    def create_engine(self) -&gt; NlpEngine:\n        \"\"\"Create an NLP engine instance.\"\"\"\n        if (\n            not self.nlp_configuration\n            or not self.nlp_configuration.get(\"models\")\n            or not self.nlp_configuration.get(\"nlp_engine_name\")\n        ):\n            raise ValueError(\n                \"Illegal nlp configuration. \"\n                \"Configuration should include nlp_engine_name and models \"\n                \"(list of model_name for each lang_code).\"\n            )\n        nlp_engine_name = self.nlp_configuration[\"nlp_engine_name\"]\n        if nlp_engine_name not in self.nlp_engines:\n            raise ValueError(\n                f\"NLP engine '{nlp_engine_name}' is not available. \"\n                \"Make sure you have all required packages installed\"\n            )\n        try:\n            nlp_engine_class = self.nlp_engines[nlp_engine_name]\n            nlp_models = self.nlp_configuration[\"models\"]\n\n            ner_model_configuration = self.nlp_configuration.get(\n                \"ner_model_configuration\"\n            )\n            if ner_model_configuration:\n                ner_model_configuration = NerModelConfiguration.from_dict(\n                    ner_model_configuration\n                )\n\n            engine = nlp_engine_class(\n                models=nlp_models, ner_model_configuration=ner_model_configuration\n            )\n            engine.load()\n            logger.info(\n                f\"Created NLP engine: {engine.engine_name}. \"\n                f\"Loaded models: {list(engine.nlp.keys())}\"\n            )\n            return engine\n        except KeyError:\n            raise ValueError(\"Wrong NLP engine configuration\")\n\n    @staticmethod\n    def _read_nlp_conf(conf_file: Union[Path, str]) -&gt; dict:\n        \"\"\"\n        Read the nlp configuration from a provided yaml file.\n\n        :param conf_file: The conf file path to read\n        \"\"\"\n\n        with open(conf_file) as file:\n            nlp_configuration = yaml.safe_load(file)\n\n        if \"ner_model_configuration\" not in nlp_configuration:\n            logger.warning(\n                \"configuration file is missing 'ner_model_configuration'. Using default\"\n            )\n\n        return nlp_configuration\n\n    @staticmethod\n    def _get_full_conf_path(\n        default_conf_file: Union[Path, str] = \"default.yaml\",\n    ) -&gt; Path:\n        \"\"\"Return a Path to the default conf file.\"\"\"\n        return Path(Path(__file__).parent.parent, \"conf\", default_conf_file)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.nlp_engine.NlpEngineProvider.create_engine","title":"create_engine","text":"<pre><code>create_engine() -&gt; NlpEngine\n</code></pre> <p>Create an NLP engine instance.</p> Source code in <code>presidio_analyzer/nlp_engine/nlp_engine_provider.py</code> <pre><code>def create_engine(self) -&gt; NlpEngine:\n    \"\"\"Create an NLP engine instance.\"\"\"\n    if (\n        not self.nlp_configuration\n        or not self.nlp_configuration.get(\"models\")\n        or not self.nlp_configuration.get(\"nlp_engine_name\")\n    ):\n        raise ValueError(\n            \"Illegal nlp configuration. \"\n            \"Configuration should include nlp_engine_name and models \"\n            \"(list of model_name for each lang_code).\"\n        )\n    nlp_engine_name = self.nlp_configuration[\"nlp_engine_name\"]\n    if nlp_engine_name not in self.nlp_engines:\n        raise ValueError(\n            f\"NLP engine '{nlp_engine_name}' is not available. \"\n            \"Make sure you have all required packages installed\"\n        )\n    try:\n        nlp_engine_class = self.nlp_engines[nlp_engine_name]\n        nlp_models = self.nlp_configuration[\"models\"]\n\n        ner_model_configuration = self.nlp_configuration.get(\n            \"ner_model_configuration\"\n        )\n        if ner_model_configuration:\n            ner_model_configuration = NerModelConfiguration.from_dict(\n                ner_model_configuration\n            )\n\n        engine = nlp_engine_class(\n            models=nlp_models, ner_model_configuration=ner_model_configuration\n        )\n        engine.load()\n        logger.info(\n            f\"Created NLP engine: {engine.engine_name}. \"\n            f\"Loaded models: {list(engine.nlp.keys())}\"\n        )\n        return engine\n    except KeyError:\n        raise ValueError(\"Wrong NLP engine configuration\")\n</code></pre>"},{"location":"api/analyzer_python/#predefined-recognizers","title":"Predefined Recognizers","text":""},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers","title":"presidio_analyzer.predefined_recognizers","text":"<p>Predefined recognizers package. Holds all the default recognizers.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer","title":"TransformersRecognizer","text":"<p>               Bases: <code>SpacyRecognizer</code></p> <p>Recognize entities using the spacy-huggingface-pipeline package.</p> <p>The recognizer doesn't run transformers models, but loads the output from the NlpArtifacts See:  - https://huggingface.co/docs/transformers/main/en/index for transformer models  - https://github.com/explosion/spacy-huggingface-pipelines on the spaCy wrapper to transformers</p> METHOD DESCRIPTION <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_dict</code> <p>Create EntityRecognizer from a dict input.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>build_explanation</code> <p>Create explanation for why this result was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/nlp_engine_recognizers/transformers_recognizer.py</code> <pre><code>class TransformersRecognizer(SpacyRecognizer):\n    \"\"\"\n    Recognize entities using the spacy-huggingface-pipeline package.\n\n    The recognizer doesn't run transformers models,\n    but loads the output from the NlpArtifacts\n    See:\n     - https://huggingface.co/docs/transformers/main/en/index for transformer models\n     - https://github.com/explosion/spacy-huggingface-pipelines on the spaCy wrapper to transformers\n    \"\"\"  # noqa E501\n\n    ENTITIES = [\n        \"PERSON\",\n        \"LOCATION\",\n        \"ORGANIZATION\",\n        \"AGE\",\n        \"ID\",\n        \"EMAIL\",\n        \"DATE_TIME\",\n        \"PHONE_NUMBER\",\n    ]\n\n    def __init__(self, **kwargs):  # noqa ANN003\n        self.DEFAULT_EXPLANATION = self.DEFAULT_EXPLANATION.replace(\n            \"Spacy\", \"Transformers\"\n        )\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; EntityRecognizer\n</code></pre> <p>Create EntityRecognizer from a dict input.</p> PARAMETER DESCRIPTION <code>entity_recognizer_dict</code> <p>Dict containing keys and values for instantiation</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.TransformersRecognizer.build_explanation","title":"build_explanation","text":"<pre><code>build_explanation(\n    original_score: float, explanation: str\n) -&gt; AnalysisExplanation\n</code></pre> <p>Create explanation for why this result was detected.</p> PARAMETER DESCRIPTION <code>original_score</code> <p>Score given by this recognizer</p> <p> TYPE: <code>float</code> </p> <code>explanation</code> <p>Explanation string</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> Source code in <code>presidio_analyzer/predefined_recognizers/nlp_engine_recognizers/spacy_recognizer.py</code> <pre><code>def build_explanation(\n    self, original_score: float, explanation: str\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Create explanation for why this result was detected.\n\n    :param original_score: Score given by this recognizer\n    :param explanation: Explanation string\n    :return:\n    \"\"\"\n    explanation = AnalysisExplanation(\n        recognizer=self.name,\n        original_score=original_score,\n        textual_explanation=explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer","title":"AuAbnRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes Australian Business Number (\"ABN\").</p> <p>The Australian Business Number (ABN) is a unique 11 digit identifier issued to all entities registered in the Australian Business Register (ABR). The 11 digit ABN is structured as a 9 digit identifier with two leading check digits. The leading check digits are derived using a modulus 89 calculation. This recognizer identifies ABN using regex, context words and checksum. Reference: https://abr.business.gov.au/Help/AbnFormat</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'AU_ABN'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/australia/au_abn_recognizer.py</code> <pre><code>class AuAbnRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes Australian Business Number (\"ABN\").\n\n    The Australian Business Number (ABN) is a unique 11\n    digit identifier issued to all entities registered in\n    the Australian Business Register (ABR).\n    The 11 digit ABN is structured as a 9 digit identifier\n    with two leading check digits.\n    The leading check digits are derived using a modulus 89 calculation.\n    This recognizer identifies ABN using regex, context words and checksum.\n    Reference: https://abr.business.gov.au/Help/AbnFormat\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"ABN (Medium)\",\n            r\"\\b\\d{2}\\s\\d{3}\\s\\d{3}\\s\\d{3}\\b\",\n            0.1,\n        ),\n        Pattern(\n            \"ABN (Low)\",\n            r\"\\b\\d{11}\\b\",\n            0.01,\n        ),\n    ]\n\n    CONTEXT = [\n        \"australian business number\",\n        \"abn\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"AU_ABN\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs if replacement_pairs else [(\"-\", \"\"), (\" \", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the validation was successful.\n        \"\"\"\n        # Pre-processing before validation checks\n        text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n        abn_list = [int(digit) for digit in text if not digit.isspace()]\n\n        # Set weights based on digit position\n        weight = [10, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n\n        # Perform checksums\n        abn_list[0] = 9 if abn_list[0] == 0 else abn_list[0] - 1\n        sum_product = 0\n        for i in range(11):\n            sum_product += abn_list[i] * weight[i]\n        remainder = sum_product % 89\n        return remainder == 0\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAbnRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/australia/au_abn_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    # Pre-processing before validation checks\n    text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n    abn_list = [int(digit) for digit in text if not digit.isspace()]\n\n    # Set weights based on digit position\n    weight = [10, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n\n    # Perform checksums\n    abn_list[0] = 9 if abn_list[0] == 0 else abn_list[0] - 1\n    sum_product = 0\n    for i in range(11):\n        sum_product += abn_list[i] * weight[i]\n    remainder = sum_product % 89\n    return remainder == 0\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer","title":"AuAcnRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes Australian Company Number (\"ACN\").</p> <p>The Australian Company Number (ACN) is a nine digit number with the last digit being a check digit calculated using a modified modulus 10 calculation. This recognizer identifies ACN using regex, context words, and checksum. Reference: https://asic.gov.au/</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'AU_ACN'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/australia/au_acn_recognizer.py</code> <pre><code>class AuAcnRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes Australian Company Number (\"ACN\").\n\n    The Australian Company Number (ACN) is a nine digit number\n    with the last digit being a check digit calculated using a\n    modified modulus 10 calculation.\n    This recognizer identifies ACN using regex, context words, and checksum.\n    Reference: https://asic.gov.au/\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"ACN (Medium)\",\n            r\"\\b\\d{3}\\s\\d{3}\\s\\d{3}\\b\",\n            0.1,\n        ),\n        Pattern(\n            \"ACN (Low)\",\n            r\"\\b\\d{9}\\b\",\n            0.01,\n        ),\n    ]\n\n    CONTEXT = [\n        \"australian company number\",\n        \"acn\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"AU_ACN\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs if replacement_pairs else [(\"-\", \"\"), (\" \", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the validation was successful.\n        \"\"\"\n        # Pre-processing before validation checks\n        text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n        acn_list = [int(digit) for digit in text if not digit.isspace()]\n\n        # Set weights based on digit position\n        weight = [8, 7, 6, 5, 4, 3, 2, 1]\n\n        # Perform checksums\n        sum_product = 0\n        for i in range(8):\n            sum_product += acn_list[i] * weight[i]\n        remainder = sum_product % 10\n        complement = 10 - remainder\n        return complement == acn_list[-1]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuAcnRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/australia/au_acn_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    # Pre-processing before validation checks\n    text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n    acn_list = [int(digit) for digit in text if not digit.isspace()]\n\n    # Set weights based on digit position\n    weight = [8, 7, 6, 5, 4, 3, 2, 1]\n\n    # Perform checksums\n    sum_product = 0\n    for i in range(8):\n        sum_product += acn_list[i] * weight[i]\n    remainder = sum_product % 10\n    complement = 10 - remainder\n    return complement == acn_list[-1]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer","title":"AuMedicareRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes Australian Medicare number using regex, context words, and checksum.</p> <p>Medicare number is a unique identifier issued by Australian Government that enables the cardholder to receive a rebates of medical expenses under Australia's Medicare system. It uses a modulus 10 checksum scheme to validate the number. Reference: https://en.wikipedia.org/wiki/Medicare_card_(Australia)</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'AU_MEDICARE'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/australia/au_medicare_recognizer.py</code> <pre><code>class AuMedicareRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes Australian Medicare number using regex, context words, and checksum.\n\n    Medicare number is a unique identifier issued by Australian Government\n    that enables the cardholder to receive a rebates of medical expenses\n    under Australia's Medicare system.\n    It uses a modulus 10 checksum scheme to validate the number.\n    Reference: https://en.wikipedia.org/wiki/Medicare_card_(Australia)\n\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"Australian Medicare Number (Medium)\",\n            r\"\\b[2-6]\\d{3}\\s\\d{5}\\s\\d\\b\",\n            0.1,\n        ),\n        Pattern(\n            \"Australian Medicare Number (Low)\",\n            r\"\\b[2-6]\\d{9}\\b\",\n            0.01,\n        ),\n    ]\n\n    CONTEXT = [\n        \"medicare\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"AU_MEDICARE\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs if replacement_pairs else [(\"-\", \"\"), (\" \", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the validation was successful.\n        \"\"\"\n        # Pre-processing before validation checks\n        text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n        medicare_list = [int(digit) for digit in text if not digit.isspace()]\n\n        # Set weights based on digit position\n        weight = [1, 3, 7, 9, 1, 3, 7, 9]\n\n        # Perform checksums\n        sum_product = 0\n        for i in range(8):\n            sum_product += medicare_list[i] * weight[i]\n        remainder = sum_product % 10\n        return remainder == medicare_list[8]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuMedicareRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/australia/au_medicare_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    # Pre-processing before validation checks\n    text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n    medicare_list = [int(digit) for digit in text if not digit.isspace()]\n\n    # Set weights based on digit position\n    weight = [1, 3, 7, 9, 1, 3, 7, 9]\n\n    # Perform checksums\n    sum_product = 0\n    for i in range(8):\n        sum_product += medicare_list[i] * weight[i]\n    remainder = sum_product % 10\n    return remainder == medicare_list[8]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer","title":"AuTfnRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes Australian Tax File Numbers (\"TFN\").</p> <p>The tax file number (TFN) is a unique identifier issued by the Australian Taxation Office to each taxpaying entity \u2014 an individual, company, superannuation fund, partnership, or trust. The TFN consists of a nine digit number, usually presented in the format NNN NNN NNN. TFN includes a check digit for detecting erroneous number based on simple modulo 11. This recognizer uses regex, context words, and checksum to identify TFN. Reference: https://www.ato.gov.au/individuals/tax-file-number/</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'AU_TFN'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/australia/au_tfn_recognizer.py</code> <pre><code>class AuTfnRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes Australian Tax File Numbers (\"TFN\").\n\n    The tax file number (TFN) is a unique identifier\n    issued by the Australian Taxation Office\n    to each taxpaying entity \u2014 an individual, company,\n    superannuation fund, partnership, or trust.\n    The TFN consists of a nine digit number, usually\n    presented in the format NNN NNN NNN.\n    TFN includes a check digit for detecting erroneous\n    number based on simple modulo 11.\n    This recognizer uses regex, context words,\n    and checksum to identify TFN.\n    Reference: https://www.ato.gov.au/individuals/tax-file-number/\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"TFN (Medium)\",\n            r\"\\b\\d{3}\\s\\d{3}\\s\\d{3}\\b\",\n            0.1,\n        ),\n        Pattern(\n            \"TFN (Low)\",\n            r\"\\b\\d{9}\\b\",\n            0.01,\n        ),\n    ]\n\n    CONTEXT = [\n        \"tax file number\",\n        \"tfn\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"AU_TFN\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs if replacement_pairs else [(\"-\", \"\"), (\" \", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the validation was successful.\n        \"\"\"\n        # Pre-processing before validation checks\n        text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n        tfn_list = [int(digit) for digit in text if not digit.isspace()]\n\n        # Set weights based on digit position\n        weight = [1, 4, 3, 7, 5, 8, 6, 9, 10]\n\n        # Perform checksums\n        sum_product = 0\n        for i in range(9):\n            sum_product += tfn_list[i] * weight[i]\n        remainder = sum_product % 11\n        return remainder == 0\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AuTfnRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/australia/au_tfn_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    # Pre-processing before validation checks\n    text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n    tfn_list = [int(digit) for digit in text if not digit.isspace()]\n\n    # Set weights based on digit position\n    weight = [1, 4, 3, 7, 5, 8, 6, 9, 10]\n\n    # Perform checksums\n    sum_product = 0\n    for i in range(9):\n        sum_product += tfn_list[i] * weight[i]\n    remainder = sum_product % 11\n    return remainder == 0\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer","title":"FiPersonalIdentityCodeRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes and validates Finnish Personal Identity Codes (Henkil\u00f6tunnus).</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'fi'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'FI_PERSONAL_IDENTITY_CODE'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern by using the control character.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/finland/fi_personal_identity_code_recognizer.py</code> <pre><code>class FiPersonalIdentityCodeRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes and validates Finnish Personal Identity Codes (Henkil\u00f6tunnus).\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"Finnish Personal Identity Code (Medium)\",\n            r\"\\b(\\d{6})([+-ABCDEFYXWVU])(\\d{3})([0123456789ABCDEFHJKLMNPRSTUVWXY])\\b\",\n            0.5,\n        ),\n        Pattern(\n            \"Finnish Personal Identity Code (Very Weak)\",\n            r\"(\\d{6})([+-ABCDEFYXWVU])(\\d{3})([0123456789ABCDEFHJKLMNPRSTUVWXY])\",\n            0.1,\n        ),\n    ]\n    CONTEXT = [\"hetu\", \"henkil\u00f6tunnus\", \"personbeteckningen\", \"personal identity code\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"fi\",\n        supported_entity: str = \"FI_PERSONAL_IDENTITY_CODE\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n        \"\"\"Validate the pattern by using the control character.\"\"\"\n\n        # More information on the validation logic from:\n        # https://dvv.fi/en/personal-identity-code\n        # Under \"How is the control character for a personal identity code calculated?\".\n        if len(pattern_text) != 11:\n            return False\n\n        date_part = pattern_text[0:6]\n        try:\n            # Checking if we do not have invalid dates e.g. 310211.\n            datetime.strptime(date_part, \"%d%m%y\")\n        except ValueError:\n            return False\n        individual_number = pattern_text[7:10]\n        control_character = pattern_text[-1]\n        valid_control_characters = \"0123456789ABCDEFHJKLMNPRSTUVWXY\"\n        number_to_check = int(date_part + individual_number)\n        return valid_control_characters[number_to_check % 31] == control_character\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.FiPersonalIdentityCodeRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern by using the control character.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/finland/fi_personal_identity_code_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"Validate the pattern by using the control character.\"\"\"\n\n    # More information on the validation logic from:\n    # https://dvv.fi/en/personal-identity-code\n    # Under \"How is the control character for a personal identity code calculated?\".\n    if len(pattern_text) != 11:\n        return False\n\n    date_part = pattern_text[0:6]\n    try:\n        # Checking if we do not have invalid dates e.g. 310211.\n        datetime.strptime(date_part, \"%d%m%y\")\n    except ValueError:\n        return False\n    individual_number = pattern_text[7:10]\n    control_character = pattern_text[-1]\n    valid_control_characters = \"0123456789ABCDEFHJKLMNPRSTUVWXY\"\n    number_to_check = int(date_part + individual_number)\n    return valid_control_characters[number_to_check % 31] == control_character\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer","title":"InVehicleRegistrationRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes Indian Vehicle Registration Number issued by RTO.</p> <p>Reference(s): https://en.wikipedia.org/wiki/Vehicle_registration_plates_of_India https://en.wikipedia.org/wiki/Regional_Transport_Office https://en.wikipedia.org/wiki/List_of_Regional_Transport_Office_districts_in_India</p> <p>The registration scheme changed over time with multiple formats in play over the years India has multiple active patterns for registration plates issued to different vehicle categories</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IN_VEHICLE_REGISTRATION'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input e.g. by removing dashes or spaces</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Determine absolute value based on calculation.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/india/in_vehicle_registration_recognizer.py</code> <pre><code>class InVehicleRegistrationRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes Indian Vehicle Registration Number issued by RTO.\n\n    Reference(s):\n    https://en.wikipedia.org/wiki/Vehicle_registration_plates_of_India\n    https://en.wikipedia.org/wiki/Regional_Transport_Office\n    https://en.wikipedia.org/wiki/List_of_Regional_Transport_Office_districts_in_India\n\n    The registration scheme changed over time with multiple formats\n    in play over the years\n    India has multiple active patterns for registration plates issued to different\n    vehicle categories\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n        for different strings to be used during pattern matching.\n    This can allow a greater variety in input e.g. by removing dashes or spaces\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"India Vehicle Registration (Very Weak)\",\n            r\"\\b[A-Z]{1}(?!0000)[0-9]{4}\\b\",\n            0.01,\n        ),\n        Pattern(\n            \"India Vehicle Registration (Very Weak)\",\n            r\"\\b[A-Z]{2}(?!0000)\\d{4}\\b\",\n            0.01,\n        ),\n        Pattern(\n            \"India Vehicle Registration (Very Weak)\",\n            r\"\\b(I)(?!00000)\\d{5}\\b\",\n            0.01,\n        ),\n        Pattern(\n            \"India Vehicle Registration (Weak)\",\n            r\"\\b[A-Z]{3}(?!0000)\\d{4}\\b\",\n            0.20,\n        ),\n        Pattern(\n            \"India Vehicle Registration (Medium)\",\n            r\"\\b\\d{1,3}(CD|CC|UN)[1-9]{1}[0-9]{1,3}\\b\",\n            0.40,\n        ),\n        Pattern(\n            \"India Vehicle Registration\",\n            r\"\\b[A-Z]{2}\\d{1}[A-Z]{1,3}(?!0000)\\d{4}\\b\",\n            0.50,\n        ),\n        Pattern(\n            \"India Vehicle Registration\",\n            r\"\\b[A-Z]{2}\\d{2}[A-Z]{1,2}(?!0000)\\d{4}\\b\",\n            0.50,\n        ),\n        Pattern(\n            \"India Vehicle Registration\",\n            r\"\\b[2-9]{1}[1-9]{1}(BH)(?!0000)\\d{4}[A-HJ-NP-Z]{2}\\b\",\n            0.85,\n        ),\n        Pattern(\n            \"India Vehicle Registration\",\n            r\"\\b(?!00)\\d{2}(A|B|C|D|E|F|H|K|P|R|X)\\d{6}[A-Z]{1}\\b\",\n            0.85,\n        ),\n    ]\n\n    CONTEXT = [\"RTO\", \"vehicle\", \"plate\", \"registration\"]\n\n    # fmt: off\n    in_vehicle_foreign_mission_codes = {\n        84, 85, 89, 93, 94, 95, 97, 98, 99, 102, 104, 105, 106, 109, 111, 112,\n        113, 117, 119, 120, 121, 122, 123, 125, 126, 128, 133, 134, 135, 137,\n        141, 145, 147, 149, 152, 153, 155, 156, 157, 159, 160\n    }\n\n    in_vehicle_armed_forces_codes = {\n        'A', 'B', 'C', 'D', 'E', 'F', 'H', 'K', 'P', 'R', 'X'}\n    in_vehicle_diplomatic_codes = {\"CC\", \"CD\", \"UN\"}\n    in_vehicle_dist_an = {\"01\"}\n    in_vehicle_dist_ap = {\"39\", \"40\"}\n    in_vehicle_dist_ar = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"19\", \"20\", \"22\"\n    }\n    in_vehicle_dist_as = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\"\n    }\n    in_vehicle_dist_br = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"19\",\n        \"21\", \"22\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\",\n        \"34\", \"37\", \"38\", \"39\", \"43\", \"44\", \"45\", \"46\", \"50\", \"51\", \"52\", \"53\",\n        \"55\", \"56\"\n    }\n    in_vehicle_dist_cg = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\",\n        \"25\", \"26\", \"27\", \"28\", \"29\", \"30\"\n    }\n    in_vehicle_dist_ch = {\"01\", \"02\", \"03\", \"04\"}\n    in_vehicle_dist_dd = {\"01\", \"02\", \"03\"}\n    in_vehicle_dist_dn = {\"09\"}  # old list\n    in_vehicle_dist_dl = {\n        \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\"}\n    in_vehicle_dist_ga = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"}\n    in_vehicle_dist_gj = {\n        \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\",\n        \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\"\n    }\n    in_vehicle_dist_hp = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\",\n        \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\", \"61\",\n        \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\",\n        \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\",\n        \"86\", \"87\", \"88\", \"89\", \"90\", \"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\",\n        \"98\", \"99\"\n    }\n    in_vehicle_dist_hr = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\",\n        \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\", \"61\",\n        \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\",\n        \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\",\n        \"86\", \"87\", \"88\", \"89\", \"90\", \"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\",\n        \"98\", \"99\"\n    }\n    in_vehicle_dist_jh = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\"\n    }\n    in_vehicle_dist_jk = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\"\n    }\n    in_vehicle_dist_ka = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\",\n        \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\", \"61\",\n        \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\"\n    }\n    in_vehicle_dist_kl = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\",\n        \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\", \"61\",\n        \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\",\n        \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\",\n        \"86\", \"87\", \"88\", \"89\", \"90\", \"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\",\n        \"98\", \"99\"\n    }\n    in_vehicle_dist_la = {\"01\", \"02\"}\n    in_vehicle_dist_ld = {\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"}\n    in_vehicle_dist_mh = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\",\n        \"50\", \"51\"\n    }\n    in_vehicle_dist_ml = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"}\n    in_vehicle_dist_mn = {\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\"}\n    in_vehicle_dist_mp = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\",\n        \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\", \"61\",\n        \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\"\n    }\n    in_vehicle_dist_mz = {\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"}\n    in_vehicle_dist_nl = {\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"}\n    in_vehicle_dist_od = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\"\n    }\n    in_vehicle_dist_or = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\"\n    }  # old list\n    in_vehicle_dist_pb = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\",\n        \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\", \"61\",\n        \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\",\n        \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\",\n        \"86\", \"87\", \"88\", \"89\", \"90\", \"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\",\n        \"98\", \"99\"\n    }\n    in_vehicle_dist_py = {\"01\", \"02\", \"03\", \"04\", \"05\"}\n    in_vehicle_dist_rj = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\",\n        \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\"\n    }\n    in_vehicle_dist_sk = {\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"}\n    in_vehicle_dist_tn = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\",\n        \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\", \"61\",\n        \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\",\n        \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\",\n        \"86\", \"87\", \"88\", \"89\", \"90\", \"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\",\n        \"98\", \"99\"\n    }\n    in_vehicle_dist_tr = {\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"}\n    in_vehicle_dist_ts = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\"\n    }\n    in_vehicle_dist_uk = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"\n    }\n    in_vehicle_dist_up = {\n        \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\",\n        \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\",\n        \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\",\n        \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\",\n        \"60\", \"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\",\n        \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\",\n        \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\", \"91\", \"92\", \"93\", \"94\", \"95\",\n        \"96\"\n    }\n    in_vehicle_dist_wb = {\n        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"22\", \"23\", \"24\", \"25\",\n        \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\",\n        \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\",\n        \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\", \"61\",\n        \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\",\n        \"74\", \"75\", \"76\", \"77\", \"78\", \"79\", \"80\", \"81\", \"82\", \"83\", \"84\", \"85\",\n        \"86\", \"87\", \"88\", \"89\", \"90\", \"91\", \"92\", \"93\", \"94\", \"95\", \"96\", \"97\",\n        \"98\"\n    }\n    in_union_territories = {\"AN\", \"CH\", \"DH\", \"DL\", \"JK\", \"LA\", \"LD\", \"PY\"}\n    in_old_union_territories = {\"CT\", \"DN\"}\n    in_states = {\n        \"AP\", \"AR\", \"AS\", \"BR\", \"CG\", \"GA\", \"GJ\", \"HR\", \"HP\", \"JH\", \"KA\", \"KL\",\n        \"MP\", \"MH\", \"MN\", \"ML\", \"MZ\", \"NL\", \"OD\", \"PB\", \"RJ\", \"SK\", \"TN\", \"TS\",\n        \"TR\", \"UP\", \"UK\", \"WB\", \"UT\"\n    }\n    in_old_states = {\"UL\", \"OR\", \"UA\"}\n    in_non_standard_state_or_ut = {\"DD\"}\n\n    state_rto_district_map = {\n        \"AN\": in_vehicle_dist_an,\n        \"AP\": in_vehicle_dist_ap,\n        \"AR\": in_vehicle_dist_ar,\n        \"AS\": in_vehicle_dist_as,\n        \"BR\": in_vehicle_dist_br,\n        \"CG\": in_vehicle_dist_cg,\n        \"CH\": in_vehicle_dist_ch,\n        \"DD\": in_vehicle_dist_dd,\n        \"DN\": in_vehicle_dist_dn,\n        \"DL\": in_vehicle_dist_dl,\n        \"GA\": in_vehicle_dist_ga,\n        \"GJ\": in_vehicle_dist_gj,\n        \"HP\": in_vehicle_dist_hp,\n        \"HR\": in_vehicle_dist_hr,\n        \"JH\": in_vehicle_dist_jh,\n        \"JK\": in_vehicle_dist_jk,\n        \"KA\": in_vehicle_dist_ka,\n        \"KL\": in_vehicle_dist_kl,\n        \"LA\": in_vehicle_dist_la,\n        \"LD\": in_vehicle_dist_ld,\n        \"MH\": in_vehicle_dist_mh,\n        \"ML\": in_vehicle_dist_ml,\n        \"MN\": in_vehicle_dist_mn,\n        \"MP\": in_vehicle_dist_mp,\n        \"MZ\": in_vehicle_dist_mz,\n        \"NL\": in_vehicle_dist_nl,\n        \"OD\": in_vehicle_dist_od,\n        \"OR\": in_vehicle_dist_or,\n        \"PB\": in_vehicle_dist_pb,\n        \"PY\": in_vehicle_dist_py,\n        \"RJ\": in_vehicle_dist_rj,\n        \"SK\": in_vehicle_dist_sk,\n        \"TN\": in_vehicle_dist_tn,\n        \"TR\": in_vehicle_dist_tr,\n        \"TS\": in_vehicle_dist_ts,\n        \"UK\": in_vehicle_dist_uk,\n        \"UP\": in_vehicle_dist_up,\n        \"WB\": in_vehicle_dist_wb,\n    }\n\n    two_factor_registration_prefix = set()\n    two_factor_registration_prefix |= in_union_territories\n    two_factor_registration_prefix |= in_states\n    two_factor_registration_prefix |= in_old_states\n    two_factor_registration_prefix |= in_old_union_territories\n    two_factor_registration_prefix |= in_non_standard_state_or_ut\n    # fmt: on\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"IN_VEHICLE_REGISTRATION\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs\n            if replacement_pairs\n            else [(\"-\", \"\"), (\" \", \"\"), (\":\", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"Determine absolute value based on calculation.\"\"\"\n        sanitized_value = EntityRecognizer.sanitize_value(\n            pattern_text, self.replacement_pairs\n        )\n        return self.__check_vehicle_registration(sanitized_value)\n\n    def __check_vehicle_registration(self, sanitized_value: str) -&gt; bool:\n        # print('check function called')\n        is_valid_registration = None  # deliberately not typecasted or set to bool False\n        # logic here\n        if len(sanitized_value) &gt;= 8:\n            first_two_char = sanitized_value[:2].upper()\n            dist_code: str = \"\"\n\n            if first_two_char in self.two_factor_registration_prefix:\n                if sanitized_value[2].isdigit():\n                    if sanitized_value[3].isdigit():\n                        dist_code = sanitized_value[2:4]\n                    else:\n                        dist_code = sanitized_value[2:3]\n\n                    registration_digits = sanitized_value[-4:]\n                    if registration_digits.isnumeric():\n                        if 0 &lt; int(registration_digits) &lt;= 9999:\n                            if (\n                                dist_code\n                                and dist_code\n                                in self.state_rto_district_map.get(first_two_char, \"\")\n                            ):\n                                is_valid_registration = True\n\n                for diplomatic_vehicle_code in self.in_vehicle_diplomatic_codes:\n                    if diplomatic_vehicle_code in sanitized_value:\n                        vehicle_prefix = sanitized_value.partition(\n                            diplomatic_vehicle_code\n                        )[0]\n                        if vehicle_prefix.isnumeric() and (\n                            1 &lt;= int(vehicle_prefix) &lt;= 80\n                            or int(vehicle_prefix)\n                            in self.in_vehicle_foreign_mission_codes\n                        ):\n                            is_valid_registration = True\n\n        return is_valid_registration\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVehicleRegistrationRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Determine absolute value based on calculation.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/india/in_vehicle_registration_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"Determine absolute value based on calculation.\"\"\"\n    sanitized_value = EntityRecognizer.sanitize_value(\n        pattern_text, self.replacement_pairs\n    )\n    return self.__check_vehicle_registration(sanitized_value)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer","title":"InAadhaarRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes Indian UIDAI Person Identification Number (\"AADHAAR\").</p> <p>Reference: https://en.wikipedia.org/wiki/Aadhaar A 12 digit unique number that is issued to each individual by Government of India</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IN_AADHAAR'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Determine absolute value based on calculation.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/india/in_aadhaar_recognizer.py</code> <pre><code>class InAadhaarRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes Indian UIDAI Person Identification Number (\"AADHAAR\").\n\n    Reference: https://en.wikipedia.org/wiki/Aadhaar\n    A 12 digit unique number that is issued to each individual by Government of India\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"AADHAAR (Very Weak)\",\n            r\"\\b[0-9]{12}\\b\",\n            0.01,\n        ),\n        Pattern(\"AADHAR (Very Weak)\", r\"\\b[0-9]{4}[- :][0-9]{4}[- :][0-9]{4}\\b\", 0.01),\n    ]\n\n    CONTEXT = [\n        \"aadhaar\",\n        \"uidai\",\n    ]\n\n    utils = None\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"IN_AADHAAR\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ) -&gt; None:\n        self.replacement_pairs = (\n            replacement_pairs\n            if replacement_pairs\n            else [(\"-\", \"\"), (\" \", \"\"), (\":\", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"Determine absolute value based on calculation.\"\"\"\n        sanitized_value = EntityRecognizer.sanitize_value(\n            pattern_text, self.replacement_pairs\n        )\n        return self.__check_aadhaar(sanitized_value)\n\n    def __check_aadhaar(self, sanitized_value: str) -&gt; bool:\n        is_valid_aadhaar: bool = False\n        if (\n            len(sanitized_value) == 12\n            and sanitized_value.isnumeric() is True\n            and int(sanitized_value[0]) &gt;= 2\n            and self._is_verhoeff_number(int(sanitized_value)) is True\n            and self._is_palindrome(sanitized_value) is False\n        ):\n            is_valid_aadhaar = True\n        return is_valid_aadhaar\n\n    @staticmethod\n    def _is_palindrome(text: str, case_insensitive: bool = False) -&gt; bool:\n        \"\"\"\n        Validate if input text is a true palindrome.\n\n        :param text: input text string to check for palindrome\n        :param case_insensitive: optional flag to check palindrome with no case\n        :return: True / False\n        \"\"\"\n        palindrome_text = text\n        if case_insensitive:\n            palindrome_text = palindrome_text.replace(\" \", \"\").lower()\n        return palindrome_text == palindrome_text[::-1]\n\n    @staticmethod\n    def _is_verhoeff_number(input_number: int) -&gt; bool:\n        \"\"\"\n        Check if the input number is a true verhoeff number.\n\n        :param input_number:\n        :return:\n        \"\"\"\n        __d__ = [\n            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n            [1, 2, 3, 4, 0, 6, 7, 8, 9, 5],\n            [2, 3, 4, 0, 1, 7, 8, 9, 5, 6],\n            [3, 4, 0, 1, 2, 8, 9, 5, 6, 7],\n            [4, 0, 1, 2, 3, 9, 5, 6, 7, 8],\n            [5, 9, 8, 7, 6, 0, 4, 3, 2, 1],\n            [6, 5, 9, 8, 7, 1, 0, 4, 3, 2],\n            [7, 6, 5, 9, 8, 2, 1, 0, 4, 3],\n            [8, 7, 6, 5, 9, 3, 2, 1, 0, 4],\n            [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n        ]\n        __p__ = [\n            [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n            [1, 5, 7, 6, 2, 8, 3, 0, 9, 4],\n            [5, 8, 0, 3, 7, 9, 6, 1, 4, 2],\n            [8, 9, 1, 6, 0, 4, 3, 5, 2, 7],\n            [9, 4, 5, 3, 1, 2, 6, 8, 7, 0],\n            [4, 2, 8, 6, 5, 7, 3, 9, 0, 1],\n            [2, 7, 9, 3, 8, 0, 6, 4, 1, 5],\n            [7, 0, 4, 6, 9, 1, 3, 2, 5, 8],\n        ]\n        __inv__ = [0, 4, 3, 2, 1, 5, 6, 7, 8, 9]\n\n        c = 0\n        inverted_number = list(map(int, reversed(str(input_number))))\n        for i in range(len(inverted_number)):\n            c = __d__[c][__p__[i % 8][inverted_number[i]]]\n        return __inv__[c] == 0\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InAadhaarRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Determine absolute value based on calculation.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/india/in_aadhaar_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"Determine absolute value based on calculation.\"\"\"\n    sanitized_value = EntityRecognizer.sanitize_value(\n        pattern_text, self.replacement_pairs\n    )\n    return self.__check_aadhaar(sanitized_value)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer","title":"InPanRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes Indian Permanent Account Number (\"PAN\").</p> <p>The Permanent Account Number (PAN) is a ten digit alpha-numeric code with the last digit being a check digit calculated using a modified modulus 10 calculation. This recognizer identifies PAN using regex and context words. Reference: https://en.wikipedia.org/wiki/Permanent_account_number,            https://incometaxindia.gov.in/Forms/tps/1.Permanent%20Account%20Number%20(PAN).pdf</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IN_PAN'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/india/in_pan_recognizer.py</code> <pre><code>class InPanRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes Indian Permanent Account Number (\"PAN\").\n\n    The Permanent Account Number (PAN) is a ten digit alpha-numeric code\n    with the last digit being a check digit calculated using a\n    modified modulus 10 calculation.\n    This recognizer identifies PAN using regex and context words.\n    Reference: https://en.wikipedia.org/wiki/Permanent_account_number,\n               https://incometaxindia.gov.in/Forms/tps/1.Permanent%20Account%20Number%20(PAN).pdf\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"PAN (High)\",\n            r\"\\b([A-Za-z]{3}[AaBbCcFfGgHhJjLlPpTt]{1}[A-Za-z]{1}[0-9]{4}[A-Za-z]{1})\\b\",\n            0.5,\n        ),\n        Pattern(\n            \"PAN (Medium)\",\n            r\"\\b([A-Za-z]{5}[0-9]{4}[A-Za-z]{1})\\b\",\n            0.1,\n        ),\n        Pattern(\n            \"PAN (Low)\",\n            r\"\\b((?=.*?[a-zA-Z])(?=.*?[0-9]{4})[\\w@#$%^?~-]{10})\\b\",\n            0.01,\n        ),\n    ]\n\n    CONTEXT = [\n        \"permanent account number\",\n        \"pan\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"IN_PAN\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs if replacement_pairs else [(\"-\", \"\"), (\" \", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPanRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer","title":"InPassportRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes Indian Passport Number.</p> <p>Indian Passport Number is a eight digit alphanumeric number.</p> <p>Reference: https://www.bajajallianz.com/blog/travel-insurance-articles/where-is-passport-number-in-indian-passport.html</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IN_PASSPORT'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/india/in_passport_recognizer.py</code> <pre><code>class InPassportRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes Indian Passport Number.\n\n    Indian Passport Number is a eight digit alphanumeric number.\n\n    Reference:\n    https://www.bajajallianz.com/blog/travel-insurance-articles/where-is-passport-number-in-indian-passport.html\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"PASSPORT\",\n            r\"\\b[A-Z][1-9]\\d\\s?\\d{4}[1-9]\\b\",\n            0.1,\n        ),\n    ]\n\n    CONTEXT = [\"passport\", \"indian passport\", \"passport number\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"IN_PASSPORT\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InPassportRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer","title":"InVoterRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize Indian Voter/Election Id(EPIC).</p> <p>The Elector's Photo Identity Card or Voter id is a ten digit alpha-numeric code issued by Election Commission of India to adult domiciles who have reached the age of 18 Ref: https://en.wikipedia.org/wiki/Voter_ID_(India)</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IN_VOTER'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/india/in_voter_recognizer.py</code> <pre><code>class InVoterRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize Indian Voter/Election Id(EPIC).\n\n    The Elector's Photo Identity Card or Voter id is a ten digit\n    alpha-numeric code issued by Election Commission of India\n    to adult domiciles who have reached the age of 18\n    Ref: https://en.wikipedia.org/wiki/Voter_ID_(India)\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"VOTER\",\n            r\"\\b([A-Za-z]{1}[ABCDGHJKMNPRSYabcdghjkmnprsy]{1}[A-Za-z]{1}([0-9]){7})\\b\",\n            0.4,\n        ),\n        Pattern(\n            \"VOTER\",\n            r\"\\b([A-Za-z]){3}([0-9]){7}\\b\",\n            0.3,\n        ),\n    ]\n\n    CONTEXT = [\n        \"voter\",\n        \"epic\",\n        \"elector photo identity card\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"IN_VOTER\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n            supported_entity=supported_entity,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.InVoterRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer","title":"ItDriverLicenseRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes IT Driver License using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'it'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IT_DRIVER_LICENSE'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/italy/it_driver_license_recognizer.py</code> <pre><code>class ItDriverLicenseRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes IT Driver License using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"Driver License\",\n            (\n                r\"\\b(?i)(([A-Z]{2}\\d{7}[A-Z])\"\n                r\"|(^[U]1[BCDEFGHLJKMNPRSTUWYXZ0-9]{7}[A-Z]))\\b\"\n            ),\n            0.2,\n        ),\n    ]\n    CONTEXT = [\"patente\", \"patente di guida\", \"licenza\", \"licenza di guida\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"it\",\n        supported_entity: str = \"IT_DRIVER_LICENSE\",\n    ) -&gt; None:\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItDriverLicenseRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer","title":"ItFiscalCodeRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes IT Fiscal Code using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'it'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IT_FISCAL_CODE'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/italy/it_fiscal_code_recognizer.py</code> <pre><code>class ItFiscalCodeRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes IT Fiscal Code using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"Fiscal Code\",\n            (\n                r\"(?i)((?:[A-Z][AEIOU][AEIOUX]|[AEIOU]X{2}\"\n                r\"|[B-DF-HJ-NP-TV-Z]{2}[A-Z]){2}\"\n                r\"(?:[\\dLMNP-V]{2}(?:[A-EHLMPR-T](?:[04LQ][1-9MNP-V]|[15MR][\\dLMNP-V]\"\n                r\"|[26NS][0-8LMNP-U])|[DHPS][37PT][0L]|[ACELMRT][37PT][01LM]\"\n                r\"|[AC-EHLMPR-T][26NS][9V])|(?:[02468LNQSU][048LQU]\"\n                r\"|[13579MPRTV][26NS])B[26NS][9V])(?:[A-MZ][1-9MNP-V][\\dLMNP-V]{2}\"\n                r\"|[A-M][0L](?:[1-9MNP-V][\\dLMNP-V]|[0L][1-9MNP-V]))[A-Z])\"\n            ),\n            0.3,\n        ),\n    ]\n    CONTEXT = [\"codice fiscale\", \"cf\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"it\",\n        supported_entity: str = \"IT_FISCAL_CODE\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the validation was successful.\n        \"\"\"\n        pattern_text = pattern_text.upper()\n        control = pattern_text[-1]\n        text_to_validate = pattern_text[:-1]\n        odd_values = text_to_validate[0::2]\n        even_values = text_to_validate[1::2]\n\n        # Odd values\n        map_odd = {\n            \"0\": 1,\n            \"1\": 0,\n            \"2\": 5,\n            \"3\": 7,\n            \"4\": 9,\n            \"5\": 13,\n            \"6\": 15,\n            \"7\": 17,\n            \"8\": 19,\n            \"9\": 21,\n            \"A\": 1,\n            \"B\": 0,\n            \"C\": 5,\n            \"D\": 7,\n            \"E\": 9,\n            \"F\": 13,\n            \"G\": 15,\n            \"H\": 17,\n            \"I\": 19,\n            \"J\": 21,\n            \"K\": 2,\n            \"L\": 4,\n            \"M\": 18,\n            \"N\": 20,\n            \"O\": 11,\n            \"P\": 3,\n            \"Q\": 6,\n            \"R\": 8,\n            \"S\": 12,\n            \"T\": 14,\n            \"U\": 16,\n            \"V\": 10,\n            \"W\": 22,\n            \"X\": 25,\n            \"Y\": 24,\n            \"Z\": 23,\n        }\n\n        odd_sum = 0\n        for char in odd_values:\n            odd_sum += map_odd[char]\n\n        # Even values\n        map_even = {\n            \"0\": 0,\n            \"1\": 1,\n            \"2\": 2,\n            \"3\": 3,\n            \"4\": 4,\n            \"5\": 5,\n            \"6\": 6,\n            \"7\": 7,\n            \"8\": 8,\n            \"9\": 9,\n            \"A\": 0,\n            \"B\": 1,\n            \"C\": 2,\n            \"D\": 3,\n            \"E\": 4,\n            \"F\": 5,\n            \"G\": 6,\n            \"H\": 7,\n            \"I\": 8,\n            \"J\": 9,\n            \"K\": 10,\n            \"L\": 11,\n            \"M\": 12,\n            \"N\": 13,\n            \"O\": 14,\n            \"P\": 15,\n            \"Q\": 16,\n            \"R\": 17,\n            \"S\": 18,\n            \"T\": 19,\n            \"U\": 20,\n            \"V\": 21,\n            \"W\": 22,\n            \"X\": 23,\n            \"Y\": 24,\n            \"Z\": 25,\n        }\n\n        even_sum = 0\n        for char in even_values:\n            even_sum += map_even[char]\n\n        # Mod value\n        map_mod = {\n            0: \"A\",\n            1: \"B\",\n            2: \"C\",\n            3: \"D\",\n            4: \"E\",\n            5: \"F\",\n            6: \"G\",\n            7: \"H\",\n            8: \"I\",\n            9: \"J\",\n            10: \"K\",\n            11: \"L\",\n            12: \"M\",\n            13: \"N\",\n            14: \"O\",\n            15: \"P\",\n            16: \"Q\",\n            17: \"R\",\n            18: \"S\",\n            19: \"T\",\n            20: \"U\",\n            21: \"V\",\n            22: \"W\",\n            23: \"X\",\n            24: \"Y\",\n            25: \"Z\",\n        }\n        check_value = map_mod[((odd_sum + even_sum) % 26)]\n\n        if check_value == control:\n            result = True\n        else:\n            result = None\n\n        return result\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItFiscalCodeRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/italy/it_fiscal_code_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    pattern_text = pattern_text.upper()\n    control = pattern_text[-1]\n    text_to_validate = pattern_text[:-1]\n    odd_values = text_to_validate[0::2]\n    even_values = text_to_validate[1::2]\n\n    # Odd values\n    map_odd = {\n        \"0\": 1,\n        \"1\": 0,\n        \"2\": 5,\n        \"3\": 7,\n        \"4\": 9,\n        \"5\": 13,\n        \"6\": 15,\n        \"7\": 17,\n        \"8\": 19,\n        \"9\": 21,\n        \"A\": 1,\n        \"B\": 0,\n        \"C\": 5,\n        \"D\": 7,\n        \"E\": 9,\n        \"F\": 13,\n        \"G\": 15,\n        \"H\": 17,\n        \"I\": 19,\n        \"J\": 21,\n        \"K\": 2,\n        \"L\": 4,\n        \"M\": 18,\n        \"N\": 20,\n        \"O\": 11,\n        \"P\": 3,\n        \"Q\": 6,\n        \"R\": 8,\n        \"S\": 12,\n        \"T\": 14,\n        \"U\": 16,\n        \"V\": 10,\n        \"W\": 22,\n        \"X\": 25,\n        \"Y\": 24,\n        \"Z\": 23,\n    }\n\n    odd_sum = 0\n    for char in odd_values:\n        odd_sum += map_odd[char]\n\n    # Even values\n    map_even = {\n        \"0\": 0,\n        \"1\": 1,\n        \"2\": 2,\n        \"3\": 3,\n        \"4\": 4,\n        \"5\": 5,\n        \"6\": 6,\n        \"7\": 7,\n        \"8\": 8,\n        \"9\": 9,\n        \"A\": 0,\n        \"B\": 1,\n        \"C\": 2,\n        \"D\": 3,\n        \"E\": 4,\n        \"F\": 5,\n        \"G\": 6,\n        \"H\": 7,\n        \"I\": 8,\n        \"J\": 9,\n        \"K\": 10,\n        \"L\": 11,\n        \"M\": 12,\n        \"N\": 13,\n        \"O\": 14,\n        \"P\": 15,\n        \"Q\": 16,\n        \"R\": 17,\n        \"S\": 18,\n        \"T\": 19,\n        \"U\": 20,\n        \"V\": 21,\n        \"W\": 22,\n        \"X\": 23,\n        \"Y\": 24,\n        \"Z\": 25,\n    }\n\n    even_sum = 0\n    for char in even_values:\n        even_sum += map_even[char]\n\n    # Mod value\n    map_mod = {\n        0: \"A\",\n        1: \"B\",\n        2: \"C\",\n        3: \"D\",\n        4: \"E\",\n        5: \"F\",\n        6: \"G\",\n        7: \"H\",\n        8: \"I\",\n        9: \"J\",\n        10: \"K\",\n        11: \"L\",\n        12: \"M\",\n        13: \"N\",\n        14: \"O\",\n        15: \"P\",\n        16: \"Q\",\n        17: \"R\",\n        18: \"S\",\n        19: \"T\",\n        20: \"U\",\n        21: \"V\",\n        22: \"W\",\n        23: \"X\",\n        24: \"Y\",\n        25: \"Z\",\n    }\n    check_value = map_mod[((odd_sum + even_sum) % 26)]\n\n    if check_value == control:\n        result = True\n    else:\n        result = None\n\n    return result\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer","title":"ItIdentityCardRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes Italian Identity Card number using case-insensitive regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'it'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IT_IDENTITY_CARD'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/italy/it_identity_card_recognizer.py</code> <pre><code>class ItIdentityCardRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes Italian Identity Card number using case-insensitive regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"Paper-based Identity Card (very weak)\",\n            # The number is composed of 2 letters, space (optional), 7 digits\n            r\"(?i)\\b[A-Z]{2}\\s?\\d{7}\\b\",  # noqa: E501\n            0.01,\n        ),\n        Pattern(\n            \"Electronic Identity Card (CIE) 2.0 (very weak)\",\n            r\"(?i)\\b\\d{7}[A-Z]{2}\\b\",  # noqa: E501\n            0.01,\n        ),\n        Pattern(\n            \"Electronic Identity Card (CIE) 3.0 (very weak)\",\n            r\"(?i)\\b[A-Z]{2}\\d{5}[A-Z]{2}\\b\",  # noqa: E501\n            0.01,\n        ),\n    ]\n\n    CONTEXT = [\n        \"carta\",\n        \"identit\u00e0\",\n        \"elettronica\",\n        \"cie\",\n        \"documento\",\n        \"riconoscimento\",\n        \"espatrio\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"it\",\n        supported_entity: str = \"IT_IDENTITY_CARD\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItIdentityCardRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer","title":"ItPassportRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes IT Passport number using case-insensitive regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'it'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IT_PASSPORT'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/italy/it_passport_recognizer.py</code> <pre><code>class ItPassportRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes IT Passport number using case-insensitive regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"Passport (very weak)\",\n            r\"(?i)\\b[A-Z]{2}\\d{7}\\b\",  # noqa: E501\n            0.01,\n        ),\n    ]\n\n    CONTEXT = [\n        \"passaporto\",\n        \"elettronico\",\n        \"italiano\",\n        \"viaggio\",\n        \"viaggiare\",\n        \"estero\",\n        \"documento\",\n        \"dogana\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"it\",\n        supported_entity: str = \"IT_PASSPORT\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItPassportRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer","title":"ItVatCodeRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes Italian VAT code using regex and checksum.</p> <p>For more information about italian VAT code:     https://en.wikipedia.org/wiki/VAT_identification_number#:~:text=%5B2%5D)-,Italy,-Partita%20IVA</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'it'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IT_VAT_CODE'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/italy/it_vat_code.py</code> <pre><code>class ItVatCodeRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes Italian VAT code using regex and checksum.\n\n    For more information about italian VAT code:\n        https://en.wikipedia.org/wiki/VAT_identification_number#:~:text=%5B2%5D)-,Italy,-Partita%20IVA\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    # Class variables\n    PATTERNS = [\n        Pattern(\n            \"IT Vat code (piva)\",\n            r\"\\b([0-9][ _]?){11}\\b\",\n            0.1,\n        )\n    ]\n\n    CONTEXT = [\"piva\", \"partita iva\", \"pi\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"it\",\n        supported_entity: str = \"IT_VAT_CODE\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs\n            if replacement_pairs\n            else [(\"-\", \"\"), (\" \", \"\"), (\"_\", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the validation was successful.\n        \"\"\"\n\n        # Pre-processing before validation checks\n        text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n\n        # Edge-case that passes the checksum even though it is not a\n        # valid italian vat code.\n        if text == \"00000000000\":\n            return False\n\n        x = 0\n        y = 0\n\n        for i in range(0, 5):\n            x += int(text[2 * i])\n            tmp_y = int(text[2 * i + 1]) * 2\n            if tmp_y &gt; 9:\n                tmp_y = tmp_y - 9\n            y += tmp_y\n\n        t = (x + y) % 10\n        c = (10 - t) % 10\n\n        if c == int(text[10]):\n            result = True\n        else:\n            result = False\n\n        return result\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.ItVatCodeRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/italy/it_vat_code.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n\n    # Pre-processing before validation checks\n    text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n\n    # Edge-case that passes the checksum even though it is not a\n    # valid italian vat code.\n    if text == \"00000000000\":\n        return False\n\n    x = 0\n    y = 0\n\n    for i in range(0, 5):\n        x += int(text[2 * i])\n        tmp_y = int(text[2 * i + 1]) * 2\n        if tmp_y &gt; 9:\n            tmp_y = tmp_y - 9\n        y += tmp_y\n\n    t = (x + y) % 10\n    c = (10 - t) % 10\n\n    if c == int(text[10]):\n        result = True\n    else:\n        result = False\n\n    return result\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer","title":"KrRrnRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize Korean Resident Registration Number (RRN).</p> <p>The Korean Resident Registration Number (RRN) is a 13-digit number issued to all Korean residents.</p> <p>The format is YYMMDD-GHIJKLX where: - YYMMDD represents the birth date - G determines gender and century of birth</p> <p>For RRNs issued before October 2020: - HIJKL is a serial number assigned by district - X is a check digit calculated using the preceding 12 digits</p> <p>For RRNs issued after October 2020: - HIJKLX is a random number</p> <p>Reference: https://en.wikipedia.org/wiki/Resident_registration_number</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'kr'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'KR_RRN'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/korea/kr_rrn_recognizer.py</code> <pre><code>class KrRrnRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize Korean Resident Registration Number (RRN).\n\n    The Korean Resident Registration Number (RRN) is\n    a 13-digit number issued to all Korean residents.\n\n    The format is YYMMDD-GHIJKLX where:\n    - YYMMDD represents the birth date\n    - G determines gender and century of birth\n\n    For RRNs issued before October 2020:\n    - HIJKL is a serial number assigned by district\n    - X is a check digit calculated using the preceding 12 digits\n\n    For RRNs issued after October 2020:\n    - HIJKLX is a random number\n\n    Reference: https://en.wikipedia.org/wiki/Resident_registration_number\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"RRN (Medium)\",\n            r\"\\b\\d{2}(0[1-9]|1[0-2])(0[1-9]|[1-2][0-9]|3[0-1])(-?)\\d{7}\\b\",\n            0.5,\n        )\n    ]\n\n    CONTEXT = [\n        \"Korean RRN\",\n        \"Korean Resident Registration Number\",\n        \"Resident Registration Number\",\n        \"RRN\",\n        \"rrn\",\n        \"rrn#\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"kr\",\n        supported_entity: str = \"KR_RRN\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = replacement_pairs if replacement_pairs else [(\"-\", \"\")]\n\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; Union[bool, None]:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        This validation is only for RRNs issued before October 2020.\n        Therefore, it returns None, not False, at the end of the method.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool or None, indicating whether the validation was successful.\n        \"\"\"\n        # Pre-processing before validation checks\n        sanitized_value = EntityRecognizer.sanitize_value(\n            pattern_text, self.replacement_pairs\n        )\n\n        # Check if the sanitized value has the correct length (13 digits)\n        if len(sanitized_value) != 13:\n            return False\n\n        # Check if all characters are digits\n        if not sanitized_value.isdigit():\n            return False\n\n        # Validate region code (HI) and checksum (X)\n        region_code = int(sanitized_value[7:9])  # HI\n        if self._validate_region_code(region_code) and self._validate_checksum(\n            sanitized_value\n        ):\n            return True\n\n        return None\n\n    def _validate_region_code(self, region_code: int) -&gt; bool:\n        \"\"\"\n        Validate the region code of Korean RRN.\n\n        :param region_code: The region code to validate\n        :return: True if region code is valid, False otherwise\n        \"\"\"\n        return True if 0 &lt;= region_code &lt;= 95 else False\n\n    def _validate_checksum(self, rrn: str) -&gt; bool:\n        \"\"\"\n        Validate the checksum of Korean RRN.\n\n        The checksum is calculated using the preceding 12 digits.\n        X = (11 - (2A+3B+4C+5D+6E+7F+8G+9H+2I+3J+4K+5L) mod 11) mod 10\n\n        :param rrn: The RRN to validate\n        :return: True if checksum is valid, False otherwise\n        \"\"\"\n\n        digit_sum = (\n            2 * int(rrn[0])\n            + 3 * int(rrn[1])\n            + 4 * int(rrn[2])\n            + 5 * int(rrn[3])\n            + 6 * int(rrn[4])\n            + 7 * int(rrn[5])\n            + 8 * int(rrn[6])\n            + 9 * int(rrn[7])\n            + 2 * int(rrn[8])\n            + 3 * int(rrn[9])\n            + 4 * int(rrn[10])\n            + 5 * int(rrn[11])\n        )\n        checksum = (11 - (digit_sum % 11)) % 10\n\n        return checksum == int(rrn[12])\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.KrRrnRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Union[bool, None]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <p>This validation is only for RRNs issued before October 2020. Therefore, it returns None, not False, at the end of the method.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Union[bool, None]</code> <p>A bool or None, indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/korea/kr_rrn_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Union[bool, None]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    This validation is only for RRNs issued before October 2020.\n    Therefore, it returns None, not False, at the end of the method.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool or None, indicating whether the validation was successful.\n    \"\"\"\n    # Pre-processing before validation checks\n    sanitized_value = EntityRecognizer.sanitize_value(\n        pattern_text, self.replacement_pairs\n    )\n\n    # Check if the sanitized value has the correct length (13 digits)\n    if len(sanitized_value) != 13:\n        return False\n\n    # Check if all characters are digits\n    if not sanitized_value.isdigit():\n        return False\n\n    # Validate region code (HI) and checksum (X)\n    region_code = int(sanitized_value[7:9])  # HI\n    if self._validate_region_code(region_code) and self._validate_checksum(\n        sanitized_value\n    ):\n        return True\n\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer","title":"PlPeselRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize PESEL number using regex and checksum.</p> <p>For more information about PESEL: https://en.wikipedia.org/wiki/PESEL</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pl'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'PL_PESEL'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/poland/pl_pesel_recognizer.py</code> <pre><code>class PlPeselRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize PESEL number using regex and checksum.\n\n    For more information about PESEL: https://en.wikipedia.org/wiki/PESEL\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"PESEL\",\n            r\"[0-9]{2}([02468][1-9]|[13579][012])(0[1-9]|1[0-9]|2[0-9]|3[01])[0-9]{5}\",\n            0.4,\n        ),\n    ]\n\n    CONTEXT = [\"PESEL\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"pl\",\n        supported_entity: str = \"PL_PESEL\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:  # noqa D102\n        digits = [int(digit) for digit in pattern_text]\n        weights = [1, 3, 7, 9, 1, 3, 7, 9, 1, 3]\n\n        checksum = sum(digit * weight for digit, weight in zip(digits[:10], weights))\n        checksum %= 10\n\n        return checksum == digits[10]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PlPeselRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer","title":"SgFinRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize SG FIN/NRIC number using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'SG_NRIC_FIN'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/singapore/sg_fin_recognizer.py</code> <pre><code>class SgFinRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize SG FIN/NRIC number using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\"Nric (weak)\", r\"(?i)(\\b[A-Z][0-9]{7}[A-Z]\\b)\", 0.3),\n        Pattern(\"Nric (medium)\", r\"(?i)(\\b[STFGM][0-9]{7}[A-Z]\\b)\", 0.5),\n    ]\n\n    CONTEXT = [\"fin\", \"fin#\", \"nric\", \"nric#\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"SG_NRIC_FIN\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgFinRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer","title":"SgUenRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize Singapore UEN (Unique Entity Number) using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'SG_UEN'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>validate_uen_format_a</code> <p>Validate the UEN format A using checksum.</p> <code>validate_uen_format_b</code> <p>Validate the UEN format B using checksum.</p> <code>validate_uen_format_c</code> <p>Validate the UEN format C using checksum.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/singapore/sg_uen_recognizer.py</code> <pre><code>class SgUenRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize Singapore UEN (Unique Entity Number) using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"UEN (low)\",\n            r\"\\b\\d{8}[A-Z]\\b|\\b\\d{9}[A-Z]\\b|\\b(T|S)\\d{2}[A-Z]{2}\\d{4}[A-Z]\\b\",\n            0.3,\n        )\n    ]\n\n    CONTEXT = [\"uen\", \"unique entity number\", \"business registration\", \"ACRA\"]\n\n    UEN_FORMAT_A_WEIGHT = (10, 4, 9, 3, 8, 2, 7, 1)\n    UEN_FORMAT_A_ALPHABET = \"XMKECAWLJDB\"\n    UEN_FORMAT_B_WEIGHT = (10, 8, 6, 4, 9, 7, 5, 3, 1)\n    UEN_FORMAT_B_ALPHABET = \"ZKCMDNERGWH\"\n    UEN_FORMAT_C_WEIGHT = (4, 3, 5, 3, 10, 2, 2, 5, 7)\n    UEN_FORMAT_C_ALPHABET = \"ABCDEFGHJKLMNPQRSTUVWX0123456789\"\n    UEN_FORMAT_C_PREFIX = {\"T\", \"S\", \"R\"}\n    UEN_FORMAT_C_ENTITY_TYPE = {\n        \"LP\",\n        \"LL\",\n        \"FC\",\n        \"PF\",\n        \"RF\",\n        \"MQ\",\n        \"MM\",\n        \"NB\",\n        \"CC\",\n        \"CS\",\n        \"MB\",\n        \"FM\",\n        \"GS\",\n        \"DP\",\n        \"CP\",\n        \"NR\",\n        \"CM\",\n        \"CD\",\n        \"MD\",\n        \"HS\",\n        \"VH\",\n        \"CH\",\n        \"MH\",\n        \"CL\",\n        \"XL\",\n        \"CX\",\n        \"HC\",\n        \"RP\",\n        \"TU\",\n        \"TC\",\n        \"FB\",\n        \"FN\",\n        \"PA\",\n        \"PB\",\n        \"SS\",\n        \"MC\",\n        \"SM\",\n        \"GA\",\n        \"GB\",\n    }\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"SG_UEN\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the validation was successful.\n        \"\"\"\n\n        if len(pattern_text) == 9:\n            # Checksum validation for UEN format A\n            return self.validate_uen_format_a(pattern_text)\n        elif len(pattern_text) == 10 and pattern_text[0].isalpha():\n            # Checksum validation for UEN format C\n            return self.validate_uen_format_c(pattern_text)\n        elif len(pattern_text) == 10:\n            # Checksum validation for UEN format B\n            return self.validate_uen_format_b(pattern_text)\n\n        return False\n\n    @staticmethod\n    def validate_uen_format_a(uen: str) -&gt; bool:\n        \"\"\"\n        Validate the UEN format A using checksum.\n\n        :param uen: The UEN to validate.\n        :return: True if the UEN is valid according to its respective\n        format, False otherwise.\n        \"\"\"\n        check_digit = uen[-1]\n\n        weighted_sum = sum(\n            int(n) * w for n, w in zip(uen[:-1], SgUenRecognizer.UEN_FORMAT_A_WEIGHT)\n        )\n\n        checksum = SgUenRecognizer.UEN_FORMAT_A_ALPHABET[weighted_sum % 11]\n\n        return check_digit == checksum\n\n    @staticmethod\n    def validate_uen_format_b(uen: str) -&gt; bool:\n        \"\"\"\n        Validate the UEN format B using checksum.\n\n        :param uen: The UEN to validate.\n        :return: True if the UEN is valid according to its respective\n        format, False otherwise.\n        \"\"\"\n        check_digit = uen[-1]\n        year_of_registration = int(uen[0:4])\n\n        # Check if the year of registration is not in the future\n        if year_of_registration &gt; date.today().year:\n            return False\n\n        weighted_sum = sum(\n            int(n) * w for n, w in zip(uen[:-1], SgUenRecognizer.UEN_FORMAT_B_WEIGHT)\n        )\n\n        checksum = SgUenRecognizer.UEN_FORMAT_B_ALPHABET[weighted_sum % 11]\n\n        return check_digit == checksum\n\n    @staticmethod\n    def validate_uen_format_c(uen: str) -&gt; bool:\n        \"\"\"\n        Validate the UEN format C using checksum.\n\n        :param uen: The UEN to validate.\n        :return: True if the UEN is valid according to its respective\n        format, False otherwise.\n        \"\"\"\n        check_digit = uen[-1]\n\n        if uen[0] not in SgUenRecognizer.UEN_FORMAT_C_PREFIX:\n            return False\n\n        entity_type = uen[3:5]\n\n        if entity_type not in SgUenRecognizer.UEN_FORMAT_C_ENTITY_TYPE:\n            return False\n\n        weighted_sum = sum(\n            SgUenRecognizer.UEN_FORMAT_C_ALPHABET.index(n) * w\n            for n, w in zip(uen[:-1], SgUenRecognizer.UEN_FORMAT_C_WEIGHT)\n        )\n\n        checksum = SgUenRecognizer.UEN_FORMAT_C_ALPHABET[(weighted_sum - 5) % 11]\n\n        return check_digit == checksum\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/singapore/sg_uen_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n\n    if len(pattern_text) == 9:\n        # Checksum validation for UEN format A\n        return self.validate_uen_format_a(pattern_text)\n    elif len(pattern_text) == 10 and pattern_text[0].isalpha():\n        # Checksum validation for UEN format C\n        return self.validate_uen_format_c(pattern_text)\n    elif len(pattern_text) == 10:\n        # Checksum validation for UEN format B\n        return self.validate_uen_format_b(pattern_text)\n\n    return False\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.validate_uen_format_a","title":"validate_uen_format_a  <code>staticmethod</code>","text":"<pre><code>validate_uen_format_a(uen: str) -&gt; bool\n</code></pre> <p>Validate the UEN format A using checksum.</p> PARAMETER DESCRIPTION <code>uen</code> <p>The UEN to validate.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the UEN is valid according to its respective format, False otherwise.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/singapore/sg_uen_recognizer.py</code> <pre><code>@staticmethod\ndef validate_uen_format_a(uen: str) -&gt; bool:\n    \"\"\"\n    Validate the UEN format A using checksum.\n\n    :param uen: The UEN to validate.\n    :return: True if the UEN is valid according to its respective\n    format, False otherwise.\n    \"\"\"\n    check_digit = uen[-1]\n\n    weighted_sum = sum(\n        int(n) * w for n, w in zip(uen[:-1], SgUenRecognizer.UEN_FORMAT_A_WEIGHT)\n    )\n\n    checksum = SgUenRecognizer.UEN_FORMAT_A_ALPHABET[weighted_sum % 11]\n\n    return check_digit == checksum\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.validate_uen_format_b","title":"validate_uen_format_b  <code>staticmethod</code>","text":"<pre><code>validate_uen_format_b(uen: str) -&gt; bool\n</code></pre> <p>Validate the UEN format B using checksum.</p> PARAMETER DESCRIPTION <code>uen</code> <p>The UEN to validate.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the UEN is valid according to its respective format, False otherwise.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/singapore/sg_uen_recognizer.py</code> <pre><code>@staticmethod\ndef validate_uen_format_b(uen: str) -&gt; bool:\n    \"\"\"\n    Validate the UEN format B using checksum.\n\n    :param uen: The UEN to validate.\n    :return: True if the UEN is valid according to its respective\n    format, False otherwise.\n    \"\"\"\n    check_digit = uen[-1]\n    year_of_registration = int(uen[0:4])\n\n    # Check if the year of registration is not in the future\n    if year_of_registration &gt; date.today().year:\n        return False\n\n    weighted_sum = sum(\n        int(n) * w for n, w in zip(uen[:-1], SgUenRecognizer.UEN_FORMAT_B_WEIGHT)\n    )\n\n    checksum = SgUenRecognizer.UEN_FORMAT_B_ALPHABET[weighted_sum % 11]\n\n    return check_digit == checksum\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SgUenRecognizer.validate_uen_format_c","title":"validate_uen_format_c  <code>staticmethod</code>","text":"<pre><code>validate_uen_format_c(uen: str) -&gt; bool\n</code></pre> <p>Validate the UEN format C using checksum.</p> PARAMETER DESCRIPTION <code>uen</code> <p>The UEN to validate.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the UEN is valid according to its respective format, False otherwise.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/singapore/sg_uen_recognizer.py</code> <pre><code>@staticmethod\ndef validate_uen_format_c(uen: str) -&gt; bool:\n    \"\"\"\n    Validate the UEN format C using checksum.\n\n    :param uen: The UEN to validate.\n    :return: True if the UEN is valid according to its respective\n    format, False otherwise.\n    \"\"\"\n    check_digit = uen[-1]\n\n    if uen[0] not in SgUenRecognizer.UEN_FORMAT_C_PREFIX:\n        return False\n\n    entity_type = uen[3:5]\n\n    if entity_type not in SgUenRecognizer.UEN_FORMAT_C_ENTITY_TYPE:\n        return False\n\n    weighted_sum = sum(\n        SgUenRecognizer.UEN_FORMAT_C_ALPHABET.index(n) * w\n        for n, w in zip(uen[:-1], SgUenRecognizer.UEN_FORMAT_C_WEIGHT)\n    )\n\n    checksum = SgUenRecognizer.UEN_FORMAT_C_ALPHABET[(weighted_sum - 5) % 11]\n\n    return check_digit == checksum\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer","title":"EsNieRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize NIE number using regex and checksum.</p> <p>Reference(s): https://es.wikipedia.org/wiki/N%C3%BAmero_de_identidad_de_extranjero https://www.interior.gob.es/opencms/ca/servicios-al-ciudadano/tramites-y-gestiones/dni/calculo-del-digito-de-control-del-nif-nie/</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'es'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ES_NIE'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern by using the control character.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/spain/es_nie_recognizer.py</code> <pre><code>class EsNieRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize NIE number using regex and checksum.\n\n    Reference(s):\n    https://es.wikipedia.org/wiki/N%C3%BAmero_de_identidad_de_extranjero\n    https://www.interior.gob.es/opencms/ca/servicios-al-ciudadano/tramites-y-gestiones/dni/calculo-del-digito-de-control-del-nif-nie/\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes\n    or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"NIE\",\n            r\"\\b[X-Z]?[0-9]?[0-9]{7}[-]?[A-Z]\\b\",\n            0.5,\n        ),\n    ]\n\n    CONTEXT = [\"n\u00famero de identificaci\u00f3n de extranjero\", \"NIE\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"es\",\n        supported_entity: str = \"ES_NIE\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs if replacement_pairs else [(\"-\", \"\"), (\" \", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"Validate the pattern by using the control character.\"\"\"\n\n        pattern_text = EntityRecognizer.sanitize_value(\n            pattern_text, self.replacement_pairs\n        )\n\n        letters = \"TRWAGMYFPDXBNJZSQVHLCKE\"\n        letter = pattern_text[-1]\n\n        # check last is a letter, and first is in X,Y,Z\n        if not pattern_text[1:-1].isdigit or pattern_text[:1] not in \"XYZ\":\n            return False\n        # check size is 8 or 9\n        if len(pattern_text) &lt; 8 or len(pattern_text) &gt; 9:\n            return False\n\n        # replace XYZ with 012, and check the mod 23\n        number = int(str(\"XYZ\".index(pattern_text[0])) + pattern_text[1:-1])\n        return letter == letters[number % 23]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNieRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Validate the pattern by using the control character.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/spain/es_nie_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"Validate the pattern by using the control character.\"\"\"\n\n    pattern_text = EntityRecognizer.sanitize_value(\n        pattern_text, self.replacement_pairs\n    )\n\n    letters = \"TRWAGMYFPDXBNJZSQVHLCKE\"\n    letter = pattern_text[-1]\n\n    # check last is a letter, and first is in X,Y,Z\n    if not pattern_text[1:-1].isdigit or pattern_text[:1] not in \"XYZ\":\n        return False\n    # check size is 8 or 9\n    if len(pattern_text) &lt; 8 or len(pattern_text) &gt; 9:\n        return False\n\n    # replace XYZ with 012, and check the mod 23\n    number = int(str(\"XYZ\".index(pattern_text[0])) + pattern_text[1:-1])\n    return letter == letters[number % 23]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer","title":"EsNifRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize NIF number using regex and checksum.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'es'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ES_NIF'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/spain/es_nif_recognizer.py</code> <pre><code>class EsNifRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize NIF number using regex and checksum.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"NIF\",\n            r\"\\b[0-9]?[0-9]{7}[-]?[A-Z]\\b\",\n            0.5,\n        ),\n    ]\n\n    CONTEXT = [\"documento nacional de identidad\", \"DNI\", \"NIF\", \"identificaci\u00f3n\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"es\",\n        supported_entity: str = \"ES_NIF\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs if replacement_pairs else [(\"-\", \"\"), (\" \", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:  # noqa D102\n        pattern_text = EntityRecognizer.sanitize_value(\n            pattern_text, self.replacement_pairs\n        )\n        letter = pattern_text[-1]\n        number = int(\"\".join(filter(str.isdigit, pattern_text)))\n        letters = \"TRWAGMYFPDXBNJZSQVHLCKE\"\n        return letter == letters[number % 23]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EsNifRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer","title":"NhsRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes NHS number using regex and checksum.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'UK_NHS'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/uk/uk_nhs_recognizer.py</code> <pre><code>class NhsRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes NHS number using regex and checksum.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"NHS (medium)\",\n            r\"\\b([0-9]{3})[- ]?([0-9]{3})[- ]?([0-9]{4})\\b\",\n            0.5,\n        ),\n    ]\n\n    CONTEXT = [\n        \"national health service\",\n        \"nhs\",\n        \"health services authority\",\n        \"health authority\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"UK_NHS\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs if replacement_pairs else [(\"-\", \"\"), (\" \", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the validation was successful.\n        \"\"\"\n        text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n        total = sum(\n            [int(c) * multiplier for c, multiplier in zip(text, reversed(range(11)))]\n        )\n        remainder = total % 11\n        check_remainder = remainder == 0\n\n        return check_remainder\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.NhsRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/uk/uk_nhs_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    text = EntityRecognizer.sanitize_value(pattern_text, self.replacement_pairs)\n    total = sum(\n        [int(c) * multiplier for c, multiplier in zip(text, reversed(range(11)))]\n    )\n    remainder = total % 11\n    check_remainder = remainder == 0\n\n    return check_remainder\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer","title":"UkNinoRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes UK National Insurance Number using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'UK_NINO'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/uk/uk_nino_recognizer.py</code> <pre><code>class UkNinoRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes UK National Insurance Number using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"NINO (medium)\",\n            r\"\\b(?!bg|gb|nk|kn|nt|tn|zz|BG|GB|NK|KN|NT|TN|ZZ) ?([a-ceghj-pr-tw-zA-CEGHJ-PR-TW-Z]{1}[a-ceghj-npr-tw-zA-CEGHJ-NPR-TW-Z]{1}) ?([0-9]{2}) ?([0-9]{2}) ?([0-9]{2}) ?([a-dA-D{1}])\\b\",  # noqa: E501\n            0.5,\n        ),\n    ]\n\n    CONTEXT = [\"national insurance\", \"ni number\", \"nino\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"UK_NINO\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UkNinoRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer","title":"AbaRoutingRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize American Banking Association (ABA) routing number.</p> <p>Also known as routing transit number (RTN) and used to identify financial institutions and process transactions.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ABA_ROUTING_NUMBER'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/us/aba_routing_recognizer.py</code> <pre><code>class AbaRoutingRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize American Banking Association (ABA) routing number.\n\n    Also known as routing transit number (RTN) and used to identify financial\n    institutions and process transactions.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"ABA routing number (weak)\",\n            r\"\\b[0123678]\\d{8}\\b\",\n            0.05,\n        ),\n        Pattern(\n            \"ABA routing number\",\n            r\"\\b[0123678]\\d{3}-\\d{4}-\\d\\b\",\n            0.3,\n        ),\n    ]\n\n    CONTEXT = [\n        \"aba\",\n        \"routing\",\n        \"abarouting\",\n        \"association\",\n        \"bankrouting\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"ABA_ROUTING_NUMBER\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = replacement_pairs or [(\"-\", \"\")]\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:  # noqa D102\n        sanitized_value = EntityRecognizer.sanitize_value(\n            pattern_text, self.replacement_pairs\n        )\n        return self.__checksum(sanitized_value)\n\n    @staticmethod\n    def __checksum(sanitized_value: str) -&gt; bool:\n        s = 0\n        for idx, m in enumerate([3, 7, 1, 3, 7, 1, 3, 7, 1]):\n            s += int(sanitized_value[idx]) * m\n        return s % 10 == 0\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AbaRoutingRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer","title":"MedicalLicenseRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize common Medical license numbers using regex + checksum.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'MEDICAL_LICENSE'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/us/medical_license_recognizer.py</code> <pre><code>class MedicalLicenseRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize common Medical license numbers using regex + checksum.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"USA DEA Certificate Number (weak)\",\n            r\"[abcdefghjklmprstuxABCDEFGHJKLMPRSTUX]{1}[a-zA-Z]{1}\\d{7}|\"\n            r\"[abcdefghjklmprstuxABCDEFGHJKLMPRSTUX]{1}9\\d{7}\",\n            0.4,\n        ),\n    ]\n\n    CONTEXT = [\"medical\", \"certificate\", \"DEA\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"MEDICAL_LICENSE\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs if replacement_pairs else [(\"-\", \"\"), (\" \", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:  # noqa D102\n        sanitized_value = EntityRecognizer.sanitize_value(\n            pattern_text, self.replacement_pairs\n        )\n        checksum = self.__luhn_checksum(sanitized_value)\n\n        return checksum\n\n    @staticmethod\n    def __luhn_checksum(sanitized_value: str) -&gt; bool:\n        def digits_of(n: str) -&gt; List[int]:\n            return [int(dig) for dig in str(n)]\n\n        digits = digits_of(sanitized_value[2:])\n        checksum = digits.pop()\n        even_digits = digits[-1::-2]\n        odd_digits = digits[-2::-2]\n        checksum *= -1\n        checksum += 2 * sum(even_digits) + sum(odd_digits)\n        return checksum % 10 == 0\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.MedicalLicenseRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer","title":"UsBankRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes US bank number using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'US_BANK_NUMBER'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/us/us_bank_recognizer.py</code> <pre><code>class UsBankRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes US bank number using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"Bank Account (weak)\",\n            r\"\\b[0-9]{8,17}\\b\",\n            0.05,\n        ),\n    ]\n\n    CONTEXT = [\n        # Task #603: Support keyphrases: change to \"checking account\"\n        # as part of keyphrase change\n        \"check\",\n        \"account\",\n        \"account#\",\n        \"acct\",\n        \"bank\",\n        \"save\",\n        \"debit\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"US_BANK_NUMBER\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsBankRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer","title":"UsLicenseRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes US driver license using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'US_DRIVER_LICENSE'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/us/us_driver_license_recognizer.py</code> <pre><code>class UsLicenseRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes US driver license using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"Driver License - Alphanumeric (weak)\",\n            r\"\\b([A-Z][0-9]{3,6}|[A-Z][0-9]{5,9}|[A-Z][0-9]{6,8}|[A-Z][0-9]{4,8}|[A-Z][0-9]{9,11}|[A-Z]{1,2}[0-9]{5,6}|H[0-9]{8}|V[0-9]{6}|X[0-9]{8}|A-Z]{2}[0-9]{2,5}|[A-Z]{2}[0-9]{3,7}|[0-9]{2}[A-Z]{3}[0-9]{5,6}|[A-Z][0-9]{13,14}|[A-Z][0-9]{18}|[A-Z][0-9]{6}R|[A-Z][0-9]{9}|[A-Z][0-9]{1,12}|[0-9]{9}[A-Z]|[A-Z]{2}[0-9]{6}[A-Z]|[0-9]{8}[A-Z]{2}|[0-9]{3}[A-Z]{2}[0-9]{4}|[A-Z][0-9][A-Z][0-9][A-Z]|[0-9]{7,8}[A-Z])\\b\",  # noqa: E501\n            0.3,\n        ),\n        Pattern(\n            \"Driver License - Digits (very weak)\",\n            r\"\\b([0-9]{6,14}|[0-9]{16})\\b\",  # noqa: E501\n            0.01,\n        ),\n    ]\n\n    CONTEXT = [\n        \"driver\",\n        \"license\",\n        \"permit\",\n        \"lic\",\n        \"identification\",\n        \"dls\",\n        \"cdls\",\n        \"lic#\",\n        \"driving\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"US_DRIVER_LICENSE\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            supported_language=supported_language,\n            patterns=patterns,\n            context=context,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsLicenseRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer","title":"UsItinRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes US ITIN (Individual Taxpayer Identification Number) using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'US_ITIN'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/us/us_itin_recognizer.py</code> <pre><code>class UsItinRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes US ITIN (Individual Taxpayer Identification Number) using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"Itin (very weak)\",\n            r\"\\b9\\d{2}[- ](5\\d|6[0-5]|7\\d|8[0-8]|9([0-2]|[4-9]))\\d{4}\\b|\\b9\\d{2}(5\\d|6[0-5]|7\\d|8[0-8]|9([0-2]|[4-9]))[- ]\\d{4}\\b\",  # noqa: E501\n            0.05,\n        ),\n        Pattern(\n            \"Itin (weak)\",\n            r\"\\b9\\d{2}(5\\d|6[0-5]|7\\d|8[0-8]|9([0-2]|[4-9]))\\d{4}\\b\",  # noqa: E501\n            0.3,\n        ),\n        Pattern(\n            \"Itin (medium)\",\n            r\"\\b9\\d{2}[- ](5\\d|6[0-5]|7\\d|8[0-8]|9([0-2]|[4-9]))[- ]\\d{4}\\b\",  # noqa: E501\n            0.5,\n        ),\n    ]\n\n    CONTEXT = [\"individual\", \"taxpayer\", \"itin\", \"tax\", \"payer\", \"taxid\", \"tin\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"US_ITIN\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsItinRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer","title":"UsPassportRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognizes US Passport number using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'US_PASSPORT'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/us/us_passport_recognizer.py</code> <pre><code>class UsPassportRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognizes US Passport number using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    # Weak pattern: all passport numbers are a weak match, e.g., 14019033\n    PATTERNS = [\n        Pattern(\"Passport (very weak)\", r\"(\\b[0-9]{9}\\b)\", 0.05),\n        Pattern(\"Passport Next Generation (very weak)\", r\"(\\b[A-Z][0-9]{8}\\b)\", 0.1),\n    ]\n    CONTEXT = [\"us\", \"united\", \"states\", \"passport\", \"passport#\", \"travel\", \"document\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"US_PASSPORT\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsPassportRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer","title":"UsSsnRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize US Social Security Number (SSN) using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'US_SSN'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>invalidate_result</code> <p>Check if the pattern text cannot be validated as a US_SSN entity.</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/us/us_ssn_recognizer.py</code> <pre><code>class UsSsnRecognizer(PatternRecognizer):\n    \"\"\"Recognize US Social Security Number (SSN) using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\"SSN1 (very weak)\", r\"\\b([0-9]{5})-([0-9]{4})\\b\", 0.05),  # noqa E501\n        Pattern(\"SSN2 (very weak)\", r\"\\b([0-9]{3})-([0-9]{6})\\b\", 0.05),  # noqa E501\n        Pattern(\"SSN3 (very weak)\", r\"\\b(([0-9]{3})-([0-9]{2})-([0-9]{4}))\\b\", 0.05),  # noqa E501\n        Pattern(\"SSN4 (very weak)\", r\"\\b[0-9]{9}\\b\", 0.05),\n        Pattern(\"SSN5 (medium)\", r\"\\b([0-9]{3})[- .]([0-9]{2})[- .]([0-9]{4})\\b\", 0.5),\n    ]\n\n    CONTEXT = [\n        \"social\",\n        \"security\",\n        # \"sec\", # Task #603: Support keyphrases (\"social sec\")\n        \"ssn\",\n        \"ssns\",\n        # \"ssn#\",  # iss:1452 - a # does not work with LemmaContextAwareEnhancer\n        # \"ss#\",  # iss:1452 - a # does not work with LemmaContextAwareEnhancer\n        \"ssid\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"US_SSN\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def invalidate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"\n        Check if the pattern text cannot be validated as a US_SSN entity.\n\n        :param pattern_text: Text detected as pattern by regex\n        :return: True if invalidated\n        \"\"\"\n        # if there are delimiters, make sure both delimiters are the same\n        delimiter_counts = defaultdict(int)\n        for c in pattern_text:\n            if c in (\".\", \"-\", \" \"):\n                delimiter_counts[c] += 1\n        if len(delimiter_counts.keys()) &gt; 1:\n            # mismatched delimiters\n            return True\n\n        only_digits = \"\".join(c for c in pattern_text if c.isdigit())\n        if all(only_digits[0] == c for c in only_digits):\n            # cannot be all same digit\n            return True\n\n        if only_digits[3:5] == \"00\" or only_digits[5:] == \"0000\":\n            # groups cannot be all zeros\n            return True\n\n        for sample_ssn in (\"000\", \"666\", \"123456789\", \"98765432\", \"078051120\"):\n            if only_digits.startswith(sample_ssn):\n                return True\n\n        return False\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UsSsnRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Check if the pattern text cannot be validated as a US_SSN entity.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>Text detected as pattern by regex</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if invalidated</p> Source code in <code>presidio_analyzer/predefined_recognizers/country_specific/us/us_ssn_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"\n    Check if the pattern text cannot be validated as a US_SSN entity.\n\n    :param pattern_text: Text detected as pattern by regex\n    :return: True if invalidated\n    \"\"\"\n    # if there are delimiters, make sure both delimiters are the same\n    delimiter_counts = defaultdict(int)\n    for c in pattern_text:\n        if c in (\".\", \"-\", \" \"):\n            delimiter_counts[c] += 1\n    if len(delimiter_counts.keys()) &gt; 1:\n        # mismatched delimiters\n        return True\n\n    only_digits = \"\".join(c for c in pattern_text if c.isdigit())\n    if all(only_digits[0] == c for c in only_digits):\n        # cannot be all same digit\n        return True\n\n    if only_digits[3:5] == \"00\" or only_digits[5:] == \"0000\":\n        # groups cannot be all zeros\n        return True\n\n    for sample_ssn in (\"000\", \"666\", \"123456789\", \"98765432\", \"078051120\"):\n        if only_digits.startswith(sample_ssn):\n            return True\n\n    return False\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer","title":"CreditCardRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize common credit card numbers using regex + checksum.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'CREDIT_CARD'</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/credit_card_recognizer.py</code> <pre><code>class CreditCardRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize common credit card numbers using regex + checksum.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"All Credit Cards (weak)\",\n            r\"\\b(?!1\\d{12}(?!\\d))((4\\d{3})|(5[0-5]\\d{2})|(6\\d{3})|(1\\d{3})|(3\\d{3}))[- ]?(\\d{3,4})[- ]?(\\d{3,4})[- ]?(\\d{3,5})\\b\",  # noqa: E501\n            0.3,\n        ),\n    ]\n\n    CONTEXT = [\n        \"credit\",\n        \"card\",\n        \"visa\",\n        \"mastercard\",\n        \"cc \",\n        \"amex\",\n        \"discover\",\n        \"jcb\",\n        \"diners\",\n        \"maestro\",\n        \"instapayment\",\n    ]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"CREDIT_CARD\",\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = (\n            replacement_pairs if replacement_pairs else [(\"-\", \"\"), (\" \", \"\")]\n        )\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:  # noqa D102\n        sanitized_value = EntityRecognizer.sanitize_value(\n            pattern_text, self.replacement_pairs\n        )\n        checksum = self.__luhn_checksum(sanitized_value)\n\n        return checksum\n\n    @staticmethod\n    def __luhn_checksum(sanitized_value: str) -&gt; bool:\n        def digits_of(n: str) -&gt; List[int]:\n            return [int(dig) for dig in str(n)]\n\n        digits = digits_of(sanitized_value)\n        odd_digits = digits[-1::-2]\n        even_digits = digits[-2::-2]\n        checksum = sum(odd_digits)\n        for d in even_digits:\n            checksum += sum(digits_of(str(d * 2)))\n        return checksum % 10 == 0\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CreditCardRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer","title":"CryptoRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize common crypto account numbers using regex + checksum.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'CRYPTO'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>validate_result</code> <p>Validate the Bitcoin address using checksum.</p> <code>bech32_polymod</code> <p>Compute the Bech32 checksum.</p> <code>bech32_hrp_expand</code> <p>Expand the HRP into values for checksum computation.</p> <code>bech32_verify_checksum</code> <p>Verify a checksum given HRP and converted data characters.</p> <code>bech32_decode</code> <p>Validate a Bech32/Bech32m string, and determine HRP and data.</p> <code>validate_bech32_address</code> <p>Validate a Bech32 or Bech32m address.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/crypto_recognizer.py</code> <pre><code>class CryptoRecognizer(PatternRecognizer):\n    \"\"\"Recognize common crypto account numbers using regex + checksum.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\"Crypto (Medium)\", r\"(bc1|[13])[a-zA-HJ-NP-Z0-9]{25,59}\", 0.5),\n    ]\n\n    CONTEXT = [\"wallet\", \"btc\", \"bitcoin\", \"crypto\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"CRYPTO\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"Validate the Bitcoin address using checksum.\n\n        :param pattern_text: The cryptocurrency address to validate.\n        :return: True if the address is valid according to its respective\n        format, False otherwise.\n        \"\"\"\n        if pattern_text.startswith(\"1\") or pattern_text.startswith(\"3\"):\n            # P2PKH or P2SH address validation\n            try:\n                bcbytes = self.__decode_base58(str.encode(pattern_text))\n                checksum = sha256(sha256(bcbytes[:-4]).digest()).digest()[:4]\n                return bcbytes[-4:] == checksum\n            except ValueError:\n                return False\n        elif pattern_text.startswith(\"bc1\"):\n            # Bech32 or Bech32m address validation\n            if CryptoRecognizer.validate_bech32_address(pattern_text)[0]:\n                return True\n        return False\n\n    @staticmethod\n    def __decode_base58(bc: bytes) -&gt; bytes:\n        digits58 = b\"123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz\"\n        origlen = len(bc)\n        bc = bc.lstrip(digits58[0:1])\n\n        n = 0\n        for char in bc:\n            n = n * 58 + digits58.index(char)\n        return n.to_bytes(origlen - len(bc) + (n.bit_length() + 7) // 8, \"big\")\n\n    @staticmethod\n    def bech32_polymod(values):\n        \"\"\"Compute the Bech32 checksum.\"\"\"\n        generator = [0x3B6A57B2, 0x26508E6D, 0x1EA119FA, 0x3D4233DD, 0x2A1462B3]\n        chk = 1\n        for value in values:\n            top = chk &gt;&gt; 25\n            chk = (chk &amp; 0x1FFFFFF) &lt;&lt; 5 ^ value\n            for i in range(5):\n                chk ^= generator[i] if ((top &gt;&gt; i) &amp; 1) else 0\n        return chk\n\n    @staticmethod\n    def bech32_hrp_expand(hrp):\n        \"\"\"Expand the HRP into values for checksum computation.\"\"\"\n        return [ord(x) &gt;&gt; 5 for x in hrp] + [0] + [ord(x) &amp; 31 for x in hrp]\n\n    @staticmethod\n    def bech32_verify_checksum(hrp, data):\n        \"\"\"Verify a checksum given HRP and converted data characters.\"\"\"\n        const = CryptoRecognizer.bech32_polymod(\n            CryptoRecognizer.bech32_hrp_expand(hrp) + data\n        )\n        if const == 1:\n            return BECH32\n        if const == BECH32M_CONST:\n            return BECH32M\n        return None\n\n    @staticmethod\n    def bech32_decode(bech):\n        \"\"\"Validate a Bech32/Bech32m string, and determine HRP and data.\"\"\"\n        if (any(ord(x) &lt; 33 or ord(x) &gt; 126 for x in bech)) or (\n            bech.lower() != bech and bech.upper() != bech\n        ):\n            return (None, None, None)\n        bech = bech.lower()\n        pos = bech.rfind(\"1\")\n        if pos &lt; 1 or pos + 7 &gt; len(bech) or len(bech) &gt; 90:\n            return (None, None, None)\n        if not all(x in CHARSET for x in bech[pos + 1 :]):\n            return (None, None, None)\n        hrp = bech[:pos]\n        data = [CHARSET.find(x) for x in bech[pos + 1 :]]\n        spec = CryptoRecognizer.bech32_verify_checksum(hrp, data)\n        if spec is None:\n            return (None, None, None)\n        return (hrp, data[:-6], spec)\n\n    @staticmethod\n    def validate_bech32_address(address):\n        \"\"\"Validate a Bech32 or Bech32m address.\"\"\"\n        hrp, data, spec = CryptoRecognizer.bech32_decode(address)\n        if hrp is not None and data is not None:\n            return True, spec\n        return False, None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Validate the Bitcoin address using checksum.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>The cryptocurrency address to validate.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the address is valid according to its respective format, False otherwise.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/crypto_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"Validate the Bitcoin address using checksum.\n\n    :param pattern_text: The cryptocurrency address to validate.\n    :return: True if the address is valid according to its respective\n    format, False otherwise.\n    \"\"\"\n    if pattern_text.startswith(\"1\") or pattern_text.startswith(\"3\"):\n        # P2PKH or P2SH address validation\n        try:\n            bcbytes = self.__decode_base58(str.encode(pattern_text))\n            checksum = sha256(sha256(bcbytes[:-4]).digest()).digest()[:4]\n            return bcbytes[-4:] == checksum\n        except ValueError:\n            return False\n    elif pattern_text.startswith(\"bc1\"):\n        # Bech32 or Bech32m address validation\n        if CryptoRecognizer.validate_bech32_address(pattern_text)[0]:\n            return True\n    return False\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.bech32_polymod","title":"bech32_polymod  <code>staticmethod</code>","text":"<pre><code>bech32_polymod(values)\n</code></pre> <p>Compute the Bech32 checksum.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/crypto_recognizer.py</code> <pre><code>@staticmethod\ndef bech32_polymod(values):\n    \"\"\"Compute the Bech32 checksum.\"\"\"\n    generator = [0x3B6A57B2, 0x26508E6D, 0x1EA119FA, 0x3D4233DD, 0x2A1462B3]\n    chk = 1\n    for value in values:\n        top = chk &gt;&gt; 25\n        chk = (chk &amp; 0x1FFFFFF) &lt;&lt; 5 ^ value\n        for i in range(5):\n            chk ^= generator[i] if ((top &gt;&gt; i) &amp; 1) else 0\n    return chk\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.bech32_hrp_expand","title":"bech32_hrp_expand  <code>staticmethod</code>","text":"<pre><code>bech32_hrp_expand(hrp)\n</code></pre> <p>Expand the HRP into values for checksum computation.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/crypto_recognizer.py</code> <pre><code>@staticmethod\ndef bech32_hrp_expand(hrp):\n    \"\"\"Expand the HRP into values for checksum computation.\"\"\"\n    return [ord(x) &gt;&gt; 5 for x in hrp] + [0] + [ord(x) &amp; 31 for x in hrp]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.bech32_verify_checksum","title":"bech32_verify_checksum  <code>staticmethod</code>","text":"<pre><code>bech32_verify_checksum(hrp, data)\n</code></pre> <p>Verify a checksum given HRP and converted data characters.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/crypto_recognizer.py</code> <pre><code>@staticmethod\ndef bech32_verify_checksum(hrp, data):\n    \"\"\"Verify a checksum given HRP and converted data characters.\"\"\"\n    const = CryptoRecognizer.bech32_polymod(\n        CryptoRecognizer.bech32_hrp_expand(hrp) + data\n    )\n    if const == 1:\n        return BECH32\n    if const == BECH32M_CONST:\n        return BECH32M\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.bech32_decode","title":"bech32_decode  <code>staticmethod</code>","text":"<pre><code>bech32_decode(bech)\n</code></pre> <p>Validate a Bech32/Bech32m string, and determine HRP and data.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/crypto_recognizer.py</code> <pre><code>@staticmethod\ndef bech32_decode(bech):\n    \"\"\"Validate a Bech32/Bech32m string, and determine HRP and data.\"\"\"\n    if (any(ord(x) &lt; 33 or ord(x) &gt; 126 for x in bech)) or (\n        bech.lower() != bech and bech.upper() != bech\n    ):\n        return (None, None, None)\n    bech = bech.lower()\n    pos = bech.rfind(\"1\")\n    if pos &lt; 1 or pos + 7 &gt; len(bech) or len(bech) &gt; 90:\n        return (None, None, None)\n    if not all(x in CHARSET for x in bech[pos + 1 :]):\n        return (None, None, None)\n    hrp = bech[:pos]\n    data = [CHARSET.find(x) for x in bech[pos + 1 :]]\n    spec = CryptoRecognizer.bech32_verify_checksum(hrp, data)\n    if spec is None:\n        return (None, None, None)\n    return (hrp, data[:-6], spec)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.CryptoRecognizer.validate_bech32_address","title":"validate_bech32_address  <code>staticmethod</code>","text":"<pre><code>validate_bech32_address(address)\n</code></pre> <p>Validate a Bech32 or Bech32m address.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/crypto_recognizer.py</code> <pre><code>@staticmethod\ndef validate_bech32_address(address):\n    \"\"\"Validate a Bech32 or Bech32m address.\"\"\"\n    hrp, data, spec = CryptoRecognizer.bech32_decode(address)\n    if hrp is not None and data is not None:\n        return True, spec\n    return False, None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer","title":"DateRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize date using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'DATE_TIME'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/date_recognizer.py</code> <pre><code>class DateRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize date using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"ISO 8601 datetime\",\n            r\"\\b(\\d{4}-[01]\\d-[0-3]\\dT[0-2]\\d:[0-5]\\d:[0-5]\\d\\.\\d+([+-][0-2]\\d:[0-5]\\d|Z))|(\\d{4}-[01]\\d-[0-3]\\dT[0-2]\\d:[0-5]\\d:[0-5]\\d([+-][0-2]\\d:[0-5]\\d|Z))|(\\d{4}-[01]\\d-[0-3]\\dT[0-2]\\d:[0-5]\\d([+-][0-2]\\d:[0-5]\\d|Z))\\b\",  # noqa: E501\n            0.8,\n        ),\n        Pattern(\n            \"mm/dd/yyyy or mm/dd/yy\",\n            r\"\\b(([1-9]|0[1-9]|1[0-2])/([1-9]|0[1-9]|[1-2][0-9]|3[0-1])/(\\d{4}|\\d{2}))\\b\",  # noqa: E501\n            0.6,\n        ),\n        Pattern(\n            \"dd/mm/yyyy or dd/mm/yy\",\n            r\"\\b(([1-9]|0[1-9]|[1-2][0-9]|3[0-1])/([1-9]|0[1-9]|1[0-2])/(\\d{4}|\\d{2}))\\b\",  # noqa: E501\n            0.6,\n        ),\n        Pattern(\n            \"yyyy/mm/dd\",\n            r\"\\b(\\d{4}/([1-9]|0[1-9]|1[0-2])/([1-9]|0[1-9]|[1-2][0-9]|3[0-1]))\\b\",\n            0.6,\n        ),\n        Pattern(\n            \"mm-dd-yyyy\",\n            r\"\\b(([1-9]|0[1-9]|1[0-2])-([1-9]|0[1-9]|[1-2][0-9]|3[0-1])-\\d{4})\\b\",\n            0.6,\n        ),\n        Pattern(\n            \"dd-mm-yyyy\",\n            r\"\\b(([1-9]|0[1-9]|[1-2][0-9]|3[0-1])-([1-9]|0[1-9]|1[0-2])-\\d{4})\\b\",\n            0.6,\n        ),\n        Pattern(\n            \"yyyy-mm-dd\",\n            r\"\\b(\\d{4}-([1-9]|0[1-9]|1[0-2])-([1-9]|0[1-9]|[1-2][0-9]|3[0-1]))\\b\",\n            0.6,\n        ),\n        Pattern(\n            \"dd.mm.yyyy or dd.mm.yy\",\n            r\"\\b(([1-9]|0[1-9]|[1-2][0-9]|3[0-1])\\.([1-9]|0[1-9]|1[0-2])\\.(\\d{4}|\\d{2}))\\b\",  # noqa: E501\n            0.6,\n        ),\n        Pattern(\n            \"dd-MMM-yyyy or dd-MMM-yy\",\n            r\"\\b(([1-9]|0[1-9]|[1-2][0-9]|3[0-1])-(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)-(\\d{4}|\\d{2}))\\b\",  # noqa: E501\n            0.6,\n        ),\n        Pattern(\n            \"MMM-yyyy or MMM-yy\",\n            r\"\\b((JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)-(\\d{4}|\\d{2}))\\b\",  # noqa: E501\n            0.6,\n        ),\n        Pattern(\n            \"dd-MMM or dd-MMM\",\n            r\"\\b(([1-9]|0[1-9]|[1-2][0-9]|3[0-1])-(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC))\\b\",  # noqa: E501\n            0.6,\n        ),\n        Pattern(\n            \"mm/yyyy or m/yyyy\",\n            r\"\\b(([1-9]|0[1-9]|1[0-2])/\\d{4})\\b\",\n            0.2,\n        ),\n        Pattern(\n            \"mm/yy or m/yy\",\n            r\"\\b(([1-9]|0[1-9]|1[0-2])/\\d{2})\\b\",\n            0.1,\n        ),\n    ]\n\n    CONTEXT = [\"date\", \"birthday\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"DATE_TIME\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.DateRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer","title":"EmailRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize email addresses using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'EMAIL_ADDRESS'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/email_recognizer.py</code> <pre><code>class EmailRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize email addresses using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"Email (Medium)\",\n            r\"\\b((([!#$%&amp;'*+\\-/=?^_`{|}~\\w])|([!#$%&amp;'*+\\-/=?^_`{|}~\\w][!#$%&amp;'*+\\-/=?^_`{|}~\\.\\w]{0,}[!#$%&amp;'*+\\-/=?^_`{|}~\\w]))[@]\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*)\\b\",  # noqa: E501\n            0.5,\n        ),\n    ]\n\n    CONTEXT = [\"email\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"EMAIL_ADDRESS\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def validate_result(self, pattern_text: str):  # noqa D102\n        result = tldextract.extract(pattern_text)\n        return result.fqdn != \"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.EmailRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer","title":"IbanRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize IBAN code using regex and checksum.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IBAN_CODE'</code> </p> <code>exact_match</code> <p>Whether patterns should be exactly matched or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>bos_eos</code> <p>Tuple of strings for beginning of string (BOS) and end of string (EOS)</p> <p> TYPE: <code>Tuple[str, str]</code> DEFAULT: <code>(BOS, EOS)</code> </p> <code>regex_flags</code> <p>Regex flags options</p> <p> TYPE: <code>int</code> DEFAULT: <code>DOTALL | MULTILINE</code> </p> <code>replacement_pairs</code> <p>List of tuples with potential replacement values for different strings to be used during pattern matching. This can allow a greater variety in input, for example by removing dashes or spaces.</p> <p> TYPE: <code>Optional[List[Tuple[str, str]]]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>analyze</code> <p>Analyze IBAN.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/iban_recognizer.py</code> <pre><code>class IbanRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize IBAN code using regex and checksum.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    :param exact_match: Whether patterns should be exactly matched or not\n    :param bos_eos: Tuple of strings for beginning of string (BOS)\n    and end of string (EOS)\n    :param regex_flags: Regex flags options\n    :param replacement_pairs: List of tuples with potential replacement values\n    for different strings to be used during pattern matching.\n    This can allow a greater variety in input, for example by removing dashes or spaces.\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"IBAN Generic\",\n            r\"\\b([A-Z]{2}[ \\-]?[0-9]{2})(?=(?:[ \\-]?[A-Z0-9]){9,30})((?:[ \\-]?[A-Z0-9]{3,5}){2})\"  # noqa\n            r\"([ \\-]?[A-Z0-9]{3,5})?([ \\-]?[A-Z0-9]{3,5})?([ \\-]?[A-Z0-9]{3,5})?([ \\-]?[A-Z0-9]{3,5})?([ \\-]?[A-Z0-9]{3,5})?\"  # noqa\n            r\"([ \\-]?[A-Z0-9]{1,3})?\\b\",  # noqa\n            0.5,\n        ),\n    ]\n\n    CONTEXT = [\"iban\", \"bank\", \"transaction\"]\n\n    LETTERS: Dict[int, str] = {\n        ord(d): str(i) for i, d in enumerate(string.digits + string.ascii_uppercase)\n    }\n\n    def __init__(\n        self,\n        patterns: List[str] = None,\n        context: List[str] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"IBAN_CODE\",\n        exact_match: bool = False,\n        bos_eos: Tuple[str, str] = (BOS, EOS),\n        regex_flags: int = re.DOTALL | re.MULTILINE,\n        replacement_pairs: Optional[List[Tuple[str, str]]] = None,\n    ):\n        self.replacement_pairs = replacement_pairs or [(\"-\", \"\"), (\" \", \"\")]\n        self.exact_match = exact_match\n        self.BOSEOS = bos_eos if exact_match else ()\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n            global_regex_flags=regex_flags,\n        )\n\n    def validate_result(self, pattern_text: str):  # noqa D102\n        try:\n            pattern_text = EntityRecognizer.sanitize_value(\n                pattern_text, self.replacement_pairs\n            )\n            is_valid_checksum = (\n                self.__generate_iban_check_digits(pattern_text, self.LETTERS)\n                == pattern_text[2:4]\n            )\n            # score = EntityRecognizer.MIN_SCORE\n            result = False\n            if is_valid_checksum:\n                if self.__is_valid_format(pattern_text, self.BOSEOS):\n                    result = True\n                elif self.__is_valid_format(pattern_text.upper(), self.BOSEOS):\n                    result = None\n            return result\n        except ValueError:\n            logger.error(\"Failed to validate text %s\", pattern_text)\n            return False\n\n    def analyze(\n        self,\n        text: str,\n        entities: List[str],\n        nlp_artifacts: NlpArtifacts = None,\n        regex_flags: int = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Analyze IBAN.\"\"\"\n        results = []\n\n        if self.patterns:\n            pattern_result = self.__analyze_patterns(text)\n            results.extend(pattern_result)\n\n        return results\n\n    def __analyze_patterns(self, text: str, flags: int = None):\n        \"\"\"\n        Evaluate all patterns in the provided text.\n\n        Logic includes detecting words in the provided deny list.\n        In a sentence we could get a false positive at the end of our regex, were we\n        want to find the IBAN but not the false positive at the end of the match.\n\n        i.e. \"I want my deposit in DE89370400440532013000 2 days from today.\"\n\n        :param text: text to analyze\n        :param flags: regex flags\n        :return: A list of RecognizerResult\n        \"\"\"\n        flags = flags if flags else self.global_regex_flags\n        results = []\n        for pattern in self.patterns:\n            matches = re.finditer(pattern.regex, text, flags=flags)\n\n            for match in matches:\n                for grp_num in reversed(range(1, len(match.groups()) + 1)):\n                    start = match.span(0)[0]\n                    end = (\n                        match.span(grp_num)[1]\n                        if match.span(grp_num)[1] &gt; 0\n                        else match.span(0)[1]\n                    )\n                    current_match = text[start:end]\n\n                    # Skip empty results\n                    if current_match == \"\":\n                        continue\n\n                    score = pattern.score\n\n                    validation_result = self.validate_result(current_match)\n                    description = PatternRecognizer.build_regex_explanation(\n                        self.name,\n                        pattern.name,\n                        pattern.regex,\n                        score,\n                        validation_result,\n                        flags,\n                    )\n                    pattern_result = RecognizerResult(\n                        entity_type=self.supported_entities[0],\n                        start=start,\n                        end=end,\n                        score=score,\n                        analysis_explanation=description,\n                        recognition_metadata={\n                            RecognizerResult.RECOGNIZER_NAME_KEY: self.name,\n                            RecognizerResult.RECOGNIZER_IDENTIFIER_KEY: self.id,\n                        },\n                    )\n\n                    if validation_result is not None:\n                        if validation_result:\n                            pattern_result.score = EntityRecognizer.MAX_SCORE\n                        else:\n                            pattern_result.score = EntityRecognizer.MIN_SCORE\n\n                    if pattern_result.score &gt; EntityRecognizer.MIN_SCORE:\n                        results.append(pattern_result)\n                        break\n\n        return results\n\n    @staticmethod\n    def __number_iban(iban: str, letters: Dict[int, str]) -&gt; str:\n        return (iban[4:] + iban[:4]).translate(letters)\n\n    @staticmethod\n    def __generate_iban_check_digits(iban: str, letters: Dict[int, str]) -&gt; str:\n        transformed_iban = (iban[:2] + \"00\" + iban[4:]).upper()\n        number_iban = IbanRecognizer.__number_iban(transformed_iban, letters)\n        return f\"{98 - (int(number_iban) % 97):0&gt;2}\"\n\n    @staticmethod\n    def __is_valid_format(\n        iban: str,\n        bos_eos: Tuple[str, str] = (BOS, EOS),\n        flags: int = re.DOTALL | re.MULTILINE,\n    ) -&gt; bool:\n        country_code = iban[:2]\n        if country_code in regex_per_country:\n            country_regex = regex_per_country.get(country_code, \"\")\n            if bos_eos and country_regex:\n                country_regex = bos_eos[0] + country_regex + bos_eos[1]\n            return country_regex and re.match(country_regex, iban, flags=flags)\n\n        return False\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IbanRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: NlpArtifacts = None,\n    regex_flags: int = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyze IBAN.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/iban_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: NlpArtifacts = None,\n    regex_flags: int = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Analyze IBAN.\"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer","title":"IpRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize IP address using regex.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'IP_ADDRESS'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> <code>invalidate_result</code> <p>Check if the pattern text cannot be validated as an IP address.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/ip_recognizer.py</code> <pre><code>class IpRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize IP address using regex.\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    PATTERNS = [\n        Pattern(\n            \"IPv4\",\n            r\"\\b(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\b\",  # noqa: E501\n            0.6,\n        ),\n        Pattern(\n            \"IPv6\",\n            r\"\\b(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))\\b\",  # noqa: E501\n            0.6,\n        ),\n        Pattern(\n            \"IPv6\",\n            r\"::\",\n            0.1,\n        ),\n    ]\n\n    CONTEXT = [\"ip\", \"ipv4\", \"ipv6\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"IP_ADDRESS\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n\n    def invalidate_result(self, pattern_text: str) -&gt; bool:\n        \"\"\"\n        Check if the pattern text cannot be validated as an IP address.\n\n        :param pattern_text: Text detected as pattern by regex\n        :return: True if invalidated\n        \"\"\"\n        try:\n            ipaddress.ip_address(pattern_text)\n        except ValueError:\n            return True\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.IpRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; bool\n</code></pre> <p>Check if the pattern text cannot be validated as an IP address.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>Text detected as pattern by regex</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if invalidated</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/ip_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; bool:\n    \"\"\"\n    Check if the pattern text cannot be validated as an IP address.\n\n    :param pattern_text: Text detected as pattern by regex\n    :return: True if invalidated\n    \"\"\"\n    try:\n        ipaddress.ip_address(pattern_text)\n    except ValueError:\n        return True\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PhoneRecognizer","title":"PhoneRecognizer","text":"<p>               Bases: <code>LocalRecognizer</code></p> <p>Recognize multi-regional phone numbers.</p> <p>Using python-phonenumbers, along with fixed and regional context words.</p> PARAMETER DESCRIPTION <code>context</code> <p>Base context words for enhancing the assurance scores.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_regions</code> <p>The regions for phone number matching and validation</p> <p> DEFAULT: <code>DEFAULT_SUPPORTED_REGIONS</code> </p> <code>leniency</code> <p>The strictness level of phone number formats. Accepts values from 0 to 3, where 0 is the lenient and 3 is the most strictest.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>1</code> </p> METHOD DESCRIPTION <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_dict</code> <p>Create EntityRecognizer from a dict input.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>analyze</code> <p>Analyzes text to detect phone numbers using python-phonenumbers.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/phone_recognizer.py</code> <pre><code>class PhoneRecognizer(LocalRecognizer):\n    \"\"\"Recognize multi-regional phone numbers.\n\n     Using python-phonenumbers, along with fixed and regional context words.\n    :param context: Base context words for enhancing the assurance scores.\n    :param supported_language: Language this recognizer supports\n    :param supported_regions: The regions for phone number matching and validation\n    :param leniency: The strictness level of phone number formats.\n    Accepts values from 0 to 3, where 0 is the lenient and 3 is the most strictest.\n    \"\"\"\n\n    SCORE = 0.4\n    CONTEXT = [\"phone\", \"number\", \"telephone\", \"cell\", \"cellphone\", \"mobile\", \"call\"]\n    DEFAULT_SUPPORTED_REGIONS = (\"US\", \"UK\", \"DE\", \"FE\", \"IL\", \"IN\", \"CA\", \"BR\")\n\n    def __init__(\n        self,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        # For all regions, use phonenumbers.SUPPORTED_REGIONS\n        supported_regions=DEFAULT_SUPPORTED_REGIONS,\n        leniency: Optional[int] = 1,\n    ):\n        context = context if context else self.CONTEXT\n        self.supported_regions = supported_regions\n        self.leniency = leniency\n        super().__init__(\n            supported_entities=self.get_supported_entities(),\n            supported_language=supported_language,\n            context=context,\n        )\n\n    def load(self) -&gt; None:  # noqa D102\n        pass\n\n    def get_supported_entities(self):  # noqa D102\n        return [\"PHONE_NUMBER\"]\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Analyzes text to detect phone numbers using python-phonenumbers.\n\n        Iterates over entities, fetching regions, then matching regional\n        phone numbers patterns against the text.\n        :param text: Text to be analyzed\n        :param entities: Entities this recognizer can detect\n        :param nlp_artifacts: Additional metadata from the NLP engine\n        :return: List of phone numbers RecognizerResults\n        \"\"\"\n        results = []\n        for region in self.supported_regions:\n            for match in phonenumbers.PhoneNumberMatcher(\n                text, region, leniency=self.leniency\n            ):\n                try:\n                    parsed_number = phonenumbers.parse(text[match.start : match.end])\n                    region = phonenumbers.region_code_for_number(parsed_number)\n                    results += [\n                        self._get_recognizer_result(match, text, region, nlp_artifacts)\n                    ]\n                except NumberParseException:\n                    results += [\n                        self._get_recognizer_result(match, text, region, nlp_artifacts)\n                    ]\n\n        return EntityRecognizer.remove_duplicates(results)\n\n    def _get_recognizer_result(self, match, text, region, nlp_artifacts):\n        result = RecognizerResult(\n            entity_type=\"PHONE_NUMBER\",\n            start=match.start,\n            end=match.end,\n            score=self.SCORE,\n            analysis_explanation=self._get_analysis_explanation(region),\n            recognition_metadata={\n                RecognizerResult.RECOGNIZER_NAME_KEY: self.name,\n                RecognizerResult.RECOGNIZER_IDENTIFIER_KEY: self.id,\n            },\n        )\n\n        return result\n\n    def _get_analysis_explanation(self, region):\n        return AnalysisExplanation(\n            recognizer=PhoneRecognizer.__name__,\n            original_score=self.SCORE,\n            textual_explanation=f\"Recognized as {region} region phone number, \"\n            f\"using PhoneRecognizer\",\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PhoneRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PhoneRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PhoneRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PhoneRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PhoneRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PhoneRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; EntityRecognizer\n</code></pre> <p>Create EntityRecognizer from a dict input.</p> PARAMETER DESCRIPTION <code>entity_recognizer_dict</code> <p>Dict containing keys and values for instantiation</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PhoneRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PhoneRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.PhoneRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect phone numbers using python-phonenumbers.</p> <p>Iterates over entities, fetching regions, then matching regional phone numbers patterns against the text.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Additional metadata from the NLP engine</p> <p> TYPE: <code>NlpArtifacts</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List of phone numbers RecognizerResults</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/phone_recognizer.py</code> <pre><code>def analyze(\n    self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n) -&gt; List[RecognizerResult]:\n    \"\"\"Analyzes text to detect phone numbers using python-phonenumbers.\n\n    Iterates over entities, fetching regions, then matching regional\n    phone numbers patterns against the text.\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Additional metadata from the NLP engine\n    :return: List of phone numbers RecognizerResults\n    \"\"\"\n    results = []\n    for region in self.supported_regions:\n        for match in phonenumbers.PhoneNumberMatcher(\n            text, region, leniency=self.leniency\n        ):\n            try:\n                parsed_number = phonenumbers.parse(text[match.start : match.end])\n                region = phonenumbers.region_code_for_number(parsed_number)\n                results += [\n                    self._get_recognizer_result(match, text, region, nlp_artifacts)\n                ]\n            except NumberParseException:\n                results += [\n                    self._get_recognizer_result(match, text, region, nlp_artifacts)\n                ]\n\n    return EntityRecognizer.remove_duplicates(results)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer","title":"UrlRecognizer","text":"<p>               Bases: <code>PatternRecognizer</code></p> <p>Recognize urls using regex.</p> <p>This application uses Open Source components: Project: CommonRegex https://github.com/madisonmay/CommonRegex Copyright (c) 2014 Madison May License (MIT)  https://github.com/madisonmay/CommonRegex/blob/master/LICENSE</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to be used by this recognizer</p> <p> TYPE: <code>Optional[List[Pattern]]</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>List of context words to increase confidence in detection</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>supported_language</code> <p>Language this recognizer supports</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>supported_entity</code> <p>The entity this recognizer can detect</p> <p> TYPE: <code>str</code> DEFAULT: <code>'URL'</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize instance into a dictionary.</p> <code>from_dict</code> <p>Create instance from a serialized dict.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>validate_result</code> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <code>invalidate_result</code> <p>Logic to check for result invalidation by running pruning logic.</p> <code>build_regex_explanation</code> <p>Construct an explanation for why this entity was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/generic/url_recognizer.py</code> <pre><code>class UrlRecognizer(PatternRecognizer):\n    \"\"\"\n    Recognize urls using regex.\n\n    This application uses Open Source components:\n    Project: CommonRegex https://github.com/madisonmay/CommonRegex\n    Copyright (c) 2014 Madison May\n    License (MIT)  https://github.com/madisonmay/CommonRegex/blob/master/LICENSE\n\n    :param patterns: List of patterns to be used by this recognizer\n    :param context: List of context words to increase confidence in detection\n    :param supported_language: Language this recognizer supports\n    :param supported_entity: The entity this recognizer can detect\n    \"\"\"\n\n    BASE_URL_REGEX = r\"((www\\d{0,3}[.])?[a-z0-9.\\-]+[.](?:(?:com)|(?:edu)|(?:gov)|(?:int)|(?:mil)|(?:net)|(?:onl)|(?:org)|(?:pro)|(?:red)|(?:tel)|(?:uno)|(?:xxx)|(?:academy)|(?:accountant)|(?:accountants)|(?:actor)|(?:adult)|(?:africa)|(?:agency)|(?:airforce)|(?:apartments)|(?:app)|(?:archi)|(?:army)|(?:art)|(?:asia)|(?:associates)|(?:attorney)|(?:auction)|(?:audio)|(?:auto)|(?:autos)|(?:baby)|(?:band)|(?:bar)|(?:bargains)|(?:beer)|(?:berlin)|(?:best)|(?:bet)|(?:bid)|(?:bike)|(?:bio)|(?:black)|(?:blackfriday)|(?:blog)|(?:blue)|(?:boats)|(?:bond)|(?:boo)|(?:boston)|(?:bot)|(?:boutique)|(?:build)|(?:builders)|(?:business)|(?:buzz)|(?:cab)|(?:cafe)|(?:cam)|(?:camera)|(?:camp)|(?:capital)|(?:car)|(?:cards)|(?:care)|(?:careers)|(?:cars)|(?:casa)|(?:cash)|(?:casino)|(?:catering)|(?:center)|(?:ceo)|(?:cfd)|(?:charity)|(?:chat)|(?:cheap)|(?:christmas)|(?:church)|(?:city)|(?:claims)|(?:cleaning)|(?:click)|(?:clinic)|(?:clothing)|(?:cloud)|(?:club)|(?:codes)|(?:coffee)|(?:college)|(?:com)|(?:community)|(?:company)|(?:computer)|(?:condos)|(?:construction)|(?:consulting)|(?:contact)|(?:contractors)|(?:cooking)|(?:cool)|(?:coupons)|(?:courses)|(?:credit)|(?:creditcard)|(?:cricket)|(?:cruises)|(?:cyou)|(?:dad)|(?:dance)|(?:date)|(?:dating)|(?:day)|(?:degree)|(?:delivery)|(?:democrat)|(?:dental)|(?:dentist)|(?:desi)|(?:design)|(?:dev)|(?:diamonds)|(?:diet)|(?:digital)|(?:direct)|(?:directory)|(?:discount)|(?:doctor)|(?:dog)|(?:domains)|(?:download)|(?:earth)|(?:eco)|(?:education)|(?:email)|(?:energy)|(?:engineer)|(?:engineering)|(?:enterprises)|(?:equipment)|(?:esq)|(?:estate)|(?:events)|(?:exchange)|(?:expert)|(?:exposed)|(?:express)|(?:fail)|(?:faith)|(?:family)|(?:fans)|(?:farm)|(?:fashion)|(?:feedback)|(?:film)|(?:finance)|(?:financial)|(?:fish)|(?:fishing)|(?:fit)|(?:fitness)|(?:flights)|(?:florist)|(?:flowers)|(?:football)|(?:forsale)|(?:foundation)|(?:fun)|(?:fund)|(?:furniture)|(?:futbol)|(?:fyi)|(?:gallery)|(?:game)|(?:games)|(?:garden)|(?:gay)|(?:gdn)|(?:gifts)|(?:gives)|(?:giving)|(?:glass)|(?:global)|(?:gmbh)|(?:gold)|(?:golf)|(?:graphics)|(?:gratis)|(?:green)|(?:gripe)|(?:group)|(?:guide)|(?:guitars)|(?:guru)|(?:hair)|(?:hamburg)|(?:haus)|(?:health)|(?:healthcare)|(?:help)|(?:hiphop)|(?:hockey)|(?:holdings)|(?:holiday)|(?:homes)|(?:horse)|(?:hospital)|(?:host)|(?:hosting)|(?:house)|(?:how)|(?:icu)|(?:info)|(?:ink)|(?:institute)|(?:insure)|(?:international)|(?:investments)|(?:irish)|(?:jewelry)|(?:jetzt)|(?:juegos)|(?:kaufen)|(?:kids)|(?:kitchen)|(?:kiwi)|(?:krd)|(?:kyoto)|(?:land)|(?:lat)|(?:law)|(?:lawyer)|(?:lease)|(?:legal)|(?:lgbt)|(?:life)|(?:lighting)|(?:limited)|(?:limo)|(?:link)|(?:live)|(?:loan)|(?:loans)|(?:lol)|(?:london)|(?:love)|(?:ltd)|(?:ltda)|(?:luxury)|(?:maison)|(?:management)|(?:market)|(?:marketing)|(?:markets)|(?:mba)|(?:media)|(?:melbourne)|(?:meme)|(?:memorial)|(?:men)|(?:miami)|(?:mobi)|(?:moda)|(?:moe)|(?:mom)|(?:money)|(?:monster)|(?:mortgage)|(?:motorcycles)|(?:mov)|(?:movie)|(?:nagoya)|(?:name)|(?:navy)|(?:network)|(?:new)|(?:news)|(?:ngo)|(?:ninja)|(?:now)|(?:nyc)|(?:observer)|(?:okinawa)|(?:one)|(?:ong)|(?:onl)|(?:online)|(?:organic)|(?:osaka)|(?:page)|(?:paris)|(?:partners)|(?:parts)|(?:party)|(?:pet)|(?:phd)|(?:photo)|(?:photography)|(?:photos)|(?:pics)|(?:pictures)|(?:pink)|(?:pizza)|(?:place)|(?:plumbing)|(?:plus)|(?:poker)|(?:porn)|(?:press)|(?:pro)|(?:productions)|(?:prof)|(?:promo)|(?:properties)|(?:property)|(?:protection)|(?:pub)|(?:quest)|(?:racing)|(?:recipes)|(?:red)|(?:rehab)|(?:reise)|(?:reisen)|(?:rent)|(?:rentals)|(?:repair)|(?:report)|(?:republican)|(?:rest)|(?:restaurant)|(?:review)|(?:reviews)|(?:rip)|(?:rocks)|(?:rodeo)|(?:rsvp)|(?:run)|(?:saarland)|(?:sale)|(?:salon)|(?:sarl)|(?:sbs)|(?:school)|(?:schule)|(?:science)|(?:services)|(?:sex)|(?:sexy)|(?:sh)|(?:shoes)|(?:shop)|(?:shopping)|(?:show)|(?:singles)|(?:site)|(?:skin)|(?:soccer)|(?:social)|(?:software)|(?:solar)|(?:solutions)|(?:soy)|(?:space)|(?:spiegel)|(?:study)|(?:style)|(?:sucks)|(?:supply)|(?:support)|(?:surf)|(?:surgery)|(?:systems)|(?:tax)|(?:taxi)|(?:team)|(?:tech)|(?:technology)|(?:tel)|(?:theater)|(?:tips)|(?:tires)|(?:today)|(?:tools)|(?:top)|(?:tours)|(?:town)|(?:toys)|(?:trade)|(?:training)|(?:tube)|(?:uk)|(?:university)|(?:uno)|(?:vacations)|(?:ventures)|(?:vet)|(?:video)|(?:villas)|(?:vin)|(?:vip)|(?:vision)|(?:vlaanderen)|(?:vodka)|(?:vote)|(?:voting)|(?:voyage)|(?:wales)|(?:wang)|(?:watch)|(?:webcam)|(?:website)|(?:wedding)|(?:wiki)|(?:wine)|(?:work)|(?:works)|(?:world)|(?:wtf)|(?:xyz)|(?:yoga)|(?:yokohama)|(?:you)|(?:zone)|(?:ac)|(?:ad)|(?:ae)|(?:af)|(?:ag)|(?:ai)|(?:al)|(?:am)|(?:an)|(?:ao)|(?:aq)|(?:ar)|(?:as)|(?:at)|(?:au)|(?:aw)|(?:ax)|(?:az)|(?:ba)|(?:bb)|(?:bd)|(?:be)|(?:bf)|(?:bg)|(?:bh)|(?:bi)|(?:bj)|(?:bm)|(?:bn)|(?:bo)|(?:br)|(?:bs)|(?:bt)|(?:bv)|(?:bw)|(?:by)|(?:bz)|(?:ca)|(?:cc)|(?:cd)|(?:cf)|(?:cg)|(?:ch)|(?:ci)|(?:ck)|(?:cl)|(?:cm)|(?:cn)|(?:co)|(?:cr)|(?:cu)|(?:cv)|(?:cw)|(?:cx)|(?:cy)|(?:cz)|(?:de)|(?:dj)|(?:dk)|(?:dm)|(?:do)|(?:dz)|(?:ec)|(?:ee)|(?:eg)|(?:er)|(?:es)|(?:et)|(?:eu)|(?:fi)|(?:fj)|(?:fk)|(?:fm)|(?:fo)|(?:fr)|(?:ga)|(?:gb)|(?:gd)|(?:ge)|(?:gf)|(?:gg)|(?:gh)|(?:gi)|(?:gl)|(?:gm)|(?:gn)|(?:gp)|(?:gq)|(?:gr)|(?:gs)|(?:gt)|(?:gu)|(?:gw)|(?:gy)|(?:hk)|(?:hm)|(?:hn)|(?:hr)|(?:ht)|(?:hu)|(?:id)|(?:ie)|(?:il)|(?:im)|(?:in)|(?:io)|(?:iq)|(?:ir)|(?:is)|(?:it)|(?:je)|(?:jm)|(?:jo)|(?:jp)|(?:ke)|(?:kg)|(?:kh)|(?:ki)|(?:km)|(?:kn)|(?:kp)|(?:kr)|(?:kw)|(?:ky)|(?:kz)|(?:la)|(?:lb)|(?:lc)|(?:li)|(?:lk)|(?:lr)|(?:ls)|(?:lt)|(?:lu)|(?:lv)|(?:ly)|(?:ma)|(?:mc)|(?:md)|(?:me)|(?:mg)|(?:mh)|(?:mk)|(?:ml)|(?:mm)|(?:mn)|(?:mo)|(?:mp)|(?:mq)|(?:mr)|(?:ms)|(?:mt)|(?:mu)|(?:mv)|(?:mw)|(?:mx)|(?:my)|(?:mz)|(?:na)|(?:nc)|(?:ne)|(?:nf)|(?:ng)|(?:ni)|(?:nl)|(?:no)|(?:np)|(?:nr)|(?:nu)|(?:nz)|(?:om)|(?:pa)|(?:pe)|(?:pf)|(?:pg)|(?:ph)|(?:pk)|(?:pl)|(?:pm)|(?:pn)|(?:pr)|(?:ps)|(?:pt)|(?:pw)|(?:py)|(?:qa)|(?:re)|(?:ro)|(?:rs)|(?:ru)|(?:rw)|(?:sa)|(?:sb)|(?:sc)|(?:sd)|(?:se)|(?:sg)|(?:sh)|(?:si)|(?:sj)|(?:sk)|(?:sl)|(?:sm)|(?:sn)|(?:so)|(?:sr)|(?:st)|(?:su)|(?:sv)|(?:sx)|(?:sy)|(?:sz)|(?:tc)|(?:td)|(?:tf)|(?:tg)|(?:th)|(?:tj)|(?:tk)|(?:tl)|(?:tm)|(?:tn)|(?:to)|(?:tp)|(?:tr)|(?:tt)|(?:tv)|(?:tw)|(?:tz)|(?:ua)|(?:ug)|(?:uk)|(?:us)|(?:uy)|(?:uz)|(?:va)|(?:vc)|(?:ve)|(?:vg)|(?:vi)|(?:vn)|(?:vu)|(?:wf)|(?:ws)|(?:ye)|(?:yt)|(?:za)|(?:zm)|(?:zw))(?:/[^\\s()&lt;&gt;\\\"']*)?)\"  # noqa: E501\n\n    PATTERNS = [\n        Pattern(\"Standard Url\", \"(?i)(?:https?://)\" + BASE_URL_REGEX, 0.6),\n        Pattern(\"Non schema URL\", \"(?i)\" + BASE_URL_REGEX, 0.5),\n        Pattern(\"Quoted URL\", r'(?i)[\"\\'](https?://' + BASE_URL_REGEX + r')[\"\\']', 0.6),\n        Pattern(\n            \"Quoted Non-schema URL\", r'(?i)[\"\\'](' + BASE_URL_REGEX + r')[\"\\']', 0.5\n        ),\n    ]\n\n    CONTEXT = [\"url\", \"website\", \"link\"]\n\n    def __init__(\n        self,\n        patterns: Optional[List[Pattern]] = None,\n        context: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        supported_entity: str = \"URL\",\n    ):\n        patterns = patterns if patterns else self.PATTERNS\n        context = context if context else self.CONTEXT\n        super().__init__(\n            supported_entity=supported_entity,\n            patterns=patterns,\n            context=context,\n            supported_language=supported_language,\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyzes text to detect PII using regular expressions or deny-lists.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>Entities this recognizer can detect</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>Output values from the NLP engine</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> <code>regex_flags</code> <p>regex flags to be used in regex matching</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; PatternRecognizer\n</code></pre> <p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.validate_result","title":"validate_result","text":"<pre><code>validate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.invalidate_result","title":"invalidate_result","text":"<pre><code>invalidate_result(pattern_text: str) -&gt; Optional[bool]\n</code></pre> <p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> PARAMETER DESCRIPTION <code>pattern_text</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.UrlRecognizer.build_regex_explanation","title":"build_regex_explanation  <code>staticmethod</code>","text":"<pre><code>build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation\n</code></pre> <p>Construct an explanation for why this entity was detected.</p> PARAMETER DESCRIPTION <code>recognizer_name</code> <p>Name of recognizer detecting the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern_name</code> <p>Regex pattern name which detected the entity</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Regex pattern logic</p> <p> TYPE: <code>str</code> </p> <code>original_score</code> <p>Score given by the recognizer</p> <p> TYPE: <code>float</code> </p> <code>validation_result</code> <p>Whether validation was used and its result</p> <p> TYPE: <code>bool</code> </p> <code>regex_flags</code> <p>Regex flags used in the regex matching</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    textual_explanation = (\n        f\"Detected by `{recognizer_name}` \" f\"using pattern `{pattern_name}`\"\n    )\n\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n        textual_explanation=textual_explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer","title":"GLiNERRecognizer","text":"<p>               Bases: <code>LocalRecognizer</code></p> <p>GLiNER model based entity recognizer.</p> METHOD DESCRIPTION <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_dict</code> <p>Create EntityRecognizer from a dict input.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>load</code> <p>Load the GLiNER model.</p> <code>analyze</code> <p>Analyze text to identify entities using a GLiNER model.</p> Source code in <code>presidio_analyzer/predefined_recognizers/ner/gliner_recognizer.py</code> <pre><code>class GLiNERRecognizer(LocalRecognizer):\n    \"\"\"GLiNER model based entity recognizer.\"\"\"\n\n    def __init__(\n        self,\n        supported_entities: Optional[List[str]] = None,\n        name: str = \"GLiNERRecognizer\",\n        supported_language: str = \"en\",\n        version: str = \"0.0.1\",\n        context: Optional[List[str]] = None,\n        entity_mapping: Optional[Dict[str, str]] = None,\n        model_name: str = \"urchade/gliner_multi_pii-v1\",\n        flat_ner: bool = True,\n        multi_label: bool = False,\n        threshold: float = 0.30,\n        map_location: str = \"cpu\",\n    ):\n        \"\"\"GLiNER model based entity recognizer.\n\n        The model is based on the GLiNER library.\n\n        :param supported_entities: List of supported entities for this recognizer.\n        If None, all entities in Presidio's default configuration will be used.\n        see `NerModelConfiguration`\n        :param name: Name of the recognizer\n        :param supported_language: Language code to use for the recognizer\n        :param version: Version of the recognizer\n        :param context: N/A for this recognizer\n        :param model_name: The name of the GLiNER model to load\n        :param flat_ner: Whether to use flat NER or not (see GLiNER's documentation)\n        :param multi_label: Whether to use multi-label classification or not\n        (see GLiNER's documentation)\n        :param threshold: The threshold for the model's output\n        (see GLiNER's documentation)\n        :param map_location: The device to use for the model\n\n\n        \"\"\"\n\n        if entity_mapping:\n            if supported_entities:\n                raise ValueError(\n                    \"entity_mapping and supported_entities cannot be used together\"\n                )\n\n            self.model_to_presidio_entity_mapping = entity_mapping\n        else:\n            if not supported_entities:\n                logger.info(\n                    \"No supported entities provided, \"\n                    \"using default entities from NerModelConfiguration\"\n                )\n                self.model_to_presidio_entity_mapping = (\n                    NerModelConfiguration().model_to_presidio_entity_mapping\n                )\n            else:\n                self.model_to_presidio_entity_mapping = {\n                    entity: entity for entity in supported_entities\n                }\n\n        logger.info(\"Using entity mapping %s\", json.dumps(entity_mapping, indent=2))\n        supported_entities = list(set(self.model_to_presidio_entity_mapping.values()))\n        self.model_name = model_name\n        self.map_location = map_location\n        self.flat_ner = flat_ner\n        self.multi_label = multi_label\n        self.threshold = threshold\n\n        self.gliner = None\n\n        super().__init__(\n            supported_entities=supported_entities,\n            name=name,\n            supported_language=supported_language,\n            version=version,\n            context=context,\n        )\n\n        self.gliner_labels = list(self.model_to_presidio_entity_mapping.keys())\n\n    def load(self) -&gt; None:\n        \"\"\"Load the GLiNER model.\"\"\"\n        if not GLiNER:\n            raise ImportError(\"GLiNER is not installed. Please install it.\")\n        self.gliner = GLiNER.from_pretrained(self.model_name)\n\n    def analyze(\n        self,\n        text: str,\n        entities: List[str],\n        nlp_artifacts: Optional[NlpArtifacts] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Analyze text to identify entities using a GLiNER model.\n\n        :param text: The text to be analyzed\n        :param entities: The list of entities this recognizer is requested to return\n        :param nlp_artifacts: N/A for this recognizer\n        \"\"\"\n\n        # combine the input labels as this model allows for ad-hoc labels\n        labels = self.__create_input_labels(entities)\n\n        predictions = self.gliner.predict_entities(\n            text=text,\n            labels=labels,\n            flat_ner=self.flat_ner,\n            threshold=self.threshold,\n            multi_label=self.multi_label,\n        )\n        recognizer_results = []\n        for prediction in predictions:\n            presidio_entity = self.model_to_presidio_entity_mapping.get(\n                prediction[\"label\"], prediction[\"label\"]\n            )\n            if entities and presidio_entity not in entities:\n                continue\n\n            analysis_explanation = AnalysisExplanation(\n                recognizer=self.name,\n                original_score=prediction[\"score\"],\n                textual_explanation=f\"Identified as {presidio_entity} by GLiNER\",\n            )\n\n            recognizer_results.append(\n                RecognizerResult(\n                    entity_type=presidio_entity,\n                    start=prediction[\"start\"],\n                    end=prediction[\"end\"],\n                    score=prediction[\"score\"],\n                    analysis_explanation=analysis_explanation,\n                )\n            )\n\n        return recognizer_results\n\n    def __create_input_labels(self, entities):\n        \"\"\"Append the entities requested by the user to the list of labels if it's not there.\"\"\"  # noqa: E501\n        labels = self.gliner_labels\n        for entity in entities:\n            if (\n                entity not in self.model_to_presidio_entity_mapping.values()\n                and entity not in self.gliner_labels\n            ):\n                labels.append(entity)\n        return labels\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; EntityRecognizer\n</code></pre> <p>Create EntityRecognizer from a dict input.</p> PARAMETER DESCRIPTION <code>entity_recognizer_dict</code> <p>Dict containing keys and values for instantiation</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.load","title":"load","text":"<pre><code>load() -&gt; None\n</code></pre> <p>Load the GLiNER model.</p> Source code in <code>presidio_analyzer/predefined_recognizers/ner/gliner_recognizer.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the GLiNER model.\"\"\"\n    if not GLiNER:\n        raise ImportError(\"GLiNER is not installed. Please install it.\")\n    self.gliner = GLiNER.from_pretrained(self.model_name)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.GLiNERRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str, entities: List[str], nlp_artifacts: Optional[NlpArtifacts] = None\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyze text to identify entities using a GLiNER model.</p> PARAMETER DESCRIPTION <code>text</code> <p>The text to be analyzed</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>The list of entities this recognizer is requested to return</p> <p> TYPE: <code>List[str]</code> </p> <code>nlp_artifacts</code> <p>N/A for this recognizer</p> <p> TYPE: <code>Optional[NlpArtifacts]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/predefined_recognizers/ner/gliner_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Analyze text to identify entities using a GLiNER model.\n\n    :param text: The text to be analyzed\n    :param entities: The list of entities this recognizer is requested to return\n    :param nlp_artifacts: N/A for this recognizer\n    \"\"\"\n\n    # combine the input labels as this model allows for ad-hoc labels\n    labels = self.__create_input_labels(entities)\n\n    predictions = self.gliner.predict_entities(\n        text=text,\n        labels=labels,\n        flat_ner=self.flat_ner,\n        threshold=self.threshold,\n        multi_label=self.multi_label,\n    )\n    recognizer_results = []\n    for prediction in predictions:\n        presidio_entity = self.model_to_presidio_entity_mapping.get(\n            prediction[\"label\"], prediction[\"label\"]\n        )\n        if entities and presidio_entity not in entities:\n            continue\n\n        analysis_explanation = AnalysisExplanation(\n            recognizer=self.name,\n            original_score=prediction[\"score\"],\n            textual_explanation=f\"Identified as {presidio_entity} by GLiNER\",\n        )\n\n        recognizer_results.append(\n            RecognizerResult(\n                entity_type=presidio_entity,\n                start=prediction[\"start\"],\n                end=prediction[\"end\"],\n                score=prediction[\"score\"],\n                analysis_explanation=analysis_explanation,\n            )\n        )\n\n    return recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer","title":"SpacyRecognizer","text":"<p>               Bases: <code>LocalRecognizer</code></p> <p>Recognize PII entities using a spaCy NLP model.</p> <pre><code>Since the spaCy pipeline is ran by the AnalyzerEngine/SpacyNlpEngine,\nthis recognizer only extracts the entities from the NlpArtifacts\nand returns them.\n</code></pre> METHOD DESCRIPTION <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_dict</code> <p>Create EntityRecognizer from a dict input.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>build_explanation</code> <p>Create explanation for why this result was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/nlp_engine_recognizers/spacy_recognizer.py</code> <pre><code>class SpacyRecognizer(LocalRecognizer):\n    \"\"\"\n    Recognize PII entities using a spaCy NLP model.\n\n        Since the spaCy pipeline is ran by the AnalyzerEngine/SpacyNlpEngine,\n        this recognizer only extracts the entities from the NlpArtifacts\n        and returns them.\n\n    \"\"\"\n\n    ENTITIES = [\"DATE_TIME\", \"NRP\", \"LOCATION\", \"PERSON\", \"ORGANIZATION\"]\n\n    DEFAULT_EXPLANATION = \"Identified as {} by Spacy's Named Entity Recognition\"\n\n    # deprecated, use MODEL_TO_PRESIDIO_MAPPING in NerModelConfiguration instead\n    CHECK_LABEL_GROUPS = [\n        ({\"LOCATION\"}, {\"GPE\", \"LOC\"}),\n        ({\"PERSON\", \"PER\"}, {\"PERSON\", \"PER\"}),\n        ({\"DATE_TIME\"}, {\"DATE\", \"TIME\"}),\n        ({\"NRP\"}, {\"NORP\"}),\n        ({\"ORGANIZATION\"}, {\"ORG\"}),\n    ]\n\n    def __init__(\n        self,\n        supported_language: str = \"en\",\n        supported_entities: Optional[List[str]] = None,\n        ner_strength: float = 0.85,\n        default_explanation: Optional[str] = None,\n        check_label_groups: Optional[List[Tuple[Set, Set]]] = None,\n        context: Optional[List[str]] = None,\n    ):\n        \"\"\"\n\n        :param supported_language: Language this recognizer supports\n        :param supported_entities: The entities this recognizer can detect\n        :param ner_strength: Default confidence for NER prediction\n        :param check_label_groups: (DEPRECATED) Tuple containing Presidio entity names\n        :param default_explanation: Default explanation for the results when using return_decision_process=True\n        \"\"\"  # noqa E501\n\n        self.ner_strength = ner_strength\n        if check_label_groups:\n            warnings.warn(\n                \"check_label_groups is deprecated and isn't used;\"\n                \"entities are mapped in NerModelConfiguration\",\n                DeprecationWarning,\n                2,\n            )\n\n        self.default_explanation = (\n            default_explanation if default_explanation else self.DEFAULT_EXPLANATION\n        )\n        supported_entities = supported_entities if supported_entities else self.ENTITIES\n        super().__init__(\n            supported_entities=supported_entities,\n            supported_language=supported_language,\n            context=context,\n        )\n\n    def load(self) -&gt; None:  # noqa D102\n        # no need to load anything as the analyze method already receives\n        # preprocessed nlp artifacts\n        pass\n\n    def build_explanation(\n        self, original_score: float, explanation: str\n    ) -&gt; AnalysisExplanation:\n        \"\"\"\n        Create explanation for why this result was detected.\n\n        :param original_score: Score given by this recognizer\n        :param explanation: Explanation string\n        :return:\n        \"\"\"\n        explanation = AnalysisExplanation(\n            recognizer=self.name,\n            original_score=original_score,\n            textual_explanation=explanation,\n        )\n        return explanation\n\n    def analyze(self, text: str, entities, nlp_artifacts=None):  # noqa D102\n        results = []\n        if not nlp_artifacts:\n            logger.warning(\"Skipping SpaCy, nlp artifacts not provided...\")\n            return results\n\n        ner_entities = nlp_artifacts.entities\n        ner_scores = nlp_artifacts.scores\n\n        for ner_entity, ner_score in zip(ner_entities, ner_scores):\n            if (\n                ner_entity.label_ not in entities\n                or ner_entity.label_ not in self.supported_entities\n            ):\n                logger.debug(\n                    f\"Skipping entity {ner_entity.label_} \"\n                    f\"as it is not in the supported entities list\"\n                )\n                continue\n\n            textual_explanation = self.DEFAULT_EXPLANATION.format(ner_entity.label_)\n            explanation = self.build_explanation(ner_score, textual_explanation)\n            spacy_result = RecognizerResult(\n                entity_type=ner_entity.label_,\n                start=ner_entity.start_char,\n                end=ner_entity.end_char,\n                score=ner_score,\n                analysis_explanation=explanation,\n                recognition_metadata={\n                    RecognizerResult.RECOGNIZER_NAME_KEY: self.name,\n                    RecognizerResult.RECOGNIZER_IDENTIFIER_KEY: self.id,\n                },\n            )\n            results.append(spacy_result)\n\n        return results\n\n    @staticmethod\n    def __check_label(\n        entity: str, label: str, check_label_groups: Tuple[Set, Set]\n    ) -&gt; bool:\n        raise DeprecationWarning(\"__check_label is deprecated\")\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; EntityRecognizer\n</code></pre> <p>Create EntityRecognizer from a dict input.</p> PARAMETER DESCRIPTION <code>entity_recognizer_dict</code> <p>Dict containing keys and values for instantiation</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.SpacyRecognizer.build_explanation","title":"build_explanation","text":"<pre><code>build_explanation(\n    original_score: float, explanation: str\n) -&gt; AnalysisExplanation\n</code></pre> <p>Create explanation for why this result was detected.</p> PARAMETER DESCRIPTION <code>original_score</code> <p>Score given by this recognizer</p> <p> TYPE: <code>float</code> </p> <code>explanation</code> <p>Explanation string</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> Source code in <code>presidio_analyzer/predefined_recognizers/nlp_engine_recognizers/spacy_recognizer.py</code> <pre><code>def build_explanation(\n    self, original_score: float, explanation: str\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Create explanation for why this result was detected.\n\n    :param original_score: Score given by this recognizer\n    :param explanation: Explanation string\n    :return:\n    \"\"\"\n    explanation = AnalysisExplanation(\n        recognizer=self.name,\n        original_score=original_score,\n        textual_explanation=explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer","title":"StanzaRecognizer","text":"<p>               Bases: <code>SpacyRecognizer</code></p> <p>Recognize entities using the Stanza NLP package.</p> <p>See https://stanfordnlp.github.io/stanza/. Uses the spaCy-Stanza package (https://github.com/explosion/spacy-stanza) to align Stanza's interface with spaCy's</p> METHOD DESCRIPTION <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_dict</code> <p>Create EntityRecognizer from a dict input.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>build_explanation</code> <p>Create explanation for why this result was detected.</p> Source code in <code>presidio_analyzer/predefined_recognizers/nlp_engine_recognizers/stanza_recognizer.py</code> <pre><code>class StanzaRecognizer(SpacyRecognizer):\n    \"\"\"\n    Recognize entities using the Stanza NLP package.\n\n    See https://stanfordnlp.github.io/stanza/.\n    Uses the spaCy-Stanza package (https://github.com/explosion/spacy-stanza) to align\n    Stanza's interface with spaCy's\n    \"\"\"\n\n    def __init__(self, **kwargs):  # noqa ANN003\n        self.DEFAULT_EXPLANATION = self.DEFAULT_EXPLANATION.replace(\"Spacy\", \"Stanza\")\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; EntityRecognizer\n</code></pre> <p>Create EntityRecognizer from a dict input.</p> PARAMETER DESCRIPTION <code>entity_recognizer_dict</code> <p>Dict containing keys and values for instantiation</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.StanzaRecognizer.build_explanation","title":"build_explanation","text":"<pre><code>build_explanation(\n    original_score: float, explanation: str\n) -&gt; AnalysisExplanation\n</code></pre> <p>Create explanation for why this result was detected.</p> PARAMETER DESCRIPTION <code>original_score</code> <p>Score given by this recognizer</p> <p> TYPE: <code>float</code> </p> <code>explanation</code> <p>Explanation string</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>AnalysisExplanation</code> Source code in <code>presidio_analyzer/predefined_recognizers/nlp_engine_recognizers/spacy_recognizer.py</code> <pre><code>def build_explanation(\n    self, original_score: float, explanation: str\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Create explanation for why this result was detected.\n\n    :param original_score: Score given by this recognizer\n    :param explanation: Explanation string\n    :return:\n    \"\"\"\n    explanation = AnalysisExplanation(\n        recognizer=self.name,\n        original_score=original_score,\n        textual_explanation=explanation,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer","title":"AzureHealthDeidRecognizer","text":"<p>               Bases: <code>RemoteRecognizer</code></p> <p>Wrapper for PHI detection using Azure Health Data Services de-identification.</p> METHOD DESCRIPTION <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_dict</code> <p>Create EntityRecognizer from a dict input.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>get_supported_entities</code> <p>Return the list of entities supported by this recognizer.</p> <code>analyze</code> <p>Analyze text using Azure Health Data Services Deidentification (TAG operation).</p> Source code in <code>presidio_analyzer/predefined_recognizers/third_party/ahds_recognizer.py</code> <pre><code>class AzureHealthDeidRecognizer(RemoteRecognizer):\n    \"\"\"Wrapper for PHI detection using Azure Health Data Services de-identification.\"\"\"\n\n    def __init__(\n        self,\n        supported_entities: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        client: Optional[DeidentificationClient] = None,\n        **kwargs\n    ):\n        \"\"\"\n        Wrap PHI detection using Azure Health Data Services de-identification.\n\n        :param supported_entities: List of supported entities for this recognizer.\n        :param supported_language: Language code (not used, only 'en' supported).\n        :param client: Optional DeidentificationClient instance.\n        :param kwargs: Additional arguments required by the parent class.\n        \"\"\"\n        super().__init__(\n            supported_entities=supported_entities,\n            supported_language=supported_language,\n            name=\"Azure Health Data Services Deidentification\",\n            version=\"1.0.0\",\n            **kwargs\n        )\n\n\n        endpoint = os.getenv(\"AHDS_ENDPOINT\", None)\n\n        if client is None:\n            if endpoint is None:\n                raise ValueError(\n                    \"AHDS de-identification endpoint is required. \"\n                    \"Please provide an endpoint \"\n                    \"or set the AHDS_ENDPOINT environment variable.\"\n                )\n\n            if not DeidentificationClient:\n                raise ImportError(\n                    \"Azure Health Data Services Deidentification SDK is not available. \"\n                    \"Please install azure-health-deidentification and azure-identity.\"\n                )\n\n            credential = DefaultAzureCredential()\n            client = DeidentificationClient(endpoint, credential)\n\n        self.deid_client = client\n\n        if not supported_entities:\n            self.supported_entities = self._get_supported_entities()\n\n    @staticmethod\n    def _get_supported_entities() -&gt; List[str]:\n        if PhiCategory:\n            try:\n                # PhiCategory is an enum, try to get the actual enum names\n                return [category.name for category in PhiCategory]\n            except Exception:\n                return ImportError\n\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"\n        Return the list of entities supported by this recognizer.\n\n        Returns\n            List[str]: A list of supported entity names as strings.\n        \"\"\"\n        return self.supported_entities\n\n    def analyze(\n        self, text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyze text using Azure Health Data Services Deidentification (TAG operation).\n\n        :param text: Text to analyze\n        :param entities: List of entities to return (optional)\n        :param nlp_artifacts: Not used\n        :return: List of RecognizerResult for each PHI entity found\n        \"\"\"\n        if not entities:\n            entities = self.supported_entities\n\n        body = DeidentificationContent(\n            input_text=text,\n            operation_type=DeidentificationOperationType.TAG\n        )\n        result = self.deid_client.deidentify_text(body)\n\n        recognizer_results = []\n        if result.tagger_result and result.tagger_result.entities:\n            for entity in result.tagger_result.entities:\n                category = entity.category.upper()\n                if category not in [e.upper() for e in entities]:\n                    continue\n                analysis_explanation = AzureHealthDeidRecognizer._build_explanation(\n                    entity_type=category\n                )\n                recognizer_results.append(\n                    RecognizerResult(\n                        entity_type=category,\n                        start=entity.offset.code_point,\n                        end=entity.offset.code_point + entity.length.code_point,\n                        score=round(entity.confidence_score, 2),\n                        analysis_explanation=analysis_explanation,\n                    )\n                )\n        return recognizer_results\n\n    @staticmethod\n    def _build_explanation(entity_type: str) -&gt; AnalysisExplanation:\n        explanation = AnalysisExplanation(\n            recognizer=AzureHealthDeidRecognizer.__class__.__name__,\n            original_score=1.0,\n            textual_explanation=(\n            f\"Identified as {entity_type} by Azure Health Data Services \"\n            \"Deidentification\"\n            ),\n        )\n        return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; EntityRecognizer\n</code></pre> <p>Create EntityRecognizer from a dict input.</p> PARAMETER DESCRIPTION <code>entity_recognizer_dict</code> <p>Dict containing keys and values for instantiation</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities supported by this recognizer.</p> <p>Returns     List[str]: A list of supported entity names as strings.</p> Source code in <code>presidio_analyzer/predefined_recognizers/third_party/ahds_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities supported by this recognizer.\n\n    Returns\n        List[str]: A list of supported entity names as strings.\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureHealthDeidRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyze text using Azure Health Data Services Deidentification (TAG operation).</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to analyze</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>List of entities to return (optional)</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>nlp_artifacts</code> <p>Not used</p> <p> TYPE: <code>NlpArtifacts</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List of RecognizerResult for each PHI entity found</p> Source code in <code>presidio_analyzer/predefined_recognizers/third_party/ahds_recognizer.py</code> <pre><code>def analyze(\n    self, text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyze text using Azure Health Data Services Deidentification (TAG operation).\n\n    :param text: Text to analyze\n    :param entities: List of entities to return (optional)\n    :param nlp_artifacts: Not used\n    :return: List of RecognizerResult for each PHI entity found\n    \"\"\"\n    if not entities:\n        entities = self.supported_entities\n\n    body = DeidentificationContent(\n        input_text=text,\n        operation_type=DeidentificationOperationType.TAG\n    )\n    result = self.deid_client.deidentify_text(body)\n\n    recognizer_results = []\n    if result.tagger_result and result.tagger_result.entities:\n        for entity in result.tagger_result.entities:\n            category = entity.category.upper()\n            if category not in [e.upper() for e in entities]:\n                continue\n            analysis_explanation = AzureHealthDeidRecognizer._build_explanation(\n                entity_type=category\n            )\n            recognizer_results.append(\n                RecognizerResult(\n                    entity_type=category,\n                    start=entity.offset.code_point,\n                    end=entity.offset.code_point + entity.length.code_point,\n                    score=round(entity.confidence_score, 2),\n                    analysis_explanation=analysis_explanation,\n                )\n            )\n    return recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer","title":"AzureAILanguageRecognizer","text":"<p>               Bases: <code>RemoteRecognizer</code></p> <p>Wrapper for PII detection using Azure AI Language.</p> METHOD DESCRIPTION <code>enhance_using_context</code> <p>Enhance confidence score using context of the entity.</p> <code>get_supported_language</code> <p>Return the language this recognizer can support.</p> <code>get_version</code> <p>Return the version of this recognizer.</p> <code>to_dict</code> <p>Serialize self to dictionary.</p> <code>from_dict</code> <p>Create EntityRecognizer from a dict input.</p> <code>remove_duplicates</code> <p>Remove duplicate results.</p> <code>sanitize_value</code> <p>Cleanse the input string of the replacement pairs specified as argument.</p> <code>get_supported_entities</code> <p>Return the list of entities this recognizer can identify.</p> <code>analyze</code> <p>Analyze text using Azure AI Language.</p> Source code in <code>presidio_analyzer/predefined_recognizers/third_party/azure_ai_language.py</code> <pre><code>class AzureAILanguageRecognizer(RemoteRecognizer):\n    \"\"\"Wrapper for PII detection using Azure AI Language.\"\"\"\n\n    def __init__(\n        self,\n        supported_entities: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        ta_client: Optional[\"TextAnalyticsClient\"] = None,\n        azure_ai_key: Optional[str] = None,\n        azure_ai_endpoint: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Wrap the PII detection in Azure AI Language.\n\n        :param supported_entities: List of supported entities for this recognizer.\n        If None, all supported entities will be used.\n        :param supported_language: Language code to use for the recognizer.\n        :param ta_client: object of type TextAnalyticsClient. If missing,\n        the client will be created using the key and endpoint.\n        :param azure_ai_key: Azure AI for language key\n        :param azure_ai_endpoint: Azure AI for language endpoint\n        :param kwargs: Additional arguments required by the parent class\n\n        For more info, see https://learn.microsoft.com/en-us/azure/ai-services/language-service/personally-identifiable-information/overview\n        \"\"\"  # noqa E501\n\n        super().__init__(\n            supported_entities=supported_entities,\n            supported_language=supported_language,\n            name=\"Azure AI Language PII\",\n            version=\"5.2.0\",\n            **kwargs,\n        )\n\n        is_available = bool(TextAnalyticsClient)\n        if not ta_client and not is_available:\n            raise ValueError(\n                \"Azure AI Language is not available. \"\n                \"Please install the required dependencies:\"\n                \"1. azure-ai-textanalytics\"\n                \"2. azure-core\"\n            )\n\n        if not supported_entities:\n            self.supported_entities = self.__get_azure_ai_supported_entities()\n\n        if not ta_client:\n            ta_client = self.__authenticate_client(azure_ai_key, azure_ai_endpoint)\n        self.ta_client = ta_client\n\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"\n        Return the list of entities this recognizer can identify.\n\n        :return: A list of the supported entities by this recognizer\n        \"\"\"\n        return self.supported_entities\n\n    @staticmethod\n    def __get_azure_ai_supported_entities() -&gt; List[str]:\n        \"\"\"Return the list of all supported entities for Azure AI Language.\"\"\"\n        from azure.ai.textanalytics._models import PiiEntityCategory  # noqa\n\n        return [r.value.upper() for r in PiiEntityCategory]\n\n    @staticmethod\n    def __authenticate_client(key: str, endpoint: str) -&gt; TextAnalyticsClient:\n        \"\"\"Authenticate the client using the key and endpoint.\n\n        :param key: Azure AI Language key\n        :param endpoint: Azure AI Language endpoint\n        \"\"\"\n        key = key if key else os.getenv(\"AZURE_AI_KEY\", None)\n        endpoint = endpoint if endpoint else os.getenv(\"AZURE_AI_ENDPOINT\", None)\n        if key is None:\n            raise ValueError(\n                \"Azure AI Language key is required. \"\n                \"Please provide a key or set the AZURE_AI_KEY environment variable.\"\n            )\n        if endpoint is None:\n            raise ValueError(\n                \"Azure AI Language endpoint is required. \"\n                \"Please provide an endpoint \"\n                \"or set the AZURE_AI_ENDPOINT environment variable.\"\n            )\n\n        ta_credential = AzureKeyCredential(key)\n        text_analytics_client = TextAnalyticsClient(\n            endpoint=endpoint, credential=ta_credential\n        )\n        return text_analytics_client\n\n    def analyze(\n        self, text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyze text using Azure AI Language.\n\n        :param text: Text to analyze\n        :param entities: List of entities to return\n        :param nlp_artifacts: Object of type NlpArtifacts, not used in this recognizer.\n        :return: A list of RecognizerResult, one per each entity found in the text.\n        \"\"\"\n        if not entities:\n            entities = self.supported_entities\n        response = self.ta_client.recognize_pii_entities(\n            [text], language=self.supported_language\n        )\n        results = [doc for doc in response if not doc.is_error]\n        recognizer_results = []\n        for res in results:\n            for entity in res.entities:\n                entity.category = entity.category.upper()\n                if entity.category.lower() not in [\n                    ent.lower() for ent in self.supported_entities\n                ]:\n                    continue\n                if entity.category.lower() not in [ent.lower() for ent in entities]:\n                    continue\n                analysis_explanation = AzureAILanguageRecognizer._build_explanation(\n                    original_score=entity.confidence_score,\n                    entity_type=entity.category,\n                )\n                recognizer_results.append(\n                    RecognizerResult(\n                        entity_type=entity.category,\n                        start=entity.offset,\n                        end=entity.offset + entity.length,\n                        score=entity.confidence_score,\n                        analysis_explanation=analysis_explanation,\n                    )\n                )\n\n        return recognizer_results\n\n    @staticmethod\n    def _build_explanation(\n        original_score: float, entity_type: str\n    ) -&gt; AnalysisExplanation:\n        explanation = AnalysisExplanation(\n            recognizer=AzureAILanguageRecognizer.__class__.__name__,\n            original_score=original_score,\n            textual_explanation=f\"Identified as {entity_type} by Azure AI Language\",\n        )\n        return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer.id","title":"id  <code>property</code>","text":"<pre><code>id\n</code></pre> <p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer.enhance_using_context","title":"enhance_using_context","text":"<pre><code>enhance_using_context(\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> PARAMETER DESCRIPTION <code>text</code> <p>The actual text that was analyzed</p> <p> TYPE: <code>str</code> </p> <code>raw_recognizer_results</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>other_raw_recognizer_results</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>nlp_artifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> <p> TYPE: <code>NlpArtifacts</code> </p> <code>context</code> <p>list of context words</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer.get_supported_language","title":"get_supported_language","text":"<pre><code>get_supported_language() -&gt; str\n</code></pre> <p>Return the language this recognizer can support.</p> RETURNS DESCRIPTION <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer.get_version","title":"get_version","text":"<pre><code>get_version() -&gt; str\n</code></pre> <p>Return the version of this recognizer.</p> RETURNS DESCRIPTION <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Serialize self to dictionary.</p> RETURNS DESCRIPTION <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(entity_recognizer_dict: Dict) -&gt; EntityRecognizer\n</code></pre> <p>Create EntityRecognizer from a dict input.</p> PARAMETER DESCRIPTION <code>entity_recognizer_dict</code> <p>Dict containing keys and values for instantiation</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer.remove_duplicates","title":"remove_duplicates  <code>staticmethod</code>","text":"<pre><code>remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]\n</code></pre> <p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> PARAMETER DESCRIPTION <code>results</code> <p>List[RecognizerResult]</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer.sanitize_value","title":"sanitize_value  <code>staticmethod</code>","text":"<pre><code>sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str\n</code></pre> <p>Cleanse the input string of the replacement pairs specified as argument.</p> PARAMETER DESCRIPTION <code>text</code> <p>input string</p> <p> TYPE: <code>str</code> </p> <code>replacement_pairs</code> <p>pairs of what has to be replaced with which value</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>cleansed string</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef sanitize_value(text: str, replacement_pairs: List[Tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Cleanse the input string of the replacement pairs specified as argument.\n\n    :param text: input string\n    :param replacement_pairs: pairs of what has to be replaced with which value\n    :return: cleansed string\n    \"\"\"\n    for search_string, replacement_string in replacement_pairs:\n        text = text.replace(search_string, replacement_string)\n    return text\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer.get_supported_entities","title":"get_supported_entities","text":"<pre><code>get_supported_entities() -&gt; List[str]\n</code></pre> <p>Return the list of entities this recognizer can identify.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/predefined_recognizers/third_party/azure_ai_language.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.predefined_recognizers.AzureAILanguageRecognizer.analyze","title":"analyze","text":"<pre><code>analyze(\n    text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None\n) -&gt; List[RecognizerResult]\n</code></pre> <p>Analyze text using Azure AI Language.</p> PARAMETER DESCRIPTION <code>text</code> <p>Text to analyze</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>List of entities to return</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>nlp_artifacts</code> <p>Object of type NlpArtifacts, not used in this recognizer.</p> <p> TYPE: <code>NlpArtifacts</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[RecognizerResult]</code> <p>A list of RecognizerResult, one per each entity found in the text.</p> Source code in <code>presidio_analyzer/predefined_recognizers/third_party/azure_ai_language.py</code> <pre><code>def analyze(\n    self, text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyze text using Azure AI Language.\n\n    :param text: Text to analyze\n    :param entities: List of entities to return\n    :param nlp_artifacts: Object of type NlpArtifacts, not used in this recognizer.\n    :return: A list of RecognizerResult, one per each entity found in the text.\n    \"\"\"\n    if not entities:\n        entities = self.supported_entities\n    response = self.ta_client.recognize_pii_entities(\n        [text], language=self.supported_language\n    )\n    results = [doc for doc in response if not doc.is_error]\n    recognizer_results = []\n    for res in results:\n        for entity in res.entities:\n            entity.category = entity.category.upper()\n            if entity.category.lower() not in [\n                ent.lower() for ent in self.supported_entities\n            ]:\n                continue\n            if entity.category.lower() not in [ent.lower() for ent in entities]:\n                continue\n            analysis_explanation = AzureAILanguageRecognizer._build_explanation(\n                original_score=entity.confidence_score,\n                entity_type=entity.category,\n            )\n            recognizer_results.append(\n                RecognizerResult(\n                    entity_type=entity.category,\n                    start=entity.offset,\n                    end=entity.offset + entity.length,\n                    score=entity.confidence_score,\n                    analysis_explanation=analysis_explanation,\n                )\n            )\n\n    return recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#misc","title":"Misc","text":""},{"location":"api/analyzer_python/#presidio_analyzer.analyzer_request.AnalyzerRequest","title":"presidio_analyzer.analyzer_request.AnalyzerRequest","text":"<p>Analyzer request data.</p> PARAMETER DESCRIPTION <code>req_data</code> <p>A request dictionary with the following fields: text: the text to analyze language: the language of the text entities: List of PII entities that should be looked for in the text. If entities=None then all entities are looked for. correlation_id: cross call ID for this request score_threshold: A minimum value for which to return an identified entity log_decision_process: Should the decision points within the analysis be logged return_decision_process: Should the decision points within the analysis returned as part of the response</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_analyzer/analyzer_request.py</code> <pre><code>class AnalyzerRequest:\n    \"\"\"\n    Analyzer request data.\n\n    :param req_data: A request dictionary with the following fields:\n        text: the text to analyze\n        language: the language of the text\n        entities: List of PII entities that should be looked for in the text.\n        If entities=None then all entities are looked for.\n        correlation_id: cross call ID for this request\n        score_threshold: A minimum value for which to return an identified entity\n        log_decision_process: Should the decision points within the analysis\n        be logged\n        return_decision_process: Should the decision points within the analysis\n        returned as part of the response\n    \"\"\"\n\n    def __init__(self, req_data: Dict):\n        self.text = req_data.get(\"text\")\n        self.language = req_data.get(\"language\")\n        self.entities = req_data.get(\"entities\")\n        self.correlation_id = req_data.get(\"correlation_id\")\n        self.score_threshold = req_data.get(\"score_threshold\")\n        self.return_decision_process = req_data.get(\"return_decision_process\")\n        ad_hoc_recognizers = req_data.get(\"ad_hoc_recognizers\")\n        self.ad_hoc_recognizers = []\n        if ad_hoc_recognizers:\n            self.ad_hoc_recognizers = [\n                PatternRecognizer.from_dict(rec) for rec in ad_hoc_recognizers\n            ]\n        self.context = req_data.get(\"context\")\n        self.allow_list = req_data.get(\"allow_list\")\n        self.allow_list_match = req_data.get(\"allow_list_match\", \"exact\")\n        self.regex_flags = req_data.get(\"regex_flags\",\n                                        re.DOTALL | re.MULTILINE | re.IGNORECASE)\n</code></pre>"},{"location":"api/anonymizer_python/","title":"Presidio Anonymizer API Reference","text":""},{"location":"api/anonymizer_python/#presidio_anonymizer","title":"presidio_anonymizer","text":"<p>Anonymizer root module.</p>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine","title":"AnonymizerEngine","text":"<p>               Bases: <code>EngineBase</code></p> <p>AnonymizerEngine class.</p> <p>Handles the entire logic of the Presidio-anonymizer. Gets the original text and replaces the PII entities with the desired anonymizers.</p> METHOD DESCRIPTION <code>anonymize</code> <p>Anonymize method to anonymize the given text.</p> <code>add_anonymizer</code> <p>Add a new anonymizer to the engine.</p> <code>remove_anonymizer</code> <p>Remove an anonymizer from the engine.</p> <code>get_anonymizers</code> <p>Return a list of supported anonymizers.</p> Source code in <code>presidio_anonymizer/anonymizer_engine.py</code> <pre><code>class AnonymizerEngine(EngineBase):\n    \"\"\"\n    AnonymizerEngine class.\n\n    Handles the entire logic of the Presidio-anonymizer. Gets the original text\n    and replaces the PII entities with the desired anonymizers.\n    \"\"\"\n\n    def anonymize(\n        self,\n        text: str,\n        analyzer_results: List[RecognizerResult],\n        operators: Optional[Dict[str, OperatorConfig]] = None,\n        conflict_resolution: ConflictResolutionStrategy = (\n            ConflictResolutionStrategy.MERGE_SIMILAR_OR_CONTAINED\n        ),\n    ) -&gt; EngineResult:\n        \"\"\"Anonymize method to anonymize the given text.\n\n        :param text: the text we are anonymizing\n        :param analyzer_results: A list of RecognizerResult class -&gt; The results we\n        received from the analyzer\n        :param operators: The configuration of the anonymizers we would like\n        to use for each entity e.g.: {\"PHONE_NUMBER\":OperatorConfig(\"redact\", {})}\n        received from the analyzer\n        :param conflict_resolution: The configuration designed to handle conflicts\n        among entities\n        :return: the anonymized text and a list of information about the\n        anonymized entities.\n\n        :example:\n\n        &gt;&gt;&gt; from presidio_anonymizer import AnonymizerEngine\n        &gt;&gt;&gt; from presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n\n        &gt;&gt;&gt; # Initialize the engine with logger.\n        &gt;&gt;&gt; engine = AnonymizerEngine()\n\n        &gt;&gt;&gt; # Invoke the anonymize function with the text, analyzer results and\n        &gt;&gt;&gt; # Operators to define the anonymization type.\n        &gt;&gt;&gt; result = engine.anonymize(\n        &gt;&gt;&gt;     text=\"My name is Bond, James Bond\",\n        &gt;&gt;&gt;     analyzer_results=[RecognizerResult(entity_type=\"PERSON\",\n        &gt;&gt;&gt;                                        start=11,\n        &gt;&gt;&gt;                                        end=15,\n        &gt;&gt;&gt;                                        score=0.8),\n        &gt;&gt;&gt;                       RecognizerResult(entity_type=\"PERSON\",\n        &gt;&gt;&gt;                                        start=17,\n        &gt;&gt;&gt;                                        end=27,\n        &gt;&gt;&gt;                                        score=0.8)],\n        &gt;&gt;&gt;     operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"BIP\"})}\n        &gt;&gt;&gt; )\n\n        &gt;&gt;&gt; print(result)\n        text: My name is BIP, BIP.\n        items:\n        [\n            {'start': 16, 'end': 19, 'entity_type': 'PERSON',\n             'text': 'BIP', 'operator': 'replace'},\n            {'start': 11, 'end': 14, 'entity_type': 'PERSON',\n             'text': 'BIP', 'operator': 'replace'}\n        ]\n\n\n        \"\"\"\n        # We do this to make sure the original analyzer_results object is not\n        # modified\n        analyzer_results = self._copy_recognizer_results(analyzer_results)\n\n        # Sort because downstream processors like whitespace merging expect input to\n        # be sorted by start, end to work correctly\n        analyzer_results.sort(key=lambda x: (x.start, x.end))\n\n        analyzer_results = self._remove_conflicts_and_get_text_manipulation_data(\n            analyzer_results, conflict_resolution\n        )\n\n        merged_results = self._merge_entities_with_whitespace_between(\n            text, analyzer_results\n        )\n\n        operators = self.__check_or_add_default_operator(operators)\n\n        return self._operate(\n            text=text,\n            pii_entities=merged_results,\n            operators_metadata=operators,\n            operator_type=OperatorType.Anonymize,\n        )\n\n    def add_anonymizer(self, anonymizer_cls: Type[Operator]) -&gt; None:\n        \"\"\"\n        Add a new anonymizer to the engine.\n\n        anonymizer_cls: The anonymizer class to add to the engine.\n        \"\"\"\n        logger.info(f\"Added anonymizer {anonymizer_cls.__name__}\")\n        self.operators_factory.add_anonymize_operator(anonymizer_cls)\n\n    def remove_anonymizer(self, anonymizer_cls: Type[Operator]) -&gt; None:\n        \"\"\"\n        Remove an anonymizer from the engine.\n\n        anonymizer_cls: The anonymizer class to remove from the engine.\n        \"\"\"\n        logger.info(f\"Removed anonymizer {anonymizer_cls.__name__}\")\n        self.operators_factory.remove_anonymize_operator(anonymizer_cls)\n\n    def _remove_conflicts_and_get_text_manipulation_data(\n        self,\n        analyzer_results: List[RecognizerResult],\n        conflict_resolution: ConflictResolutionStrategy,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Iterate the list and create a sorted unique results list from it.\n\n        Only insert results which are:\n        1. Indices are not contained in other result.\n        2. Have the same indices as other results but with larger score.\n        :return: List\n        \"\"\"\n        tmp_analyzer_results = []\n        # This list contains all elements which we need to check a single result\n        # against. If a result is dropped, it can also be dropped from this list\n        # since it is intersecting with another result and we selected the other one.\n        other_elements = analyzer_results.copy()\n        for result in analyzer_results:\n            other_elements.remove(result)\n\n            is_merge_same_entity_type = False\n            for other_element in other_elements:\n                if other_element.entity_type != result.entity_type:\n                    continue\n                if result.intersects(other_element) == 0:\n                    continue\n\n                other_element.start = min(result.start, other_element.start)\n                other_element.end = max(result.end, other_element.end)\n                other_element.score = max(result.score, other_element.score)\n                is_merge_same_entity_type = True\n                break\n            if not is_merge_same_entity_type:\n                other_elements.append(result)\n                tmp_analyzer_results.append(result)\n            else:\n                self.logger.debug(\n                    f\"removing element {result} from \" f\"results list due to merge\"\n                )\n\n        unique_text_metadata_elements = []\n        # This list contains all elements which we need to check a single result\n        # against. If a result is dropped, it can also be dropped from this list\n        # since it is intersecting with another result and we selected the other one.\n        other_elements = tmp_analyzer_results.copy()\n        for result in tmp_analyzer_results:\n            other_elements.remove(result)\n            result_conflicted = self.__is_result_conflicted_with_other_elements(\n                other_elements, result\n            )\n            if not result_conflicted:\n                other_elements.append(result)\n                unique_text_metadata_elements.append(result)\n            else:\n                self.logger.debug(\n                    f\"removing element {result} from results list due to conflict\"\n                )\n\n        # This further improves the quality of handling the conflict between the\n        # various entities overlapping. This will not drop the results insted\n        # it adjust the start and end positions of overlapping results and removes\n        # All types of conflicts among entities as well as text.\n        if conflict_resolution == ConflictResolutionStrategy.REMOVE_INTERSECTIONS:\n            unique_text_metadata_elements.sort(key=lambda element: element.start)\n            elements_length = len(unique_text_metadata_elements)\n            index = 0\n            while index &lt; elements_length - 1:\n                current_entity = unique_text_metadata_elements[index]\n                next_entity = unique_text_metadata_elements[index + 1]\n                if current_entity.end &lt;= next_entity.start:\n                    index += 1\n                else:\n                    if current_entity.score &gt;= next_entity.score:\n                        next_entity.start = current_entity.end\n                    else:\n                        current_entity.end = next_entity.start\n                    unique_text_metadata_elements.sort(\n                        key=lambda element: element.start\n                    )\n            unique_text_metadata_elements = [\n                element\n                for element in unique_text_metadata_elements\n                if element.start &lt;= element.end\n            ]\n        return unique_text_metadata_elements\n\n    def _merge_entities_with_whitespace_between(\n        self, text: str, analyzer_results: List[RecognizerResult]\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Merge adjacent entities of the same type separated by whitespace.\"\"\"\n        merged_results = []\n        prev_result = None\n        for result in analyzer_results:\n            if prev_result is not None:\n                if prev_result.entity_type == result.entity_type:\n                    if re.search(r\"^( )+$\", text[prev_result.end : result.start]):\n                        merged_results.remove(prev_result)\n                        result.start = prev_result.start\n            merged_results.append(result)\n            prev_result = result\n        return merged_results\n\n    def get_anonymizers(self) -&gt; List[str]:\n        \"\"\"Return a list of supported anonymizers.\"\"\"\n        names = [p for p in self.operators_factory.get_anonymizers().keys()]\n        return names\n\n    @staticmethod\n    def __is_result_conflicted_with_other_elements(other_elements, result):\n        return any(\n            [result.has_conflict(other_element) for other_element in other_elements]\n        )\n\n    @staticmethod\n    def __check_or_add_default_operator(\n        operators: Dict[str, OperatorConfig],\n    ) -&gt; Dict[str, OperatorConfig]:\n        default_operator = OperatorConfig(DEFAULT)\n        if not operators:\n            return {\"DEFAULT\": default_operator}\n        if not operators.get(\"DEFAULT\"):\n            operators[\"DEFAULT\"] = default_operator\n        return operators\n\n    @staticmethod\n    def _copy_recognizer_results(\n        analyzer_results: List[RecognizerResult],\n    ) -&gt; List[RecognizerResult]:\n        return [\n            RecognizerResult(\n                start=result.start,\n                end=result.end,\n                entity_type=result.entity_type,\n                score=result.score,\n            )\n            for result in analyzer_results\n        ]\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.anonymize","title":"anonymize","text":"<pre><code>anonymize(\n    text: str,\n    analyzer_results: List[RecognizerResult],\n    operators: Optional[Dict[str, OperatorConfig]] = None,\n    conflict_resolution: ConflictResolutionStrategy = ConflictResolutionStrategy.MERGE_SIMILAR_OR_CONTAINED,\n) -&gt; EngineResult\n</code></pre> <p>Anonymize method to anonymize the given text.</p> <p>:example:</p> <p>from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import RecognizerResult, OperatorConfig</p> PARAMETER DESCRIPTION <code>text</code> <p>the text we are anonymizing</p> <p> TYPE: <code>str</code> </p> <code>analyzer_results</code> <p>A list of RecognizerResult class -&gt; The results we received from the analyzer</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>operators</code> <p>The configuration of the anonymizers we would like to use for each entity e.g.: {\"PHONE_NUMBER\":OperatorConfig(\"redact\", {})} received from the analyzer</p> <p> TYPE: <code>Optional[Dict[str, OperatorConfig]]</code> DEFAULT: <code>None</code> </p> <code>conflict_resolution</code> <p>The configuration designed to handle conflicts among entities</p> <p> TYPE: <code>ConflictResolutionStrategy</code> DEFAULT: <code>MERGE_SIMILAR_OR_CONTAINED</code> </p> RETURNS DESCRIPTION <code>EngineResult</code> <p>the anonymized text and a list of information about the anonymized entities.</p> Source code in <code>presidio_anonymizer/anonymizer_engine.py</code> <pre><code>def anonymize(\n    self,\n    text: str,\n    analyzer_results: List[RecognizerResult],\n    operators: Optional[Dict[str, OperatorConfig]] = None,\n    conflict_resolution: ConflictResolutionStrategy = (\n        ConflictResolutionStrategy.MERGE_SIMILAR_OR_CONTAINED\n    ),\n) -&gt; EngineResult:\n    \"\"\"Anonymize method to anonymize the given text.\n\n    :param text: the text we are anonymizing\n    :param analyzer_results: A list of RecognizerResult class -&gt; The results we\n    received from the analyzer\n    :param operators: The configuration of the anonymizers we would like\n    to use for each entity e.g.: {\"PHONE_NUMBER\":OperatorConfig(\"redact\", {})}\n    received from the analyzer\n    :param conflict_resolution: The configuration designed to handle conflicts\n    among entities\n    :return: the anonymized text and a list of information about the\n    anonymized entities.\n\n    :example:\n\n    &gt;&gt;&gt; from presidio_anonymizer import AnonymizerEngine\n    &gt;&gt;&gt; from presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n\n    &gt;&gt;&gt; # Initialize the engine with logger.\n    &gt;&gt;&gt; engine = AnonymizerEngine()\n\n    &gt;&gt;&gt; # Invoke the anonymize function with the text, analyzer results and\n    &gt;&gt;&gt; # Operators to define the anonymization type.\n    &gt;&gt;&gt; result = engine.anonymize(\n    &gt;&gt;&gt;     text=\"My name is Bond, James Bond\",\n    &gt;&gt;&gt;     analyzer_results=[RecognizerResult(entity_type=\"PERSON\",\n    &gt;&gt;&gt;                                        start=11,\n    &gt;&gt;&gt;                                        end=15,\n    &gt;&gt;&gt;                                        score=0.8),\n    &gt;&gt;&gt;                       RecognizerResult(entity_type=\"PERSON\",\n    &gt;&gt;&gt;                                        start=17,\n    &gt;&gt;&gt;                                        end=27,\n    &gt;&gt;&gt;                                        score=0.8)],\n    &gt;&gt;&gt;     operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"BIP\"})}\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; print(result)\n    text: My name is BIP, BIP.\n    items:\n    [\n        {'start': 16, 'end': 19, 'entity_type': 'PERSON',\n         'text': 'BIP', 'operator': 'replace'},\n        {'start': 11, 'end': 14, 'entity_type': 'PERSON',\n         'text': 'BIP', 'operator': 'replace'}\n    ]\n\n\n    \"\"\"\n    # We do this to make sure the original analyzer_results object is not\n    # modified\n    analyzer_results = self._copy_recognizer_results(analyzer_results)\n\n    # Sort because downstream processors like whitespace merging expect input to\n    # be sorted by start, end to work correctly\n    analyzer_results.sort(key=lambda x: (x.start, x.end))\n\n    analyzer_results = self._remove_conflicts_and_get_text_manipulation_data(\n        analyzer_results, conflict_resolution\n    )\n\n    merged_results = self._merge_entities_with_whitespace_between(\n        text, analyzer_results\n    )\n\n    operators = self.__check_or_add_default_operator(operators)\n\n    return self._operate(\n        text=text,\n        pii_entities=merged_results,\n        operators_metadata=operators,\n        operator_type=OperatorType.Anonymize,\n    )\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.anonymize--initialize-the-engine-with-logger","title":"Initialize the engine with logger.","text":"<p>engine = AnonymizerEngine()</p>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.anonymize--invoke-the-anonymize-function-with-the-text-analyzer-results-and","title":"Invoke the anonymize function with the text, analyzer results and","text":""},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.anonymize--operators-to-define-the-anonymization-type","title":"Operators to define the anonymization type.","text":"<p>result = engine.anonymize(     text=\"My name is Bond, James Bond\",     analyzer_results=[RecognizerResult(entity_type=\"PERSON\",                                        start=11,                                        end=15,                                        score=0.8),                       RecognizerResult(entity_type=\"PERSON\",                                        start=17,                                        end=27,                                        score=0.8)],     operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"BIP\"})} )</p> <p>print(result) text: My name is BIP, BIP. items: [     {'start': 16, 'end': 19, 'entity_type': 'PERSON',      'text': 'BIP', 'operator': 'replace'},     {'start': 11, 'end': 14, 'entity_type': 'PERSON',      'text': 'BIP', 'operator': 'replace'} ]</p>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.add_anonymizer","title":"add_anonymizer","text":"<pre><code>add_anonymizer(anonymizer_cls: Type[Operator]) -&gt; None\n</code></pre> <p>Add a new anonymizer to the engine.</p> <p>anonymizer_cls: The anonymizer class to add to the engine.</p> Source code in <code>presidio_anonymizer/anonymizer_engine.py</code> <pre><code>def add_anonymizer(self, anonymizer_cls: Type[Operator]) -&gt; None:\n    \"\"\"\n    Add a new anonymizer to the engine.\n\n    anonymizer_cls: The anonymizer class to add to the engine.\n    \"\"\"\n    logger.info(f\"Added anonymizer {anonymizer_cls.__name__}\")\n    self.operators_factory.add_anonymize_operator(anonymizer_cls)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.remove_anonymizer","title":"remove_anonymizer","text":"<pre><code>remove_anonymizer(anonymizer_cls: Type[Operator]) -&gt; None\n</code></pre> <p>Remove an anonymizer from the engine.</p> <p>anonymizer_cls: The anonymizer class to remove from the engine.</p> Source code in <code>presidio_anonymizer/anonymizer_engine.py</code> <pre><code>def remove_anonymizer(self, anonymizer_cls: Type[Operator]) -&gt; None:\n    \"\"\"\n    Remove an anonymizer from the engine.\n\n    anonymizer_cls: The anonymizer class to remove from the engine.\n    \"\"\"\n    logger.info(f\"Removed anonymizer {anonymizer_cls.__name__}\")\n    self.operators_factory.remove_anonymize_operator(anonymizer_cls)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.get_anonymizers","title":"get_anonymizers","text":"<pre><code>get_anonymizers() -&gt; List[str]\n</code></pre> <p>Return a list of supported anonymizers.</p> Source code in <code>presidio_anonymizer/anonymizer_engine.py</code> <pre><code>def get_anonymizers(self) -&gt; List[str]:\n    \"\"\"Return a list of supported anonymizers.\"\"\"\n    names = [p for p in self.operators_factory.get_anonymizers().keys()]\n    return names\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.BatchAnonymizerEngine","title":"BatchAnonymizerEngine","text":"<p>BatchAnonymizerEngine class.</p> <p>A class that provides functionality to anonymize in batches.</p> PARAMETER DESCRIPTION <code>anonymizer_engine</code> <p>An instance of the AnonymizerEngine class.</p> <p> TYPE: <code>Optional[AnonymizerEngine]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>anonymize_list</code> <p>Anonymize a list of strings.</p> <code>anonymize_dict</code> <p>Anonymize values in a dictionary.</p> Source code in <code>presidio_anonymizer/batch_anonymizer_engine.py</code> <pre><code>class BatchAnonymizerEngine:\n    \"\"\"\n    BatchAnonymizerEngine class.\n\n    A class that provides functionality to anonymize in batches.\n    :param anonymizer_engine: An instance of the AnonymizerEngine class.\n    \"\"\"\n\n    def __init__(self, anonymizer_engine: Optional[AnonymizerEngine] = None):\n        self.anonymizer_engine = anonymizer_engine or AnonymizerEngine()\n\n    def anonymize_list(\n        self,\n        texts: List[Optional[Union[str, bool, int, float]]],\n        recognizer_results_list: List[List[RecognizerResult]],\n        **kwargs,\n    ) -&gt; List[Union[str, Any]]:\n        \"\"\"\n        Anonymize a list of strings.\n\n        :param texts: List containing the texts to be anonymized (original texts).\n            Items with a `type` not in `(str, bool, int, float)` will not be anonymized.\n        :param recognizer_results_list: A list of lists of RecognizerResult,\n        the output of the AnalyzerEngine on each text in the list.\n        :param kwargs: Additional kwargs for the `AnonymizerEngine.anonymize` method\n        \"\"\"\n        return_list = []\n        if not recognizer_results_list:\n            recognizer_results_list = [[] for _ in range(len(texts))]\n        for text, recognizer_results in zip(texts, recognizer_results_list):\n            if type(text) in (str, bool, int, float):\n                res = self.anonymizer_engine.anonymize(\n                    text=str(text), analyzer_results=recognizer_results, **kwargs\n                )\n                return_list.append(res.text)\n            else:\n                return_list.append(text)\n\n        return return_list\n\n    def anonymize_dict(\n        self, analyzer_results: Iterable[DictRecognizerResult], **kwargs\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Anonymize values in a dictionary.\n\n        :param analyzer_results: Iterator of `DictRecognizerResult`\n        containing the output of the AnalyzerEngine.analyze_dict on the input text.\n        :param kwargs: Additional kwargs for the `AnonymizerEngine.anonymize` method\n        \"\"\"\n\n        return_dict = {}\n        for result in analyzer_results:\n            if isinstance(result.value, dict):\n                resp = self.anonymize_dict(\n                    analyzer_results=result.recognizer_results, **kwargs\n                )\n                return_dict[result.key] = resp\n\n            elif isinstance(result.value, str):\n                resp = self.anonymizer_engine.anonymize(\n                    text=result.value,\n                    analyzer_results=result.recognizer_results,\n                    **kwargs,\n                )\n                return_dict[result.key] = resp.text\n\n            elif isinstance(result.value, collections.abc.Iterable):\n                anonymize_response = self.anonymize_list(\n                    texts=result.value,\n                    recognizer_results_list=result.recognizer_results,\n                    **kwargs,\n                )\n                return_dict[result.key] = anonymize_response\n            else:\n                return_dict[result.key] = result.value\n        return return_dict\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.BatchAnonymizerEngine.anonymize_list","title":"anonymize_list","text":"<pre><code>anonymize_list(\n    texts: List[Optional[Union[str, bool, int, float]]],\n    recognizer_results_list: List[List[RecognizerResult]],\n    **kwargs\n) -&gt; List[Union[str, Any]]\n</code></pre> <p>Anonymize a list of strings.</p> PARAMETER DESCRIPTION <code>texts</code> <p>List containing the texts to be anonymized (original texts). Items with a <code>type</code> not in <code>(str, bool, int, float)</code> will not be anonymized.</p> <p> TYPE: <code>List[Optional[Union[str, bool, int, float]]]</code> </p> <code>recognizer_results_list</code> <p>A list of lists of RecognizerResult, the output of the AnalyzerEngine on each text in the list.</p> <p> TYPE: <code>List[List[RecognizerResult]]</code> </p> <code>kwargs</code> <p>Additional kwargs for the <code>AnonymizerEngine.anonymize</code> method</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>presidio_anonymizer/batch_anonymizer_engine.py</code> <pre><code>def anonymize_list(\n    self,\n    texts: List[Optional[Union[str, bool, int, float]]],\n    recognizer_results_list: List[List[RecognizerResult]],\n    **kwargs,\n) -&gt; List[Union[str, Any]]:\n    \"\"\"\n    Anonymize a list of strings.\n\n    :param texts: List containing the texts to be anonymized (original texts).\n        Items with a `type` not in `(str, bool, int, float)` will not be anonymized.\n    :param recognizer_results_list: A list of lists of RecognizerResult,\n    the output of the AnalyzerEngine on each text in the list.\n    :param kwargs: Additional kwargs for the `AnonymizerEngine.anonymize` method\n    \"\"\"\n    return_list = []\n    if not recognizer_results_list:\n        recognizer_results_list = [[] for _ in range(len(texts))]\n    for text, recognizer_results in zip(texts, recognizer_results_list):\n        if type(text) in (str, bool, int, float):\n            res = self.anonymizer_engine.anonymize(\n                text=str(text), analyzer_results=recognizer_results, **kwargs\n            )\n            return_list.append(res.text)\n        else:\n            return_list.append(text)\n\n    return return_list\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.BatchAnonymizerEngine.anonymize_dict","title":"anonymize_dict","text":"<pre><code>anonymize_dict(\n    analyzer_results: Iterable[DictRecognizerResult], **kwargs\n) -&gt; Dict[str, str]\n</code></pre> <p>Anonymize values in a dictionary.</p> PARAMETER DESCRIPTION <code>analyzer_results</code> <p>Iterator of <code>DictRecognizerResult</code> containing the output of the AnalyzerEngine.analyze_dict on the input text.</p> <p> TYPE: <code>Iterable[DictRecognizerResult]</code> </p> <code>kwargs</code> <p>Additional kwargs for the <code>AnonymizerEngine.anonymize</code> method</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>presidio_anonymizer/batch_anonymizer_engine.py</code> <pre><code>def anonymize_dict(\n    self, analyzer_results: Iterable[DictRecognizerResult], **kwargs\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Anonymize values in a dictionary.\n\n    :param analyzer_results: Iterator of `DictRecognizerResult`\n    containing the output of the AnalyzerEngine.analyze_dict on the input text.\n    :param kwargs: Additional kwargs for the `AnonymizerEngine.anonymize` method\n    \"\"\"\n\n    return_dict = {}\n    for result in analyzer_results:\n        if isinstance(result.value, dict):\n            resp = self.anonymize_dict(\n                analyzer_results=result.recognizer_results, **kwargs\n            )\n            return_dict[result.key] = resp\n\n        elif isinstance(result.value, str):\n            resp = self.anonymizer_engine.anonymize(\n                text=result.value,\n                analyzer_results=result.recognizer_results,\n                **kwargs,\n            )\n            return_dict[result.key] = resp.text\n\n        elif isinstance(result.value, collections.abc.Iterable):\n            anonymize_response = self.anonymize_list(\n                texts=result.value,\n                recognizer_results_list=result.recognizer_results,\n                **kwargs,\n            )\n            return_dict[result.key] = anonymize_response\n        else:\n            return_dict[result.key] = result.value\n    return return_dict\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.DeanonymizeEngine","title":"DeanonymizeEngine","text":"<p>               Bases: <code>EngineBase</code></p> <p>Deanonymize text that was previously anonymized.</p> METHOD DESCRIPTION <code>deanonymize</code> <p>Receive the text, entities and operators to perform deanonymization over.</p> <code>get_deanonymizers</code> <p>Return a list of supported deanonymizers.</p> <code>add_deanonymizer</code> <p>Add a new deanonymizer to the engine.</p> <code>remove_deanonymizer</code> <p>Remove a deanonymizer from the engine.</p> Source code in <code>presidio_anonymizer/deanonymize_engine.py</code> <pre><code>class DeanonymizeEngine(EngineBase):\n    \"\"\"Deanonymize text that was previously anonymized.\"\"\"\n\n    def deanonymize(\n        self,\n        text: str,\n        entities: List[OperatorResult],\n        operators: Dict[str, OperatorConfig],\n    ) -&gt; EngineResult:\n        \"\"\"\n        Receive the text, entities and operators to perform deanonymization over.\n\n        :param operators: the operators to apply on the anonymizer result entities\n        :param text: the full text with the encrypted entities\n        :param entities: list of encrypted entities\n        :return: EngineResult - the new text and data about the deanonymized entities.\n        \"\"\"\n        return self._operate(text, entities, operators, OperatorType.Deanonymize)\n\n    def get_deanonymizers(self) -&gt; List[str]:\n        \"\"\"Return a list of supported deanonymizers.\"\"\"\n        names = [p for p in self.operators_factory.get_deanonymizers().keys()]\n        return names\n\n    def add_deanonymizer(self, deanonymizer_cls: Type[Operator]) -&gt; None:\n        \"\"\"\n        Add a new deanonymizer to the engine.\n\n        anonymizer_cls: The deanonymizer class to add to the engine.\n        \"\"\"\n        logger.info(f\"Added deanonymizer {deanonymizer_cls.__name__}\")\n        self.operators_factory.add_deanonymize_operator(deanonymizer_cls)\n\n    def remove_deanonymizer(self, deanonymizer_cls: Type[Operator]) -&gt; None:\n        \"\"\"\n        Remove a deanonymizer from the engine.\n\n        deanonymizer_cls: The deanonymizer class to remove from the engine.\n        \"\"\"\n        logger.info(f\"Removed deanonymizer {deanonymizer_cls.__name__}\")\n        self.operators_factory.remove_deanonymize_operator(deanonymizer_cls)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.DeanonymizeEngine.deanonymize","title":"deanonymize","text":"<pre><code>deanonymize(\n    text: str,\n    entities: List[OperatorResult],\n    operators: Dict[str, OperatorConfig],\n) -&gt; EngineResult\n</code></pre> <p>Receive the text, entities and operators to perform deanonymization over.</p> PARAMETER DESCRIPTION <code>operators</code> <p>the operators to apply on the anonymizer result entities</p> <p> TYPE: <code>Dict[str, OperatorConfig]</code> </p> <code>text</code> <p>the full text with the encrypted entities</p> <p> TYPE: <code>str</code> </p> <code>entities</code> <p>list of encrypted entities</p> <p> TYPE: <code>List[OperatorResult]</code> </p> RETURNS DESCRIPTION <code>EngineResult</code> <p>EngineResult - the new text and data about the deanonymized entities.</p> Source code in <code>presidio_anonymizer/deanonymize_engine.py</code> <pre><code>def deanonymize(\n    self,\n    text: str,\n    entities: List[OperatorResult],\n    operators: Dict[str, OperatorConfig],\n) -&gt; EngineResult:\n    \"\"\"\n    Receive the text, entities and operators to perform deanonymization over.\n\n    :param operators: the operators to apply on the anonymizer result entities\n    :param text: the full text with the encrypted entities\n    :param entities: list of encrypted entities\n    :return: EngineResult - the new text and data about the deanonymized entities.\n    \"\"\"\n    return self._operate(text, entities, operators, OperatorType.Deanonymize)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.DeanonymizeEngine.get_deanonymizers","title":"get_deanonymizers","text":"<pre><code>get_deanonymizers() -&gt; List[str]\n</code></pre> <p>Return a list of supported deanonymizers.</p> Source code in <code>presidio_anonymizer/deanonymize_engine.py</code> <pre><code>def get_deanonymizers(self) -&gt; List[str]:\n    \"\"\"Return a list of supported deanonymizers.\"\"\"\n    names = [p for p in self.operators_factory.get_deanonymizers().keys()]\n    return names\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.DeanonymizeEngine.add_deanonymizer","title":"add_deanonymizer","text":"<pre><code>add_deanonymizer(deanonymizer_cls: Type[Operator]) -&gt; None\n</code></pre> <p>Add a new deanonymizer to the engine.</p> <p>anonymizer_cls: The deanonymizer class to add to the engine.</p> Source code in <code>presidio_anonymizer/deanonymize_engine.py</code> <pre><code>def add_deanonymizer(self, deanonymizer_cls: Type[Operator]) -&gt; None:\n    \"\"\"\n    Add a new deanonymizer to the engine.\n\n    anonymizer_cls: The deanonymizer class to add to the engine.\n    \"\"\"\n    logger.info(f\"Added deanonymizer {deanonymizer_cls.__name__}\")\n    self.operators_factory.add_deanonymize_operator(deanonymizer_cls)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.DeanonymizeEngine.remove_deanonymizer","title":"remove_deanonymizer","text":"<pre><code>remove_deanonymizer(deanonymizer_cls: Type[Operator]) -&gt; None\n</code></pre> <p>Remove a deanonymizer from the engine.</p> <p>deanonymizer_cls: The deanonymizer class to remove from the engine.</p> Source code in <code>presidio_anonymizer/deanonymize_engine.py</code> <pre><code>def remove_deanonymizer(self, deanonymizer_cls: Type[Operator]) -&gt; None:\n    \"\"\"\n    Remove a deanonymizer from the engine.\n\n    deanonymizer_cls: The deanonymizer class to remove from the engine.\n    \"\"\"\n    logger.info(f\"Removed deanonymizer {deanonymizer_cls.__name__}\")\n    self.operators_factory.remove_deanonymize_operator(deanonymizer_cls)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.ConflictResolutionStrategy","title":"ConflictResolutionStrategy","text":"<p>               Bases: <code>Enum</code></p> <p>Conflict resolution strategy.</p> <p>The strategy to use when there is a conflict between two entities.</p> <p>MERGE_SIMILAR_OR_CONTAINED: This default strategy resolves conflicts between similar or contained entities. REMOVE_INTERSECTIONS: Effectively resolves both intersection conflicts among entities and default strategy conflicts. NONE: No conflict resolution will be performed.</p> Source code in <code>presidio_anonymizer/entities/conflict_resolution_strategy.py</code> <pre><code>class ConflictResolutionStrategy(Enum):\n    \"\"\"Conflict resolution strategy.\n\n    The strategy to use when there is a conflict between two entities.\n\n    MERGE_SIMILAR_OR_CONTAINED: This default strategy resolves conflicts\n    between similar or contained entities.\n    REMOVE_INTERSECTIONS: Effectively resolves both intersection conflicts\n    among entities and default strategy conflicts.\n    NONE: No conflict resolution will be performed.\n    \"\"\"\n\n    MERGE_SIMILAR_OR_CONTAINED = \"merge_similar_or_contained\"\n    REMOVE_INTERSECTIONS = \"remove_intersections\"\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.DictRecognizerResult","title":"DictRecognizerResult  <code>dataclass</code>","text":"<p>Data class for holding the output of the Presidio Analyzer on dictionaries.</p> PARAMETER DESCRIPTION <code>key</code> <p>key in dictionary</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>value to run analysis on (either string or list of strings)</p> <p> TYPE: <code>Union[str, List[str], dict]</code> </p> <code>recognizer_results</code> <p>Analyzer output for one value. Could be either: - A list of recognizer results if the input is one string - A list of lists of recognizer results, if the input is a list of strings. - An iterator of a DictRecognizerResult, if the input is a dictionary. In this case the recognizer_results would be the iterator of the DictRecognizerResult next level in the dictionary.</p> <p> TYPE: <code>Union[List[RecognizerResult], List[List[RecognizerResult]], Iterator[DictRecognizerResult]]</code> </p> Source code in <code>presidio_anonymizer/entities/engine/dict_recognizer_result.py</code> <pre><code>@dataclass\nclass DictRecognizerResult:\n    \"\"\"\n    Data class for holding the output of the Presidio Analyzer on dictionaries.\n\n    :param key: key in dictionary\n    :param value: value to run analysis on (either string or list of strings)\n    :param recognizer_results: Analyzer output for one value.\n    Could be either:\n     - A list of recognizer results if the input is one string\n     - A list of lists of recognizer results, if the input is a list of strings.\n     - An iterator of a DictRecognizerResult, if the input is a dictionary.\n     In this case the recognizer_results would be the iterator\n     of the DictRecognizerResult next level in the dictionary.\n    \"\"\"\n\n    key: str\n    value: Union[str, List[str], dict]\n    recognizer_results: Union[\n        List[RecognizerResult],\n        List[List[RecognizerResult]],\n        Iterator[\"DictRecognizerResult\"],\n    ]\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.EngineResult","title":"EngineResult","text":"<p>Engine result.</p> METHOD DESCRIPTION <code>set_text</code> <p>Set a text.</p> <code>add_item</code> <p>Add an item.</p> <code>normalize_item_indexes</code> <p>Normalize the indexes to be index from start.</p> <code>to_json</code> <p>Return a json string serializing this instance.</p> Source code in <code>presidio_anonymizer/entities/engine/result/engine_result.py</code> <pre><code>class EngineResult:\n    \"\"\"Engine result.\"\"\"\n\n    def __init__(self, text: str = None, items: List[OperatorResult] = None):\n        \"\"\"Create EngineResult entity.\n\n        :param text: The anonymized text.\n        :param items: List of PII entities and the indices\n         of their replacements in the anonymized text.\n        \"\"\"\n        if items is None:\n            items = []\n        self.text = text\n        self.items = items\n\n    def set_text(self, text: str):\n        \"\"\"Set a text.\"\"\"\n        self.text = text\n\n    def add_item(self, item: OperatorResult):\n        \"\"\"Add an item.\n\n        :param item: an item to add to the list.\n        \"\"\"\n        self.items.append(item)\n\n    def normalize_item_indexes(self):\n        \"\"\"Normalize the indexes to be index from start.\"\"\"\n        text_len = len(self.text)\n        for result_item in self.items:\n            result_item.start = text_len - result_item.end\n            result_item.end = result_item.start + len(result_item.text)\n\n    def to_json(self) -&gt; str:\n        \"\"\"Return a json string serializing this instance.\"\"\"\n        return json.dumps(self, default=lambda x: x.__dict__)\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object.\"\"\"\n\n        items_repr = (\n            \",\\n    \".join([str(item) for item in self.items]) if self.items else \"\"\n        )\n        return f\"text: {self.text}\\nitems:\\n[\\n    {items_repr}\\n]\\n\"\n\n    def __eq__(self, other) -&gt; bool:\n        \"\"\"Verify two instances are equal.\n\n        Returns true if the two instances are equal, false otherwise.\n        \"\"\"\n        return self.text == other.text and all(\n            map(lambda x, y: x == y, self.items, other.items)\n        )\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.EngineResult.set_text","title":"set_text","text":"<pre><code>set_text(text: str)\n</code></pre> <p>Set a text.</p> Source code in <code>presidio_anonymizer/entities/engine/result/engine_result.py</code> <pre><code>def set_text(self, text: str):\n    \"\"\"Set a text.\"\"\"\n    self.text = text\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.EngineResult.add_item","title":"add_item","text":"<pre><code>add_item(item: OperatorResult)\n</code></pre> <p>Add an item.</p> PARAMETER DESCRIPTION <code>item</code> <p>an item to add to the list.</p> <p> TYPE: <code>OperatorResult</code> </p> Source code in <code>presidio_anonymizer/entities/engine/result/engine_result.py</code> <pre><code>def add_item(self, item: OperatorResult):\n    \"\"\"Add an item.\n\n    :param item: an item to add to the list.\n    \"\"\"\n    self.items.append(item)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.EngineResult.normalize_item_indexes","title":"normalize_item_indexes","text":"<pre><code>normalize_item_indexes()\n</code></pre> <p>Normalize the indexes to be index from start.</p> Source code in <code>presidio_anonymizer/entities/engine/result/engine_result.py</code> <pre><code>def normalize_item_indexes(self):\n    \"\"\"Normalize the indexes to be index from start.\"\"\"\n    text_len = len(self.text)\n    for result_item in self.items:\n        result_item.start = text_len - result_item.end\n        result_item.end = result_item.start + len(result_item.text)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.EngineResult.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Return a json string serializing this instance.</p> Source code in <code>presidio_anonymizer/entities/engine/result/engine_result.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Return a json string serializing this instance.\"\"\"\n    return json.dumps(self, default=lambda x: x.__dict__)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.InvalidParamError","title":"InvalidParamError","text":"<p>               Bases: <code>Exception</code></p> <p>Throw exception with error when user input is not valid.</p> <p>param msg: Message to be added to the exception</p> Source code in <code>presidio_anonymizer/entities/invalid_exception.py</code> <pre><code>class InvalidParamError(Exception):\n    \"\"\"Throw exception with error when user input is not valid.\n\n    param msg: Message to be added to the exception\n    \"\"\"\n\n    def __init__(self, msg: str):\n        self.err_msg = msg\n        super().__init__(self.err_msg)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.OperatorConfig","title":"OperatorConfig","text":"<p>Hold the data of the required operator.</p> METHOD DESCRIPTION <code>from_json</code> <p>Create OperatorConfig from json.</p> Source code in <code>presidio_anonymizer/entities/engine/operator_config.py</code> <pre><code>class OperatorConfig:\n    \"\"\"Hold the data of the required operator.\"\"\"\n\n    def __init__(self, operator_name: str, params: Dict = None):\n        \"\"\"\n        Create an operator config instance.\n\n        :param operator_name: the name of the operator we want to work with\n        :param params: the parameters the operator needs in order to work\n        \"\"\"\n        self.operator_name = operator_name\n        if not params:\n            params = {}\n        self.params = params\n        self.__validate_fields()\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object.\"\"\"\n        return f\"operator_name: {self.operator_name}, params: {self.params}\"\n\n    @classmethod\n    def from_json(cls, params: Dict) -&gt; \"OperatorConfig\":\n        \"\"\"\n        Create OperatorConfig from json.\n\n        :param params: json e.g.: {\n            \"type\": \"mask\",\n            \"masking_char\": \"*\",\n            \"chars_to_mask\": 4,\n            \"from_end\": true\n            }\n        :return: OperatorConfig\n        \"\"\"\n        operator_name = params.get(\"type\")\n        if operator_name:\n            params.pop(\"type\")\n        return cls(operator_name, params)\n\n    def __eq__(self, other: \"OperatorConfig\"):\n        \"\"\"Verify two OperatorConfigs are equal.\"\"\"\n        operator_name = self.operator_name == other.operator_name\n        return self.params == other.params and operator_name\n\n    def __validate_fields(self):\n        validate_parameter_not_empty(\n            self.operator_name, \"operator config\", \"operator_name\"\n        )\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.OperatorConfig.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(params: Dict) -&gt; OperatorConfig\n</code></pre> <p>Create OperatorConfig from json.</p> PARAMETER DESCRIPTION <code>params</code> <p>json e.g.: { \"type\": \"mask\", \"masking_char\": \"*\", \"chars_to_mask\": 4, \"from_end\": true }</p> <p> TYPE: <code>Dict</code> </p> RETURNS DESCRIPTION <code>OperatorConfig</code> <p>OperatorConfig</p> Source code in <code>presidio_anonymizer/entities/engine/operator_config.py</code> <pre><code>@classmethod\ndef from_json(cls, params: Dict) -&gt; \"OperatorConfig\":\n    \"\"\"\n    Create OperatorConfig from json.\n\n    :param params: json e.g.: {\n        \"type\": \"mask\",\n        \"masking_char\": \"*\",\n        \"chars_to_mask\": 4,\n        \"from_end\": true\n        }\n    :return: OperatorConfig\n    \"\"\"\n    operator_name = params.get(\"type\")\n    if operator_name:\n        params.pop(\"type\")\n    return cls(operator_name, params)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.OperatorResult","title":"OperatorResult","text":"<p>               Bases: <code>PIIEntity</code></p> <p>A class to hold data for engines results either anonymize or deanonymize.</p> METHOD DESCRIPTION <code>to_dict</code> <p>Return object as Dict.</p> <code>from_json</code> <p>Create OperatorResult from user json.</p> Source code in <code>presidio_anonymizer/entities/engine/result/operator_result.py</code> <pre><code>class OperatorResult(PIIEntity):\n    \"\"\"A class to hold data for engines results either anonymize or deanonymize.\"\"\"\n\n    def __init__(\n        self,\n        start: int,\n        end: int,\n        entity_type: str,\n        text: str = None,\n        operator: str = None,\n    ):\n        PIIEntity.__init__(self, start, end, entity_type)\n        self.text = text\n        self.operator = operator\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object.\"\"\"\n        return str(self.to_dict())\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"Return object as Dict.\"\"\"\n        return self.__dict__\n\n    def __str__(self):\n        \"\"\"Return a string representation of the object.\"\"\"\n        return str(self.to_dict())\n\n    def __eq__(self, other: \"OperatorResult\") -&gt; bool:\n        \"\"\"\n        Verify two OperatorResults are equal.\n\n        :param other: OperatorResult\n        :return: bool\n        \"\"\"\n        return (\n            self.start == other.start\n            and self.end == other.end\n            and self.entity_type == other.entity_type\n            and self.operator == other.operator\n            and self.text == other.text\n        )\n\n    @classmethod\n    def from_json(cls, json: Dict) -&gt; \"OperatorResult\":\n        \"\"\"\n        Create OperatorResult from user json.\n\n        :param json: json representation for this operator result. For example:\n        {\n            \"start\": 0,\n            \"end\": 10,\n            \"key\": \"1111111111111111\",\n            \"entity_type\":\"PERSON\",\n            \"text\":\"resulted_text\",\n            \"operator\":\"encrypt\",\n        }\n        \"\"\"\n        start = json.get(\"start\")\n        end = json.get(\"end\")\n        entity_type = json.get(\"entity_type\")\n        text = json.get(\"text\")\n        operator = json.get(\"operator\")\n        return cls(\n            start=start,\n            end=end,\n            entity_type=entity_type,\n            text=text,\n            operator=operator,\n        )\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.OperatorResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Return object as Dict.</p> Source code in <code>presidio_anonymizer/entities/engine/result/operator_result.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Return object as Dict.\"\"\"\n    return self.__dict__\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.OperatorResult.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(json: Dict) -&gt; OperatorResult\n</code></pre> <p>Create OperatorResult from user json.</p> PARAMETER DESCRIPTION <code>json</code> <p>json representation for this operator result. For example: { \"start\": 0, \"end\": 10, \"key\": \"1111111111111111\", \"entity_type\":\"PERSON\", \"text\":\"resulted_text\", \"operator\":\"encrypt\", }</p> <p> TYPE: <code>Dict</code> </p> Source code in <code>presidio_anonymizer/entities/engine/result/operator_result.py</code> <pre><code>@classmethod\ndef from_json(cls, json: Dict) -&gt; \"OperatorResult\":\n    \"\"\"\n    Create OperatorResult from user json.\n\n    :param json: json representation for this operator result. For example:\n    {\n        \"start\": 0,\n        \"end\": 10,\n        \"key\": \"1111111111111111\",\n        \"entity_type\":\"PERSON\",\n        \"text\":\"resulted_text\",\n        \"operator\":\"encrypt\",\n    }\n    \"\"\"\n    start = json.get(\"start\")\n    end = json.get(\"end\")\n    entity_type = json.get(\"entity_type\")\n    text = json.get(\"text\")\n    operator = json.get(\"operator\")\n    return cls(\n        start=start,\n        end=end,\n        entity_type=entity_type,\n        text=text,\n        operator=operator,\n    )\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.PIIEntity","title":"PIIEntity","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class to hold the text we are going to operate on metadata.</p> Source code in <code>presidio_anonymizer/entities/engine/pii_entity.py</code> <pre><code>class PIIEntity(ABC):\n    \"\"\"Abstract class to hold the text we are going to operate on metadata.\"\"\"\n\n    logger = logging.getLogger(\"presidio-anonymizer\")\n\n    def __init__(self, start: int, end: int, entity_type: str):\n        self.start = start\n        self.end = end\n        self.entity_type = entity_type\n        self.__validate_fields()\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object.\"\"\"\n        return (\n            f\"start: {self.start}\"\n            f\"end: {self.end},\"\n            f\"entity_type: {self.entity_type}\"\n        )\n\n    def __gt__(self, other):\n        \"\"\"Check one entity is greater then other by the text end index.\"\"\"\n        return self.start &gt; other.start\n\n    def __eq__(self, other):\n        \"\"\"Check two text metadata entities are equal.\"\"\"\n        return (\n            self.start == other.start\n            and self.end == other.end\n            and self.entity_type == other.entity_type\n        )\n\n    def __validate_fields(self):\n        validate_parameter_exists(self.start, \"result\", \"start\")\n        validate_type(self.start, \"start\", int)\n        validate_parameter_exists(self.end, \"result\", \"end\")\n        validate_type(self.end, \"end\", int)\n        validate_parameter_not_empty(self.entity_type, \"result\", \"entity_type\")\n        if self.start &lt; 0 or self.end &lt; 0:\n            raise InvalidParamError(\n                \"Invalid input, result start and end must be positive\"\n            )\n        if self.start &gt; self.end:\n            raise InvalidParamError(\n                f\"Invalid input, start index '{self.start}' \"\n                f\"must be smaller than end index '{self.end}'\"\n            )\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.RecognizerResult","title":"RecognizerResult","text":"<p>               Bases: <code>PIIEntity</code></p> <p>Recognizer Result represents the findings of the detected entity.</p> <p>Result of a recognizer analyzing the text.</p> PARAMETER DESCRIPTION <code>entity_type</code> <p>the type of the entity</p> <p> TYPE: <code>str</code> </p> <code>start</code> <p>the start location of the detected entity</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>the end location of the detected entity</p> <p> TYPE: <code>int</code> </p> <code>score</code> <p>the score of the detection</p> <p> TYPE: <code>float</code> </p> METHOD DESCRIPTION <code>from_json</code> <p>Create RecognizerResult from json.</p> <code>has_conflict</code> <p>Check if two recognizer results are conflicted or not.</p> <code>contains</code> <p>Check if one result is contained or equal to another result.</p> <code>equal_indices</code> <p>Check if the indices are equal between two results.</p> <code>intersects</code> <p>Check if self intersects with a different RecognizerResult.</p> Source code in <code>presidio_anonymizer/entities/engine/recognizer_result.py</code> <pre><code>class RecognizerResult(PIIEntity):\n    \"\"\"\n    Recognizer Result represents the findings of the detected entity.\n\n    Result of a recognizer analyzing the text.\n\n    :param entity_type: the type of the entity\n    :param start: the start location of the detected entity\n    :param end: the end location of the detected entity\n    :param score: the score of the detection\n    \"\"\"\n\n    logger = logging.getLogger(\"presidio-anonymizer\")\n\n    def __init__(self, entity_type: str, start: int, end: int, score: float):\n        PIIEntity.__init__(self, start, end, entity_type)\n        self.score = score\n        validate_parameter_exists(score, \"analyzer result\", \"score\")\n\n    @classmethod\n    def from_json(cls, data: Dict):\n        \"\"\"\n        Create RecognizerResult from json.\n\n        :param data: e.g. {\n            \"start\": 24,\n            \"end\": 32,\n            \"score\": 0.8,\n            \"entity_type\": \"NAME\"\n        }\n        :return: RecognizerResult\n        \"\"\"\n        score = data.get(\"score\")\n        entity_type = data.get(\"entity_type\")\n        start = data.get(\"start\")\n        end = data.get(\"end\")\n        return cls(entity_type, start, end, score)\n\n    def __gt__(self, other):\n        \"\"\"\n        Check if one result is greater by using the results indices in the text.\n\n        :param other: another RecognizerResult\n        :return: bool\n        \"\"\"\n        if self.start == other.start:\n            return self.end &gt; other.end\n        return self.start &gt; other.start\n\n    def __eq__(self, other):\n        \"\"\"\n        Check two results are equal by using all class fields.\n\n        :param other: another RecognizerResult\n        :return: bool\n        \"\"\"\n        equal_type = self.entity_type == other.entity_type\n        equal_score = self.score == other.score\n        return self.equal_indices(other) and equal_type and equal_score\n\n    def __hash__(self):\n        \"\"\"\n        Hash the result data by using all class fields.\n\n        :return: int\n        \"\"\"\n        return hash(\n            f\"{str(self.start)} {str(self.end)} {str(self.score)} {self.entity_type}\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the instance.\"\"\"\n        return (\n            f\"type: {self.entity_type}, \"\n            f\"start: {self.start}, \"\n            f\"end: {self.end}, \"\n            f\"score: {self.score}\"\n        )\n\n    def has_conflict(self, other):\n        \"\"\"\n        Check if two recognizer results are conflicted or not.\n\n        I have a conflict if:\n        1. My indices are the same as the other and my score is lower.\n        2. If my indices are contained in another.\n\n        :param other: RecognizerResult\n        :return:\n        \"\"\"\n        if self.equal_indices(other):\n            return self.score &lt;= other.score\n        return other.contains(self)\n\n    def contains(self, other):\n        \"\"\"\n        Check if one result is contained or equal to another result.\n\n        :param other: another RecognizerResult\n        :return: bool\n        \"\"\"\n        return self.start &lt;= other.start and self.end &gt;= other.end\n\n    def equal_indices(self, other):\n        \"\"\"\n        Check if the indices are equal between two results.\n\n        :param other: another RecognizerResult\n        :return:\n        \"\"\"\n        return self.start == other.start and self.end == other.end\n\n    def intersects(self, other) -&gt; int:\n        \"\"\"\n        Check if self intersects with a different RecognizerResult.\n\n        :return: If intersecting, returns the number of\n        intersecting characters.\n        If not, returns 0\n        \"\"\"\n        # if they do not overlap the intersection is 0\n        if self.end &lt; other.start or other.end &lt; self.start:\n            return 0\n\n        # otherwise the intersection is min(end) - max(start)\n        return min(self.end, other.end) - max(self.start, other.start)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.RecognizerResult.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(data: Dict)\n</code></pre> <p>Create RecognizerResult from json.</p> PARAMETER DESCRIPTION <code>data</code> <p>e.g. { \"start\": 24, \"end\": 32, \"score\": 0.8, \"entity_type\": \"NAME\" }</p> <p> TYPE: <code>Dict</code> </p> RETURNS DESCRIPTION <p>RecognizerResult</p> Source code in <code>presidio_anonymizer/entities/engine/recognizer_result.py</code> <pre><code>@classmethod\ndef from_json(cls, data: Dict):\n    \"\"\"\n    Create RecognizerResult from json.\n\n    :param data: e.g. {\n        \"start\": 24,\n        \"end\": 32,\n        \"score\": 0.8,\n        \"entity_type\": \"NAME\"\n    }\n    :return: RecognizerResult\n    \"\"\"\n    score = data.get(\"score\")\n    entity_type = data.get(\"entity_type\")\n    start = data.get(\"start\")\n    end = data.get(\"end\")\n    return cls(entity_type, start, end, score)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.RecognizerResult.has_conflict","title":"has_conflict","text":"<pre><code>has_conflict(other)\n</code></pre> <p>Check if two recognizer results are conflicted or not.</p> <p>I have a conflict if: 1. My indices are the same as the other and my score is lower. 2. If my indices are contained in another.</p> PARAMETER DESCRIPTION <code>other</code> <p>RecognizerResult</p> <p> </p> RETURNS DESCRIPTION Source code in <code>presidio_anonymizer/entities/engine/recognizer_result.py</code> <pre><code>def has_conflict(self, other):\n    \"\"\"\n    Check if two recognizer results are conflicted or not.\n\n    I have a conflict if:\n    1. My indices are the same as the other and my score is lower.\n    2. If my indices are contained in another.\n\n    :param other: RecognizerResult\n    :return:\n    \"\"\"\n    if self.equal_indices(other):\n        return self.score &lt;= other.score\n    return other.contains(self)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.RecognizerResult.contains","title":"contains","text":"<pre><code>contains(other)\n</code></pre> <p>Check if one result is contained or equal to another result.</p> PARAMETER DESCRIPTION <code>other</code> <p>another RecognizerResult</p> <p> </p> RETURNS DESCRIPTION <p>bool</p> Source code in <code>presidio_anonymizer/entities/engine/recognizer_result.py</code> <pre><code>def contains(self, other):\n    \"\"\"\n    Check if one result is contained or equal to another result.\n\n    :param other: another RecognizerResult\n    :return: bool\n    \"\"\"\n    return self.start &lt;= other.start and self.end &gt;= other.end\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.RecognizerResult.equal_indices","title":"equal_indices","text":"<pre><code>equal_indices(other)\n</code></pre> <p>Check if the indices are equal between two results.</p> PARAMETER DESCRIPTION <code>other</code> <p>another RecognizerResult</p> <p> </p> RETURNS DESCRIPTION Source code in <code>presidio_anonymizer/entities/engine/recognizer_result.py</code> <pre><code>def equal_indices(self, other):\n    \"\"\"\n    Check if the indices are equal between two results.\n\n    :param other: another RecognizerResult\n    :return:\n    \"\"\"\n    return self.start == other.start and self.end == other.end\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.RecognizerResult.intersects","title":"intersects","text":"<pre><code>intersects(other) -&gt; int\n</code></pre> <p>Check if self intersects with a different RecognizerResult.</p> RETURNS DESCRIPTION <code>int</code> <p>If intersecting, returns the number of intersecting characters. If not, returns 0</p> Source code in <code>presidio_anonymizer/entities/engine/recognizer_result.py</code> <pre><code>def intersects(self, other) -&gt; int:\n    \"\"\"\n    Check if self intersects with a different RecognizerResult.\n\n    :return: If intersecting, returns the number of\n    intersecting characters.\n    If not, returns 0\n    \"\"\"\n    # if they do not overlap the intersection is 0\n    if self.end &lt; other.start or other.end &lt; self.start:\n        return 0\n\n    # otherwise the intersection is min(end) - max(start)\n    return min(self.end, other.end) - max(self.start, other.start)\n</code></pre>"},{"location":"api/image_redactor_python/","title":"Presidio Image Redactor API Reference","text":""},{"location":"api/image_redactor_python/#presidio_image_redactor","title":"presidio_image_redactor","text":"<p>Image Redactor root module.</p>"},{"location":"api/image_redactor_python/#presidio_image_redactor.OCR","title":"OCR","text":"<p>               Bases: <code>ABC</code></p> <p>OCR class that performs OCR on a given image.</p> METHOD DESCRIPTION <code>perform_ocr</code> <p>Perform OCR on a given image.</p> <code>get_text_from_ocr_dict</code> <p>Combine the text from the OCR dict to full text.</p> Source code in <code>presidio_image_redactor/ocr.py</code> <pre><code>class OCR(ABC):\n    \"\"\"OCR class that performs OCR on a given image.\"\"\"\n\n    @abstractmethod\n    def perform_ocr(self, image: object, **kwargs) -&gt; dict:\n        \"\"\"Perform OCR on a given image.\n\n        :param image: PIL Image/numpy array or file path(str) to be processed\n        :param kwargs: Additional values for perform OCR method\n\n        :return: results dictionary containing bboxes and text for each detected word\n        \"\"\"\n        pass\n\n    @staticmethod\n    def get_text_from_ocr_dict(ocr_result: dict, separator: str = \" \") -&gt; str:\n        \"\"\"Combine the text from the OCR dict to full text.\n\n        :param ocr_result: dictionary containing the ocr results per word\n        :param separator: separator to use when joining the words\n\n        return: str containing the full extracted text as string\n        \"\"\"\n        if not ocr_result:\n            return \"\"\n        else:\n            return separator.join(ocr_result[\"text\"])\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.OCR.perform_ocr","title":"perform_ocr  <code>abstractmethod</code>","text":"<pre><code>perform_ocr(image: object, **kwargs) -&gt; dict\n</code></pre> <p>Perform OCR on a given image.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image/numpy array or file path(str) to be processed</p> <p> TYPE: <code>object</code> </p> <code>kwargs</code> <p>Additional values for perform OCR method</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>results dictionary containing bboxes and text for each detected word</p> Source code in <code>presidio_image_redactor/ocr.py</code> <pre><code>@abstractmethod\ndef perform_ocr(self, image: object, **kwargs) -&gt; dict:\n    \"\"\"Perform OCR on a given image.\n\n    :param image: PIL Image/numpy array or file path(str) to be processed\n    :param kwargs: Additional values for perform OCR method\n\n    :return: results dictionary containing bboxes and text for each detected word\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.OCR.get_text_from_ocr_dict","title":"get_text_from_ocr_dict  <code>staticmethod</code>","text":"<pre><code>get_text_from_ocr_dict(ocr_result: dict, separator: str = ' ') -&gt; str\n</code></pre> <p>Combine the text from the OCR dict to full text.</p> PARAMETER DESCRIPTION <code>ocr_result</code> <p>dictionary containing the ocr results per word</p> <p> TYPE: <code>dict</code> </p> <code>separator</code> <p>separator to use when joining the words  return: str containing the full extracted text as string</p> <p> TYPE: <code>str</code> DEFAULT: <code>' '</code> </p> Source code in <code>presidio_image_redactor/ocr.py</code> <pre><code>@staticmethod\ndef get_text_from_ocr_dict(ocr_result: dict, separator: str = \" \") -&gt; str:\n    \"\"\"Combine the text from the OCR dict to full text.\n\n    :param ocr_result: dictionary containing the ocr results per word\n    :param separator: separator to use when joining the words\n\n    return: str containing the full extracted text as string\n    \"\"\"\n    if not ocr_result:\n        return \"\"\n    else:\n        return separator.join(ocr_result[\"text\"])\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.TesseractOCR","title":"TesseractOCR","text":"<p>               Bases: <code>OCR</code></p> <p>OCR class that performs OCR on a given image.</p> METHOD DESCRIPTION <code>get_text_from_ocr_dict</code> <p>Combine the text from the OCR dict to full text.</p> <code>perform_ocr</code> <p>Perform OCR on a given image.</p> Source code in <code>presidio_image_redactor/tesseract_ocr.py</code> <pre><code>class TesseractOCR(OCR):\n    \"\"\"OCR class that performs OCR on a given image.\"\"\"\n\n    def perform_ocr(self, image: object, **kwargs) -&gt; dict:\n        \"\"\"Perform OCR on a given image.\n\n        :param image: PIL Image/numpy array or file path(str) to be processed\n        :param kwargs: Additional values for OCR image_to_data\n\n        :return: results dictionary containing bboxes and text for each detected word\n        \"\"\"\n        output_type = pytesseract.Output.DICT\n        return pytesseract.image_to_data(image, output_type=output_type, **kwargs)\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.TesseractOCR.get_text_from_ocr_dict","title":"get_text_from_ocr_dict  <code>staticmethod</code>","text":"<pre><code>get_text_from_ocr_dict(ocr_result: dict, separator: str = ' ') -&gt; str\n</code></pre> <p>Combine the text from the OCR dict to full text.</p> PARAMETER DESCRIPTION <code>ocr_result</code> <p>dictionary containing the ocr results per word</p> <p> TYPE: <code>dict</code> </p> <code>separator</code> <p>separator to use when joining the words  return: str containing the full extracted text as string</p> <p> TYPE: <code>str</code> DEFAULT: <code>' '</code> </p> Source code in <code>presidio_image_redactor/ocr.py</code> <pre><code>@staticmethod\ndef get_text_from_ocr_dict(ocr_result: dict, separator: str = \" \") -&gt; str:\n    \"\"\"Combine the text from the OCR dict to full text.\n\n    :param ocr_result: dictionary containing the ocr results per word\n    :param separator: separator to use when joining the words\n\n    return: str containing the full extracted text as string\n    \"\"\"\n    if not ocr_result:\n        return \"\"\n    else:\n        return separator.join(ocr_result[\"text\"])\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.TesseractOCR.perform_ocr","title":"perform_ocr","text":"<pre><code>perform_ocr(image: object, **kwargs) -&gt; dict\n</code></pre> <p>Perform OCR on a given image.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image/numpy array or file path(str) to be processed</p> <p> TYPE: <code>object</code> </p> <code>kwargs</code> <p>Additional values for OCR image_to_data</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>results dictionary containing bboxes and text for each detected word</p> Source code in <code>presidio_image_redactor/tesseract_ocr.py</code> <pre><code>def perform_ocr(self, image: object, **kwargs) -&gt; dict:\n    \"\"\"Perform OCR on a given image.\n\n    :param image: PIL Image/numpy array or file path(str) to be processed\n    :param kwargs: Additional values for OCR image_to_data\n\n    :return: results dictionary containing bboxes and text for each detected word\n    \"\"\"\n    output_type = pytesseract.Output.DICT\n    return pytesseract.image_to_data(image, output_type=output_type, **kwargs)\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DocumentIntelligenceOCR","title":"DocumentIntelligenceOCR","text":"<p>               Bases: <code>OCR</code></p> <p>OCR class that uses Azure AI Document Intelligence OCR engine.</p> PARAMETER DESCRIPTION <code>key</code> <p>The API key</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>endpoint</code> <p>The API endpoint</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>model_id</code> <p>Which model to use  For details, see https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'prebuilt-document'</code> </p> METHOD DESCRIPTION <code>get_text_from_ocr_dict</code> <p>Combine the text from the OCR dict to full text.</p> <code>get_imgbytes</code> <p>Retrieve the image bytes from the image object.</p> <code>analyze_document</code> <p>Analyze the document and return the result.</p> <code>perform_ocr</code> <p>Perform OCR on the image.</p> Source code in <code>presidio_image_redactor/document_intelligence_ocr.py</code> <pre><code>class DocumentIntelligenceOCR(OCR):\n    \"\"\"OCR class that uses Azure AI Document Intelligence OCR engine.\n\n    :param key: The API key\n    :param endpoint: The API endpoint\n    :param model_id: Which model to use\n\n    For details, see\n    https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/\n    \"\"\"\n\n    SUPPORTED_MODELS = [\n        \"prebuilt-document\",\n        \"prebuilt-read\",\n        \"prebuilt-layout\",\n        \"prebuilt-contract\",\n        \"prebuilt-healthInsuranceCard.us\",\n        \"prebuilt-invoice\",\n        \"prebuilt-receipt\",\n        \"prebuilt-idDocument\",\n        \"prebuilt-businessCard\",\n    ]\n\n    def __init__(\n        self,\n        endpoint: Optional[str] = None,\n        key: Optional[str] = None,\n        model_id: Optional[str] = \"prebuilt-document\",\n    ):\n        if model_id not in DocumentIntelligenceOCR.SUPPORTED_MODELS:\n            raise ValueError(\"Unsupported model id: %s\" % model_id)\n\n        # If endpoint and/or key are not passed, attempt to get from environment\n        # variables\n        if not endpoint:\n            endpoint = os.getenv(\"DOCUMENT_INTELLIGENCE_ENDPOINT\")\n\n        if not key:\n            key = os.getenv(\"DOCUMENT_INTELLIGENCE_KEY\")\n\n        if not key or not endpoint:\n            raise ValueError(\"Endpoint and key must be specified\")\n\n        self.client = DocumentAnalysisClient(\n            endpoint=endpoint, credential=AzureKeyCredential(key)\n        )\n        self.model_id = model_id\n\n    @staticmethod\n    def _polygon_to_bbox(polygon: Sequence[Point]) -&gt; tuple:\n        \"\"\"Convert polygon to a tuple of left/top/width/height.\n\n        The returned bounding box should entirely cover the passed polygon.\n\n        :param polygon: A sequence of points\n\n        :return: a tuple of left/top/width/height in pixel dimensions\n\n        \"\"\"\n        # We need at least two points for a valid bounding box.\n        if len(polygon) &lt; 2:\n            return (0, 0, 0, 0)\n\n        left = min([int(p.x) for p in polygon])\n        top = min([int(p.y) for p in polygon])\n        right = max([int(p.x) for p in polygon])\n        bottom = max([int(p.y) for p in polygon])\n        width = right - left\n        height = bottom - top\n        return (left, top, width, height)\n\n    @staticmethod\n    def _page_to_bboxes(page: DocumentPage) -&gt; dict:\n        \"\"\"Convert bounding boxes to uniform format.\n\n        Presidio supports tesseract format of output only, so we format in the same\n        way.\n        Expected format looks like:\n        {\n            \"left\": [123, 345],\n            \"top\": [0, 15],\n            \"width\": [100, 75],\n            \"height\": [25, 30],\n            \"conf\": [\"1\", \"0.87\"],\n            \"text\": [\"JOHN\", \"DOE\"],\n        }\n\n        :param page: The documentpage object from the DI client library\n\n        :return: dictionary in the expected format for presidio\n        \"\"\"\n        bounds = [\n            DocumentIntelligenceOCR._polygon_to_bbox(word.polygon)\n            for word in page.words\n        ]\n\n        return {\n            \"left\": [box[0] for box in bounds],\n            \"top\": [box[1] for box in bounds],\n            \"width\": [box[2] for box in bounds],\n            \"height\": [box[3] for box in bounds],\n            \"conf\": [w.confidence for w in page.words],\n            \"text\": [w.content for w in page.words],\n        }\n\n    def get_imgbytes(self, image: Union[bytes, np.ndarray, Image.Image]) -&gt; bytes:\n        \"\"\"Retrieve the image bytes from the image object.\n\n        :param image:  Any of bytes/numpy array /PIL image object\n\n        :return: raw image bytes\n        \"\"\"\n        if isinstance(image, bytes):\n            return image\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n            # Fallthrough to process PIL image\n        if isinstance(image, Image.Image):\n            # Image is a PIL image, write to bytes stream\n            ostream = BytesIO()\n            image.save(ostream, \"PNG\")\n            imgbytes = ostream.getvalue()\n        elif isinstance(image, str):\n            # image is a filename\n            imgbytes = open(image, \"rb\")\n        else:\n            raise ValueError(\"Unsupported image type: %s\" % type(image))\n        return imgbytes\n\n    def analyze_document(self, imgbytes: bytes, **kwargs) -&gt; AnalyzedDocument:\n        \"\"\"Analyze the document and return the result.\n\n        :param imgbytes: The bytes to send to the API endpoint\n        :param kwargs: additional arguments for begin_analyze_document\n\n        :return: the result of the poller, an AnalyzedDocument object.\n        \"\"\"\n        poller = self.client.begin_analyze_document(self.model_id, imgbytes, **kwargs)\n        return poller.result()\n\n    def perform_ocr(self, image: object, **kwargs) -&gt; dict:\n        \"\"\"Perform OCR on the image.\n\n        :param image: PIL Image/numpy array or file path(str) to be processed\n        :param kwargs: Additional values for begin_analyze_document\n\n        :return: results dictionary containing bboxes and text for each detected word\n        \"\"\"\n        imgbytes = self.get_imgbytes(image)\n        result = self.analyze_document(imgbytes, **kwargs)\n\n        # Currently cannot handle more than one page.\n        if not (len(result.pages) == 1):\n            raise ValueError(\"DocumentIntelligenceOCR only supports 1 page documents\")\n\n        return DocumentIntelligenceOCR._page_to_bboxes(result.pages[0])\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DocumentIntelligenceOCR.get_text_from_ocr_dict","title":"get_text_from_ocr_dict  <code>staticmethod</code>","text":"<pre><code>get_text_from_ocr_dict(ocr_result: dict, separator: str = ' ') -&gt; str\n</code></pre> <p>Combine the text from the OCR dict to full text.</p> PARAMETER DESCRIPTION <code>ocr_result</code> <p>dictionary containing the ocr results per word</p> <p> TYPE: <code>dict</code> </p> <code>separator</code> <p>separator to use when joining the words  return: str containing the full extracted text as string</p> <p> TYPE: <code>str</code> DEFAULT: <code>' '</code> </p> Source code in <code>presidio_image_redactor/ocr.py</code> <pre><code>@staticmethod\ndef get_text_from_ocr_dict(ocr_result: dict, separator: str = \" \") -&gt; str:\n    \"\"\"Combine the text from the OCR dict to full text.\n\n    :param ocr_result: dictionary containing the ocr results per word\n    :param separator: separator to use when joining the words\n\n    return: str containing the full extracted text as string\n    \"\"\"\n    if not ocr_result:\n        return \"\"\n    else:\n        return separator.join(ocr_result[\"text\"])\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DocumentIntelligenceOCR.get_imgbytes","title":"get_imgbytes","text":"<pre><code>get_imgbytes(image: Union[bytes, ndarray, Image]) -&gt; bytes\n</code></pre> <p>Retrieve the image bytes from the image object.</p> PARAMETER DESCRIPTION <code>image</code> <p>Any of bytes/numpy array /PIL image object</p> <p> TYPE: <code>Union[bytes, ndarray, Image]</code> </p> RETURNS DESCRIPTION <code>bytes</code> <p>raw image bytes</p> Source code in <code>presidio_image_redactor/document_intelligence_ocr.py</code> <pre><code>def get_imgbytes(self, image: Union[bytes, np.ndarray, Image.Image]) -&gt; bytes:\n    \"\"\"Retrieve the image bytes from the image object.\n\n    :param image:  Any of bytes/numpy array /PIL image object\n\n    :return: raw image bytes\n    \"\"\"\n    if isinstance(image, bytes):\n        return image\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n        # Fallthrough to process PIL image\n    if isinstance(image, Image.Image):\n        # Image is a PIL image, write to bytes stream\n        ostream = BytesIO()\n        image.save(ostream, \"PNG\")\n        imgbytes = ostream.getvalue()\n    elif isinstance(image, str):\n        # image is a filename\n        imgbytes = open(image, \"rb\")\n    else:\n        raise ValueError(\"Unsupported image type: %s\" % type(image))\n    return imgbytes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DocumentIntelligenceOCR.analyze_document","title":"analyze_document","text":"<pre><code>analyze_document(imgbytes: bytes, **kwargs) -&gt; AnalyzedDocument\n</code></pre> <p>Analyze the document and return the result.</p> PARAMETER DESCRIPTION <code>imgbytes</code> <p>The bytes to send to the API endpoint</p> <p> TYPE: <code>bytes</code> </p> <code>kwargs</code> <p>additional arguments for begin_analyze_document</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>AnalyzedDocument</code> <p>the result of the poller, an AnalyzedDocument object.</p> Source code in <code>presidio_image_redactor/document_intelligence_ocr.py</code> <pre><code>def analyze_document(self, imgbytes: bytes, **kwargs) -&gt; AnalyzedDocument:\n    \"\"\"Analyze the document and return the result.\n\n    :param imgbytes: The bytes to send to the API endpoint\n    :param kwargs: additional arguments for begin_analyze_document\n\n    :return: the result of the poller, an AnalyzedDocument object.\n    \"\"\"\n    poller = self.client.begin_analyze_document(self.model_id, imgbytes, **kwargs)\n    return poller.result()\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DocumentIntelligenceOCR.perform_ocr","title":"perform_ocr","text":"<pre><code>perform_ocr(image: object, **kwargs) -&gt; dict\n</code></pre> <p>Perform OCR on the image.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image/numpy array or file path(str) to be processed</p> <p> TYPE: <code>object</code> </p> <code>kwargs</code> <p>Additional values for begin_analyze_document</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>results dictionary containing bboxes and text for each detected word</p> Source code in <code>presidio_image_redactor/document_intelligence_ocr.py</code> <pre><code>def perform_ocr(self, image: object, **kwargs) -&gt; dict:\n    \"\"\"Perform OCR on the image.\n\n    :param image: PIL Image/numpy array or file path(str) to be processed\n    :param kwargs: Additional values for begin_analyze_document\n\n    :return: results dictionary containing bboxes and text for each detected word\n    \"\"\"\n    imgbytes = self.get_imgbytes(image)\n    result = self.analyze_document(imgbytes, **kwargs)\n\n    # Currently cannot handle more than one page.\n    if not (len(result.pages) == 1):\n        raise ValueError(\"DocumentIntelligenceOCR only supports 1 page documents\")\n\n    return DocumentIntelligenceOCR._page_to_bboxes(result.pages[0])\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BboxProcessor","title":"BboxProcessor","text":"<p>Common module for general bounding box operators.</p> METHOD DESCRIPTION <code>get_bboxes_from_ocr_results</code> <p>Get bounding boxes on padded image for all detected words from ocr_results.</p> <code>get_bboxes_from_analyzer_results</code> <p>Organize bounding box info from analyzer results.</p> <code>remove_bbox_padding</code> <p>Remove added padding in bounding box coordinates.</p> <code>match_with_source</code> <p>Match returned redacted PII bbox data with some source of truth for PII.</p> Source code in <code>presidio_image_redactor/bbox.py</code> <pre><code>class BboxProcessor:\n    \"\"\"Common module for general bounding box operators.\"\"\"\n\n    @staticmethod\n    def get_bboxes_from_ocr_results(\n        ocr_results: Dict[str, List[Union[int, str]]],\n    ) -&gt; List[Dict[str, Union[int, float, str]]]:\n        \"\"\"Get bounding boxes on padded image for all detected words from ocr_results.\n\n        :param ocr_results: Raw results from OCR.\n        :return: Bounding box information per word.\n        \"\"\"\n        bboxes = []\n        for i in range(len(ocr_results[\"text\"])):\n            detected_text = ocr_results[\"text\"][i]\n            if detected_text:\n                bbox = {\n                    \"left\": ocr_results[\"left\"][i],\n                    \"top\": ocr_results[\"top\"][i],\n                    \"width\": ocr_results[\"width\"][i],\n                    \"height\": ocr_results[\"height\"][i],\n                    \"conf\": float(ocr_results[\"conf\"][i]),\n                    \"label\": detected_text,\n                }\n                bboxes.append(bbox)\n\n        return bboxes\n\n    @staticmethod\n    def get_bboxes_from_analyzer_results(\n        analyzer_results: List[ImageRecognizerResult],\n    ) -&gt; List[Dict[str, Union[str, float, int]]]:\n        \"\"\"Organize bounding box info from analyzer results.\n\n        :param analyzer_results: Results from using ImageAnalyzerEngine.\n\n        :return: Bounding box info organized.\n        \"\"\"\n        bboxes = []\n        for i in range(len(analyzer_results)):\n            result = analyzer_results[i].to_dict()\n\n            bbox_item = {\n                \"entity_type\": result[\"entity_type\"],\n                \"score\": result[\"score\"],\n                \"left\": result[\"left\"],\n                \"top\": result[\"top\"],\n                \"width\": result[\"width\"],\n                \"height\": result[\"height\"],\n            }\n            bboxes.append(bbox_item)\n\n        return bboxes\n\n    @staticmethod\n    def remove_bbox_padding(\n        analyzer_bboxes: List[Dict[str, Union[str, float, int]]],\n        padding_width: int,\n    ) -&gt; List[Dict[str, int]]:\n        \"\"\"Remove added padding in bounding box coordinates.\n\n        :param analyzer_bboxes: The bounding boxes from analyzer results.\n        :param padding_width: Pixel width used for padding (0 if no padding).\n\n        :return: Bounding box information per word.\n        \"\"\"\n        if padding_width &lt; 0:\n            raise ValueError(\"Padding width must be a non-negative integer.\")\n\n        if len(analyzer_bboxes) &gt; 0:\n            # Get fields\n            has_label = False\n            has_entity_type = False\n            try:\n                _ = analyzer_bboxes[0][\"label\"]\n                has_label = True\n            except KeyError:\n                has_label = False\n            try:\n                _ = analyzer_bboxes[0][\"entity_type\"]\n                has_entity_type = True\n            except KeyError:\n                has_entity_type = False\n\n            # Remove padding from all bounding boxes\n            if has_label is True and has_entity_type is True:\n                bboxes = [\n                    {\n                        \"left\": max(0, bbox[\"left\"] - padding_width),\n                        \"top\": max(0, bbox[\"top\"] - padding_width),\n                        \"width\": bbox[\"width\"],\n                        \"height\": bbox[\"height\"],\n                        \"label\": bbox[\"label\"],\n                        \"entity_type\": bbox[\"entity_type\"],\n                    }\n                    for bbox in analyzer_bboxes\n                ]\n            elif has_label is True and has_entity_type is False:\n                bboxes = [\n                    {\n                        \"left\": max(0, bbox[\"left\"] - padding_width),\n                        \"top\": max(0, bbox[\"top\"] - padding_width),\n                        \"width\": bbox[\"width\"],\n                        \"height\": bbox[\"height\"],\n                        \"label\": bbox[\"label\"],\n                    }\n                    for bbox in analyzer_bboxes\n                ]\n            elif has_label is False and has_entity_type is True:\n                bboxes = [\n                    {\n                        \"left\": max(0, bbox[\"left\"] - padding_width),\n                        \"top\": max(0, bbox[\"top\"] - padding_width),\n                        \"width\": bbox[\"width\"],\n                        \"height\": bbox[\"height\"],\n                        \"entity_type\": bbox[\"entity_type\"],\n                    }\n                    for bbox in analyzer_bboxes\n                ]\n            elif has_label is False and has_entity_type is False:\n                bboxes = [\n                    {\n                        \"left\": max(0, bbox[\"left\"] - padding_width),\n                        \"top\": max(0, bbox[\"top\"] - padding_width),\n                        \"width\": bbox[\"width\"],\n                        \"height\": bbox[\"height\"],\n                    }\n                    for bbox in analyzer_bboxes\n                ]\n        else:\n            bboxes = analyzer_bboxes\n\n        return bboxes\n\n    @staticmethod\n    def match_with_source(\n        all_pos: List[Dict[str, Union[str, int, float]]],\n        pii_source_dict: List[Dict[str, Union[str, int, float]]],\n        detected_pii: Dict[str, Union[str, float, int]],\n        tolerance: int = 50,\n    ) -&gt; Tuple[List[Dict[str, Union[str, int, float]]], bool]:\n        \"\"\"Match returned redacted PII bbox data with some source of truth for PII.\n\n        :param all_pos: Dictionary storing all positives.\n        :param pii_source_dict: List of PII labels for this instance.\n        :param detected_pii: Detected PII (single entity from analyzer_results).\n        :param tolerance: Tolerance for exact coordinates and size data.\n        :return: List of all positive with PII mapped back as possible\n        and whether a match was found.\n        \"\"\"\n        all_pos_match = all_pos.copy()\n\n        # Get info from detected PII (positive)\n        results_left = detected_pii[\"left\"]\n        results_top = detected_pii[\"top\"]\n        results_width = detected_pii[\"width\"]\n        results_height = detected_pii[\"height\"]\n        try:\n            results_score = detected_pii[\"score\"]\n        except KeyError:\n            # Handle matching when no score available\n            results_score = 0\n        match_found = False\n\n        # See what in the ground truth this positive matches\n        for label in pii_source_dict:\n            source_left = label[\"left\"]\n            source_top = label[\"top\"]\n            source_width = label[\"width\"]\n            source_height = label[\"height\"]\n\n            match_left = abs(source_left - results_left) &lt;= tolerance\n            match_top = abs(source_top - results_top) &lt;= tolerance\n            match_width = abs(source_width - results_width) &lt;= tolerance\n            match_height = abs(source_height - results_height) &lt;= tolerance\n            matching = [match_left, match_top, match_width, match_height]\n\n            if False not in matching:\n                # If match is found, carry over ground truth info\n                positive = label\n                positive[\"score\"] = results_score\n                all_pos_match.append(positive)\n                match_found = True\n\n        return all_pos_match, match_found\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BboxProcessor.get_bboxes_from_ocr_results","title":"get_bboxes_from_ocr_results  <code>staticmethod</code>","text":"<pre><code>get_bboxes_from_ocr_results(\n    ocr_results: Dict[str, List[Union[int, str]]],\n) -&gt; List[Dict[str, Union[int, float, str]]]\n</code></pre> <p>Get bounding boxes on padded image for all detected words from ocr_results.</p> PARAMETER DESCRIPTION <code>ocr_results</code> <p>Raw results from OCR.</p> <p> TYPE: <code>Dict[str, List[Union[int, str]]]</code> </p> RETURNS DESCRIPTION <code>List[Dict[str, Union[int, float, str]]]</code> <p>Bounding box information per word.</p> Source code in <code>presidio_image_redactor/bbox.py</code> <pre><code>@staticmethod\ndef get_bboxes_from_ocr_results(\n    ocr_results: Dict[str, List[Union[int, str]]],\n) -&gt; List[Dict[str, Union[int, float, str]]]:\n    \"\"\"Get bounding boxes on padded image for all detected words from ocr_results.\n\n    :param ocr_results: Raw results from OCR.\n    :return: Bounding box information per word.\n    \"\"\"\n    bboxes = []\n    for i in range(len(ocr_results[\"text\"])):\n        detected_text = ocr_results[\"text\"][i]\n        if detected_text:\n            bbox = {\n                \"left\": ocr_results[\"left\"][i],\n                \"top\": ocr_results[\"top\"][i],\n                \"width\": ocr_results[\"width\"][i],\n                \"height\": ocr_results[\"height\"][i],\n                \"conf\": float(ocr_results[\"conf\"][i]),\n                \"label\": detected_text,\n            }\n            bboxes.append(bbox)\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BboxProcessor.get_bboxes_from_analyzer_results","title":"get_bboxes_from_analyzer_results  <code>staticmethod</code>","text":"<pre><code>get_bboxes_from_analyzer_results(\n    analyzer_results: List[ImageRecognizerResult],\n) -&gt; List[Dict[str, Union[str, float, int]]]\n</code></pre> <p>Organize bounding box info from analyzer results.</p> PARAMETER DESCRIPTION <code>analyzer_results</code> <p>Results from using ImageAnalyzerEngine.</p> <p> TYPE: <code>List[ImageRecognizerResult]</code> </p> RETURNS DESCRIPTION <code>List[Dict[str, Union[str, float, int]]]</code> <p>Bounding box info organized.</p> Source code in <code>presidio_image_redactor/bbox.py</code> <pre><code>@staticmethod\ndef get_bboxes_from_analyzer_results(\n    analyzer_results: List[ImageRecognizerResult],\n) -&gt; List[Dict[str, Union[str, float, int]]]:\n    \"\"\"Organize bounding box info from analyzer results.\n\n    :param analyzer_results: Results from using ImageAnalyzerEngine.\n\n    :return: Bounding box info organized.\n    \"\"\"\n    bboxes = []\n    for i in range(len(analyzer_results)):\n        result = analyzer_results[i].to_dict()\n\n        bbox_item = {\n            \"entity_type\": result[\"entity_type\"],\n            \"score\": result[\"score\"],\n            \"left\": result[\"left\"],\n            \"top\": result[\"top\"],\n            \"width\": result[\"width\"],\n            \"height\": result[\"height\"],\n        }\n        bboxes.append(bbox_item)\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BboxProcessor.remove_bbox_padding","title":"remove_bbox_padding  <code>staticmethod</code>","text":"<pre><code>remove_bbox_padding(\n    analyzer_bboxes: List[Dict[str, Union[str, float, int]]], padding_width: int\n) -&gt; List[Dict[str, int]]\n</code></pre> <p>Remove added padding in bounding box coordinates.</p> PARAMETER DESCRIPTION <code>analyzer_bboxes</code> <p>The bounding boxes from analyzer results.</p> <p> TYPE: <code>List[Dict[str, Union[str, float, int]]]</code> </p> <code>padding_width</code> <p>Pixel width used for padding (0 if no padding).</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>List[Dict[str, int]]</code> <p>Bounding box information per word.</p> Source code in <code>presidio_image_redactor/bbox.py</code> <pre><code>@staticmethod\ndef remove_bbox_padding(\n    analyzer_bboxes: List[Dict[str, Union[str, float, int]]],\n    padding_width: int,\n) -&gt; List[Dict[str, int]]:\n    \"\"\"Remove added padding in bounding box coordinates.\n\n    :param analyzer_bboxes: The bounding boxes from analyzer results.\n    :param padding_width: Pixel width used for padding (0 if no padding).\n\n    :return: Bounding box information per word.\n    \"\"\"\n    if padding_width &lt; 0:\n        raise ValueError(\"Padding width must be a non-negative integer.\")\n\n    if len(analyzer_bboxes) &gt; 0:\n        # Get fields\n        has_label = False\n        has_entity_type = False\n        try:\n            _ = analyzer_bboxes[0][\"label\"]\n            has_label = True\n        except KeyError:\n            has_label = False\n        try:\n            _ = analyzer_bboxes[0][\"entity_type\"]\n            has_entity_type = True\n        except KeyError:\n            has_entity_type = False\n\n        # Remove padding from all bounding boxes\n        if has_label is True and has_entity_type is True:\n            bboxes = [\n                {\n                    \"left\": max(0, bbox[\"left\"] - padding_width),\n                    \"top\": max(0, bbox[\"top\"] - padding_width),\n                    \"width\": bbox[\"width\"],\n                    \"height\": bbox[\"height\"],\n                    \"label\": bbox[\"label\"],\n                    \"entity_type\": bbox[\"entity_type\"],\n                }\n                for bbox in analyzer_bboxes\n            ]\n        elif has_label is True and has_entity_type is False:\n            bboxes = [\n                {\n                    \"left\": max(0, bbox[\"left\"] - padding_width),\n                    \"top\": max(0, bbox[\"top\"] - padding_width),\n                    \"width\": bbox[\"width\"],\n                    \"height\": bbox[\"height\"],\n                    \"label\": bbox[\"label\"],\n                }\n                for bbox in analyzer_bboxes\n            ]\n        elif has_label is False and has_entity_type is True:\n            bboxes = [\n                {\n                    \"left\": max(0, bbox[\"left\"] - padding_width),\n                    \"top\": max(0, bbox[\"top\"] - padding_width),\n                    \"width\": bbox[\"width\"],\n                    \"height\": bbox[\"height\"],\n                    \"entity_type\": bbox[\"entity_type\"],\n                }\n                for bbox in analyzer_bboxes\n            ]\n        elif has_label is False and has_entity_type is False:\n            bboxes = [\n                {\n                    \"left\": max(0, bbox[\"left\"] - padding_width),\n                    \"top\": max(0, bbox[\"top\"] - padding_width),\n                    \"width\": bbox[\"width\"],\n                    \"height\": bbox[\"height\"],\n                }\n                for bbox in analyzer_bboxes\n            ]\n    else:\n        bboxes = analyzer_bboxes\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BboxProcessor.match_with_source","title":"match_with_source  <code>staticmethod</code>","text":"<pre><code>match_with_source(\n    all_pos: List[Dict[str, Union[str, int, float]]],\n    pii_source_dict: List[Dict[str, Union[str, int, float]]],\n    detected_pii: Dict[str, Union[str, float, int]],\n    tolerance: int = 50,\n) -&gt; Tuple[List[Dict[str, Union[str, int, float]]], bool]\n</code></pre> <p>Match returned redacted PII bbox data with some source of truth for PII.</p> PARAMETER DESCRIPTION <code>all_pos</code> <p>Dictionary storing all positives.</p> <p> TYPE: <code>List[Dict[str, Union[str, int, float]]]</code> </p> <code>pii_source_dict</code> <p>List of PII labels for this instance.</p> <p> TYPE: <code>List[Dict[str, Union[str, int, float]]]</code> </p> <code>detected_pii</code> <p>Detected PII (single entity from analyzer_results).</p> <p> TYPE: <code>Dict[str, Union[str, float, int]]</code> </p> <code>tolerance</code> <p>Tolerance for exact coordinates and size data.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> RETURNS DESCRIPTION <code>Tuple[List[Dict[str, Union[str, int, float]]], bool]</code> <p>List of all positive with PII mapped back as possible and whether a match was found.</p> Source code in <code>presidio_image_redactor/bbox.py</code> <pre><code>@staticmethod\ndef match_with_source(\n    all_pos: List[Dict[str, Union[str, int, float]]],\n    pii_source_dict: List[Dict[str, Union[str, int, float]]],\n    detected_pii: Dict[str, Union[str, float, int]],\n    tolerance: int = 50,\n) -&gt; Tuple[List[Dict[str, Union[str, int, float]]], bool]:\n    \"\"\"Match returned redacted PII bbox data with some source of truth for PII.\n\n    :param all_pos: Dictionary storing all positives.\n    :param pii_source_dict: List of PII labels for this instance.\n    :param detected_pii: Detected PII (single entity from analyzer_results).\n    :param tolerance: Tolerance for exact coordinates and size data.\n    :return: List of all positive with PII mapped back as possible\n    and whether a match was found.\n    \"\"\"\n    all_pos_match = all_pos.copy()\n\n    # Get info from detected PII (positive)\n    results_left = detected_pii[\"left\"]\n    results_top = detected_pii[\"top\"]\n    results_width = detected_pii[\"width\"]\n    results_height = detected_pii[\"height\"]\n    try:\n        results_score = detected_pii[\"score\"]\n    except KeyError:\n        # Handle matching when no score available\n        results_score = 0\n    match_found = False\n\n    # See what in the ground truth this positive matches\n    for label in pii_source_dict:\n        source_left = label[\"left\"]\n        source_top = label[\"top\"]\n        source_width = label[\"width\"]\n        source_height = label[\"height\"]\n\n        match_left = abs(source_left - results_left) &lt;= tolerance\n        match_top = abs(source_top - results_top) &lt;= tolerance\n        match_width = abs(source_width - results_width) &lt;= tolerance\n        match_height = abs(source_height - results_height) &lt;= tolerance\n        matching = [match_left, match_top, match_width, match_height]\n\n        if False not in matching:\n            # If match is found, carry over ground truth info\n            positive = label\n            positive[\"score\"] = results_score\n            all_pos_match.append(positive)\n            match_found = True\n\n    return all_pos_match, match_found\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePreprocessor","title":"ImagePreprocessor","text":"<p>ImagePreprocessor class.</p> <p>Parent class for image preprocessing objects.</p> METHOD DESCRIPTION <code>preprocess_image</code> <p>Preprocess the image to be analyzed.</p> <code>convert_image_to_array</code> <p>Convert PIL image to numpy array.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>class ImagePreprocessor:\n    \"\"\"ImagePreprocessor class.\n\n    Parent class for image preprocessing objects.\n    \"\"\"\n\n    def __init__(self, use_greyscale: bool = True) -&gt; None:\n        \"\"\"Initialize the ImagePreprocessor class.\n\n        :param use_greyscale: Whether to convert the image to greyscale.\n        \"\"\"\n        self.use_greyscale = use_greyscale\n\n    def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n        \"\"\"Preprocess the image to be analyzed.\n\n        :param image: Loaded PIL image.\n\n        :return: The processed image and any metadata regarding the\n             preprocessing approach.\n        \"\"\"\n        return image, {}\n\n    def convert_image_to_array(self, image: Image.Image) -&gt; np.ndarray:\n        \"\"\"Convert PIL image to numpy array.\n\n        :param image: Loaded PIL image.\n\n        :return: image pixels as a numpy array.\n\n        \"\"\"\n\n        if isinstance(image, np.ndarray):\n            img = image\n        else:\n            if self.use_greyscale:\n                image = image.convert(\"L\")\n            img = np.asarray(image)\n        return img\n\n    @staticmethod\n    def _get_bg_color(\n        image: Image.Image, is_greyscale: bool, invert: bool = False\n    ) -&gt; Union[int, Tuple[int, int, int]]:\n        \"\"\"Select most common color as background color.\n\n        :param image: Loaded PIL image.\n        :param is_greyscale: Whether the image is greyscale.\n        :param invert: TRUE if you want to get the inverse of the bg color.\n\n        :return: Background color.\n        \"\"\"\n        # Invert colors if invert flag is True\n        if invert:\n            if image.mode == \"RGBA\":\n                # Handle transparency as needed\n                r, g, b, a = image.split()\n                rgb_image = Image.merge(\"RGB\", (r, g, b))\n                inverted_image = PIL.ImageOps.invert(rgb_image)\n                r2, g2, b2 = inverted_image.split()\n\n                image = Image.merge(\"RGBA\", (r2, g2, b2, a))\n\n            else:\n                image = PIL.ImageOps.invert(image)\n\n        # Get background color\n        if is_greyscale:\n            # Select most common color as color\n            bg_color = int(np.bincount(image.flatten()).argmax())\n        else:\n            # Reduce size of image to 1 pixel to get dominant color\n            tmp_image = image.copy()\n            tmp_image = tmp_image.resize((1, 1), resample=0)\n            bg_color = tmp_image.getpixel((0, 0))\n\n        return bg_color\n\n    @staticmethod\n    def _get_image_contrast(image: np.ndarray) -&gt; Tuple[float, float]:\n        \"\"\"Compute the contrast level and mean intensity of an image.\n\n        :param image: Input image pixels (as a numpy array).\n\n        :return: A tuple containing the contrast level and mean intensity of the image.\n        \"\"\"\n        contrast = np.std(image)\n        mean_intensity = np.mean(image)\n        return contrast, mean_intensity\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePreprocessor.preprocess_image","title":"preprocess_image","text":"<pre><code>preprocess_image(image: Image) -&gt; Tuple[Image.Image, dict]\n</code></pre> <p>Preprocess the image to be analyzed.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded PIL image.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>Tuple[Image, dict]</code> <p>The processed image and any metadata regarding the preprocessing approach.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n    \"\"\"Preprocess the image to be analyzed.\n\n    :param image: Loaded PIL image.\n\n    :return: The processed image and any metadata regarding the\n         preprocessing approach.\n    \"\"\"\n    return image, {}\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePreprocessor.convert_image_to_array","title":"convert_image_to_array","text":"<pre><code>convert_image_to_array(image: Image) -&gt; np.ndarray\n</code></pre> <p>Convert PIL image to numpy array.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded PIL image.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>image pixels as a numpy array.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def convert_image_to_array(self, image: Image.Image) -&gt; np.ndarray:\n    \"\"\"Convert PIL image to numpy array.\n\n    :param image: Loaded PIL image.\n\n    :return: image pixels as a numpy array.\n\n    \"\"\"\n\n    if isinstance(image, np.ndarray):\n        img = image\n    else:\n        if self.use_greyscale:\n            image = image.convert(\"L\")\n        img = np.asarray(image)\n    return img\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine","title":"ImageAnalyzerEngine","text":"<p>ImageAnalyzerEngine class.</p> PARAMETER DESCRIPTION <code>analyzer_engine</code> <p>The Presidio AnalyzerEngine instance to be used to detect PII in text</p> <p> TYPE: <code>Optional[AnalyzerEngine]</code> DEFAULT: <code>None</code> </p> <code>ocr</code> <p>the OCR object to be used to detect text in images.</p> <p> TYPE: <code>Optional[OCR]</code> DEFAULT: <code>None</code> </p> <code>image_preprocessor</code> <p>The ImagePreprocessor object to be used to preprocess the image</p> <p> TYPE: <code>Optional[ImagePreprocessor]</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>analyze</code> <p>Analyse method to analyse the given image.</p> <code>threshold_ocr_result</code> <p>Filter out OCR results below confidence threshold.</p> <code>remove_space_boxes</code> <p>Remove OCR bboxes that are for spaces.</p> <code>map_analyzer_results_to_bounding_boxes</code> <p>Map extracted PII entities to image bounding boxes.</p> <code>fig2img</code> <p>Convert a Matplotlib figure to a PIL Image and return it.</p> <code>get_pii_bboxes</code> <p>Get a list of bboxes with is_PII property.</p> <code>add_custom_bboxes</code> <p>Add custom bounding boxes to image.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>class ImageAnalyzerEngine:\n    \"\"\"ImageAnalyzerEngine class.\n\n    :param analyzer_engine: The Presidio AnalyzerEngine instance\n        to be used to detect PII in text\n    :param ocr: the OCR object to be used to detect text in images.\n    :param image_preprocessor: The ImagePreprocessor object to be\n        used to preprocess the image\n    \"\"\"\n\n    def __init__(\n        self,\n        analyzer_engine: Optional[AnalyzerEngine] = None,\n        ocr: Optional[OCR] = None,\n        image_preprocessor: Optional[ImagePreprocessor] = None,\n    ):\n        if not analyzer_engine:\n            analyzer_engine = AnalyzerEngine()\n        self.analyzer_engine = analyzer_engine\n\n        if not ocr:\n            ocr = TesseractOCR()\n        self.ocr = ocr\n\n        if not image_preprocessor:\n            image_preprocessor = ImagePreprocessor()\n        self.image_preprocessor = image_preprocessor\n\n    def analyze(\n        self, image: object, ocr_kwargs: Optional[dict] = None, **text_analyzer_kwargs\n    ) -&gt; List[ImageRecognizerResult]:\n        \"\"\"Analyse method to analyse the given image.\n\n        :param image: PIL Image/numpy array or file path(str) to be processed.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        :return: List of the extract entities with image bounding boxes.\n        \"\"\"\n        # Perform OCR\n        perform_ocr_kwargs, ocr_threshold = self._parse_ocr_kwargs(ocr_kwargs)\n        image, preprocessing_metadata = self.image_preprocessor.preprocess_image(image)\n        ocr_result = self.ocr.perform_ocr(image, **perform_ocr_kwargs)\n        ocr_result = self.remove_space_boxes(ocr_result)\n\n        if preprocessing_metadata and (\"scale_factor\" in preprocessing_metadata):\n            ocr_result = self._scale_bbox_results(\n                ocr_result, preprocessing_metadata[\"scale_factor\"]\n            )\n\n        # Apply OCR confidence threshold if it is passed in\n        if ocr_threshold:\n            ocr_result = self.threshold_ocr_result(ocr_result, ocr_threshold)\n\n        # Analyze text\n        text = self.ocr.get_text_from_ocr_dict(ocr_result)\n\n        # Difines English as default language, if not specified\n        if \"language\" not in text_analyzer_kwargs:\n            text_analyzer_kwargs[\"language\"] = \"en\"\n        analyzer_result = self.analyzer_engine.analyze(\n            text=text, **text_analyzer_kwargs\n        )\n        allow_list = self._check_for_allow_list(text_analyzer_kwargs)\n        bboxes = self.map_analyzer_results_to_bounding_boxes(\n            analyzer_result, ocr_result, text, allow_list\n        )\n\n        return bboxes\n\n    @staticmethod\n    def threshold_ocr_result(ocr_result: dict, ocr_threshold: float) -&gt; dict:\n        \"\"\"Filter out OCR results below confidence threshold.\n\n        :param ocr_result: OCR results (raw).\n        :param ocr_threshold: Threshold value between -1 and 100.\n\n        :return: OCR results with low confidence items removed.\n        \"\"\"\n        if ocr_threshold &lt; -1 or ocr_threshold &gt; 100:\n            raise ValueError(\"ocr_threshold must be between -1 and 100\")\n\n        # Get indices of items above threshold\n        idx = list()\n        for i, val in enumerate(ocr_result[\"conf\"]):\n            if float(val) &gt;= ocr_threshold:\n                idx.append(i)\n\n        # Only retain high confidence items\n        filtered_ocr_result = {}\n        for key in list(ocr_result.keys()):\n            filtered_ocr_result[key] = [ocr_result[key][i] for i in idx]\n\n        return filtered_ocr_result\n\n    @staticmethod\n    def remove_space_boxes(ocr_result: dict) -&gt; dict:\n        \"\"\"Remove OCR bboxes that are for spaces.\n\n        :param ocr_result: OCR results (raw or thresholded).\n        :return: OCR results with empty words removed.\n        \"\"\"\n        # Get indices of items with no text\n        idx = list()\n        for i, text in enumerate(ocr_result[\"text\"]):\n            is_not_space = text.isspace() is False\n            if text != \"\" and is_not_space:\n                idx.append(i)\n\n        # Only retain items with text\n        filtered_ocr_result = {}\n        for key in list(ocr_result.keys()):\n            filtered_ocr_result[key] = [ocr_result[key][i] for i in idx]\n\n        return filtered_ocr_result\n\n    @staticmethod\n    def map_analyzer_results_to_bounding_boxes(\n        text_analyzer_results: List[RecognizerResult],\n        ocr_result: dict,\n        text: str,\n        allow_list: List[str],\n    ) -&gt; List[ImageRecognizerResult]:\n        \"\"\"Map extracted PII entities to image bounding boxes.\n\n        Matching is based on the position of the recognized entity from analyzer\n        and word (in ocr dict) in the text.\n\n        :param text_analyzer_results: PII entities recognized by presidio analyzer\n        :param ocr_result: dict results with words and bboxes from OCR\n        :param text: text the results are based on\n        :param allow_list: List of words to not redact\n\n        return: list of extracted entities with image bounding boxes\n        \"\"\"\n        if (not ocr_result) or (not text_analyzer_results):\n            return []\n\n        bboxes = []\n        proc_indexes = 0\n        indexes = len(text_analyzer_results)\n\n        pos = 0\n        iter_ocr = enumerate(ocr_result[\"text\"])\n        for index, word in iter_ocr:\n            if not word:\n                pos += 1\n            else:\n                for element in text_analyzer_results:\n                    text_element = text[element.start : element.end]\n                    # check position and text of ocr word matches recognized entity\n                    if (\n                        max(pos, element.start) &lt; min(element.end, pos + len(word))\n                    ) and ((text_element in word) or (word in text_element)):\n                        yes_make_bbox_for_word = (\n                            (word is not None)\n                            and (word != \"\")\n                            and (word.isspace() is False)\n                            and (word not in allow_list)\n                        )\n                        # Do not add bbox for standalone spaces / empty strings\n                        if yes_make_bbox_for_word:\n                            bboxes.append(\n                                ImageRecognizerResult(\n                                    element.entity_type,\n                                    element.start,\n                                    element.end,\n                                    element.score,\n                                    ocr_result[\"left\"][index],\n                                    ocr_result[\"top\"][index],\n                                    ocr_result[\"width\"][index],\n                                    ocr_result[\"height\"][index],\n                                )\n                            )\n\n                            # add bounding boxes for all words in ocr dict\n                            # contained within the text of recognized entity\n                            # based on relative position in the full text\n                            while pos + len(word) &lt; element.end:\n                                prev_word = word\n                                index, word = next(iter_ocr)\n                                yes_make_bbox_for_word = (\n                                    (word is not None)\n                                    and (word != \"\")\n                                    and (word.isspace() is False)\n                                    and (word not in allow_list)\n                                )\n                                if yes_make_bbox_for_word:\n                                    bboxes.append(\n                                        ImageRecognizerResult(\n                                            element.entity_type,\n                                            element.start,\n                                            element.end,\n                                            element.score,\n                                            ocr_result[\"left\"][index],\n                                            ocr_result[\"top\"][index],\n                                            ocr_result[\"width\"][index],\n                                            ocr_result[\"height\"][index],\n                                        )\n                                    )\n                                pos += len(prev_word) + 1\n                            proc_indexes += 1\n\n                if proc_indexes == indexes:\n                    break\n                pos += len(word) + 1\n\n        return bboxes\n\n    @staticmethod\n    def _scale_bbox_results(\n        ocr_result: Dict[str, List[Union[int, str]]], scale_factor: float\n    ) -&gt; Dict[str, float]:\n        \"\"\"Scale down the bounding box results based on a scale percentage.\n\n        :param ocr_result: OCR results (raw).\n        :param scale_percent: Scale percentage for resizing the bounding box.\n\n        :return: OCR results (scaled).\n        \"\"\"\n        scaled_results = deepcopy(ocr_result)\n        coordinate_keys = [\"left\", \"top\"]\n        dimension_keys = [\"width\", \"height\"]\n\n        for coord_key in coordinate_keys:\n            scaled_results[coord_key] = [\n                int(np.ceil((x) / (scale_factor))) for x in scaled_results[coord_key]\n            ]\n\n        for dim_key in dimension_keys:\n            scaled_results[dim_key] = [\n                max(1, int(np.ceil(x / (scale_factor))))\n                for x in scaled_results[dim_key]\n            ]\n        return scaled_results\n\n    @staticmethod\n    def _remove_bbox_padding(\n        analyzer_bboxes: List[Dict[str, Union[str, float, int]]],\n        padding_width: int,\n    ) -&gt; List[Dict[str, int]]:\n        \"\"\"Remove added padding in bounding box coordinates.\n\n        :param analyzer_bboxes: The bounding boxes from analyzer results.\n        :param padding_width: Pixel width used for padding (0 if no padding).\n\n        :return: Bounding box information per word.\n        \"\"\"\n\n        unpadded_results = deepcopy(analyzer_bboxes)\n        if padding_width &lt; 0:\n            raise ValueError(\"Padding width must be a non-negative integer.\")\n\n        coordinate_keys = [\"left\", \"top\"]\n        for coord_key in coordinate_keys:\n            unpadded_results[coord_key] = [\n                max(0, x - padding_width) for x in unpadded_results[coord_key]\n            ]\n\n        return unpadded_results\n\n    @staticmethod\n    def _parse_ocr_kwargs(ocr_kwargs: dict) -&gt; Tuple[dict, float]:\n        \"\"\"Parse the OCR-related kwargs.\n\n        :param ocr_kwargs: Parameters for OCR operations.\n\n        :return: Params for ocr.perform_ocr and ocr_threshold\n        \"\"\"\n        ocr_threshold = None\n        if ocr_kwargs is not None:\n            if \"ocr_threshold\" in ocr_kwargs:\n                ocr_threshold = ocr_kwargs[\"ocr_threshold\"]\n                ocr_kwargs = {\n                    key: value\n                    for key, value in ocr_kwargs.items()\n                    if key != \"ocr_threshold\"\n                }\n        else:\n            ocr_kwargs = {}\n\n        return ocr_kwargs, ocr_threshold\n\n    @staticmethod\n    def _check_for_allow_list(text_analyzer_kwargs: dict) -&gt; List[str]:\n        \"\"\"Check the text_analyzer_kwargs for an allow_list.\n\n        :param text_analyzer_kwargs: Text analyzer kwargs.\n        :return: The allow list if it exists.\n        \"\"\"\n        allow_list = []\n        if text_analyzer_kwargs is not None:\n            if \"allow_list\" in text_analyzer_kwargs:\n                allow_list = text_analyzer_kwargs[\"allow_list\"]\n\n        return allow_list\n\n    @staticmethod\n    def fig2img(fig: matplotlib.figure.Figure) -&gt; Image:\n        \"\"\"Convert a Matplotlib figure to a PIL Image and return it.\n\n        :param fig: Matplotlib figure.\n\n        :return: Image of figure.\n        \"\"\"\n        buf = io.BytesIO()\n        fig.savefig(buf)\n        buf.seek(0)\n        img = Image.open(buf)\n\n        return img\n\n    @staticmethod\n    def get_pii_bboxes(\n        ocr_bboxes: List[dict], analyzer_bboxes: List[dict]\n    ) -&gt; List[dict]:\n        \"\"\"Get a list of bboxes with is_PII property.\n\n        :param ocr_bboxes: Bboxes from OCR results.\n        :param analyzer_bboxes: Bboxes from analyzer results.\n\n        :return: All bboxes with appropriate label for whether it is PHI or not.\n        \"\"\"\n        bboxes = []\n        for ocr_bbox in ocr_bboxes:\n            has_match = False\n\n            # Check if we have the same bbox in analyzer results\n            for analyzer_bbox in analyzer_bboxes:\n                has_same_position = (\n                    ocr_bbox[\"left\"] == analyzer_bbox[\"left\"]\n                    and ocr_bbox[\"top\"] == analyzer_bbox[\"top\"]\n                )  # noqa: E501\n                has_same_dimension = (\n                    ocr_bbox[\"width\"] == analyzer_bbox[\"width\"]\n                    and ocr_bbox[\"height\"] == analyzer_bbox[\"height\"]\n                )  # noqa: E501\n                is_same = has_same_position is True and has_same_dimension is True\n\n                if is_same is True:\n                    current_bbox = analyzer_bbox\n                    current_bbox[\"is_PII\"] = True\n                    has_match = True\n                    break\n\n            if has_match is False:\n                current_bbox = ocr_bbox\n                current_bbox[\"is_PII\"] = False\n\n            bboxes.append(current_bbox)\n\n        return bboxes\n\n    @classmethod\n    def add_custom_bboxes(\n        cls,\n        image: Image,\n        bboxes: List[dict],\n        show_text_annotation: bool = True,\n        use_greyscale_cmap: bool = False,\n    ) -&gt; Image:\n        \"\"\"Add custom bounding boxes to image.\n\n        :param image: Standard image of DICOM pixels.\n        :param bboxes: List of bounding boxes to display (with is_PII field).\n        :param show_text_annotation: True if you want text annotation for\n        PHI status to display.\n        :param use_greyscale_cmap: Use greyscale color map.\n        :return: Image with bounding boxes drawn on.\n        \"\"\"\n        image_custom = ImageChops.duplicate(image)\n        image_x, image_y = image_custom.size\n\n        fig, ax = plt.subplots()\n        image_r = 70\n        fig.set_size_inches(image_x / image_r, image_y / image_r)\n\n        if len(bboxes) == 0:\n            ax.imshow(image_custom)\n            return image_custom\n        else:\n            for box in bboxes:\n                try:\n                    entity_type = box[\"entity_type\"]\n                except KeyError:\n                    entity_type = \"UNKNOWN\"\n\n                try:\n                    if box[\"is_PII\"]:\n                        bbox_color = \"r\"\n                    else:\n                        bbox_color = \"b\"\n                except KeyError:\n                    bbox_color = \"b\"\n\n                # Get coordinates and dimensions\n                x0 = box[\"left\"]\n                y0 = box[\"top\"]\n                x1 = x0 + box[\"width\"]\n                y1 = y0 + box[\"height\"]\n                rect = matplotlib.patches.Rectangle(\n                    (x0, y0), x1 - x0, y1 - y0, edgecolor=bbox_color, facecolor=\"none\"\n                )\n                ax.add_patch(rect)\n                if show_text_annotation:\n                    ax.annotate(\n                        entity_type,\n                        xy=(x0 - 3, y0 - 3),\n                        xycoords=\"data\",\n                        bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"0.9\"),\n                    )\n            if use_greyscale_cmap:\n                ax.imshow(image_custom, cmap=\"gray\")\n            else:\n                ax.imshow(image_custom)\n            im_from_fig = cls.fig2img(fig)\n            im_resized = im_from_fig.resize((image_x, image_y))\n\n        return im_resized\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.analyze","title":"analyze","text":"<pre><code>analyze(\n    image: object, ocr_kwargs: Optional[dict] = None, **text_analyzer_kwargs\n) -&gt; List[ImageRecognizerResult]\n</code></pre> <p>Analyse method to analyse the given image.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image/numpy array or file path(str) to be processed.</p> <p> TYPE: <code>object</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[ImageRecognizerResult]</code> <p>List of the extract entities with image bounding boxes.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>def analyze(\n    self, image: object, ocr_kwargs: Optional[dict] = None, **text_analyzer_kwargs\n) -&gt; List[ImageRecognizerResult]:\n    \"\"\"Analyse method to analyse the given image.\n\n    :param image: PIL Image/numpy array or file path(str) to be processed.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: List of the extract entities with image bounding boxes.\n    \"\"\"\n    # Perform OCR\n    perform_ocr_kwargs, ocr_threshold = self._parse_ocr_kwargs(ocr_kwargs)\n    image, preprocessing_metadata = self.image_preprocessor.preprocess_image(image)\n    ocr_result = self.ocr.perform_ocr(image, **perform_ocr_kwargs)\n    ocr_result = self.remove_space_boxes(ocr_result)\n\n    if preprocessing_metadata and (\"scale_factor\" in preprocessing_metadata):\n        ocr_result = self._scale_bbox_results(\n            ocr_result, preprocessing_metadata[\"scale_factor\"]\n        )\n\n    # Apply OCR confidence threshold if it is passed in\n    if ocr_threshold:\n        ocr_result = self.threshold_ocr_result(ocr_result, ocr_threshold)\n\n    # Analyze text\n    text = self.ocr.get_text_from_ocr_dict(ocr_result)\n\n    # Difines English as default language, if not specified\n    if \"language\" not in text_analyzer_kwargs:\n        text_analyzer_kwargs[\"language\"] = \"en\"\n    analyzer_result = self.analyzer_engine.analyze(\n        text=text, **text_analyzer_kwargs\n    )\n    allow_list = self._check_for_allow_list(text_analyzer_kwargs)\n    bboxes = self.map_analyzer_results_to_bounding_boxes(\n        analyzer_result, ocr_result, text, allow_list\n    )\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.threshold_ocr_result","title":"threshold_ocr_result  <code>staticmethod</code>","text":"<pre><code>threshold_ocr_result(ocr_result: dict, ocr_threshold: float) -&gt; dict\n</code></pre> <p>Filter out OCR results below confidence threshold.</p> PARAMETER DESCRIPTION <code>ocr_result</code> <p>OCR results (raw).</p> <p> TYPE: <code>dict</code> </p> <code>ocr_threshold</code> <p>Threshold value between -1 and 100.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>OCR results with low confidence items removed.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@staticmethod\ndef threshold_ocr_result(ocr_result: dict, ocr_threshold: float) -&gt; dict:\n    \"\"\"Filter out OCR results below confidence threshold.\n\n    :param ocr_result: OCR results (raw).\n    :param ocr_threshold: Threshold value between -1 and 100.\n\n    :return: OCR results with low confidence items removed.\n    \"\"\"\n    if ocr_threshold &lt; -1 or ocr_threshold &gt; 100:\n        raise ValueError(\"ocr_threshold must be between -1 and 100\")\n\n    # Get indices of items above threshold\n    idx = list()\n    for i, val in enumerate(ocr_result[\"conf\"]):\n        if float(val) &gt;= ocr_threshold:\n            idx.append(i)\n\n    # Only retain high confidence items\n    filtered_ocr_result = {}\n    for key in list(ocr_result.keys()):\n        filtered_ocr_result[key] = [ocr_result[key][i] for i in idx]\n\n    return filtered_ocr_result\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.remove_space_boxes","title":"remove_space_boxes  <code>staticmethod</code>","text":"<pre><code>remove_space_boxes(ocr_result: dict) -&gt; dict\n</code></pre> <p>Remove OCR bboxes that are for spaces.</p> PARAMETER DESCRIPTION <code>ocr_result</code> <p>OCR results (raw or thresholded).</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>OCR results with empty words removed.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@staticmethod\ndef remove_space_boxes(ocr_result: dict) -&gt; dict:\n    \"\"\"Remove OCR bboxes that are for spaces.\n\n    :param ocr_result: OCR results (raw or thresholded).\n    :return: OCR results with empty words removed.\n    \"\"\"\n    # Get indices of items with no text\n    idx = list()\n    for i, text in enumerate(ocr_result[\"text\"]):\n        is_not_space = text.isspace() is False\n        if text != \"\" and is_not_space:\n            idx.append(i)\n\n    # Only retain items with text\n    filtered_ocr_result = {}\n    for key in list(ocr_result.keys()):\n        filtered_ocr_result[key] = [ocr_result[key][i] for i in idx]\n\n    return filtered_ocr_result\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.map_analyzer_results_to_bounding_boxes","title":"map_analyzer_results_to_bounding_boxes  <code>staticmethod</code>","text":"<pre><code>map_analyzer_results_to_bounding_boxes(\n    text_analyzer_results: List[RecognizerResult],\n    ocr_result: dict,\n    text: str,\n    allow_list: List[str],\n) -&gt; List[ImageRecognizerResult]\n</code></pre> <p>Map extracted PII entities to image bounding boxes.</p> <p>Matching is based on the position of the recognized entity from analyzer and word (in ocr dict) in the text.</p> PARAMETER DESCRIPTION <code>text_analyzer_results</code> <p>PII entities recognized by presidio analyzer</p> <p> TYPE: <code>List[RecognizerResult]</code> </p> <code>ocr_result</code> <p>dict results with words and bboxes from OCR</p> <p> TYPE: <code>dict</code> </p> <code>text</code> <p>text the results are based on</p> <p> TYPE: <code>str</code> </p> <code>allow_list</code> <p>List of words to not redact  return: list of extracted entities with image bounding boxes</p> <p> TYPE: <code>List[str]</code> </p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@staticmethod\ndef map_analyzer_results_to_bounding_boxes(\n    text_analyzer_results: List[RecognizerResult],\n    ocr_result: dict,\n    text: str,\n    allow_list: List[str],\n) -&gt; List[ImageRecognizerResult]:\n    \"\"\"Map extracted PII entities to image bounding boxes.\n\n    Matching is based on the position of the recognized entity from analyzer\n    and word (in ocr dict) in the text.\n\n    :param text_analyzer_results: PII entities recognized by presidio analyzer\n    :param ocr_result: dict results with words and bboxes from OCR\n    :param text: text the results are based on\n    :param allow_list: List of words to not redact\n\n    return: list of extracted entities with image bounding boxes\n    \"\"\"\n    if (not ocr_result) or (not text_analyzer_results):\n        return []\n\n    bboxes = []\n    proc_indexes = 0\n    indexes = len(text_analyzer_results)\n\n    pos = 0\n    iter_ocr = enumerate(ocr_result[\"text\"])\n    for index, word in iter_ocr:\n        if not word:\n            pos += 1\n        else:\n            for element in text_analyzer_results:\n                text_element = text[element.start : element.end]\n                # check position and text of ocr word matches recognized entity\n                if (\n                    max(pos, element.start) &lt; min(element.end, pos + len(word))\n                ) and ((text_element in word) or (word in text_element)):\n                    yes_make_bbox_for_word = (\n                        (word is not None)\n                        and (word != \"\")\n                        and (word.isspace() is False)\n                        and (word not in allow_list)\n                    )\n                    # Do not add bbox for standalone spaces / empty strings\n                    if yes_make_bbox_for_word:\n                        bboxes.append(\n                            ImageRecognizerResult(\n                                element.entity_type,\n                                element.start,\n                                element.end,\n                                element.score,\n                                ocr_result[\"left\"][index],\n                                ocr_result[\"top\"][index],\n                                ocr_result[\"width\"][index],\n                                ocr_result[\"height\"][index],\n                            )\n                        )\n\n                        # add bounding boxes for all words in ocr dict\n                        # contained within the text of recognized entity\n                        # based on relative position in the full text\n                        while pos + len(word) &lt; element.end:\n                            prev_word = word\n                            index, word = next(iter_ocr)\n                            yes_make_bbox_for_word = (\n                                (word is not None)\n                                and (word != \"\")\n                                and (word.isspace() is False)\n                                and (word not in allow_list)\n                            )\n                            if yes_make_bbox_for_word:\n                                bboxes.append(\n                                    ImageRecognizerResult(\n                                        element.entity_type,\n                                        element.start,\n                                        element.end,\n                                        element.score,\n                                        ocr_result[\"left\"][index],\n                                        ocr_result[\"top\"][index],\n                                        ocr_result[\"width\"][index],\n                                        ocr_result[\"height\"][index],\n                                    )\n                                )\n                            pos += len(prev_word) + 1\n                        proc_indexes += 1\n\n            if proc_indexes == indexes:\n                break\n            pos += len(word) + 1\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.fig2img","title":"fig2img  <code>staticmethod</code>","text":"<pre><code>fig2img(fig: Figure) -&gt; Image\n</code></pre> <p>Convert a Matplotlib figure to a PIL Image and return it.</p> PARAMETER DESCRIPTION <code>fig</code> <p>Matplotlib figure.</p> <p> TYPE: <code>Figure</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Image of figure.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@staticmethod\ndef fig2img(fig: matplotlib.figure.Figure) -&gt; Image:\n    \"\"\"Convert a Matplotlib figure to a PIL Image and return it.\n\n    :param fig: Matplotlib figure.\n\n    :return: Image of figure.\n    \"\"\"\n    buf = io.BytesIO()\n    fig.savefig(buf)\n    buf.seek(0)\n    img = Image.open(buf)\n\n    return img\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.get_pii_bboxes","title":"get_pii_bboxes  <code>staticmethod</code>","text":"<pre><code>get_pii_bboxes(\n    ocr_bboxes: List[dict], analyzer_bboxes: List[dict]\n) -&gt; List[dict]\n</code></pre> <p>Get a list of bboxes with is_PII property.</p> PARAMETER DESCRIPTION <code>ocr_bboxes</code> <p>Bboxes from OCR results.</p> <p> TYPE: <code>List[dict]</code> </p> <code>analyzer_bboxes</code> <p>Bboxes from analyzer results.</p> <p> TYPE: <code>List[dict]</code> </p> RETURNS DESCRIPTION <code>List[dict]</code> <p>All bboxes with appropriate label for whether it is PHI or not.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@staticmethod\ndef get_pii_bboxes(\n    ocr_bboxes: List[dict], analyzer_bboxes: List[dict]\n) -&gt; List[dict]:\n    \"\"\"Get a list of bboxes with is_PII property.\n\n    :param ocr_bboxes: Bboxes from OCR results.\n    :param analyzer_bboxes: Bboxes from analyzer results.\n\n    :return: All bboxes with appropriate label for whether it is PHI or not.\n    \"\"\"\n    bboxes = []\n    for ocr_bbox in ocr_bboxes:\n        has_match = False\n\n        # Check if we have the same bbox in analyzer results\n        for analyzer_bbox in analyzer_bboxes:\n            has_same_position = (\n                ocr_bbox[\"left\"] == analyzer_bbox[\"left\"]\n                and ocr_bbox[\"top\"] == analyzer_bbox[\"top\"]\n            )  # noqa: E501\n            has_same_dimension = (\n                ocr_bbox[\"width\"] == analyzer_bbox[\"width\"]\n                and ocr_bbox[\"height\"] == analyzer_bbox[\"height\"]\n            )  # noqa: E501\n            is_same = has_same_position is True and has_same_dimension is True\n\n            if is_same is True:\n                current_bbox = analyzer_bbox\n                current_bbox[\"is_PII\"] = True\n                has_match = True\n                break\n\n        if has_match is False:\n            current_bbox = ocr_bbox\n            current_bbox[\"is_PII\"] = False\n\n        bboxes.append(current_bbox)\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.add_custom_bboxes","title":"add_custom_bboxes  <code>classmethod</code>","text":"<pre><code>add_custom_bboxes(\n    image: Image,\n    bboxes: List[dict],\n    show_text_annotation: bool = True,\n    use_greyscale_cmap: bool = False,\n) -&gt; Image\n</code></pre> <p>Add custom bounding boxes to image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Standard image of DICOM pixels.</p> <p> TYPE: <code>Image</code> </p> <code>bboxes</code> <p>List of bounding boxes to display (with is_PII field).</p> <p> TYPE: <code>List[dict]</code> </p> <code>show_text_annotation</code> <p>True if you want text annotation for PHI status to display.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_greyscale_cmap</code> <p>Use greyscale color map.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Image with bounding boxes drawn on.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@classmethod\ndef add_custom_bboxes(\n    cls,\n    image: Image,\n    bboxes: List[dict],\n    show_text_annotation: bool = True,\n    use_greyscale_cmap: bool = False,\n) -&gt; Image:\n    \"\"\"Add custom bounding boxes to image.\n\n    :param image: Standard image of DICOM pixels.\n    :param bboxes: List of bounding boxes to display (with is_PII field).\n    :param show_text_annotation: True if you want text annotation for\n    PHI status to display.\n    :param use_greyscale_cmap: Use greyscale color map.\n    :return: Image with bounding boxes drawn on.\n    \"\"\"\n    image_custom = ImageChops.duplicate(image)\n    image_x, image_y = image_custom.size\n\n    fig, ax = plt.subplots()\n    image_r = 70\n    fig.set_size_inches(image_x / image_r, image_y / image_r)\n\n    if len(bboxes) == 0:\n        ax.imshow(image_custom)\n        return image_custom\n    else:\n        for box in bboxes:\n            try:\n                entity_type = box[\"entity_type\"]\n            except KeyError:\n                entity_type = \"UNKNOWN\"\n\n            try:\n                if box[\"is_PII\"]:\n                    bbox_color = \"r\"\n                else:\n                    bbox_color = \"b\"\n            except KeyError:\n                bbox_color = \"b\"\n\n            # Get coordinates and dimensions\n            x0 = box[\"left\"]\n            y0 = box[\"top\"]\n            x1 = x0 + box[\"width\"]\n            y1 = y0 + box[\"height\"]\n            rect = matplotlib.patches.Rectangle(\n                (x0, y0), x1 - x0, y1 - y0, edgecolor=bbox_color, facecolor=\"none\"\n            )\n            ax.add_patch(rect)\n            if show_text_annotation:\n                ax.annotate(\n                    entity_type,\n                    xy=(x0 - 3, y0 - 3),\n                    xycoords=\"data\",\n                    bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"0.9\"),\n                )\n        if use_greyscale_cmap:\n            ax.imshow(image_custom, cmap=\"gray\")\n        else:\n            ax.imshow(image_custom)\n        im_from_fig = cls.fig2img(fig)\n        im_resized = im_from_fig.resize((image_x, image_y))\n\n    return im_resized\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageRedactorEngine","title":"ImageRedactorEngine","text":"<p>ImageRedactorEngine performs OCR + PII detection + bounding box redaction.</p> PARAMETER DESCRIPTION <code>image_analyzer_engine</code> <p>Engine which performs OCR + PII detection.</p> <p> TYPE: <code>ImageAnalyzerEngine</code> DEFAULT: <code>None</code> </p> METHOD DESCRIPTION <code>redact</code> <p>Redact method to redact the given image.</p> Source code in <code>presidio_image_redactor/image_redactor_engine.py</code> <pre><code>class ImageRedactorEngine:\n    \"\"\"ImageRedactorEngine performs OCR + PII detection + bounding box redaction.\n\n    :param image_analyzer_engine: Engine which performs OCR + PII detection.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_analyzer_engine: ImageAnalyzerEngine = None,\n    ):\n        if not image_analyzer_engine:\n            self.image_analyzer_engine = ImageAnalyzerEngine()\n        else:\n            self.image_analyzer_engine = image_analyzer_engine\n\n        self.bbox_processor = BboxProcessor()\n\n    def redact(\n        self,\n        image: Image,\n        fill: Union[int, Tuple[int, int, int]] = (0, 0, 0),\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; Image:\n        \"\"\"Redact method to redact the given image.\n\n        Please notice, this method duplicates the image, creates a new instance and\n        manipulate it.\n        :param image: PIL Image to be processed.\n        :param fill: colour to fill the shape - int (0-255) for\n        grayscale or Tuple(R, G, B) for RGB.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        :return: the redacted image\n        \"\"\"\n\n        image = ImageChops.duplicate(image)\n\n        # Check the ad-hoc recognizers list\n        self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n        # Detect PII\n        if ad_hoc_recognizers is None:\n            bboxes = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                **text_analyzer_kwargs,\n            )\n        else:\n            bboxes = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                ad_hoc_recognizers=ad_hoc_recognizers,\n                **text_analyzer_kwargs,\n            )\n\n        draw = ImageDraw.Draw(image)\n\n        for box in bboxes:\n            x0 = box.left\n            y0 = box.top\n            x1 = x0 + box.width\n            y1 = y0 + box.height\n            draw.rectangle([x0, y0, x1, y1], fill=fill)\n\n        return image\n\n    @staticmethod\n    def _check_ad_hoc_recognizer_list(\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    ):\n        \"\"\"Check if the provided ad-hoc recognizer list is valid.\n\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        \"\"\"\n        if isinstance(ad_hoc_recognizers, (list, type(None))):\n            if isinstance(ad_hoc_recognizers, list):\n                if len(ad_hoc_recognizers) &gt;= 1:\n                    are_recognizers = all(\n                        isinstance(\n                            x, presidio_analyzer.pattern_recognizer.PatternRecognizer\n                        )\n                        for x in ad_hoc_recognizers\n                    )\n                    if are_recognizers is False:\n                        raise TypeError(\n                            \"\"\"All items in ad_hoc_recognizers list must be\n                            PatternRecognizer objects\"\"\"\n                        )\n                else:\n                    raise TypeError(\n                        \"ad_hoc_recognizers must be None or list of PatternRecognizer\"\n                    )\n        else:\n            raise TypeError(\n                \"ad_hoc_recognizers must be None or list of PatternRecognizer\"\n            )\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageRedactorEngine.redact","title":"redact","text":"<pre><code>redact(\n    image: Image,\n    fill: Union[int, Tuple[int, int, int]] = (0, 0, 0),\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; Image\n</code></pre> <p>Redact method to redact the given image.</p> <p>Please notice, this method duplicates the image, creates a new instance and manipulate it.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to be processed.</p> <p> TYPE: <code>Image</code> </p> <code>fill</code> <p>colour to fill the shape - int (0-255) for grayscale or Tuple(R, G, B) for RGB.</p> <p> TYPE: <code>Union[int, Tuple[int, int, int]]</code> DEFAULT: <code>(0, 0, 0)</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>the redacted image</p> Source code in <code>presidio_image_redactor/image_redactor_engine.py</code> <pre><code>def redact(\n    self,\n    image: Image,\n    fill: Union[int, Tuple[int, int, int]] = (0, 0, 0),\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Image:\n    \"\"\"Redact method to redact the given image.\n\n    Please notice, this method duplicates the image, creates a new instance and\n    manipulate it.\n    :param image: PIL Image to be processed.\n    :param fill: colour to fill the shape - int (0-255) for\n    grayscale or Tuple(R, G, B) for RGB.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: the redacted image\n    \"\"\"\n\n    image = ImageChops.duplicate(image)\n\n    # Check the ad-hoc recognizers list\n    self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n    # Detect PII\n    if ad_hoc_recognizers is None:\n        bboxes = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            **text_analyzer_kwargs,\n        )\n    else:\n        bboxes = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n    draw = ImageDraw.Draw(image)\n\n    for box in bboxes:\n        x0 = box.left\n        y0 = box.top\n        x1 = x0 + box.width\n        y1 = y0 + box.height\n        draw.rectangle([x0, y0, x1, y1], fill=fill)\n\n    return image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePiiVerifyEngine","title":"ImagePiiVerifyEngine","text":"<p>               Bases: <code>ImageRedactorEngine</code></p> <p>ImagePiiVerifyEngine class only supporting Pii verification currently.</p> METHOD DESCRIPTION <code>redact</code> <p>Redact method to redact the given image.</p> <code>verify</code> <p>Annotate image with the detect PII entity.</p> Source code in <code>presidio_image_redactor/image_pii_verify_engine.py</code> <pre><code>class ImagePiiVerifyEngine(ImageRedactorEngine):\n    \"\"\"ImagePiiVerifyEngine class only supporting Pii verification currently.\"\"\"\n\n    def verify(\n        self,\n        image: Image,\n        is_greyscale: bool = False,\n        display_image: bool = True,\n        show_text_annotation: bool = True,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; Image:\n        \"\"\"Annotate image with the detect PII entity.\n\n        Please notice, this method duplicates the image, creates a\n        new instance and manipulate it.\n\n        :param image: PIL Image to be processed.\n        :param is_greyscale: Whether the image is greyscale or not.\n        :param display_image: If the verificationimage is displayed and returned.\n        :param show_text_annotation: True to display entity type when displaying\n        image with bounding boxes.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in ImageAnalyzerEngine.\n\n        :return: the annotated image\n        \"\"\"\n        image = ImageChops.duplicate(image)\n\n        # Check the ad-hoc recognizers list\n        self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n        # Detect text\n        perform_ocr_kwargs, ocr_threshold = (\n            self.image_analyzer_engine._parse_ocr_kwargs(ocr_kwargs)\n        )  # noqa: E501\n        ocr_results = self.image_analyzer_engine.ocr.perform_ocr(\n            image, **perform_ocr_kwargs\n        )\n        if ocr_threshold:\n            ocr_results = self.image_analyzer_engine.threshold_ocr_result(\n                ocr_results, ocr_threshold\n            )\n        ocr_bboxes = self.bbox_processor.get_bboxes_from_ocr_results(ocr_results)\n\n        # Detect PII\n        if ad_hoc_recognizers is None:\n            analyzer_results = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                **text_analyzer_kwargs,\n            )\n        else:\n            analyzer_results = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                ad_hoc_recognizers=ad_hoc_recognizers,\n                **text_analyzer_kwargs,\n            )\n        analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n            analyzer_results\n        )\n\n        # Prepare for plotting\n        pii_bboxes = self.image_analyzer_engine.get_pii_bboxes(\n            ocr_bboxes, analyzer_bboxes\n        )\n        if is_greyscale:\n            use_greyscale_cmap = True\n        else:\n            use_greyscale_cmap = False\n\n        # Get image with verification boxes\n        verify_image = (\n            self.image_analyzer_engine.add_custom_bboxes(\n                image, pii_bboxes, show_text_annotation, use_greyscale_cmap\n            )\n            if display_image\n            else None\n        )\n\n        return verify_image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePiiVerifyEngine.redact","title":"redact","text":"<pre><code>redact(\n    image: Image,\n    fill: Union[int, Tuple[int, int, int]] = (0, 0, 0),\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; Image\n</code></pre> <p>Redact method to redact the given image.</p> <p>Please notice, this method duplicates the image, creates a new instance and manipulate it.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to be processed.</p> <p> TYPE: <code>Image</code> </p> <code>fill</code> <p>colour to fill the shape - int (0-255) for grayscale or Tuple(R, G, B) for RGB.</p> <p> TYPE: <code>Union[int, Tuple[int, int, int]]</code> DEFAULT: <code>(0, 0, 0)</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>the redacted image</p> Source code in <code>presidio_image_redactor/image_redactor_engine.py</code> <pre><code>def redact(\n    self,\n    image: Image,\n    fill: Union[int, Tuple[int, int, int]] = (0, 0, 0),\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Image:\n    \"\"\"Redact method to redact the given image.\n\n    Please notice, this method duplicates the image, creates a new instance and\n    manipulate it.\n    :param image: PIL Image to be processed.\n    :param fill: colour to fill the shape - int (0-255) for\n    grayscale or Tuple(R, G, B) for RGB.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: the redacted image\n    \"\"\"\n\n    image = ImageChops.duplicate(image)\n\n    # Check the ad-hoc recognizers list\n    self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n    # Detect PII\n    if ad_hoc_recognizers is None:\n        bboxes = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            **text_analyzer_kwargs,\n        )\n    else:\n        bboxes = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n    draw = ImageDraw.Draw(image)\n\n    for box in bboxes:\n        x0 = box.left\n        y0 = box.top\n        x1 = x0 + box.width\n        y1 = y0 + box.height\n        draw.rectangle([x0, y0, x1, y1], fill=fill)\n\n    return image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePiiVerifyEngine.verify","title":"verify","text":"<pre><code>verify(\n    image: Image,\n    is_greyscale: bool = False,\n    display_image: bool = True,\n    show_text_annotation: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; Image\n</code></pre> <p>Annotate image with the detect PII entity.</p> <p>Please notice, this method duplicates the image, creates a new instance and manipulate it.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to be processed.</p> <p> TYPE: <code>Image</code> </p> <code>is_greyscale</code> <p>Whether the image is greyscale or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>display_image</code> <p>If the verificationimage is displayed and returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_text_annotation</code> <p>True to display entity type when displaying image with bounding boxes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in ImageAnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>the annotated image</p> Source code in <code>presidio_image_redactor/image_pii_verify_engine.py</code> <pre><code>def verify(\n    self,\n    image: Image,\n    is_greyscale: bool = False,\n    display_image: bool = True,\n    show_text_annotation: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Image:\n    \"\"\"Annotate image with the detect PII entity.\n\n    Please notice, this method duplicates the image, creates a\n    new instance and manipulate it.\n\n    :param image: PIL Image to be processed.\n    :param is_greyscale: Whether the image is greyscale or not.\n    :param display_image: If the verificationimage is displayed and returned.\n    :param show_text_annotation: True to display entity type when displaying\n    image with bounding boxes.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in ImageAnalyzerEngine.\n\n    :return: the annotated image\n    \"\"\"\n    image = ImageChops.duplicate(image)\n\n    # Check the ad-hoc recognizers list\n    self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n    # Detect text\n    perform_ocr_kwargs, ocr_threshold = (\n        self.image_analyzer_engine._parse_ocr_kwargs(ocr_kwargs)\n    )  # noqa: E501\n    ocr_results = self.image_analyzer_engine.ocr.perform_ocr(\n        image, **perform_ocr_kwargs\n    )\n    if ocr_threshold:\n        ocr_results = self.image_analyzer_engine.threshold_ocr_result(\n            ocr_results, ocr_threshold\n        )\n    ocr_bboxes = self.bbox_processor.get_bboxes_from_ocr_results(ocr_results)\n\n    # Detect PII\n    if ad_hoc_recognizers is None:\n        analyzer_results = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            **text_analyzer_kwargs,\n        )\n    else:\n        analyzer_results = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n    analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n        analyzer_results\n    )\n\n    # Prepare for plotting\n    pii_bboxes = self.image_analyzer_engine.get_pii_bboxes(\n        ocr_bboxes, analyzer_bboxes\n    )\n    if is_greyscale:\n        use_greyscale_cmap = True\n    else:\n        use_greyscale_cmap = False\n\n    # Get image with verification boxes\n    verify_image = (\n        self.image_analyzer_engine.add_custom_bboxes(\n            image, pii_bboxes, show_text_annotation, use_greyscale_cmap\n        )\n        if display_image\n        else None\n    )\n\n    return verify_image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine","title":"DicomImageRedactorEngine","text":"<p>               Bases: <code>ImageRedactorEngine</code></p> <p>Performs OCR + PII detection + bounding box redaction.</p> METHOD DESCRIPTION <code>redact_and_return_bbox</code> <p>Redact method to redact the given DICOM image and return redacted bboxes.</p> <code>redact</code> <p>Redact method to redact the given DICOM image.</p> <code>redact_from_file</code> <p>Redact method to redact from a given file.</p> <code>redact_from_directory</code> <p>Redact method to redact from a directory of files.</p> <code>augment_word</code> <p>Apply multiple types of casing to the provided string.</p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>class DicomImageRedactorEngine(ImageRedactorEngine):\n    \"\"\"Performs OCR + PII detection + bounding box redaction.\"\"\"\n\n    def redact_and_return_bbox(\n        self,\n        image: pydicom.dataset.FileDataset,\n        fill: str = \"contrast\",\n        padding_width: int = 25,\n        crop_ratio: float = 0.75,\n        use_metadata: bool = True,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; Tuple[pydicom.dataset.FileDataset, List[Dict[str, int]]]:\n        \"\"\"Redact method to redact the given DICOM image and return redacted bboxes.\n\n        Please note, this method duplicates the image, creates a\n        new instance and manipulates it.\n\n        :param image: Loaded DICOM instance including pixel data and metadata.\n        :param fill: Fill setting to use for redaction box (\"contrast\" or \"background\").\n        :param padding_width: Padding width to use when running OCR.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        :return: DICOM instance with redacted pixel data.\n        \"\"\"\n        # Check input\n        if type(image) not in [pydicom.dataset.FileDataset, pydicom.dataset.Dataset]:\n            raise TypeError(\"The provided image must be a loaded DICOM instance.\")\n        try:\n            image.PixelData\n        except AttributeError as e:\n            raise AttributeError(f\"Provided DICOM instance lacks pixel data: {e}\")\n        except PermissionError as e:\n            raise PermissionError(f\"Unable to access pixel data (may not exist): {e}\")\n        except IsADirectoryError as e:\n            raise IsADirectoryError(f\"DICOM instance is a directory: {e}\")\n\n        instance = deepcopy(image)\n\n        is_greyscale = self._check_if_greyscale(instance)\n        image_np = self._rescale_dcm_pixel_array(instance, is_greyscale)\n        if is_greyscale:\n            # model L for grayscale, and has 8 bit-pixel to store the pixel value\n            image_pil = Image.fromarray(image_np, mode=\"L\")\n        else:\n            # model RGB, has 3x8 bit pixel available to store the value\n            image_pil = Image.fromarray(image_np, mode=\"RGB\")\n        padded_image_pil = self._add_padding(image_pil, is_greyscale, padding_width)\n\n\n        # Detect PII\n        analyzer_results = self._get_analyzer_results(\n            padded_image_pil,\n            instance,\n            use_metadata,\n            ocr_kwargs,\n            ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n        # Redact all bounding boxes from DICOM file\n        analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n            analyzer_results\n        )\n        bboxes = self.bbox_processor.remove_bbox_padding(analyzer_bboxes, padding_width)\n        redacted_image = self._add_redact_box(instance, bboxes, crop_ratio, fill)\n\n        return redacted_image, bboxes\n\n    def redact(\n        self,\n        image: pydicom.dataset.FileDataset,\n        fill: str = \"contrast\",\n        padding_width: int = 25,\n        crop_ratio: float = 0.75,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; pydicom.dataset.FileDataset:\n        \"\"\"Redact method to redact the given DICOM image.\n\n        Please note, this method duplicates the image, creates a\n        new instance and manipulates it.\n\n        :param image: Loaded DICOM instance including pixel data and metadata.\n        :param fill: Fill setting to use for redaction box (\"contrast\" or \"background\").\n        :param padding_width: Padding width to use when running OCR.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        :return: DICOM instance with redacted pixel data.\n        \"\"\"\n        redacted_image, _ = self.redact_and_return_bbox(\n            image=image,\n            fill=fill,\n            padding_width=padding_width,\n            crop_ratio=crop_ratio,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n        return redacted_image\n\n    def redact_from_file(\n        self,\n        input_dicom_path: str,\n        output_dir: str,\n        padding_width: int = 25,\n        crop_ratio: float = 0.75,\n        fill: str = \"contrast\",\n        use_metadata: bool = True,\n        save_bboxes: bool = False,\n        verbose: bool = True,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; None:\n        \"\"\"Redact method to redact from a given file.\n\n        :param input_dicom_path: String path to DICOM image.\n        :param output_dir: String path to parent output directory.\n        :param padding_width: Padding width to use when running OCR.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Color setting to use for redaction box\n        (\"contrast\" or \"background\").\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param save_bboxes: True if we want to save boundings boxes.\n        :param verbose: True to print where redacted file was written to.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        Please notice, this method duplicates the file, creates\n        new instance and manipulate them.\n\n        \"\"\"\n        # Verify the given paths\n        if Path(input_dicom_path).is_dir() is True:\n            raise TypeError(\"input_dicom_path must be file (not dir)\")\n        if Path(input_dicom_path).is_file() is False:\n            raise TypeError(\"input_dicom_path must be a valid file\")\n        if Path(output_dir).is_file() is True:\n            raise TypeError(\n                \"output_dir must be a directory (does not need to exist yet)\"\n            )\n\n        # Create duplicate\n        dst_path = self._copy_files_for_processing(input_dicom_path, output_dir)\n\n        # Process DICOM file\n        output_location = self._redact_single_dicom_image(\n            dcm_path=dst_path,\n            crop_ratio=crop_ratio,\n            fill=fill,\n            padding_width=padding_width,\n            use_metadata=use_metadata,\n            overwrite=True,\n            dst_parent_dir=\".\",\n            save_bboxes=save_bboxes,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n        if verbose:\n            print(f\"Output written to {output_location}\")\n\n        return None\n\n    def redact_from_directory(\n        self,\n        input_dicom_path: str,\n        output_dir: str,\n        padding_width: int = 25,\n        crop_ratio: float = 0.75,\n        fill: str = \"contrast\",\n        use_metadata: bool = True,\n        save_bboxes: bool = False,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; None:\n        \"\"\"Redact method to redact from a directory of files.\n\n        :param input_dicom_path: String path to directory of DICOM images.\n        :param output_dir: String path to parent output directory.\n        :param padding_width: Padding width to use when running OCR.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Color setting to use for redaction box\n        (\"contrast\" or \"background\").\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param save_bboxes: True if we want to save bounding boxes.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        Please notice, this method duplicates the files, creates\n        new instances and manipulate them.\n\n        \"\"\"\n        # Verify the given paths\n        if Path(input_dicom_path).is_dir() is False:\n            raise TypeError(\"input_dicom_path must be a valid directory\")\n        if Path(input_dicom_path).is_file() is True:\n            raise TypeError(\"input_dicom_path must be a directory (not file)\")\n        if Path(output_dir).is_file() is True:\n            raise TypeError(\n                \"output_dir must be a directory (does not need to exist yet)\"\n            )\n\n        # Create duplicates\n        dst_path = self._copy_files_for_processing(input_dicom_path, output_dir)\n\n        # Process DICOM files\n        output_location = self._redact_multiple_dicom_images(\n            dcm_dir=dst_path,\n            crop_ratio=crop_ratio,\n            fill=fill,\n            padding_width=padding_width,\n            use_metadata=use_metadata,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            overwrite=True,\n            dst_parent_dir=\".\",\n            save_bboxes=save_bboxes,\n            ocr_kwargs=ocr_kwargs,\n            **text_analyzer_kwargs,\n        )\n\n        print(f\"Output written to {output_location}\")\n\n        return None\n\n    @staticmethod\n    def _get_all_dcm_files(dcm_dir: Path) -&gt; List[Path]:\n        \"\"\"Return paths to all DICOM files in a directory and its sub-directories.\n\n        :param dcm_dir: pathlib Path to a directory containing at least one .dcm file.\n\n        :return: List of pathlib Path objects.\n        \"\"\"\n        # Define applicable extensions\n        extensions = [\"[dD][cC][mM]\", \"[dD][iI][cC][oO][mM]\"]\n\n        # Get all files with any applicable extension\n        all_files = []\n        for extension in extensions:\n            p = dcm_dir.glob(f\"**/*.{extension}\")\n            files = [x for x in p if x.is_file()]\n            all_files += files\n\n        return all_files\n\n    @staticmethod\n    def _check_if_greyscale(instance: pydicom.dataset.FileDataset) -&gt; bool:\n        \"\"\"Check if a DICOM image is in greyscale.\n\n        :param instance: A single DICOM instance.\n\n        :return: FALSE if the Photometric Interpretation is RGB.\n        \"\"\"\n        # Check if image is grayscale using the Photometric Interpretation element\n        try:\n            color_scale = instance.PhotometricInterpretation\n        except AttributeError:\n            color_scale = None\n        is_greyscale = color_scale in [\"MONOCHROME1\", \"MONOCHROME2\"]\n\n        return is_greyscale\n\n    @staticmethod\n    def _rescale_dcm_pixel_array(\n        instance: pydicom.dataset.FileDataset, is_greyscale: bool\n    ) -&gt; np.ndarray:\n        \"\"\"Rescale DICOM pixel_array.\n\n        :param instance: A singe DICOM instance.\n        :param is_greyscale: FALSE if the Photometric Interpretation is RGB.\n\n        :return: Rescaled DICOM pixel_array.\n        \"\"\"\n        # Normalize contrast\n        if \"WindowWidth\" in instance:\n            if is_greyscale:\n                image_2d = apply_voi_lut(instance.pixel_array, instance)\n            else:\n                image_2d = instance.pixel_array\n        else:\n            image_2d = instance.pixel_array\n\n        # Convert to float to avoid overflow or underflow losses.\n        image_2d_float = image_2d.astype(float)\n\n        if not is_greyscale:\n            image_2d_scaled = image_2d_float\n        else:\n            # Rescaling grey scale between 0-255\n            image_2d_scaled = (\n                (image_2d_float.max() - image_2d_float)\n                / (image_2d_float.max() - image_2d_float.min())\n            ) * 255.0\n\n        # Convert to uint\n        image_2d_scaled = np.uint8(image_2d_scaled)\n\n        return image_2d_scaled\n\n    @staticmethod\n    def _get_bg_color(\n        image: Image.Image, is_greyscale: bool, invert: bool = False\n    ) -&gt; Union[int, Tuple[int, int, int]]:\n        \"\"\"Select most common color as background color.\n\n        :param image: Loaded PIL image.\n        :param colorscale: Colorscale of image (e.g., 'grayscale', 'RGB')\n        :param invert: TRUE if you want to get the inverse of the bg color.\n\n        :return: Background color.\n        \"\"\"\n        # Invert colors if invert flag is True\n        if invert:\n            if image.mode == \"RGBA\":\n                # Handle transparency as needed\n                r, g, b, a = image.split()\n                rgb_image = Image.merge(\"RGB\", (r, g, b))\n                inverted_image = ImageOps.invert(rgb_image)\n                r2, g2, b2 = inverted_image.split()\n\n                image = Image.merge(\"RGBA\", (r2, g2, b2, a))\n\n            else:\n                image = ImageOps.invert(image)\n\n        # Get background color\n        if is_greyscale:\n            # Select most common color as color\n            bg_color = int(np.bincount(list(image.getdata())).argmax())\n        else:\n            # Reduce size of image to 1 pixel to get dominant color\n            tmp_image = image.copy()\n            tmp_image = tmp_image.resize((1, 1), resample=0)\n            bg_color = tmp_image.getpixel((0, 0))\n\n        return bg_color\n\n    @staticmethod\n    def _get_array_corners(pixel_array: np.ndarray, crop_ratio: float) -&gt; np.ndarray:\n        \"\"\"Crop a pixel array to just return the corners in a single array.\n\n        :param pixel_array: Numpy array containing the pixel data.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n\n        :return: Cropped input array.\n        \"\"\"\n        if crop_ratio &gt;= 1.0 or crop_ratio &lt;= 0:\n            raise ValueError(\"crop_ratio must be between 0 and 1\")\n\n        # Set dimensions\n        width = pixel_array.shape[0]\n        height = pixel_array.shape[1]\n        crop_width = int(np.floor(width * crop_ratio / 2))\n        crop_height = int(np.floor(height * crop_ratio / 2))\n\n        # Get coordinates for corners\n        # (left, top, right, bottom)\n        box_top_left = (0, 0, crop_width, crop_height)\n        box_top_right = (width - crop_width, 0, width, crop_height)\n        box_bottom_left = (0, height - crop_height, crop_width, height)\n        box_bottom_right = (width - crop_width, height - crop_height, width, height)\n        boxes = [box_top_left, box_top_right, box_bottom_left, box_bottom_right]\n\n        # Only keep box pixels\n        cropped_pixel_arrays = [\n            pixel_array[box[0] : box[2], box[1] : box[3]] for box in boxes\n        ]\n\n        # Combine the cropped pixel arrays\n        cropped_array = np.vstack(cropped_pixel_arrays)\n\n        return cropped_array\n\n    @classmethod\n    def _get_most_common_pixel_value(\n        cls,\n        instance: pydicom.dataset.FileDataset,\n        crop_ratio: float,\n        fill: str = \"contrast\",\n    ) -&gt; Union[int, Tuple[int, int, int]]:\n        \"\"\"Find the most common pixel value.\n\n        :param instance: A singe DICOM instance.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Determines how box color is selected.\n        'contrast' - Masks stand out relative to background.\n        'background' - Masks are same color as background.\n\n        :return: Most or least common pixel value (depending on fill).\n        \"\"\"\n        # Crop down to just only look at image corners\n        cropped_array = cls._get_array_corners(instance.pixel_array, crop_ratio)\n\n        # Get flattened pixel array\n        flat_pixel_array = np.array(cropped_array).flatten()\n\n        is_greyscale = cls._check_if_greyscale(instance)\n        if is_greyscale:\n            # Get most common value\n            values, counts = np.unique(flat_pixel_array, return_counts=True)\n            flat_pixel_array = np.array(flat_pixel_array)\n            common_value = values[np.argmax(counts)]\n        else:\n            raise TypeError(\n                \"Most common pixel value retrieval is only supported for greyscale images at this point.\"  # noqa: E501\n            )\n\n        # Invert color as necessary\n        if fill.lower() in [\"contrast\", \"invert\", \"inverted\", \"inverse\"]:\n            pixel_value = np.max(flat_pixel_array) - common_value\n        elif fill.lower() in [\"background\", \"bg\"]:\n            pixel_value = common_value\n\n        return pixel_value\n\n    @classmethod\n    def _add_padding(\n        cls,\n        image: Image.Image,\n        is_greyscale: bool,\n        padding_width: int,\n    ) -&gt; Image.Image:\n        \"\"\"Add border to image using most common color.\n\n        :param image: Loaded PIL image.\n        :param is_greyscale: Whether image is in grayscale or not.\n        :param padding_width: Pixel width of padding (uniform).\n\n        :return: PIL image with padding.\n        \"\"\"\n        # Check padding width value\n        if padding_width &lt;= 0:\n            raise ValueError(\"Enter a positive value for padding\")\n        elif padding_width &gt;= 100:\n            raise ValueError(\n                \"Excessive padding width entered. Please use a width under 100 pixels.\"  # noqa: E501\n            )\n\n        # Select most common color as border color\n        border_color = cls._get_bg_color(image, is_greyscale)\n\n        # Add padding\n        right = padding_width\n        left = padding_width\n        top = padding_width\n        bottom = padding_width\n\n        width, height = image.size\n\n        new_width = width + right + left\n        new_height = height + top + bottom\n\n        image_with_padding = Image.new(\n            image.mode, (new_width, new_height), border_color\n        )\n        image_with_padding.paste(image, (left, top))\n\n        return image_with_padding\n\n    @staticmethod\n    def _copy_files_for_processing(src_path: str, dst_parent_dir: str) -&gt; Path:\n        \"\"\"Copy DICOM files. All processing should be done on the copies.\n\n        :param src_path: String path to DICOM file or directory containing DICOM files.\n        :param dst_parent_dir: String path to parent directory of output location.\n\n        :return: Output location of the file(s).\n        \"\"\"\n        # Identify output path\n        tail = list(Path(src_path).parts)[-1]\n        dst_path = Path(dst_parent_dir, tail)\n\n        # Copy file(s)\n        if Path(src_path).is_dir() is True:\n            try:\n                shutil.copytree(src_path, dst_path)\n            except FileExistsError:\n                raise FileExistsError(\n                    \"Destination files already exist. Please clear the destination files or specify a different dst_parent_dir.\"  # noqa: E501\n                )\n        elif Path(src_path).is_file() is True:\n            # Create the output dir manually if working with a single file\n            os.makedirs(Path(dst_path).parent, exist_ok=True)\n            shutil.copyfile(src_path, dst_path)\n        else:\n            raise FileNotFoundError(f\"{src_path} does not exist\")\n\n        return dst_path\n\n    @staticmethod\n    def _get_text_metadata(\n        instance: pydicom.dataset.FileDataset,\n    ) -&gt; Tuple[list, list, list]:\n        \"\"\"Retrieve all text metadata from the DICOM image.\n\n        :param instance: Loaded DICOM instance.\n\n        :return: List of all the instance's element values (excluding pixel data),\n        bool for if the element is specified as being a name,\n        bool for if the element is specified as being related to the patient.\n        \"\"\"\n        metadata_text = list()\n        is_name = list()\n        is_patient = list()\n\n        for element in instance:\n            # Save all metadata except the DICOM image itself\n            if element.name != \"Pixel Data\":\n                # Save the metadata\n                metadata_text.append(element.value)\n\n                # Track whether this particular element is a name\n                if \"name\" in element.name.lower():\n                    is_name.append(True)\n                else:\n                    is_name.append(False)\n\n                # Track whether this particular element is directly tied to the patient\n                if \"patient\" in element.name.lower():\n                    is_patient.append(True)\n                else:\n                    is_patient.append(False)\n            else:\n                metadata_text.append(\"\")\n                is_name.append(False)\n                is_patient.append(False)\n\n        return metadata_text, is_name, is_patient\n\n    @staticmethod\n    def augment_word(word: str, case_sensitive: bool = False) -&gt; list:\n        \"\"\"Apply multiple types of casing to the provided string.\n\n        :param word: String containing the word or term of interest.\n        :param case_sensitive: True if we want to preserve casing.\n\n        :return: List of the same string with different casings and spacing.\n        \"\"\"\n        word_list = []\n        if word != \"\":\n            # Replacing separator character with space, if any\n            text_no_separator = word.replace(\"^\", \" \")\n            text_no_separator = text_no_separator.replace(\"-\", \" \")\n            text_no_separator = \" \".join(text_no_separator.split())\n\n            if case_sensitive:\n                word_list.append(text_no_separator)\n                word_list.extend(\n                    [\n                        text_no_separator.split(\" \"),\n                    ]\n                )\n            else:\n                # Capitalize all characters in string\n                text_upper = text_no_separator.upper()\n\n                # Lowercase all characters in string\n                text_lower = text_no_separator.lower()\n\n                # Capitalize first letter in each part of string\n                text_title = text_no_separator.title()\n\n                # Append iterations\n                word_list.extend(\n                    [text_no_separator, text_upper, text_lower, text_title]\n                )\n\n                # Adding each term as a separate item in the list\n                word_list.extend(\n                    [\n                        text_no_separator.split(\" \"),\n                        text_upper.split(\" \"),\n                        text_lower.split(\" \"),\n                        text_title.split(\" \"),\n                    ]\n                )\n\n            # Flatten list\n            flat_list = []\n            for item in word_list:\n                if isinstance(item, list):\n                    flat_list.extend(item)\n                else:\n                    flat_list.append(item)\n\n            # Remove any duplicates and empty strings\n            word_list = list(set(flat_list))\n            word_list = list(filter(None, word_list))\n\n        return word_list\n\n    @classmethod\n    def _process_names(cls, text_metadata: list, is_name: list) -&gt; list:\n        \"\"\"Process names to have multiple iterations in our PHI list.\n\n        :param metadata_text: List of all the instance's element values\n        (excluding pixel data).\n        :param is_name: True if the element is specified as being a name.\n\n        :return: Metadata text with additional name iterations appended.\n        \"\"\"\n        phi_list = text_metadata.copy()\n\n        for i in range(0, len(text_metadata)):\n            if is_name[i] is True:\n                original_text = str(text_metadata[i])\n                phi_list += cls.augment_word(original_text)\n\n        return phi_list\n\n    @staticmethod\n    def _add_known_generic_phi(phi_list: list) -&gt; list:\n        \"\"\"Add known potential generic PHI values.\n\n        :param phi_list: List of PHI to use with Presidio ad-hoc recognizer.\n\n        :return: Same list with added known values.\n        \"\"\"\n        known_generic_phi = [\"[M]\", \"[F]\", \"[X]\", \"[U]\", \"M\", \"F\", \"X\", \"U\"]\n        phi_list.extend(known_generic_phi)\n\n        return phi_list\n\n    @classmethod\n    def _make_phi_list(\n        cls,\n        original_metadata: List[Union[pydicom.multival.MultiValue, list, tuple]],\n        is_name: List[bool],\n        is_patient: List[bool],\n    ) -&gt; list:\n        \"\"\"Make the list of PHI to use in Presidio ad-hoc recognizer.\n\n        :param original_metadata: List of all the instance's element values\n        (excluding pixel data).\n        :param is_name: True if the element is specified as being a name.\n        :param is_patient: True if the element is specified as being\n        related to the patient.\n\n        :return: List of PHI (str) to use with Presidio ad-hoc recognizer.\n        \"\"\"\n        # Process names\n        phi_list = cls._process_names(original_metadata, is_name)\n\n        # Add known potential phi values\n        phi_list = cls._add_known_generic_phi(phi_list)\n\n        # Flatten any nested lists\n        for phi in phi_list:\n            if type(phi) in [pydicom.multival.MultiValue, list, tuple]:\n                for item in phi:\n                    phi_list.append(item)\n                phi_list.remove(phi)\n\n        # Convert all items to strings\n        phi_str_list = [str(phi) for phi in phi_list]\n\n        # Remove duplicates\n        phi_str_list = list(set(phi_str_list))\n\n        return phi_str_list\n\n    @classmethod\n    def _set_bbox_color(\n        cls, instance: pydicom.dataset.FileDataset, fill: str\n    ) -&gt; Union[int, Tuple[int, int, int]]:\n        \"\"\"Set the bounding box color.\n\n        :param instance: A single DICOM instance.\n        :param fill: Determines how box color is selected.\n        'contrast' - Masks stand out relative to background.\n        'background' - Masks are same color as background.\n\n        :return: int or tuple of int values determining masking box color.\n        \"\"\"\n        # Check if we want the box color to contrast with the background\n        if fill.lower() in [\"contrast\", \"invert\", \"inverted\", \"inverse\"]:\n            invert_flag = True\n        elif fill.lower() in [\"background\", \"bg\"]:\n            invert_flag = False\n        else:\n            raise ValueError(\"fill must be 'contrast' or 'background'\")\n\n        is_greyscale = cls._check_if_greyscale(instance)\n        if is_greyscale:\n            # model L for grayscale, and has 8 bit-pixel to store the pixel value\n            image_pil = Image.fromarray(instance.pixel_array, mode=\"L\")\n        else:\n            # model RGB, has 3x8 bit pixel available to store the value\n            image_pil = Image.fromarray(instance.pixel_array, mode=\"RGB\")\n        box_color = cls._get_bg_color(image_pil, is_greyscale, invert_flag)\n\n        return box_color\n\n    @staticmethod\n    def _check_if_compressed(instance: pydicom.dataset.FileDataset) -&gt; bool:\n        \"\"\"Check if the pixel data is compressed.\n\n        :param instance: DICOM instance.\n\n        :return: Boolean for whether the pixel data is compressed.\n        \"\"\"\n        # Calculate expected bytes\n        rows = instance.Rows\n        columns = instance.Columns\n        samples_per_pixel = instance.SamplesPerPixel\n        bits_allocated = instance.BitsAllocated\n        try:\n            number_of_frames = instance[0x0028, 0x0008].value\n        except KeyError:\n            number_of_frames = 1\n        expected_num_bytes = (\n            rows * columns * number_of_frames * samples_per_pixel * (bits_allocated / 8)\n        )\n\n        # Compare expected vs actual\n        is_compressed = (int(expected_num_bytes)) &gt; len(instance.PixelData)\n\n        return is_compressed\n\n    @staticmethod\n    def _compress_pixel_data(\n        instance: pydicom.dataset.FileDataset,\n    ) -&gt; pydicom.dataset.FileDataset:\n        \"\"\"Recompress pixel data that was decompressed during redaction.\n\n        :param instance: Loaded DICOM instance.\n\n        :return: Instance with compressed pixel data.\n        \"\"\"\n        compression_method = pydicom.uid.RLELossless\n\n        # Temporarily change syntax to an \"uncompressed\" method\n        instance.file_meta.TransferSyntaxUID = pydicom.uid.UID(\"1.2.840.10008.1.2\")\n\n        # Compress and update syntax\n        instance.compress(compression_method, encoding_plugin=\"gdcm\")\n        instance.file_meta.TransferSyntaxUID = compression_method\n\n        return instance\n\n    @staticmethod\n    def _check_if_has_image_icon_sequence(\n        instance: pydicom.dataset.FileDataset,\n    ) -&gt; bool:\n        \"\"\"Check if there is an image icon sequence tag in the metadata.\n\n        This leads to pixel data being present in multiple locations.\n\n        :param instance: DICOM instance.\n\n        :return: Boolean for whether the instance has an image icon sequence tag.\n        \"\"\"\n        has_image_icon_sequence = False\n        try:\n            _ = instance[0x0088, 0x0200]\n            has_image_icon_sequence = True\n        except KeyError:\n            has_image_icon_sequence = False\n\n        return has_image_icon_sequence\n\n    @classmethod\n    def _add_redact_box(\n        cls,\n        instance: pydicom.dataset.FileDataset,\n        bounding_boxes_coordinates: list,\n        crop_ratio: float,\n        fill: str = \"contrast\",\n    ) -&gt; pydicom.dataset.FileDataset:\n        \"\"\"Add redaction bounding boxes on a DICOM instance.\n\n        :param instance: A single DICOM instance.\n        :param bounding_boxes_coordinates: Bounding box coordinates.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Determines how box color is selected.\n        'contrast' - Masks stand out relative to background.\n        'background' - Masks are same color as background.\n\n        :return: A new dicom instance with redaction bounding boxes.\n        \"\"\"\n        # Copy instance\n        redacted_instance = deepcopy(instance)\n        is_compressed = cls._check_if_compressed(redacted_instance)\n        has_image_icon_sequence = cls._check_if_has_image_icon_sequence(\n            redacted_instance\n        )\n\n        # Select masking box color\n        is_greyscale = cls._check_if_greyscale(instance)\n        if is_greyscale:\n            box_color = cls._get_most_common_pixel_value(instance, crop_ratio, fill)\n        else:\n            box_color = cls._set_bbox_color(redacted_instance, fill)\n\n        # Apply mask\n        for i in range(0, len(bounding_boxes_coordinates)):\n            bbox = bounding_boxes_coordinates[i]\n            top = bbox[\"top\"]\n            left = bbox[\"left\"]\n            width = bbox[\"width\"]\n            height = bbox[\"height\"]\n            redacted_instance.pixel_array[top : top + height, left : left + width] = (\n                box_color\n            )\n\n        redacted_instance.PixelData = redacted_instance.pixel_array.tobytes()\n\n        # If original pixel data is compressed, recompress after redaction\n        if is_compressed or has_image_icon_sequence:\n            # Temporary \"fix\" to manually set all YBR photometric interp as YBR_FULL\n            if \"YBR\" in redacted_instance.PhotometricInterpretation:\n                redacted_instance.PhotometricInterpretation = \"YBR_FULL\"\n            redacted_instance = cls._compress_pixel_data(redacted_instance)\n\n        return redacted_instance\n\n    def _get_analyzer_results(\n        self,\n        image: Image.Image,\n        instance: pydicom.dataset.FileDataset,\n        use_metadata: bool,\n        ocr_kwargs: Optional[dict],\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]],\n        **text_analyzer_kwargs,\n    ) -&gt; List[ImageRecognizerResult]:\n        \"\"\"Analyze image with selected redaction approach.\n\n        :param image: DICOM pixel data as PIL image.\n        :param instance: DICOM instance (with metadata).\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine (e.g., allow_list).\n\n        :return: Analyzer results.\n        \"\"\"\n        # Check the ad-hoc recognizers list\n        self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n        # Create custom recognizer using DICOM metadata\n        if use_metadata:\n            original_metadata, is_name, is_patient = self._get_text_metadata(instance)\n            phi_list = self._make_phi_list(original_metadata, is_name, is_patient)\n            deny_list_recognizer = PatternRecognizer(\n                supported_entity=\"PERSON\", deny_list=phi_list\n            )\n\n            if ad_hoc_recognizers is None:\n                ad_hoc_recognizers = [deny_list_recognizer]\n            elif isinstance(ad_hoc_recognizers, list):\n                ad_hoc_recognizers.append(deny_list_recognizer)\n\n        # Detect PII\n        if ad_hoc_recognizers is None:\n            analyzer_results = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                **text_analyzer_kwargs,\n            )\n        else:\n            analyzer_results = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                ad_hoc_recognizers=ad_hoc_recognizers,\n                **text_analyzer_kwargs,\n            )\n\n        return analyzer_results\n\n    @staticmethod\n    def _save_bbox_json(output_dcm_path: str, bboxes: List[Dict[str, int]]) -&gt; None:\n        \"\"\"Save the redacted bounding box info as a json file.\n\n        :param output_dcm_path: Path to the redacted DICOM file.\n\n        :param bboxes: Bounding boxes used in redaction.\n        \"\"\"\n        output_json_path = Path(output_dcm_path).with_suffix(\".json\")\n\n        with open(output_json_path, \"w\") as write_file:\n            json.dump(bboxes, write_file, indent=4)\n\n    def _redact_single_dicom_image(\n        self,\n        dcm_path: str,\n        crop_ratio: float,\n        fill: str,\n        padding_width: int,\n        use_metadata: bool,\n        overwrite: bool,\n        dst_parent_dir: str,\n        save_bboxes: bool,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; str:\n        \"\"\"Redact text PHI present on a DICOM image.\n\n        :param dcm_path: String path to the DICOM file.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Color setting to use for bounding boxes\n        (\"contrast\" or \"background\").\n        :param padding_width: Pixel width of padding (uniform).\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param overwrite: Only set to True if you are providing the\n        duplicated DICOM path in dcm_path.\n        :param dst_parent_dir: String path to parent directory of where to store copies.\n        :param save_bboxes: True if we want to save boundings boxes.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        :return: Path to the output DICOM file.\n        \"\"\"\n        # Ensure we are working on a single file\n        if Path(dcm_path).is_dir():\n            raise FileNotFoundError(\"Please ensure dcm_path is a single file\")\n        elif Path(dcm_path).is_file() is False:\n            raise FileNotFoundError(f\"{dcm_path} does not exist\")\n\n        # Copy file before processing if overwrite==False\n        if overwrite is False:\n            dst_path = self._copy_files_for_processing(dcm_path, dst_parent_dir)\n        else:\n            dst_path = dcm_path\n\n        # Load instance\n        instance = pydicom.dcmread(dst_path)\n\n        try:\n            instance.PixelData\n        except AttributeError:\n            raise AttributeError(\"Provided DICOM file lacks pixel data.\")\n\n        is_greyscale = self._check_if_greyscale(instance)\n        image = self._rescale_dcm_pixel_array(instance, is_greyscale)\n        if is_greyscale:\n            # model L for grayscale, and has 8 bit-pixel to store the pixel value\n            loaded_image = Image.fromarray(image, mode=\"L\")\n        else:\n            loaded_image = Image.fromarray(image, mode=\"RGB\")\n        image = self._add_padding(loaded_image, is_greyscale, padding_width)\n\n        # Detect PII\n        analyzer_results = self._get_analyzer_results(\n            image,\n            instance,\n            use_metadata,\n            ocr_kwargs,\n            ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n        # Redact all bounding boxes from DICOM file\n        analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n            analyzer_results\n        )\n        bboxes = self.bbox_processor.remove_bbox_padding(analyzer_bboxes, padding_width)\n        redacted_dicom_instance = self._add_redact_box(\n            instance, bboxes, crop_ratio, fill\n        )\n        redacted_dicom_instance.save_as(dst_path)\n\n        # Save redacted bboxes\n        if save_bboxes:\n            self._save_bbox_json(dst_path, bboxes)\n\n        return dst_path\n\n    def _redact_multiple_dicom_images(\n        self,\n        dcm_dir: str,\n        crop_ratio: float,\n        fill: str,\n        padding_width: int,\n        use_metadata: bool,\n        overwrite: bool,\n        dst_parent_dir: str,\n        save_bboxes: bool,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; str:\n        \"\"\"Redact text PHI present on all DICOM images in a directory.\n\n        :param dcm_dir: String path to directory containing DICOM files (can be nested).\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Color setting to use for bounding boxes\n        (\"contrast\" or \"background\").\n        :param padding_width: Pixel width of padding (uniform).\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param overwrite: Only set to True if you are providing\n        the duplicated DICOM dir in dcm_dir.\n        :param dst_parent_dir: String path to parent directory of where to store copies.\n        :param save_bboxes: True if we want to save boundings boxes.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        Return:\n            dst_dir (str): Path to the output DICOM directory.\n        \"\"\"\n        # Ensure we are working on a directory (can have sub-directories)\n        if Path(dcm_dir).is_file():\n            raise FileNotFoundError(\"Please ensure dcm_path is a directory\")\n        elif Path(dcm_dir).is_dir() is False:\n            raise FileNotFoundError(f\"{dcm_dir} does not exist\")\n\n        # List of files to process directly\n        if overwrite is False:\n            dst_dir = self._copy_files_for_processing(dcm_dir, dst_parent_dir)\n        else:\n            dst_dir = dcm_dir\n\n        # Process each DICOM file directly\n        all_dcm_files = self._get_all_dcm_files(Path(dst_dir))\n        for dst_path in all_dcm_files:\n            self._redact_single_dicom_image(\n                dst_path,\n                crop_ratio,\n                fill,\n                padding_width,\n                use_metadata,\n                overwrite,\n                dst_parent_dir,\n                save_bboxes,\n                ocr_kwargs=ocr_kwargs,\n                ad_hoc_recognizers=ad_hoc_recognizers,\n                **text_analyzer_kwargs,\n            )\n\n        return dst_dir\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine.redact_and_return_bbox","title":"redact_and_return_bbox","text":"<pre><code>redact_and_return_bbox(\n    image: FileDataset,\n    fill: str = \"contrast\",\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; Tuple[pydicom.dataset.FileDataset, List[Dict[str, int]]]\n</code></pre> <p>Redact method to redact the given DICOM image and return redacted bboxes.</p> <p>Please note, this method duplicates the image, creates a new instance and manipulates it.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded DICOM instance including pixel data and metadata.</p> <p> TYPE: <code>FileDataset</code> </p> <code>fill</code> <p>Fill setting to use for redaction box (\"contrast\" or \"background\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'contrast'</code> </p> <code>padding_width</code> <p>Padding width to use when running OCR.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>crop_ratio</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>use_metadata</code> <p>Whether to redact text in the image that are present in the metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Tuple[FileDataset, List[Dict[str, int]]]</code> <p>DICOM instance with redacted pixel data.</p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact_and_return_bbox(\n    self,\n    image: pydicom.dataset.FileDataset,\n    fill: str = \"contrast\",\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Tuple[pydicom.dataset.FileDataset, List[Dict[str, int]]]:\n    \"\"\"Redact method to redact the given DICOM image and return redacted bboxes.\n\n    Please note, this method duplicates the image, creates a\n    new instance and manipulates it.\n\n    :param image: Loaded DICOM instance including pixel data and metadata.\n    :param fill: Fill setting to use for redaction box (\"contrast\" or \"background\").\n    :param padding_width: Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: DICOM instance with redacted pixel data.\n    \"\"\"\n    # Check input\n    if type(image) not in [pydicom.dataset.FileDataset, pydicom.dataset.Dataset]:\n        raise TypeError(\"The provided image must be a loaded DICOM instance.\")\n    try:\n        image.PixelData\n    except AttributeError as e:\n        raise AttributeError(f\"Provided DICOM instance lacks pixel data: {e}\")\n    except PermissionError as e:\n        raise PermissionError(f\"Unable to access pixel data (may not exist): {e}\")\n    except IsADirectoryError as e:\n        raise IsADirectoryError(f\"DICOM instance is a directory: {e}\")\n\n    instance = deepcopy(image)\n\n    is_greyscale = self._check_if_greyscale(instance)\n    image_np = self._rescale_dcm_pixel_array(instance, is_greyscale)\n    if is_greyscale:\n        # model L for grayscale, and has 8 bit-pixel to store the pixel value\n        image_pil = Image.fromarray(image_np, mode=\"L\")\n    else:\n        # model RGB, has 3x8 bit pixel available to store the value\n        image_pil = Image.fromarray(image_np, mode=\"RGB\")\n    padded_image_pil = self._add_padding(image_pil, is_greyscale, padding_width)\n\n\n    # Detect PII\n    analyzer_results = self._get_analyzer_results(\n        padded_image_pil,\n        instance,\n        use_metadata,\n        ocr_kwargs,\n        ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n\n    # Redact all bounding boxes from DICOM file\n    analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n        analyzer_results\n    )\n    bboxes = self.bbox_processor.remove_bbox_padding(analyzer_bboxes, padding_width)\n    redacted_image = self._add_redact_box(instance, bboxes, crop_ratio, fill)\n\n    return redacted_image, bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine.redact","title":"redact","text":"<pre><code>redact(\n    image: FileDataset,\n    fill: str = \"contrast\",\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; pydicom.dataset.FileDataset\n</code></pre> <p>Redact method to redact the given DICOM image.</p> <p>Please note, this method duplicates the image, creates a new instance and manipulates it.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded DICOM instance including pixel data and metadata.</p> <p> TYPE: <code>FileDataset</code> </p> <code>fill</code> <p>Fill setting to use for redaction box (\"contrast\" or \"background\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'contrast'</code> </p> <code>padding_width</code> <p>Padding width to use when running OCR.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>crop_ratio</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileDataset</code> <p>DICOM instance with redacted pixel data.</p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact(\n    self,\n    image: pydicom.dataset.FileDataset,\n    fill: str = \"contrast\",\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; pydicom.dataset.FileDataset:\n    \"\"\"Redact method to redact the given DICOM image.\n\n    Please note, this method duplicates the image, creates a\n    new instance and manipulates it.\n\n    :param image: Loaded DICOM instance including pixel data and metadata.\n    :param fill: Fill setting to use for redaction box (\"contrast\" or \"background\").\n    :param padding_width: Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: DICOM instance with redacted pixel data.\n    \"\"\"\n    redacted_image, _ = self.redact_and_return_bbox(\n        image=image,\n        fill=fill,\n        padding_width=padding_width,\n        crop_ratio=crop_ratio,\n        ocr_kwargs=ocr_kwargs,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n\n    return redacted_image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine.redact_from_file","title":"redact_from_file","text":"<pre><code>redact_from_file(\n    input_dicom_path: str,\n    output_dir: str,\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    fill: str = \"contrast\",\n    use_metadata: bool = True,\n    save_bboxes: bool = False,\n    verbose: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; None\n</code></pre> <p>Redact method to redact from a given file.</p> PARAMETER DESCRIPTION <code>input_dicom_path</code> <p>String path to DICOM image.</p> <p> TYPE: <code>str</code> </p> <code>output_dir</code> <p>String path to parent output directory.</p> <p> TYPE: <code>str</code> </p> <code>padding_width</code> <p>Padding width to use when running OCR.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>crop_ratio</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>fill</code> <p>Color setting to use for redaction box (\"contrast\" or \"background\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'contrast'</code> </p> <code>use_metadata</code> <p>Whether to redact text in the image that are present in the metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>save_bboxes</code> <p>True if we want to save boundings boxes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>verbose</code> <p>True to print where redacted file was written to.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.  Please notice, this method duplicates the file, creates new instance and manipulate them.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact_from_file(\n    self,\n    input_dicom_path: str,\n    output_dir: str,\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    fill: str = \"contrast\",\n    use_metadata: bool = True,\n    save_bboxes: bool = False,\n    verbose: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; None:\n    \"\"\"Redact method to redact from a given file.\n\n    :param input_dicom_path: String path to DICOM image.\n    :param output_dir: String path to parent output directory.\n    :param padding_width: Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param fill: Color setting to use for redaction box\n    (\"contrast\" or \"background\").\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param save_bboxes: True if we want to save boundings boxes.\n    :param verbose: True to print where redacted file was written to.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    Please notice, this method duplicates the file, creates\n    new instance and manipulate them.\n\n    \"\"\"\n    # Verify the given paths\n    if Path(input_dicom_path).is_dir() is True:\n        raise TypeError(\"input_dicom_path must be file (not dir)\")\n    if Path(input_dicom_path).is_file() is False:\n        raise TypeError(\"input_dicom_path must be a valid file\")\n    if Path(output_dir).is_file() is True:\n        raise TypeError(\n            \"output_dir must be a directory (does not need to exist yet)\"\n        )\n\n    # Create duplicate\n    dst_path = self._copy_files_for_processing(input_dicom_path, output_dir)\n\n    # Process DICOM file\n    output_location = self._redact_single_dicom_image(\n        dcm_path=dst_path,\n        crop_ratio=crop_ratio,\n        fill=fill,\n        padding_width=padding_width,\n        use_metadata=use_metadata,\n        overwrite=True,\n        dst_parent_dir=\".\",\n        save_bboxes=save_bboxes,\n        ocr_kwargs=ocr_kwargs,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n\n    if verbose:\n        print(f\"Output written to {output_location}\")\n\n    return None\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine.redact_from_directory","title":"redact_from_directory","text":"<pre><code>redact_from_directory(\n    input_dicom_path: str,\n    output_dir: str,\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    fill: str = \"contrast\",\n    use_metadata: bool = True,\n    save_bboxes: bool = False,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; None\n</code></pre> <p>Redact method to redact from a directory of files.</p> PARAMETER DESCRIPTION <code>input_dicom_path</code> <p>String path to directory of DICOM images.</p> <p> TYPE: <code>str</code> </p> <code>output_dir</code> <p>String path to parent output directory.</p> <p> TYPE: <code>str</code> </p> <code>padding_width</code> <p>Padding width to use when running OCR.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>crop_ratio</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>fill</code> <p>Color setting to use for redaction box (\"contrast\" or \"background\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'contrast'</code> </p> <code>use_metadata</code> <p>Whether to redact text in the image that are present in the metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>save_bboxes</code> <p>True if we want to save bounding boxes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.  Please notice, this method duplicates the files, creates new instances and manipulate them.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact_from_directory(\n    self,\n    input_dicom_path: str,\n    output_dir: str,\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    fill: str = \"contrast\",\n    use_metadata: bool = True,\n    save_bboxes: bool = False,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; None:\n    \"\"\"Redact method to redact from a directory of files.\n\n    :param input_dicom_path: String path to directory of DICOM images.\n    :param output_dir: String path to parent output directory.\n    :param padding_width: Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param fill: Color setting to use for redaction box\n    (\"contrast\" or \"background\").\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param save_bboxes: True if we want to save bounding boxes.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    Please notice, this method duplicates the files, creates\n    new instances and manipulate them.\n\n    \"\"\"\n    # Verify the given paths\n    if Path(input_dicom_path).is_dir() is False:\n        raise TypeError(\"input_dicom_path must be a valid directory\")\n    if Path(input_dicom_path).is_file() is True:\n        raise TypeError(\"input_dicom_path must be a directory (not file)\")\n    if Path(output_dir).is_file() is True:\n        raise TypeError(\n            \"output_dir must be a directory (does not need to exist yet)\"\n        )\n\n    # Create duplicates\n    dst_path = self._copy_files_for_processing(input_dicom_path, output_dir)\n\n    # Process DICOM files\n    output_location = self._redact_multiple_dicom_images(\n        dcm_dir=dst_path,\n        crop_ratio=crop_ratio,\n        fill=fill,\n        padding_width=padding_width,\n        use_metadata=use_metadata,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        overwrite=True,\n        dst_parent_dir=\".\",\n        save_bboxes=save_bboxes,\n        ocr_kwargs=ocr_kwargs,\n        **text_analyzer_kwargs,\n    )\n\n    print(f\"Output written to {output_location}\")\n\n    return None\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine.augment_word","title":"augment_word  <code>staticmethod</code>","text":"<pre><code>augment_word(word: str, case_sensitive: bool = False) -&gt; list\n</code></pre> <p>Apply multiple types of casing to the provided string.</p> PARAMETER DESCRIPTION <code>word</code> <p>String containing the word or term of interest.</p> <p> TYPE: <code>str</code> </p> <code>case_sensitive</code> <p>True if we want to preserve casing.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list</code> <p>List of the same string with different casings and spacing.</p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>@staticmethod\ndef augment_word(word: str, case_sensitive: bool = False) -&gt; list:\n    \"\"\"Apply multiple types of casing to the provided string.\n\n    :param word: String containing the word or term of interest.\n    :param case_sensitive: True if we want to preserve casing.\n\n    :return: List of the same string with different casings and spacing.\n    \"\"\"\n    word_list = []\n    if word != \"\":\n        # Replacing separator character with space, if any\n        text_no_separator = word.replace(\"^\", \" \")\n        text_no_separator = text_no_separator.replace(\"-\", \" \")\n        text_no_separator = \" \".join(text_no_separator.split())\n\n        if case_sensitive:\n            word_list.append(text_no_separator)\n            word_list.extend(\n                [\n                    text_no_separator.split(\" \"),\n                ]\n            )\n        else:\n            # Capitalize all characters in string\n            text_upper = text_no_separator.upper()\n\n            # Lowercase all characters in string\n            text_lower = text_no_separator.lower()\n\n            # Capitalize first letter in each part of string\n            text_title = text_no_separator.title()\n\n            # Append iterations\n            word_list.extend(\n                [text_no_separator, text_upper, text_lower, text_title]\n            )\n\n            # Adding each term as a separate item in the list\n            word_list.extend(\n                [\n                    text_no_separator.split(\" \"),\n                    text_upper.split(\" \"),\n                    text_lower.split(\" \"),\n                    text_title.split(\" \"),\n                ]\n            )\n\n        # Flatten list\n        flat_list = []\n        for item in word_list:\n            if isinstance(item, list):\n                flat_list.extend(item)\n            else:\n                flat_list.append(item)\n\n        # Remove any duplicates and empty strings\n        word_list = list(set(flat_list))\n        word_list = list(filter(None, word_list))\n\n    return word_list\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine","title":"DicomImagePiiVerifyEngine","text":"<p>               Bases: <code>ImagePiiVerifyEngine</code>, <code>DicomImageRedactorEngine</code></p> <p>Class to handle verification and evaluation for DICOM de-identification.</p> METHOD DESCRIPTION <code>redact</code> <p>Redact method to redact the given DICOM image.</p> <code>redact_and_return_bbox</code> <p>Redact method to redact the given DICOM image and return redacted bboxes.</p> <code>redact_from_file</code> <p>Redact method to redact from a given file.</p> <code>redact_from_directory</code> <p>Redact method to redact from a directory of files.</p> <code>augment_word</code> <p>Apply multiple types of casing to the provided string.</p> <code>verify</code> <p>Annotate image with the detect PII entity.</p> <code>verify_dicom_instance</code> <p>Verify PII on a single DICOM instance.</p> <code>eval_dicom_instance</code> <p>Evaluate performance for a single DICOM instance.</p> <code>calculate_precision</code> <p>Calculate precision.</p> <code>calculate_recall</code> <p>Calculate recall.</p> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>class DicomImagePiiVerifyEngine(ImagePiiVerifyEngine, DicomImageRedactorEngine):\n    \"\"\"Class to handle verification and evaluation for DICOM de-identification.\"\"\"\n\n    def __init__(\n        self,\n        ocr_engine: Optional[OCR] = None,\n        image_analyzer_engine: Optional[ImageAnalyzerEngine] = None,\n    ):\n        \"\"\"Initialize DicomImagePiiVerifyEngine object.\n\n        :param ocr_engine: OCR engine to use.\n        :param image_analyzer_engine: Image analyzer engine to use.\n        \"\"\"\n        # Initialize OCR engine\n        if not ocr_engine:\n            self.ocr_engine = TesseractOCR()\n        else:\n            self.ocr_engine = ocr_engine\n\n        # Initialize image analyzer engine\n        if not image_analyzer_engine:\n            self.image_analyzer_engine = ImageAnalyzerEngine()\n        else:\n            self.image_analyzer_engine = image_analyzer_engine\n\n        # Initialize bbox processor\n        self.bbox_processor = BboxProcessor()\n\n    def verify_dicom_instance(\n        self,\n        instance: pydicom.dataset.FileDataset,\n        padding_width: int = 25,\n        display_image: bool = True,\n        show_text_annotation: bool = True,\n        use_metadata: bool = True,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; Tuple[Optional[PIL.Image.Image], list, list]:\n        \"\"\"Verify PII on a single DICOM instance.\n\n        :param instance: Loaded DICOM instance including pixel data and metadata.\n        :param padding_width: Padding width to use when running OCR.\n        :param display_image: If the verificationimage is displayed and returned.\n        :param show_text_annotation: True to display entity type when displaying\n        image with bounding boxes.\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in ImageAnalyzerEngine.\n\n        :return: Image with boxes identifying PHI, OCR results,\n        and analyzer results.\n        \"\"\"\n        instance_copy = deepcopy(instance)\n\n        try:\n            instance_copy.PixelData\n        except AttributeError:\n            raise AttributeError(\"Provided DICOM instance lacks pixel data.\")\n\n        is_greyscale = self._check_if_greyscale(instance_copy)\n        image = self._rescale_dcm_pixel_array(instance, is_greyscale)\n        if is_greyscale:\n            # model L for grayscale, and has 8 bit-pixel to store the pixel value\n            image_pil = Image.fromarray(image, mode=\"L\")\n        else:\n            # model RGB, has 3x8 bit pixel available to store the value\n            image_pil = Image.fromarray(image, mode=\"RGB\")\n        image = self._add_padding(image_pil, is_greyscale, padding_width)\n\n        # Get OCR results\n        perform_ocr_kwargs, ocr_threshold = (\n            self.image_analyzer_engine._parse_ocr_kwargs(ocr_kwargs)\n        )  # noqa: E501\n        ocr_results = self.ocr_engine.perform_ocr(image, **perform_ocr_kwargs)\n        if ocr_threshold:\n            ocr_results = self.image_analyzer_engine.threshold_ocr_result(\n                ocr_results, ocr_threshold\n            )\n        ocr_bboxes = self.bbox_processor.get_bboxes_from_ocr_results(ocr_results)\n\n        # Get analyzer results\n        analyzer_results = self._get_analyzer_results(\n            image,\n            instance,\n            use_metadata,\n            ocr_kwargs,\n            ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n        analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n            analyzer_results\n        )\n\n        # Prepare for plotting\n        pii_bboxes = self.image_analyzer_engine.get_pii_bboxes(\n            ocr_bboxes, analyzer_bboxes\n        )\n        if is_greyscale:\n            use_greyscale_cmap = True\n        else:\n            use_greyscale_cmap = False\n\n        # Get image with verification boxes\n        verify_image = (\n            self.image_analyzer_engine.add_custom_bboxes(\n                image, pii_bboxes, show_text_annotation, use_greyscale_cmap\n            )\n            if display_image\n            else None\n        )\n\n        return verify_image, ocr_bboxes, analyzer_bboxes\n\n    def eval_dicom_instance(\n        self,\n        instance: pydicom.dataset.FileDataset,\n        ground_truth: dict,\n        padding_width: int = 25,\n        tolerance: int = 50,\n        display_image: bool = False,\n        use_metadata: bool = True,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; Tuple[Optional[PIL.Image.Image], dict]:\n        \"\"\"Evaluate performance for a single DICOM instance.\n\n        :param instance: Loaded DICOM instance including pixel data and metadata.\n        :param ground_truth: Dictionary containing ground truth labels for the instance.\n        :param padding_width: Padding width to use when running OCR.\n        :param tolerance: Pixel distance tolerance for matching to ground truth.\n        :param display_image: If the verificationimage is displayed and returned.\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in ImageAnalyzerEngine.\n\n        :return: Evaluation comparing redactor engine results vs ground truth.\n        \"\"\"\n        # Verify detected PHI\n        verify_image, ocr_results, analyzer_results = self.verify_dicom_instance(\n            instance,\n            padding_width,\n            display_image,\n            use_metadata,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n        formatted_ocr_results = self.bbox_processor.get_bboxes_from_ocr_results(\n            ocr_results\n        )\n        detected_phi = self.bbox_processor.get_bboxes_from_analyzer_results(\n            analyzer_results\n        )\n\n        # Remove duplicate entities in results\n        detected_phi = self._remove_duplicate_entities(detected_phi)\n\n        # Get correct PHI text (all TP and FP)\n        all_pos = self._label_all_positives(\n            ground_truth, formatted_ocr_results, detected_phi, tolerance\n        )\n\n        # Calculate evaluation metrics\n        precision = self.calculate_precision(ground_truth, all_pos)\n        recall = self.calculate_recall(ground_truth, all_pos)\n\n        eval_results = {\n            \"all_positives\": all_pos,\n            \"ground_truth\": ground_truth,\n            \"precision\": precision,\n            \"recall\": recall,\n        }\n\n        return verify_image, eval_results\n\n    @staticmethod\n    def _remove_duplicate_entities(\n        results: List[dict], dup_pix_tolerance: int = 5\n    ) -&gt; List[dict]:\n        \"\"\"Handle when a word is detected multiple times as different types of entities.\n\n        :param results: List of detected PHI with bbox info.\n        :param dup_pix_tolerance: Pixel difference tolerance for identifying duplicates.\n        :return: Detected PHI with no duplicate entities.\n        \"\"\"\n        dups = []\n        sorted(results, key=lambda x: x[\"score\"], reverse=True)\n        results_no_dups = []\n        dims = [\"left\", \"top\", \"width\", \"height\"]\n\n        # Check for duplicates\n        for i in range(len(results) - 1):\n            i_dims = {dim: results[i][dim] for dim in dims}\n\n            # Ignore if we've already detected this dup combination\n            for other in range(i + 1, len(results)):\n                if i not in results_no_dups:\n                    other_dims = {dim: results[other][dim] for dim in dims}\n                    matching_dims = {\n                        dim: abs(i_dims[dim] - other_dims[dim]) &lt;= dup_pix_tolerance\n                        for dim in dims\n                    }\n                    matching = list(matching_dims.values())\n\n                    if all(matching):\n                        lower_scored_index = (\n                            other\n                            if results[other][\"score\"] &lt; results[i][\"score\"]\n                            else i\n                        )\n                        dups.append(lower_scored_index)\n\n        # Remove duplicates\n        for i in range(len(results)):\n            if i not in dups:\n                results_no_dups.append(results[i])\n\n        return results_no_dups\n\n    def _label_all_positives(\n        self,\n        gt_labels_dict: dict,\n        ocr_results: List[dict],\n        detected_phi: List[dict],\n        tolerance: int = 50,\n    ) -&gt; List[dict]:\n        \"\"\"Label all entities detected as PHI by using ground truth and OCR results.\n\n        All positives (detected_phi) do not contain PHI labels and are thus\n        difficult to work with intuitively. This method maps back to the\n        actual PHI to each detected sensitive entity.\n\n        :param gt_labels_dict: Dictionary with ground truth labels for a\n        single DICOM instance.\n        :param ocr_results: All detected text.\n        :param detected_phi: Formatted analyzer_results.\n        :param tolerance: Tolerance for exact coordinates and size data.\n        :return: List of all positives, labeled.\n        \"\"\"\n        all_pos = []\n\n        # Cycle through each positive (TP or FP)\n        for analyzer_result in detected_phi:\n            # See if there are any ground truth matches\n            all_pos, gt_match_found = self.bbox_processor.match_with_source(\n                all_pos, gt_labels_dict, analyzer_result, tolerance\n            )\n\n            # If not, check back with OCR\n            if not gt_match_found:\n                all_pos, _ = self.bbox_processor.match_with_source(\n                    all_pos, ocr_results, analyzer_result, tolerance\n                )\n\n        # Remove any duplicates\n        all_pos = self._remove_duplicate_entities(all_pos)\n\n        return all_pos\n\n    @staticmethod\n    def calculate_precision(gt: List[dict], all_pos: List[dict]) -&gt; float:\n        \"\"\"Calculate precision.\n\n        :param gt: List of ground truth labels.\n        :param all_pos: All Detected PHI (mapped back to have actual PHI text).\n        :return: Precision value.\n        \"\"\"\n        # Find True Positive (TP) and precision\n        tp = [i for i in all_pos if i in gt]\n        try:\n            precision = len(tp) / len(all_pos)\n        except ZeroDivisionError:\n            precision = 0\n\n        return precision\n\n    @staticmethod\n    def calculate_recall(gt: List[dict], all_pos: List[dict]) -&gt; float:\n        \"\"\"Calculate recall.\n\n        :param gt: List of ground truth labels.\n        :param all_pos: All Detected PHI (mapped back to have actual PHI text).\n        :return: Recall value.\n        \"\"\"\n        # Find True Positive (TP) and precision\n        tp = [i for i in all_pos if i in gt]\n        try:\n            recall = len(tp) / len(gt)\n        except ZeroDivisionError:\n            recall = 0\n\n        return recall\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.redact","title":"redact","text":"<pre><code>redact(\n    image: FileDataset,\n    fill: str = \"contrast\",\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; pydicom.dataset.FileDataset\n</code></pre> <p>Redact method to redact the given DICOM image.</p> <p>Please note, this method duplicates the image, creates a new instance and manipulates it.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded DICOM instance including pixel data and metadata.</p> <p> TYPE: <code>FileDataset</code> </p> <code>fill</code> <p>Fill setting to use for redaction box (\"contrast\" or \"background\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'contrast'</code> </p> <code>padding_width</code> <p>Padding width to use when running OCR.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>crop_ratio</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileDataset</code> <p>DICOM instance with redacted pixel data.</p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact(\n    self,\n    image: pydicom.dataset.FileDataset,\n    fill: str = \"contrast\",\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; pydicom.dataset.FileDataset:\n    \"\"\"Redact method to redact the given DICOM image.\n\n    Please note, this method duplicates the image, creates a\n    new instance and manipulates it.\n\n    :param image: Loaded DICOM instance including pixel data and metadata.\n    :param fill: Fill setting to use for redaction box (\"contrast\" or \"background\").\n    :param padding_width: Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: DICOM instance with redacted pixel data.\n    \"\"\"\n    redacted_image, _ = self.redact_and_return_bbox(\n        image=image,\n        fill=fill,\n        padding_width=padding_width,\n        crop_ratio=crop_ratio,\n        ocr_kwargs=ocr_kwargs,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n\n    return redacted_image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.redact_and_return_bbox","title":"redact_and_return_bbox","text":"<pre><code>redact_and_return_bbox(\n    image: FileDataset,\n    fill: str = \"contrast\",\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; Tuple[pydicom.dataset.FileDataset, List[Dict[str, int]]]\n</code></pre> <p>Redact method to redact the given DICOM image and return redacted bboxes.</p> <p>Please note, this method duplicates the image, creates a new instance and manipulates it.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded DICOM instance including pixel data and metadata.</p> <p> TYPE: <code>FileDataset</code> </p> <code>fill</code> <p>Fill setting to use for redaction box (\"contrast\" or \"background\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'contrast'</code> </p> <code>padding_width</code> <p>Padding width to use when running OCR.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>crop_ratio</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>use_metadata</code> <p>Whether to redact text in the image that are present in the metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Tuple[FileDataset, List[Dict[str, int]]]</code> <p>DICOM instance with redacted pixel data.</p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact_and_return_bbox(\n    self,\n    image: pydicom.dataset.FileDataset,\n    fill: str = \"contrast\",\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Tuple[pydicom.dataset.FileDataset, List[Dict[str, int]]]:\n    \"\"\"Redact method to redact the given DICOM image and return redacted bboxes.\n\n    Please note, this method duplicates the image, creates a\n    new instance and manipulates it.\n\n    :param image: Loaded DICOM instance including pixel data and metadata.\n    :param fill: Fill setting to use for redaction box (\"contrast\" or \"background\").\n    :param padding_width: Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: DICOM instance with redacted pixel data.\n    \"\"\"\n    # Check input\n    if type(image) not in [pydicom.dataset.FileDataset, pydicom.dataset.Dataset]:\n        raise TypeError(\"The provided image must be a loaded DICOM instance.\")\n    try:\n        image.PixelData\n    except AttributeError as e:\n        raise AttributeError(f\"Provided DICOM instance lacks pixel data: {e}\")\n    except PermissionError as e:\n        raise PermissionError(f\"Unable to access pixel data (may not exist): {e}\")\n    except IsADirectoryError as e:\n        raise IsADirectoryError(f\"DICOM instance is a directory: {e}\")\n\n    instance = deepcopy(image)\n\n    is_greyscale = self._check_if_greyscale(instance)\n    image_np = self._rescale_dcm_pixel_array(instance, is_greyscale)\n    if is_greyscale:\n        # model L for grayscale, and has 8 bit-pixel to store the pixel value\n        image_pil = Image.fromarray(image_np, mode=\"L\")\n    else:\n        # model RGB, has 3x8 bit pixel available to store the value\n        image_pil = Image.fromarray(image_np, mode=\"RGB\")\n    padded_image_pil = self._add_padding(image_pil, is_greyscale, padding_width)\n\n\n    # Detect PII\n    analyzer_results = self._get_analyzer_results(\n        padded_image_pil,\n        instance,\n        use_metadata,\n        ocr_kwargs,\n        ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n\n    # Redact all bounding boxes from DICOM file\n    analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n        analyzer_results\n    )\n    bboxes = self.bbox_processor.remove_bbox_padding(analyzer_bboxes, padding_width)\n    redacted_image = self._add_redact_box(instance, bboxes, crop_ratio, fill)\n\n    return redacted_image, bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.redact_from_file","title":"redact_from_file","text":"<pre><code>redact_from_file(\n    input_dicom_path: str,\n    output_dir: str,\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    fill: str = \"contrast\",\n    use_metadata: bool = True,\n    save_bboxes: bool = False,\n    verbose: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; None\n</code></pre> <p>Redact method to redact from a given file.</p> PARAMETER DESCRIPTION <code>input_dicom_path</code> <p>String path to DICOM image.</p> <p> TYPE: <code>str</code> </p> <code>output_dir</code> <p>String path to parent output directory.</p> <p> TYPE: <code>str</code> </p> <code>padding_width</code> <p>Padding width to use when running OCR.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>crop_ratio</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>fill</code> <p>Color setting to use for redaction box (\"contrast\" or \"background\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'contrast'</code> </p> <code>use_metadata</code> <p>Whether to redact text in the image that are present in the metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>save_bboxes</code> <p>True if we want to save boundings boxes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>verbose</code> <p>True to print where redacted file was written to.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.  Please notice, this method duplicates the file, creates new instance and manipulate them.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact_from_file(\n    self,\n    input_dicom_path: str,\n    output_dir: str,\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    fill: str = \"contrast\",\n    use_metadata: bool = True,\n    save_bboxes: bool = False,\n    verbose: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; None:\n    \"\"\"Redact method to redact from a given file.\n\n    :param input_dicom_path: String path to DICOM image.\n    :param output_dir: String path to parent output directory.\n    :param padding_width: Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param fill: Color setting to use for redaction box\n    (\"contrast\" or \"background\").\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param save_bboxes: True if we want to save boundings boxes.\n    :param verbose: True to print where redacted file was written to.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    Please notice, this method duplicates the file, creates\n    new instance and manipulate them.\n\n    \"\"\"\n    # Verify the given paths\n    if Path(input_dicom_path).is_dir() is True:\n        raise TypeError(\"input_dicom_path must be file (not dir)\")\n    if Path(input_dicom_path).is_file() is False:\n        raise TypeError(\"input_dicom_path must be a valid file\")\n    if Path(output_dir).is_file() is True:\n        raise TypeError(\n            \"output_dir must be a directory (does not need to exist yet)\"\n        )\n\n    # Create duplicate\n    dst_path = self._copy_files_for_processing(input_dicom_path, output_dir)\n\n    # Process DICOM file\n    output_location = self._redact_single_dicom_image(\n        dcm_path=dst_path,\n        crop_ratio=crop_ratio,\n        fill=fill,\n        padding_width=padding_width,\n        use_metadata=use_metadata,\n        overwrite=True,\n        dst_parent_dir=\".\",\n        save_bboxes=save_bboxes,\n        ocr_kwargs=ocr_kwargs,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n\n    if verbose:\n        print(f\"Output written to {output_location}\")\n\n    return None\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.redact_from_directory","title":"redact_from_directory","text":"<pre><code>redact_from_directory(\n    input_dicom_path: str,\n    output_dir: str,\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    fill: str = \"contrast\",\n    use_metadata: bool = True,\n    save_bboxes: bool = False,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; None\n</code></pre> <p>Redact method to redact from a directory of files.</p> PARAMETER DESCRIPTION <code>input_dicom_path</code> <p>String path to directory of DICOM images.</p> <p> TYPE: <code>str</code> </p> <code>output_dir</code> <p>String path to parent output directory.</p> <p> TYPE: <code>str</code> </p> <code>padding_width</code> <p>Padding width to use when running OCR.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>crop_ratio</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>fill</code> <p>Color setting to use for redaction box (\"contrast\" or \"background\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'contrast'</code> </p> <code>use_metadata</code> <p>Whether to redact text in the image that are present in the metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>save_bboxes</code> <p>True if we want to save bounding boxes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.  Please notice, this method duplicates the files, creates new instances and manipulate them.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact_from_directory(\n    self,\n    input_dicom_path: str,\n    output_dir: str,\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    fill: str = \"contrast\",\n    use_metadata: bool = True,\n    save_bboxes: bool = False,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; None:\n    \"\"\"Redact method to redact from a directory of files.\n\n    :param input_dicom_path: String path to directory of DICOM images.\n    :param output_dir: String path to parent output directory.\n    :param padding_width: Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param fill: Color setting to use for redaction box\n    (\"contrast\" or \"background\").\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param save_bboxes: True if we want to save bounding boxes.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    Please notice, this method duplicates the files, creates\n    new instances and manipulate them.\n\n    \"\"\"\n    # Verify the given paths\n    if Path(input_dicom_path).is_dir() is False:\n        raise TypeError(\"input_dicom_path must be a valid directory\")\n    if Path(input_dicom_path).is_file() is True:\n        raise TypeError(\"input_dicom_path must be a directory (not file)\")\n    if Path(output_dir).is_file() is True:\n        raise TypeError(\n            \"output_dir must be a directory (does not need to exist yet)\"\n        )\n\n    # Create duplicates\n    dst_path = self._copy_files_for_processing(input_dicom_path, output_dir)\n\n    # Process DICOM files\n    output_location = self._redact_multiple_dicom_images(\n        dcm_dir=dst_path,\n        crop_ratio=crop_ratio,\n        fill=fill,\n        padding_width=padding_width,\n        use_metadata=use_metadata,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        overwrite=True,\n        dst_parent_dir=\".\",\n        save_bboxes=save_bboxes,\n        ocr_kwargs=ocr_kwargs,\n        **text_analyzer_kwargs,\n    )\n\n    print(f\"Output written to {output_location}\")\n\n    return None\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.augment_word","title":"augment_word  <code>staticmethod</code>","text":"<pre><code>augment_word(word: str, case_sensitive: bool = False) -&gt; list\n</code></pre> <p>Apply multiple types of casing to the provided string.</p> PARAMETER DESCRIPTION <code>word</code> <p>String containing the word or term of interest.</p> <p> TYPE: <code>str</code> </p> <code>case_sensitive</code> <p>True if we want to preserve casing.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list</code> <p>List of the same string with different casings and spacing.</p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>@staticmethod\ndef augment_word(word: str, case_sensitive: bool = False) -&gt; list:\n    \"\"\"Apply multiple types of casing to the provided string.\n\n    :param word: String containing the word or term of interest.\n    :param case_sensitive: True if we want to preserve casing.\n\n    :return: List of the same string with different casings and spacing.\n    \"\"\"\n    word_list = []\n    if word != \"\":\n        # Replacing separator character with space, if any\n        text_no_separator = word.replace(\"^\", \" \")\n        text_no_separator = text_no_separator.replace(\"-\", \" \")\n        text_no_separator = \" \".join(text_no_separator.split())\n\n        if case_sensitive:\n            word_list.append(text_no_separator)\n            word_list.extend(\n                [\n                    text_no_separator.split(\" \"),\n                ]\n            )\n        else:\n            # Capitalize all characters in string\n            text_upper = text_no_separator.upper()\n\n            # Lowercase all characters in string\n            text_lower = text_no_separator.lower()\n\n            # Capitalize first letter in each part of string\n            text_title = text_no_separator.title()\n\n            # Append iterations\n            word_list.extend(\n                [text_no_separator, text_upper, text_lower, text_title]\n            )\n\n            # Adding each term as a separate item in the list\n            word_list.extend(\n                [\n                    text_no_separator.split(\" \"),\n                    text_upper.split(\" \"),\n                    text_lower.split(\" \"),\n                    text_title.split(\" \"),\n                ]\n            )\n\n        # Flatten list\n        flat_list = []\n        for item in word_list:\n            if isinstance(item, list):\n                flat_list.extend(item)\n            else:\n                flat_list.append(item)\n\n        # Remove any duplicates and empty strings\n        word_list = list(set(flat_list))\n        word_list = list(filter(None, word_list))\n\n    return word_list\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.verify","title":"verify","text":"<pre><code>verify(\n    image: Image,\n    is_greyscale: bool = False,\n    display_image: bool = True,\n    show_text_annotation: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; Image\n</code></pre> <p>Annotate image with the detect PII entity.</p> <p>Please notice, this method duplicates the image, creates a new instance and manipulate it.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to be processed.</p> <p> TYPE: <code>Image</code> </p> <code>is_greyscale</code> <p>Whether the image is greyscale or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>display_image</code> <p>If the verificationimage is displayed and returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_text_annotation</code> <p>True to display entity type when displaying image with bounding boxes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in ImageAnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>the annotated image</p> Source code in <code>presidio_image_redactor/image_pii_verify_engine.py</code> <pre><code>def verify(\n    self,\n    image: Image,\n    is_greyscale: bool = False,\n    display_image: bool = True,\n    show_text_annotation: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Image:\n    \"\"\"Annotate image with the detect PII entity.\n\n    Please notice, this method duplicates the image, creates a\n    new instance and manipulate it.\n\n    :param image: PIL Image to be processed.\n    :param is_greyscale: Whether the image is greyscale or not.\n    :param display_image: If the verificationimage is displayed and returned.\n    :param show_text_annotation: True to display entity type when displaying\n    image with bounding boxes.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in ImageAnalyzerEngine.\n\n    :return: the annotated image\n    \"\"\"\n    image = ImageChops.duplicate(image)\n\n    # Check the ad-hoc recognizers list\n    self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n    # Detect text\n    perform_ocr_kwargs, ocr_threshold = (\n        self.image_analyzer_engine._parse_ocr_kwargs(ocr_kwargs)\n    )  # noqa: E501\n    ocr_results = self.image_analyzer_engine.ocr.perform_ocr(\n        image, **perform_ocr_kwargs\n    )\n    if ocr_threshold:\n        ocr_results = self.image_analyzer_engine.threshold_ocr_result(\n            ocr_results, ocr_threshold\n        )\n    ocr_bboxes = self.bbox_processor.get_bboxes_from_ocr_results(ocr_results)\n\n    # Detect PII\n    if ad_hoc_recognizers is None:\n        analyzer_results = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            **text_analyzer_kwargs,\n        )\n    else:\n        analyzer_results = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n    analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n        analyzer_results\n    )\n\n    # Prepare for plotting\n    pii_bboxes = self.image_analyzer_engine.get_pii_bboxes(\n        ocr_bboxes, analyzer_bboxes\n    )\n    if is_greyscale:\n        use_greyscale_cmap = True\n    else:\n        use_greyscale_cmap = False\n\n    # Get image with verification boxes\n    verify_image = (\n        self.image_analyzer_engine.add_custom_bboxes(\n            image, pii_bboxes, show_text_annotation, use_greyscale_cmap\n        )\n        if display_image\n        else None\n    )\n\n    return verify_image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.verify_dicom_instance","title":"verify_dicom_instance","text":"<pre><code>verify_dicom_instance(\n    instance: FileDataset,\n    padding_width: int = 25,\n    display_image: bool = True,\n    show_text_annotation: bool = True,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; Tuple[Optional[PIL.Image.Image], list, list]\n</code></pre> <p>Verify PII on a single DICOM instance.</p> PARAMETER DESCRIPTION <code>instance</code> <p>Loaded DICOM instance including pixel data and metadata.</p> <p> TYPE: <code>FileDataset</code> </p> <code>padding_width</code> <p>Padding width to use when running OCR.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>display_image</code> <p>If the verificationimage is displayed and returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_text_annotation</code> <p>True to display entity type when displaying image with bounding boxes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_metadata</code> <p>Whether to redact text in the image that are present in the metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in ImageAnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Tuple[Optional[Image], list, list]</code> <p>Image with boxes identifying PHI, OCR results, and analyzer results.</p> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>def verify_dicom_instance(\n    self,\n    instance: pydicom.dataset.FileDataset,\n    padding_width: int = 25,\n    display_image: bool = True,\n    show_text_annotation: bool = True,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Tuple[Optional[PIL.Image.Image], list, list]:\n    \"\"\"Verify PII on a single DICOM instance.\n\n    :param instance: Loaded DICOM instance including pixel data and metadata.\n    :param padding_width: Padding width to use when running OCR.\n    :param display_image: If the verificationimage is displayed and returned.\n    :param show_text_annotation: True to display entity type when displaying\n    image with bounding boxes.\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in ImageAnalyzerEngine.\n\n    :return: Image with boxes identifying PHI, OCR results,\n    and analyzer results.\n    \"\"\"\n    instance_copy = deepcopy(instance)\n\n    try:\n        instance_copy.PixelData\n    except AttributeError:\n        raise AttributeError(\"Provided DICOM instance lacks pixel data.\")\n\n    is_greyscale = self._check_if_greyscale(instance_copy)\n    image = self._rescale_dcm_pixel_array(instance, is_greyscale)\n    if is_greyscale:\n        # model L for grayscale, and has 8 bit-pixel to store the pixel value\n        image_pil = Image.fromarray(image, mode=\"L\")\n    else:\n        # model RGB, has 3x8 bit pixel available to store the value\n        image_pil = Image.fromarray(image, mode=\"RGB\")\n    image = self._add_padding(image_pil, is_greyscale, padding_width)\n\n    # Get OCR results\n    perform_ocr_kwargs, ocr_threshold = (\n        self.image_analyzer_engine._parse_ocr_kwargs(ocr_kwargs)\n    )  # noqa: E501\n    ocr_results = self.ocr_engine.perform_ocr(image, **perform_ocr_kwargs)\n    if ocr_threshold:\n        ocr_results = self.image_analyzer_engine.threshold_ocr_result(\n            ocr_results, ocr_threshold\n        )\n    ocr_bboxes = self.bbox_processor.get_bboxes_from_ocr_results(ocr_results)\n\n    # Get analyzer results\n    analyzer_results = self._get_analyzer_results(\n        image,\n        instance,\n        use_metadata,\n        ocr_kwargs,\n        ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n    analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n        analyzer_results\n    )\n\n    # Prepare for plotting\n    pii_bboxes = self.image_analyzer_engine.get_pii_bboxes(\n        ocr_bboxes, analyzer_bboxes\n    )\n    if is_greyscale:\n        use_greyscale_cmap = True\n    else:\n        use_greyscale_cmap = False\n\n    # Get image with verification boxes\n    verify_image = (\n        self.image_analyzer_engine.add_custom_bboxes(\n            image, pii_bboxes, show_text_annotation, use_greyscale_cmap\n        )\n        if display_image\n        else None\n    )\n\n    return verify_image, ocr_bboxes, analyzer_bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.eval_dicom_instance","title":"eval_dicom_instance","text":"<pre><code>eval_dicom_instance(\n    instance: FileDataset,\n    ground_truth: dict,\n    padding_width: int = 25,\n    tolerance: int = 50,\n    display_image: bool = False,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; Tuple[Optional[PIL.Image.Image], dict]\n</code></pre> <p>Evaluate performance for a single DICOM instance.</p> PARAMETER DESCRIPTION <code>instance</code> <p>Loaded DICOM instance including pixel data and metadata.</p> <p> TYPE: <code>FileDataset</code> </p> <code>ground_truth</code> <p>Dictionary containing ground truth labels for the instance.</p> <p> TYPE: <code>dict</code> </p> <code>padding_width</code> <p>Padding width to use when running OCR.</p> <p> TYPE: <code>int</code> DEFAULT: <code>25</code> </p> <code>tolerance</code> <p>Pixel distance tolerance for matching to ground truth.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> <code>display_image</code> <p>If the verificationimage is displayed and returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_metadata</code> <p>Whether to redact text in the image that are present in the metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ocr_kwargs</code> <p>Additional params for OCR methods.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>ad_hoc_recognizers</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <p> TYPE: <code>Optional[List[PatternRecognizer]]</code> DEFAULT: <code>None</code> </p> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in ImageAnalyzerEngine.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Tuple[Optional[Image], dict]</code> <p>Evaluation comparing redactor engine results vs ground truth.</p> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>def eval_dicom_instance(\n    self,\n    instance: pydicom.dataset.FileDataset,\n    ground_truth: dict,\n    padding_width: int = 25,\n    tolerance: int = 50,\n    display_image: bool = False,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Tuple[Optional[PIL.Image.Image], dict]:\n    \"\"\"Evaluate performance for a single DICOM instance.\n\n    :param instance: Loaded DICOM instance including pixel data and metadata.\n    :param ground_truth: Dictionary containing ground truth labels for the instance.\n    :param padding_width: Padding width to use when running OCR.\n    :param tolerance: Pixel distance tolerance for matching to ground truth.\n    :param display_image: If the verificationimage is displayed and returned.\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in ImageAnalyzerEngine.\n\n    :return: Evaluation comparing redactor engine results vs ground truth.\n    \"\"\"\n    # Verify detected PHI\n    verify_image, ocr_results, analyzer_results = self.verify_dicom_instance(\n        instance,\n        padding_width,\n        display_image,\n        use_metadata,\n        ocr_kwargs=ocr_kwargs,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n    formatted_ocr_results = self.bbox_processor.get_bboxes_from_ocr_results(\n        ocr_results\n    )\n    detected_phi = self.bbox_processor.get_bboxes_from_analyzer_results(\n        analyzer_results\n    )\n\n    # Remove duplicate entities in results\n    detected_phi = self._remove_duplicate_entities(detected_phi)\n\n    # Get correct PHI text (all TP and FP)\n    all_pos = self._label_all_positives(\n        ground_truth, formatted_ocr_results, detected_phi, tolerance\n    )\n\n    # Calculate evaluation metrics\n    precision = self.calculate_precision(ground_truth, all_pos)\n    recall = self.calculate_recall(ground_truth, all_pos)\n\n    eval_results = {\n        \"all_positives\": all_pos,\n        \"ground_truth\": ground_truth,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n\n    return verify_image, eval_results\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.calculate_precision","title":"calculate_precision  <code>staticmethod</code>","text":"<pre><code>calculate_precision(gt: List[dict], all_pos: List[dict]) -&gt; float\n</code></pre> <p>Calculate precision.</p> PARAMETER DESCRIPTION <code>gt</code> <p>List of ground truth labels.</p> <p> TYPE: <code>List[dict]</code> </p> <code>all_pos</code> <p>All Detected PHI (mapped back to have actual PHI text).</p> <p> TYPE: <code>List[dict]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Precision value.</p> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>@staticmethod\ndef calculate_precision(gt: List[dict], all_pos: List[dict]) -&gt; float:\n    \"\"\"Calculate precision.\n\n    :param gt: List of ground truth labels.\n    :param all_pos: All Detected PHI (mapped back to have actual PHI text).\n    :return: Precision value.\n    \"\"\"\n    # Find True Positive (TP) and precision\n    tp = [i for i in all_pos if i in gt]\n    try:\n        precision = len(tp) / len(all_pos)\n    except ZeroDivisionError:\n        precision = 0\n\n    return precision\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.calculate_recall","title":"calculate_recall  <code>staticmethod</code>","text":"<pre><code>calculate_recall(gt: List[dict], all_pos: List[dict]) -&gt; float\n</code></pre> <p>Calculate recall.</p> PARAMETER DESCRIPTION <code>gt</code> <p>List of ground truth labels.</p> <p> TYPE: <code>List[dict]</code> </p> <code>all_pos</code> <p>All Detected PHI (mapped back to have actual PHI text).</p> <p> TYPE: <code>List[dict]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Recall value.</p> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>@staticmethod\ndef calculate_recall(gt: List[dict], all_pos: List[dict]) -&gt; float:\n    \"\"\"Calculate recall.\n\n    :param gt: List of ground truth labels.\n    :param all_pos: All Detected PHI (mapped back to have actual PHI text).\n    :return: Recall value.\n    \"\"\"\n    # Find True Positive (TP) and precision\n    tp = [i for i in all_pos if i in gt]\n    try:\n        recall = len(tp) / len(gt)\n    except ZeroDivisionError:\n        recall = 0\n\n    return recall\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ContrastSegmentedImageEnhancer","title":"ContrastSegmentedImageEnhancer","text":"<p>               Bases: <code>ImagePreprocessor</code></p> <p>Class containing all logic to perform contrastive segmentation.</p> <p>Contrastive segmentation is a preprocessing step that aims to enhance the text in an image by increasing the contrast between the text and the background. The parameters used to run the preprocessing are selected based on the contrast level of the image.</p> METHOD DESCRIPTION <code>convert_image_to_array</code> <p>Convert PIL image to numpy array.</p> <code>preprocess_image</code> <p>Preprocess the image to be analyzed.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>class ContrastSegmentedImageEnhancer(ImagePreprocessor):\n    \"\"\"Class containing all logic to perform contrastive segmentation.\n\n    Contrastive segmentation is a preprocessing step that aims to enhance the\n    text in an image by increasing the contrast between the text and the\n    background. The parameters used to run the preprocessing are selected based\n    on the contrast level of the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        bilateral_filter: Optional[BilateralFilter] = None,\n        adaptive_threshold: Optional[SegmentedAdaptiveThreshold] = None,\n        image_rescaling: Optional[ImageRescaling] = None,\n        low_contrast_threshold: int = 40,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\n\n        :param bilateral_filter: Optional BilateralFilter instance.\n        :param adaptive_threshold: Optional AdaptiveThreshold instance.\n        :param image_rescaling: Optional ImageRescaling instance.\n        :param low_contrast_threshold: Threshold for low contrast images.\n        \"\"\"\n\n        super().__init__(use_greyscale=True)\n        if not bilateral_filter:\n            self.bilateral_filter = BilateralFilter()\n        else:\n            self.bilateral_filter = bilateral_filter\n\n        if not adaptive_threshold:\n            self.adaptive_threshold = SegmentedAdaptiveThreshold()\n        else:\n            self.adaptive_threshold = adaptive_threshold\n\n        if not image_rescaling:\n            self.image_rescaling = ImageRescaling()\n        else:\n            self.image_rescaling = image_rescaling\n\n        self.low_contrast_threshold = low_contrast_threshold\n\n    def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n        \"\"\"Preprocess the image to be analyzed.\n\n        :param image: Loaded PIL image.\n\n        :return: The processed image and metadata (background color, scale percentage,\n             contrast level, and C value).\n        \"\"\"\n        image = self.convert_image_to_array(image)\n\n        # Apply bilateral filtering\n        filtered_image, _ = self.bilateral_filter.preprocess_image(image)\n\n        # Convert to grayscale\n        pil_filtered_image = Image.fromarray(np.uint8(filtered_image))\n        pil_grayscale_image = pil_filtered_image.convert(\"L\")\n        grayscale_image = np.asarray(pil_grayscale_image)\n\n        # Improve contrast\n        adjusted_image, _, adjusted_contrast = self._improve_contrast(grayscale_image)\n\n        # Adaptive Thresholding\n        adaptive_threshold_image, _ = self.adaptive_threshold.preprocess_image(\n            adjusted_image\n        )\n        # Increase contrast\n        _, threshold_image = cv2.threshold(\n            np.asarray(adaptive_threshold_image),\n            0,\n            255,\n            cv2.THRESH_BINARY | cv2.THRESH_OTSU,\n        )\n\n        # Rescale image\n        rescaled_image, scale_metadata = self.image_rescaling.preprocess_image(\n            threshold_image\n        )\n\n        return rescaled_image, scale_metadata\n\n    def _improve_contrast(self, image: np.ndarray) -&gt; Tuple[np.ndarray, str, str]:\n        \"\"\"Improve the contrast of an image based on its initial contrast level.\n\n        :param image: Input image.\n\n        :return: A tuple containing the improved image, the initial contrast level,\n             and the adjusted contrast level.\n        \"\"\"\n        contrast, mean_intensity = self._get_image_contrast(image)\n\n        if contrast &lt;= self.low_contrast_threshold:\n            alpha = 1.5\n            beta = -mean_intensity * alpha\n            adjusted_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n            adjusted_contrast, _ = self._get_image_contrast(adjusted_image)\n        else:\n            adjusted_image = image\n            adjusted_contrast = contrast\n        return adjusted_image, contrast, adjusted_contrast\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ContrastSegmentedImageEnhancer.convert_image_to_array","title":"convert_image_to_array","text":"<pre><code>convert_image_to_array(image: Image) -&gt; np.ndarray\n</code></pre> <p>Convert PIL image to numpy array.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded PIL image.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>image pixels as a numpy array.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def convert_image_to_array(self, image: Image.Image) -&gt; np.ndarray:\n    \"\"\"Convert PIL image to numpy array.\n\n    :param image: Loaded PIL image.\n\n    :return: image pixels as a numpy array.\n\n    \"\"\"\n\n    if isinstance(image, np.ndarray):\n        img = image\n    else:\n        if self.use_greyscale:\n            image = image.convert(\"L\")\n        img = np.asarray(image)\n    return img\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ContrastSegmentedImageEnhancer.preprocess_image","title":"preprocess_image","text":"<pre><code>preprocess_image(image: Image) -&gt; Tuple[Image.Image, dict]\n</code></pre> <p>Preprocess the image to be analyzed.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded PIL image.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>Tuple[Image, dict]</code> <p>The processed image and metadata (background color, scale percentage, contrast level, and C value).</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n    \"\"\"Preprocess the image to be analyzed.\n\n    :param image: Loaded PIL image.\n\n    :return: The processed image and metadata (background color, scale percentage,\n         contrast level, and C value).\n    \"\"\"\n    image = self.convert_image_to_array(image)\n\n    # Apply bilateral filtering\n    filtered_image, _ = self.bilateral_filter.preprocess_image(image)\n\n    # Convert to grayscale\n    pil_filtered_image = Image.fromarray(np.uint8(filtered_image))\n    pil_grayscale_image = pil_filtered_image.convert(\"L\")\n    grayscale_image = np.asarray(pil_grayscale_image)\n\n    # Improve contrast\n    adjusted_image, _, adjusted_contrast = self._improve_contrast(grayscale_image)\n\n    # Adaptive Thresholding\n    adaptive_threshold_image, _ = self.adaptive_threshold.preprocess_image(\n        adjusted_image\n    )\n    # Increase contrast\n    _, threshold_image = cv2.threshold(\n        np.asarray(adaptive_threshold_image),\n        0,\n        255,\n        cv2.THRESH_BINARY | cv2.THRESH_OTSU,\n    )\n\n    # Rescale image\n    rescaled_image, scale_metadata = self.image_rescaling.preprocess_image(\n        threshold_image\n    )\n\n    return rescaled_image, scale_metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BilateralFilter","title":"BilateralFilter","text":"<p>               Bases: <code>ImagePreprocessor</code></p> <p>BilateralFilter class.</p> <p>The class applies bilateral filtering to an image. and returns the filtered   image and metadata.</p> METHOD DESCRIPTION <code>convert_image_to_array</code> <p>Convert PIL image to numpy array.</p> <code>preprocess_image</code> <p>Preprocess the image to be analyzed.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>class BilateralFilter(ImagePreprocessor):\n    \"\"\"BilateralFilter class.\n\n    The class applies bilateral filtering to an image. and returns the filtered\n      image and metadata.\n    \"\"\"\n\n    def __init__(\n        self, diameter: int = 3, sigma_color: int = 40, sigma_space: int = 40\n    ) -&gt; None:\n        \"\"\"Initialize the BilateralFilter class.\n\n        :param diameter: Diameter of each pixel neighborhood.\n        :param sigma_color: value of sigma in the color space.\n        :param sigma_space: value of sigma in the coordinate space.\n        \"\"\"\n        super().__init__(use_greyscale=True)\n\n        self.diameter = diameter\n        self.sigma_color = sigma_color\n        self.sigma_space = sigma_space\n\n    def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n        \"\"\"Preprocess the image to be analyzed.\n\n        :param image: Loaded PIL image.\n\n        :return: The processed image and metadata (diameter, sigma_color, sigma_space).\n        \"\"\"\n        image = self.convert_image_to_array(image)\n\n        # Apply bilateral filtering\n        filtered_image = cv2.bilateralFilter(\n            image,\n            self.diameter,\n            self.sigma_color,\n            self.sigma_space,\n        )\n\n        metadata = {\n            \"diameter\": self.diameter,\n            \"sigma_color\": self.sigma_color,\n            \"sigma_space\": self.sigma_space,\n        }\n\n        return Image.fromarray(filtered_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BilateralFilter.convert_image_to_array","title":"convert_image_to_array","text":"<pre><code>convert_image_to_array(image: Image) -&gt; np.ndarray\n</code></pre> <p>Convert PIL image to numpy array.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded PIL image.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>image pixels as a numpy array.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def convert_image_to_array(self, image: Image.Image) -&gt; np.ndarray:\n    \"\"\"Convert PIL image to numpy array.\n\n    :param image: Loaded PIL image.\n\n    :return: image pixels as a numpy array.\n\n    \"\"\"\n\n    if isinstance(image, np.ndarray):\n        img = image\n    else:\n        if self.use_greyscale:\n            image = image.convert(\"L\")\n        img = np.asarray(image)\n    return img\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BilateralFilter.preprocess_image","title":"preprocess_image","text":"<pre><code>preprocess_image(image: Image) -&gt; Tuple[Image.Image, dict]\n</code></pre> <p>Preprocess the image to be analyzed.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded PIL image.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>Tuple[Image, dict]</code> <p>The processed image and metadata (diameter, sigma_color, sigma_space).</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n    \"\"\"Preprocess the image to be analyzed.\n\n    :param image: Loaded PIL image.\n\n    :return: The processed image and metadata (diameter, sigma_color, sigma_space).\n    \"\"\"\n    image = self.convert_image_to_array(image)\n\n    # Apply bilateral filtering\n    filtered_image = cv2.bilateralFilter(\n        image,\n        self.diameter,\n        self.sigma_color,\n        self.sigma_space,\n    )\n\n    metadata = {\n        \"diameter\": self.diameter,\n        \"sigma_color\": self.sigma_color,\n        \"sigma_space\": self.sigma_space,\n    }\n\n    return Image.fromarray(filtered_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.SegmentedAdaptiveThreshold","title":"SegmentedAdaptiveThreshold","text":"<p>               Bases: <code>ImagePreprocessor</code></p> <p>SegmentedAdaptiveThreshold class.</p> <p>The class applies adaptive thresholding to an image and returns the thresholded image and metadata. The parameters used to run the adaptivethresholding are selected based on the contrast level of the image.</p> METHOD DESCRIPTION <code>convert_image_to_array</code> <p>Convert PIL image to numpy array.</p> <code>preprocess_image</code> <p>Preprocess the image.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>class SegmentedAdaptiveThreshold(ImagePreprocessor):\n    \"\"\"SegmentedAdaptiveThreshold class.\n\n    The class applies adaptive thresholding to an image\n    and returns the thresholded image and metadata.\n    The parameters used to run the adaptivethresholding are selected based on\n    the contrast level of the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int = 5,\n        contrast_threshold: int = 40,\n        c_low_contrast: int = 10,\n        c_high_contrast: int = 40,\n        bg_threshold: int = 122,\n    ) -&gt; None:\n        \"\"\"Initialize the SegmentedAdaptiveThreshold class.\n\n        :param block_size: Size of the neighborhood area for threshold calculation.\n        :param contrast_threshold: Threshold for low contrast images.\n        :param c_low_contrast: Constant added to the mean for low contrast images.\n        :param c_high_contrast: Constant added to the mean for high contrast images.\n        :param bg_threshold: Threshold for background color.\n        \"\"\"\n\n        super().__init__(use_greyscale=True)\n        self.block_size = block_size\n        self.c_low_contrast = c_low_contrast\n        self.c_high_contrast = c_high_contrast\n        self.bg_threshold = bg_threshold\n        self.contrast_threshold = contrast_threshold\n\n    def preprocess_image(\n        self, image: Union[Image.Image, np.ndarray]\n    ) -&gt; Tuple[Image.Image, dict]:\n        \"\"\"Preprocess the image.\n\n        :param image: Loaded PIL image.\n\n        :return: The processed image and metadata (C, background_color, contrast).\n        \"\"\"\n        if not isinstance(image, np.ndarray):\n            image = self.convert_image_to_array(image)\n\n        # Determine background color\n        background_color = self._get_bg_color(image, True)\n        contrast, _ = self._get_image_contrast(image)\n\n        c = (\n            self.c_low_contrast\n            if contrast &lt;= self.contrast_threshold\n            else self.c_high_contrast\n        )\n\n        if background_color &lt; self.bg_threshold:\n            adaptive_threshold_image = cv2.adaptiveThreshold(\n                image,\n                255,\n                cv2.ADAPTIVE_THRESH_MEAN_C,\n                cv2.THRESH_BINARY_INV,\n                self.block_size,\n                -c,\n            )\n        else:\n            adaptive_threshold_image = cv2.adaptiveThreshold(\n                image,\n                255,\n                cv2.ADAPTIVE_THRESH_MEAN_C,\n                cv2.THRESH_BINARY,\n                self.block_size,\n                c,\n            )\n\n        metadata = {\"C\": c, \"background_color\": background_color, \"contrast\": contrast}\n        return Image.fromarray(adaptive_threshold_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.SegmentedAdaptiveThreshold.convert_image_to_array","title":"convert_image_to_array","text":"<pre><code>convert_image_to_array(image: Image) -&gt; np.ndarray\n</code></pre> <p>Convert PIL image to numpy array.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded PIL image.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>image pixels as a numpy array.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def convert_image_to_array(self, image: Image.Image) -&gt; np.ndarray:\n    \"\"\"Convert PIL image to numpy array.\n\n    :param image: Loaded PIL image.\n\n    :return: image pixels as a numpy array.\n\n    \"\"\"\n\n    if isinstance(image, np.ndarray):\n        img = image\n    else:\n        if self.use_greyscale:\n            image = image.convert(\"L\")\n        img = np.asarray(image)\n    return img\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.SegmentedAdaptiveThreshold.preprocess_image","title":"preprocess_image","text":"<pre><code>preprocess_image(image: Union[Image, ndarray]) -&gt; Tuple[Image.Image, dict]\n</code></pre> <p>Preprocess the image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded PIL image.</p> <p> TYPE: <code>Union[Image, ndarray]</code> </p> RETURNS DESCRIPTION <code>Tuple[Image, dict]</code> <p>The processed image and metadata (C, background_color, contrast).</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def preprocess_image(\n    self, image: Union[Image.Image, np.ndarray]\n) -&gt; Tuple[Image.Image, dict]:\n    \"\"\"Preprocess the image.\n\n    :param image: Loaded PIL image.\n\n    :return: The processed image and metadata (C, background_color, contrast).\n    \"\"\"\n    if not isinstance(image, np.ndarray):\n        image = self.convert_image_to_array(image)\n\n    # Determine background color\n    background_color = self._get_bg_color(image, True)\n    contrast, _ = self._get_image_contrast(image)\n\n    c = (\n        self.c_low_contrast\n        if contrast &lt;= self.contrast_threshold\n        else self.c_high_contrast\n    )\n\n    if background_color &lt; self.bg_threshold:\n        adaptive_threshold_image = cv2.adaptiveThreshold(\n            image,\n            255,\n            cv2.ADAPTIVE_THRESH_MEAN_C,\n            cv2.THRESH_BINARY_INV,\n            self.block_size,\n            -c,\n        )\n    else:\n        adaptive_threshold_image = cv2.adaptiveThreshold(\n            image,\n            255,\n            cv2.ADAPTIVE_THRESH_MEAN_C,\n            cv2.THRESH_BINARY,\n            self.block_size,\n            c,\n        )\n\n    metadata = {\"C\": c, \"background_color\": background_color, \"contrast\": contrast}\n    return Image.fromarray(adaptive_threshold_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageRescaling","title":"ImageRescaling","text":"<p>               Bases: <code>ImagePreprocessor</code></p> <p>ImageRescaling class. Rescales images based on their size.</p> METHOD DESCRIPTION <code>convert_image_to_array</code> <p>Convert PIL image to numpy array.</p> <code>preprocess_image</code> <p>Preprocess the image to be analyzed.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>class ImageRescaling(ImagePreprocessor):\n    \"\"\"ImageRescaling class. Rescales images based on their size.\"\"\"\n\n    def __init__(\n        self,\n        small_size: int = 1048576,\n        large_size: int = 4000000,\n        factor: int = 2,\n        interpolation: int = cv2.INTER_AREA,\n    ) -&gt; None:\n        \"\"\"Initialize the ImageRescaling class.\n\n        :param small_size: Threshold for small image size.\n        :param large_size: Threshold for large image size.\n        :param factor: Scaling factor for resizing.\n        :param interpolation: Interpolation method for resizing.\n        \"\"\"\n        super().__init__(use_greyscale=True)\n\n        self.small_size = small_size\n        self.large_size = large_size\n        self.factor = factor\n        self.interpolation = interpolation\n\n    def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n        \"\"\"Preprocess the image to be analyzed.\n\n        :param image: Loaded PIL image.\n\n        :return: The processed image and metadata (scale_factor).\n        \"\"\"\n\n        scale_factor = 1\n        if image.size &lt; self.small_size:\n            scale_factor = self.factor\n        elif image.size &gt; self.large_size:\n            scale_factor = 1 / self.factor\n\n        width = int(image.shape[1] * scale_factor)\n        height = int(image.shape[0] * scale_factor)\n        dimensions = (width, height)\n\n        # resize image\n        rescaled_image = cv2.resize(image, dimensions, interpolation=self.interpolation)\n        metadata = {\"scale_factor\": scale_factor}\n        return Image.fromarray(rescaled_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageRescaling.convert_image_to_array","title":"convert_image_to_array","text":"<pre><code>convert_image_to_array(image: Image) -&gt; np.ndarray\n</code></pre> <p>Convert PIL image to numpy array.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded PIL image.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>image pixels as a numpy array.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def convert_image_to_array(self, image: Image.Image) -&gt; np.ndarray:\n    \"\"\"Convert PIL image to numpy array.\n\n    :param image: Loaded PIL image.\n\n    :return: image pixels as a numpy array.\n\n    \"\"\"\n\n    if isinstance(image, np.ndarray):\n        img = image\n    else:\n        if self.use_greyscale:\n            image = image.convert(\"L\")\n        img = np.asarray(image)\n    return img\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageRescaling.preprocess_image","title":"preprocess_image","text":"<pre><code>preprocess_image(image: Image) -&gt; Tuple[Image.Image, dict]\n</code></pre> <p>Preprocess the image to be analyzed.</p> PARAMETER DESCRIPTION <code>image</code> <p>Loaded PIL image.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>Tuple[Image, dict]</code> <p>The processed image and metadata (scale_factor).</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n    \"\"\"Preprocess the image to be analyzed.\n\n    :param image: Loaded PIL image.\n\n    :return: The processed image and metadata (scale_factor).\n    \"\"\"\n\n    scale_factor = 1\n    if image.size &lt; self.small_size:\n        scale_factor = self.factor\n    elif image.size &gt; self.large_size:\n        scale_factor = 1 / self.factor\n\n    width = int(image.shape[1] * scale_factor)\n    height = int(image.shape[0] * scale_factor)\n    dimensions = (width, height)\n\n    # resize image\n    rescaled_image = cv2.resize(image, dimensions, interpolation=self.interpolation)\n    metadata = {\"scale_factor\": scale_factor}\n    return Image.fromarray(rescaled_image), metadata\n</code></pre>"},{"location":"api/structured_python/","title":"Presidio Structured API Reference","text":"<p>handler: python</p>"},{"location":"api/structured_python/#presidio_structured","title":"presidio_structured","text":"<p>presidio-structured root module.</p>"},{"location":"api/structured_python/#presidio_structured.JsonAnalysisBuilder","title":"JsonAnalysisBuilder","text":"<p>               Bases: <code>AnalysisBuilder</code></p> <p>Concrete configuration generator for JSON data.</p> METHOD DESCRIPTION <code>generate_analysis</code> <p>Generate a configuration from the given JSON data.</p> Source code in <code>presidio_structured/analysis_builder.py</code> <pre><code>class JsonAnalysisBuilder(AnalysisBuilder):\n    \"\"\"Concrete configuration generator for JSON data.\"\"\"\n\n    def generate_analysis(\n        self,\n        data: Dict,\n        language: str = \"en\",\n    ) -&gt; StructuredAnalysis:\n        \"\"\"\n        Generate a configuration from the given JSON data.\n\n        :param data: The input JSON data.\n        :return: The generated configuration.\n        \"\"\"\n        logger.debug(\"Starting JSON BatchAnalyzer analysis\")\n        analyzer_results = self.batch_analyzer.analyze_dict(\n            input_dict=data,\n            language=language,\n            n_process=self.n_process,\n            batch_size=self.batch_size\n        )\n\n        key_recognizer_result_map = self._generate_analysis_from_results_json(\n            analyzer_results\n        )\n\n        key_entity_map = {\n            key: result.entity_type for key, result in key_recognizer_result_map.items()\n        }\n\n        return StructuredAnalysis(entity_mapping=key_entity_map)\n\n    def _generate_analysis_from_results_json(\n        self, analyzer_results: Iterator[DictAnalyzerResult], prefix: str = \"\"\n    ) -&gt; Dict[str, RecognizerResult]:\n        \"\"\"\n        Generate a configuration from the given analyzer results. Always uses the first recognizer result if there are more than one.\n\n        :param analyzer_results: The analyzer results.\n        :param prefix: The prefix for the configuration keys.\n        :return: The generated configuration.\n        \"\"\"  # noqa: E501\n        key_recognizer_result_map = {}\n\n        if not isinstance(analyzer_results, Iterable):\n            logger.debug(\n                \"No analyzer results found, returning empty StructuredAnalysis\"\n            )\n            return key_recognizer_result_map\n\n        for result in analyzer_results:\n            current_key = prefix + result.key\n\n            if isinstance(result.value, dict) and isinstance(\n                result.recognizer_results, Iterator\n            ):\n                nested_mappings = self._generate_analysis_from_results_json(\n                    result.recognizer_results, prefix=current_key + \".\"\n                )\n                key_recognizer_result_map.update(nested_mappings)\n            first_recognizer_result = next(iter(result.recognizer_results), None)\n            if isinstance(first_recognizer_result, RecognizerResult):\n                logger.debug(\n                    f\"Found result with entity {first_recognizer_result.entity_type} \\\n                        in {current_key}\"\n                )\n                key_recognizer_result_map[current_key] = first_recognizer_result\n        return key_recognizer_result_map\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.JsonAnalysisBuilder.generate_analysis","title":"generate_analysis","text":"<pre><code>generate_analysis(data: Dict, language: str = 'en') -&gt; StructuredAnalysis\n</code></pre> <p>Generate a configuration from the given JSON data.</p> PARAMETER DESCRIPTION <code>data</code> <p>The input JSON data.</p> <p> TYPE: <code>Dict</code> </p> RETURNS DESCRIPTION <code>StructuredAnalysis</code> <p>The generated configuration.</p> Source code in <code>presidio_structured/analysis_builder.py</code> <pre><code>def generate_analysis(\n    self,\n    data: Dict,\n    language: str = \"en\",\n) -&gt; StructuredAnalysis:\n    \"\"\"\n    Generate a configuration from the given JSON data.\n\n    :param data: The input JSON data.\n    :return: The generated configuration.\n    \"\"\"\n    logger.debug(\"Starting JSON BatchAnalyzer analysis\")\n    analyzer_results = self.batch_analyzer.analyze_dict(\n        input_dict=data,\n        language=language,\n        n_process=self.n_process,\n        batch_size=self.batch_size\n    )\n\n    key_recognizer_result_map = self._generate_analysis_from_results_json(\n        analyzer_results\n    )\n\n    key_entity_map = {\n        key: result.entity_type for key, result in key_recognizer_result_map.items()\n    }\n\n    return StructuredAnalysis(entity_mapping=key_entity_map)\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.PandasAnalysisBuilder","title":"PandasAnalysisBuilder","text":"<p>               Bases: <code>TabularAnalysisBuilder</code></p> <p>Concrete configuration generator for tabular data.</p> METHOD DESCRIPTION <code>generate_analysis</code> <p>Generate a configuration from the given tabular data.</p> Source code in <code>presidio_structured/analysis_builder.py</code> <pre><code>class PandasAnalysisBuilder(TabularAnalysisBuilder):\n    \"\"\"Concrete configuration generator for tabular data.\"\"\"\n\n    entity_selection_strategies = {\"highest_confidence\", \"mixed\", \"most_common\"}\n\n    def generate_analysis(\n        self,\n        df: DataFrame,\n        n: Optional[int] = None,\n        language: str = \"en\",\n        selection_strategy: str = \"most_common\",\n        mixed_strategy_threshold: float = 0.5,\n    ) -&gt; StructuredAnalysis:\n        \"\"\"\n        Generate a configuration from the given tabular data.\n\n        :param df: The input tabular data (dataframe).\n        :param n: The number of samples to be taken from the dataframe.\n        :param language: The language to be used for analysis.\n        :param selection_strategy: A string that specifies the entity selection strategy\n        ('highest_confidence', 'mixed', or default to most common).\n        :param mixed_strategy_threshold: A float value for the threshold to be used in\n        the entity selection mixed strategy.\n        :return: A StructuredAnalysis object containing the analysis results.\n        \"\"\"\n        if not n:\n            n = len(df)\n        elif n &gt; len(df):\n            logger.debug(\n                f\"Number of samples ({n}) is larger than the number of rows \\\n                    ({len(df)}), using all rows\"\n            )\n            n = len(df)\n\n        df = df.sample(n, random_state=123)\n\n        key_recognizer_result_map = self._generate_key_rec_results_map(\n            df, language, selection_strategy, mixed_strategy_threshold\n        )\n\n        key_entity_map = {\n            key: result.entity_type\n            for key, result in key_recognizer_result_map.items()\n            if result.entity_type != NON_PII_ENTITY_TYPE\n        }\n\n        return StructuredAnalysis(entity_mapping=key_entity_map)\n\n    def _generate_key_rec_results_map(\n        self,\n        df: DataFrame,\n        language: str,\n        selection_strategy: str = \"most_common\",\n        mixed_strategy_threshold: float = 0.5,\n    ) -&gt; Dict[str, RecognizerResult]:\n        \"\"\"\n        Find the most common entity in a dataframe column.\n\n        If more than one entity is found in a cell, the first one is used.\n\n        :param df: The dataframe where entities will be searched.\n        :param language: Language to be used in the analysis engine.\n        :param selection_strategy: A string that specifies the entity selection strategy\n        ('highest_confidence', 'mixed', or default to most common).\n        :param mixed_strategy_threshold: A float value for the threshold to be used in\n        the entity selection mixed strategy.\n        :return: A dictionary mapping column names to the most common RecognizerResult.\n        \"\"\"\n        column_analyzer_results_map = self._batch_analyze_df(df, language)\n        key_recognizer_result_map = {}\n        for column, analyzer_result in column_analyzer_results_map.items():\n            key_recognizer_result_map[column] = self._find_entity_based_on_strategy(\n                analyzer_result, selection_strategy, mixed_strategy_threshold\n            )\n        return key_recognizer_result_map\n\n    def _batch_analyze_df(\n        self, df: DataFrame, language: str\n    ) -&gt; Dict[str, List[List[RecognizerResult]]]:\n        \"\"\"\n        Analyze each column in the dataframe for entities using the batch analyzer.\n\n        :param df: The dataframe to be analyzed.\n        :param language: The language configuration for the analyzer.\n        :return: A dictionary mapping each column name to a \\\n            list of lists of RecognizerResults.\n        \"\"\"\n        column_analyzer_results_map = {}\n        for column in df.columns:\n            logger.debug(f\"Finding most common PII entity for column {column}\")\n            analyzer_results = self.batch_analyzer.analyze_iterator(\n                [val for val in df[column]],\n                language=language,\n                n_process=self.n_process,\n                batch_size=self.batch_size\n            )\n            column_analyzer_results_map[column] = analyzer_results\n\n        return column_analyzer_results_map\n\n    def _find_entity_based_on_strategy(\n        self,\n        analyzer_results: List[List[RecognizerResult]],\n        selection_strategy: str,\n        mixed_strategy_threshold: float,\n    ) -&gt; RecognizerResult:\n        \"\"\"\n        Determine the most suitable entity based on the specified selection strategy.\n\n        :param analyzer_results: A nested list of RecognizerResult objects from the\n        analysis results.\n        :param selection_strategy: A string that specifies the entity selection strategy\n        ('highest_confidence', 'mixed', or default to most common).\n        :return: A RecognizerResult object representing the selected entity based on the\n        given strategy.\n        \"\"\"\n        if selection_strategy not in self.entity_selection_strategies:\n            raise ValueError(\n                f\"Unsupported entity selection strategy: {selection_strategy}.\"\n            )\n\n        if not any(analyzer_results):\n            return RecognizerResult(\n                entity_type=NON_PII_ENTITY_TYPE, start=0, end=1, score=1.0\n            )\n\n        flat_results = self._flatten_results(analyzer_results)\n\n        # Select the entity based on the desired strategy\n        if selection_strategy == \"highest_confidence\":\n            return self._select_highest_confidence_entity(flat_results)\n        elif selection_strategy == \"mixed\":\n            return self._select_mixed_strategy_entity(\n                flat_results, mixed_strategy_threshold\n            )\n\n        return self._select_most_common_entity(flat_results)\n\n    def _select_most_common_entity(self, flat_results):\n        \"\"\"\n        Select the most common entity from the flattened analysis results.\n\n        :param flat_results: A list of tuples containing index and RecognizerResult\n        objects from the flattened analysis results.\n        :return: A RecognizerResult object for the most commonly found entity type.\n        \"\"\"\n        # Count occurrences of each entity type\n        type_counter = Counter(res.entity_type for _, res in flat_results)\n        most_common_type, most_common_count = type_counter.most_common(1)[0]\n\n        # Calculate the score as the proportion of occurrences\n        score = most_common_count / len(flat_results)\n\n        return RecognizerResult(\n            entity_type=most_common_type, start=0, end=1, score=score\n        )\n\n    def _select_highest_confidence_entity(self, flat_results):\n        \"\"\"\n        Select the entity with the highest confidence score.\n\n        :param flat_results: A list of tuples containing index and RecognizerResult\n        objects from the flattened analysis results.\n        :return: A RecognizerResult object for the entity with the highest confidence\n        score.\n        \"\"\"\n        score_aggregator = self._aggregate_scores(flat_results)\n\n        # Find the highest score across all entities\n        highest_score = max(\n            max(scores) for scores in score_aggregator.values() if scores\n        )\n\n        # Find the entities with the highest score and count their occurrences\n        entities_highest_score = {\n            entity: scores.count(highest_score)\n            for entity, scores in score_aggregator.items()\n            if highest_score in scores\n        }\n\n        # Find the entity(ies) with the most number of high scores\n        max_occurrences = max(entities_highest_score.values())\n        highest_confidence_entities = [\n            entity\n            for entity, count in entities_highest_score.items()\n            if count == max_occurrences\n        ]\n\n        return RecognizerResult(\n            entity_type=highest_confidence_entities[0],\n            start=0,\n            end=1,\n            score=highest_score,\n        )\n\n    def _select_mixed_strategy_entity(self, flat_results, mixed_strategy_threshold):\n        \"\"\"\n        Select an entity using a mixed strategy.\n\n        Chooses an entity based on the highest confidence score if it is above the\n        threshold. Otherwise, it defaults to the most common entity.\n\n        :param flat_results: A list of tuples containing index and RecognizerResult\n        objects from the flattened analysis results.\n        :return: A RecognizerResult object selected based on the mixed strategy.\n        \"\"\"\n        # Check if mixed strategy threshold is within the valid range\n        if not 0 &lt;= mixed_strategy_threshold &lt;= 1:\n            raise ValueError(\n                f\"Invalid mixed strategy threshold: {mixed_strategy_threshold}.\"\n            )\n\n        score_aggregator = self._aggregate_scores(flat_results)\n\n        # Check if the highest score is greater than threshold and select accordingly\n        highest_score = max(\n            max(scores) for scores in score_aggregator.values() if scores\n        )\n        if highest_score &gt; mixed_strategy_threshold:\n            return self._select_highest_confidence_entity(flat_results)\n        else:\n            return self._select_most_common_entity(flat_results)\n\n    @staticmethod\n    def _aggregate_scores(flat_results):\n        \"\"\"\n        Aggregate the scores for each entity type from the flattened analysis results.\n\n        :param flat_results: A list of tuples containing index and RecognizerResult\n        objects from the flattened analysis results.\n        :return: A dictionary with entity types as keys and lists of scores as values.\n        \"\"\"\n        score_aggregator = {}\n        for _, res in flat_results:\n            if res.entity_type not in score_aggregator:\n                score_aggregator[res.entity_type] = []\n            score_aggregator[res.entity_type].append(res.score)\n        return score_aggregator\n\n    @staticmethod\n    def _flatten_results(analyzer_results):\n        \"\"\"\n        Flattens a nested lists of RecognizerResult objects into a list of tuples.\n\n        :param analyzer_results: A nested list of RecognizerResult objects from\n        the analysis results.\n        :return: A flattened list of tuples containing index and RecognizerResult\n        objects.\n        \"\"\"\n        return [\n            (cell_idx, res)\n            for cell_idx, cell_results in enumerate(analyzer_results)\n            for res in cell_results\n        ]\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.PandasAnalysisBuilder.generate_analysis","title":"generate_analysis","text":"<pre><code>generate_analysis(\n    df: DataFrame,\n    n: Optional[int] = None,\n    language: str = \"en\",\n    selection_strategy: str = \"most_common\",\n    mixed_strategy_threshold: float = 0.5,\n) -&gt; StructuredAnalysis\n</code></pre> <p>Generate a configuration from the given tabular data.</p> PARAMETER DESCRIPTION <code>df</code> <p>The input tabular data (dataframe).</p> <p> TYPE: <code>DataFrame</code> </p> <code>n</code> <p>The number of samples to be taken from the dataframe.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>The language to be used for analysis.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>selection_strategy</code> <p>A string that specifies the entity selection strategy ('highest_confidence', 'mixed', or default to most common).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'most_common'</code> </p> <code>mixed_strategy_threshold</code> <p>A float value for the threshold to be used in the entity selection mixed strategy.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>StructuredAnalysis</code> <p>A StructuredAnalysis object containing the analysis results.</p> Source code in <code>presidio_structured/analysis_builder.py</code> <pre><code>def generate_analysis(\n    self,\n    df: DataFrame,\n    n: Optional[int] = None,\n    language: str = \"en\",\n    selection_strategy: str = \"most_common\",\n    mixed_strategy_threshold: float = 0.5,\n) -&gt; StructuredAnalysis:\n    \"\"\"\n    Generate a configuration from the given tabular data.\n\n    :param df: The input tabular data (dataframe).\n    :param n: The number of samples to be taken from the dataframe.\n    :param language: The language to be used for analysis.\n    :param selection_strategy: A string that specifies the entity selection strategy\n    ('highest_confidence', 'mixed', or default to most common).\n    :param mixed_strategy_threshold: A float value for the threshold to be used in\n    the entity selection mixed strategy.\n    :return: A StructuredAnalysis object containing the analysis results.\n    \"\"\"\n    if not n:\n        n = len(df)\n    elif n &gt; len(df):\n        logger.debug(\n            f\"Number of samples ({n}) is larger than the number of rows \\\n                ({len(df)}), using all rows\"\n        )\n        n = len(df)\n\n    df = df.sample(n, random_state=123)\n\n    key_recognizer_result_map = self._generate_key_rec_results_map(\n        df, language, selection_strategy, mixed_strategy_threshold\n    )\n\n    key_entity_map = {\n        key: result.entity_type\n        for key, result in key_recognizer_result_map.items()\n        if result.entity_type != NON_PII_ENTITY_TYPE\n    }\n\n    return StructuredAnalysis(entity_mapping=key_entity_map)\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.StructuredAnalysis","title":"StructuredAnalysis  <code>dataclass</code>","text":"<p>Dataclass containing entity analysis from structured data.</p> <p>Currently, this class only contains entity mapping.</p> <p>param entity_mapping : dict. Mapping column/key names to entity types, e.g., {     \"person.name\": \"PERSON\",     \"person.address\": \"LOCATION\"     }</p> Source code in <code>presidio_structured/config/structured_analysis.py</code> <pre><code>@dataclass\nclass StructuredAnalysis:\n    \"\"\"\n    Dataclass containing entity analysis from structured data.\n\n    Currently, this class only contains entity mapping.\n\n    param entity_mapping : dict. Mapping column/key names to entity types, e.g., {\n        \"person.name\": \"PERSON\",\n        \"person.address\": \"LOCATION\"\n        }\n    \"\"\"\n\n    entity_mapping: Dict[str, str]\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.CsvReader","title":"CsvReader","text":"<p>               Bases: <code>ReaderBase</code></p> <p>Reader for reading csv files.</p> <p>Usage::</p> <pre><code>reader = CsvReader()\ndata = reader.read(path=\"filepath.csv\")\n</code></pre> METHOD DESCRIPTION <code>read</code> <p>Read csv file to pandas dataframe.</p> Source code in <code>presidio_structured/data/data_reader.py</code> <pre><code>class CsvReader(ReaderBase):\n    \"\"\"\n    Reader for reading csv files.\n\n    Usage::\n\n        reader = CsvReader()\n        data = reader.read(path=\"filepath.csv\")\n\n    \"\"\"\n\n    def read(self, path: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n        \"\"\"\n        Read csv file to pandas dataframe.\n\n        :param path: String defining the location of the csv file to read.\n        :return: Pandas DataFrame with the data read from the csv file.\n        \"\"\"\n        return pd.read_csv(path, **kwargs)\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.CsvReader.read","title":"read","text":"<pre><code>read(path: Union[str, Path], **kwargs) -&gt; pd.DataFrame\n</code></pre> <p>Read csv file to pandas dataframe.</p> PARAMETER DESCRIPTION <code>path</code> <p>String defining the location of the csv file to read.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Pandas DataFrame with the data read from the csv file.</p> Source code in <code>presidio_structured/data/data_reader.py</code> <pre><code>def read(self, path: Union[str, Path], **kwargs) -&gt; pd.DataFrame:\n    \"\"\"\n    Read csv file to pandas dataframe.\n\n    :param path: String defining the location of the csv file to read.\n    :return: Pandas DataFrame with the data read from the csv file.\n    \"\"\"\n    return pd.read_csv(path, **kwargs)\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.JsonDataProcessor","title":"JsonDataProcessor","text":"<p>               Bases: <code>DataProcessorBase</code></p> <p>JSON Data Processor, Supports arbitrary nesting of dictionaries and lists.</p> METHOD DESCRIPTION <code>operate</code> <p>Perform operations over the text using the operators, as per the structured analysis.</p> Source code in <code>presidio_structured/data/data_processors.py</code> <pre><code>class JsonDataProcessor(DataProcessorBase):\n    \"\"\"JSON Data Processor, Supports arbitrary nesting of dictionaries and lists.\"\"\"\n\n    @staticmethod\n    def _get_nested_value(data: Union[Dict, List, None], path: List[str]) -&gt; Any:\n        \"\"\"\n        Recursively retrieves the value from nested data using a given path.\n\n        :param data: Nested data (list or dictionary).\n        :param path: List of keys/indexes representing the path.\n        :return: Retrieved value.\n        \"\"\"\n        for i, key in enumerate(path):\n            if isinstance(data, list):\n                if key.isdigit():\n                    data = data[int(key)]\n                else:\n                    return [\n                        JsonDataProcessor._get_nested_value(item, path[i:])\n                        for item in data\n                    ]\n            elif isinstance(data, dict):\n                data = data.get(key)\n            else:\n                return data\n        return data\n\n    @staticmethod\n    def _set_nested_value(data: Union[Dict, List], path: List[str], value: Any) -&gt; None:\n        \"\"\"\n        Recursively sets a value in nested data using a given path.\n\n        :param data: Nested data (JSON-like).\n        :param path: List of keys/indexes representing the path.\n        :param value: Value to be set.\n        \"\"\"\n        for i, key in enumerate(path):\n            if isinstance(data, list):\n                if i + 1 &lt; len(path) and path[i + 1].isdigit():\n                    idx = int(path[i + 1])\n                    while len(data) &lt;= idx:\n                        data.append({})\n                    data = data[idx]\n                    continue\n                else:\n                    for item in data:\n                        JsonDataProcessor._set_nested_value(item, path[i:], value)\n                    return\n            elif isinstance(data, dict):\n                if i == len(path) - 1:\n                    data[key] = value\n                else:\n                    data = data.setdefault(key, {})\n\n    def _process(\n        self,\n        data: Union[Dict, List],\n        key_to_operator_mapping: Dict[str, Callable],\n    ) -&gt; Union[Dict, List]:\n        \"\"\"\n        Operates on the given JSON-like data based on the provided configuration.\n\n        :param data: JSON-like data to be operated on.\n        :param key_to_operator_mapping: maps keys to Callable operators.\n        :return: JSON-like data after the operation.\n        \"\"\"\n\n        if not isinstance(data, (dict, list)):\n            raise ValueError(\"Data must be a JSON-like object\")\n\n        for key, operator_callable in key_to_operator_mapping.items():\n            self.logger.debug(f\"Operating on key {key}\")\n            keys = key.split(\".\")\n            if isinstance(data, list):\n                for item in data:\n                    self._process(item, key_to_operator_mapping)\n            else:\n                text_to_operate_on = self._get_nested_value(data, keys)\n                if text_to_operate_on:\n                    if isinstance(text_to_operate_on, list):\n                        for text in text_to_operate_on:\n                            operated_text = self._operate_on_text(\n                                text, operator_callable\n                            )\n                            self._set_nested_value(data, keys, operated_text)\n                    else:\n                        operated_text = self._operate_on_text(\n                            text_to_operate_on, operator_callable\n                        )\n                        self._set_nested_value(data, keys, operated_text)\n        return data\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.JsonDataProcessor.operate","title":"operate","text":"<pre><code>operate(\n    data: Any,\n    structured_analysis: StructuredAnalysis,\n    operators: Dict[str, OperatorConfig],\n) -&gt; Any\n</code></pre> <p>Perform operations over the text using the operators, as per the structured analysis.</p> PARAMETER DESCRIPTION <code>data</code> <p>Data to be operated on.</p> <p> TYPE: <code>Any</code> </p> <code>structured_analysis</code> <p>Analysis schema as per the structured data.</p> <p> TYPE: <code>StructuredAnalysis</code> </p> <code>operators</code> <p>Dictionary containing operator configuration objects.</p> <p> TYPE: <code>Dict[str, OperatorConfig]</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Data after being operated upon.</p> Source code in <code>presidio_structured/data/data_processors.py</code> <pre><code>def operate(\n    self,\n    data: Any,\n    structured_analysis: StructuredAnalysis,\n    operators: Dict[str, OperatorConfig],\n) -&gt; Any:\n    \"\"\"\n    Perform operations over the text using the operators, as per the structured analysis.\n\n    :param data: Data to be operated on.\n    :param structured_analysis: Analysis schema as per the structured data.\n    :param operators: Dictionary containing operator configuration objects.\n    :return: Data after being operated upon.\n    \"\"\"  # noqa: E501\n    key_to_operator_mapping = self._generate_operator_mapping(\n        structured_analysis, operators\n    )\n    return self._process(data, key_to_operator_mapping)\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.JsonReader","title":"JsonReader","text":"<p>               Bases: <code>ReaderBase</code></p> <p>Reader for reading json files.</p> <p>Usage::</p> <pre><code>reader = JsonReader()\ndata = reader.read(path=\"filepath.json\")\n</code></pre> METHOD DESCRIPTION <code>read</code> <p>Read json file to dict.</p> Source code in <code>presidio_structured/data/data_reader.py</code> <pre><code>class JsonReader(ReaderBase):\n    \"\"\"\n    Reader for reading json files.\n\n    Usage::\n\n        reader = JsonReader()\n        data = reader.read(path=\"filepath.json\")\n\n    \"\"\"\n\n    def read(self, path: Union[str, Path], **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        Read json file to dict.\n\n        :param path: String defining the location of the json file to read.\n        :return: dictionary with the data read from the json file.\n        \"\"\"\n        with open(path) as f:\n            data = json.load(f, **kwargs)\n        return data\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.JsonReader.read","title":"read","text":"<pre><code>read(path: Union[str, Path], **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Read json file to dict.</p> PARAMETER DESCRIPTION <code>path</code> <p>String defining the location of the json file to read.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>dictionary with the data read from the json file.</p> Source code in <code>presidio_structured/data/data_reader.py</code> <pre><code>def read(self, path: Union[str, Path], **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Read json file to dict.\n\n    :param path: String defining the location of the json file to read.\n    :return: dictionary with the data read from the json file.\n    \"\"\"\n    with open(path) as f:\n        data = json.load(f, **kwargs)\n    return data\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.PandasDataProcessor","title":"PandasDataProcessor","text":"<p>               Bases: <code>DataProcessorBase</code></p> <p>Pandas Data Processor.</p> METHOD DESCRIPTION <code>operate</code> <p>Perform operations over the text using the operators, as per the structured analysis.</p> Source code in <code>presidio_structured/data/data_processors.py</code> <pre><code>class PandasDataProcessor(DataProcessorBase):\n    \"\"\"Pandas Data Processor.\"\"\"\n\n    def _process(\n        self, data: DataFrame, key_to_operator_mapping: Dict[str, Callable]\n    ) -&gt; DataFrame:\n        \"\"\"\n        Operates on the given pandas DataFrame based on the provided operators.\n\n        :param data: DataFrame to be operated on.\n        :param key_to_operator_mapping: Mapping of keys to operator callables.\n        :return: DataFrame after the operation.\n        \"\"\"\n\n        if not isinstance(data, DataFrame):\n            raise ValueError(\"Data must be a pandas DataFrame\")\n\n        for key, operator_callable in key_to_operator_mapping.items():\n            self.logger.debug(f\"Operating on column {key}\")\n            for row in data.itertuples(index=True):\n                text_to_operate_on = getattr(row, key)\n                operated_text = self._operate_on_text(\n                    text_to_operate_on, operator_callable\n                )\n                data.at[row.Index, key] = operated_text\n        return data\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.PandasDataProcessor.operate","title":"operate","text":"<pre><code>operate(\n    data: Any,\n    structured_analysis: StructuredAnalysis,\n    operators: Dict[str, OperatorConfig],\n) -&gt; Any\n</code></pre> <p>Perform operations over the text using the operators, as per the structured analysis.</p> PARAMETER DESCRIPTION <code>data</code> <p>Data to be operated on.</p> <p> TYPE: <code>Any</code> </p> <code>structured_analysis</code> <p>Analysis schema as per the structured data.</p> <p> TYPE: <code>StructuredAnalysis</code> </p> <code>operators</code> <p>Dictionary containing operator configuration objects.</p> <p> TYPE: <code>Dict[str, OperatorConfig]</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Data after being operated upon.</p> Source code in <code>presidio_structured/data/data_processors.py</code> <pre><code>def operate(\n    self,\n    data: Any,\n    structured_analysis: StructuredAnalysis,\n    operators: Dict[str, OperatorConfig],\n) -&gt; Any:\n    \"\"\"\n    Perform operations over the text using the operators, as per the structured analysis.\n\n    :param data: Data to be operated on.\n    :param structured_analysis: Analysis schema as per the structured data.\n    :param operators: Dictionary containing operator configuration objects.\n    :return: Data after being operated upon.\n    \"\"\"  # noqa: E501\n    key_to_operator_mapping = self._generate_operator_mapping(\n        structured_analysis, operators\n    )\n    return self._process(data, key_to_operator_mapping)\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.StructuredEngine","title":"StructuredEngine","text":"<p>Class to implement methods for anonymizing tabular data.</p> METHOD DESCRIPTION <code>anonymize</code> <p>Anonymize the given data using the given configuration.</p> Source code in <code>presidio_structured/structured_engine.py</code> <pre><code>class StructuredEngine:\n    \"\"\"Class to implement methods for anonymizing tabular data.\"\"\"\n\n    def __init__(self, data_processor: Optional[DataProcessorBase] = None) -&gt; None:\n        \"\"\"\n        Initialize the class with a data processor.\n\n        :param data_processor: Instance of DataProcessorBase.\n        \"\"\"\n        if data_processor is None:\n            self.data_processor = PandasDataProcessor()\n        else:\n            self.data_processor = data_processor\n\n        self.logger = logging.getLogger(\"presidio-structured\")\n\n    def anonymize(\n        self,\n        data: Union[Dict, DataFrame],\n        structured_analysis: StructuredAnalysis,\n        operators: Union[Dict[str, OperatorConfig], None] = None,\n    ) -&gt; Union[Dict, DataFrame]:\n        \"\"\"\n        Anonymize the given data using the given configuration.\n\n        :param data: input data as dictionary or pandas DataFrame.\n        :param structured_analysis: structured analysis configuration.\n        :param operators: a dictionary of operator configurations, optional.\n        :return: Anonymized dictionary or DataFrame.\n        \"\"\"\n        self.logger.debug(\"Starting anonymization\")\n        operators = self.__check_or_add_default_operator(operators)\n\n        return self.data_processor.operate(data, structured_analysis, operators)\n\n    def __check_or_add_default_operator(\n        self, operators: Union[Dict[str, OperatorConfig], None]\n    ) -&gt; Dict[str, OperatorConfig]:\n        \"\"\"\n        Check if the provided operators dictionary has a default operator. If not, add a default operator.\n\n        :param operators: dictionary of operator configurations.\n        :return: operators dictionary with the default operator added \\\n            if it was not initially present.\n        \"\"\"  # noqa: E501\n        default_operator = OperatorConfig(DEFAULT)\n        if not operators:\n            self.logger.debug(\"No operators provided, using default operator\")\n            return {\"DEFAULT\": default_operator}\n        if not operators.get(\"DEFAULT\"):\n            self.logger.debug(\"No default operator provided, using default operator\")\n            operators[\"DEFAULT\"] = default_operator\n        return operators\n</code></pre>"},{"location":"api/structured_python/#presidio_structured.StructuredEngine.anonymize","title":"anonymize","text":"<pre><code>anonymize(\n    data: Union[Dict, DataFrame],\n    structured_analysis: StructuredAnalysis,\n    operators: Union[Dict[str, OperatorConfig], None] = None,\n) -&gt; Union[Dict, DataFrame]\n</code></pre> <p>Anonymize the given data using the given configuration.</p> PARAMETER DESCRIPTION <code>data</code> <p>input data as dictionary or pandas DataFrame.</p> <p> TYPE: <code>Union[Dict, DataFrame]</code> </p> <code>structured_analysis</code> <p>structured analysis configuration.</p> <p> TYPE: <code>StructuredAnalysis</code> </p> <code>operators</code> <p>a dictionary of operator configurations, optional.</p> <p> TYPE: <code>Union[Dict[str, OperatorConfig], None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[Dict, DataFrame]</code> <p>Anonymized dictionary or DataFrame.</p> Source code in <code>presidio_structured/structured_engine.py</code> <pre><code>def anonymize(\n    self,\n    data: Union[Dict, DataFrame],\n    structured_analysis: StructuredAnalysis,\n    operators: Union[Dict[str, OperatorConfig], None] = None,\n) -&gt; Union[Dict, DataFrame]:\n    \"\"\"\n    Anonymize the given data using the given configuration.\n\n    :param data: input data as dictionary or pandas DataFrame.\n    :param structured_analysis: structured analysis configuration.\n    :param operators: a dictionary of operator configurations, optional.\n    :return: Anonymized dictionary or DataFrame.\n    \"\"\"\n    self.logger.debug(\"Starting anonymization\")\n    operators = self.__check_or_add_default_operator(operators)\n\n    return self.data_processor.operate(data, structured_analysis, operators)\n</code></pre>"},{"location":"evaluation/","title":"Evaluating PII detection with Presidio","text":""},{"location":"evaluation/#why-evaluate-pii-detection","title":"Why evaluate PII detection?","text":"<p>No de-identification system is perfect. It is important to evaluate the performance of a PII detection system for your specific use case. This evaluation can help you understand where the system makes mistakes and how to iteratively improve the detection mechanisms, which recognizers and models to use, and how to configure them.</p>"},{"location":"evaluation/#common-evaluation-metrics","title":"Common evaluation metrics","text":"<p>The most common evaluation metrics are <code>precision</code>, <code>recall</code>, and <code>F\u03b2 score</code>, which is a combination of precision and recall. These metrics are calculated based on the number of true positives, false positives, and false negatives. For every use case, the false positive and false negative rates should be balanced to achieve the desired level of accuracy.</p> <ul> <li>Precision measures the proportion of true positive results among the positive results: <code>TP / (TP + FP)</code>.</li> <li>Recall measures the proportion of true positive results among the actual positives: <code>TP / (TP + FN)</code>.</li> <li>F\u03b2 score is a weighted harmonic mean of precision and recall: <code>(1 + \u03b2^2) * (precision * recall) / (\u03b2^2 * precision + recall)</code>.</li> </ul> <p>Click here for more definitions.</p> <p>Note</p> <p>In PII detection, recall is often more important than precision, as we'd like to avoid missing any PII.  In such cases, we recommend to use the \u03b2=2 score, which gives more importance to recall.</p>"},{"location":"evaluation/#how-to-evaluate-pii-detection-with-presidio","title":"How to evaluate PII detection with Presidio","text":"<p>Presidio provides a set of tools to evaluate the performance of the PII detection system. In addition, it provides simple data generation tools to help you create a dataset for evaluation.</p>"},{"location":"evaluation/#evaluating-the-presidio-analyzer-using-presidio-research","title":"Evaluating the Presidio Analyzer using Presidio-Research","text":"<p>Presidio-Research is a python package with a set of tools that help you evaluate the performance of the Presidio Analyzer. To get started, follow the instructions in the Presidio-Research repository.</p> <p>The easiest way to get started is by reviewing the notebooks:</p> <ul> <li>Notebook 1: Shows how to use the PII data generator.</li> <li>Notebook 2: Shows a simple analysis of the PII dataset.</li> <li>Notebook 3: Provides tools to split the dataset into train/test/validation sets while avoiding leakage due to the same pattern appearing in multiple folds (only applicable for synthetically generated data).</li> <li>Notebook 4: Shows how to use the evaluation tools to evaluate how well Presidio detects PII. Note that this is using the vanilla Presidio, and the results aren't very accurate.</li> <li>Notebook 5: Shows how one can configure Presidio to detect PII much more accurately, and boost the f score in ~30%.</li> </ul> <p>For more information and advanced usage, refer to the Presidio-Research repository.</p>"},{"location":"evaluation/#evaluating-dicom-redaction-with-presidio-image-redactor","title":"Evaluating DICOM redaction with Presidio Image Redactor","text":"<p>See Evaluating DICOM redaction for more information. For a full demonstration, see the evaluation notebook.</p>"},{"location":"getting_started/getting_started_images/","title":"Getting started with image de-identification with Presidio","text":"<p>Presidio provides a simple way to de-identify image data by detecting and anonymizing personally identifiable information (PII). This guide shows you how to get started with image de-identification using Presidio's Python packages.</p> <p>Presidio has two main modules for image de-identification: General purpose, and specifically for DICOM (medical) images.</p>"},{"location":"getting_started/getting_started_images/#simple-flow-python-package","title":"Simple flow - Python package","text":"Anonymize PII in imagesRedact text PII in DICOM images <ol> <li> <p>Install presidio-image-redactor</p> <pre><code>pip install presidio-image-redactor\n</code></pre> </li> <li> <p>Redact PII from image</p> <pre><code>from presidio_image_redactor import ImageRedactorEngine\nfrom PIL import Image\n\nimage = Image.open(path_to_image_file)\n\nredactor = ImageRedactorEngine()\nredactor.redact(image=image)\n</code></pre> </li> </ol> <ol> <li> <p>Install presidio-image-redactor</p> <pre><code>pip install presidio-image-redactor\n</code></pre> </li> <li> <p>Redact text PII from DICOM image</p> <pre><code>import pydicom\nfrom presidio_image_redactor import DicomImageRedactorEngine\n\n# Set input and output paths\ninput_path = \"path/to/your/dicom/file.dcm\"\noutput_dir = \"./output\"\n\n# Initialize the engine\nengine = DicomImageRedactorEngine()\n\n# Option 1: Redact from a loaded DICOM image\ndicom_image = pydicom.dcmread(input_path)\nredacted_dicom_image = engine.redact(dicom_image, fill=\"contrast\")\n\n# Option 2: Redact from DICOM file\nengine.redact_from_file(input_path, output_dir, padding_width=25, fill=\"contrast\")\n\n# Option 3: Redact from directory\nengine.redact_from_directory(\"path/to/your/dicom\", output_dir, padding_width=25, fill=\"contrast\")\n</code></pre> </li> </ol>"},{"location":"getting_started/getting_started_images/#simple-flow-docker-container","title":"Simple flow - Docker container","text":"<p>Presidio provides a Docker containers that you can use to de-identify image data.</p> <ol> <li>Download Docker image</li> </ol> <pre><code>docker pull mcr.microsoft.com/presidio-image-redactor\n</code></pre> <ol> <li>Run container</li> </ol> <pre><code>docker run -d -p 5003:3000 mcr.microsoft.com/presidio-image-redactor\n</code></pre> <ol> <li>Use the API</li> </ol> <pre><code>curl -XPOST \"http://localhost:5003/redact\" -H \"content-type: multipart/form-data\" -F \"image=@img.png\" -F \"data=\\\"{'color_fill':'255'}\\\"\" &gt; out.png \n</code></pre>"},{"location":"getting_started/getting_started_images/#read-more","title":"Read more","text":"<ul> <li>Installing Presidio</li> <li>PII detection in images</li> <li>Samples</li> <li>Python API reference - Image Redactor</li> </ul>"},{"location":"getting_started/getting_started_structured/","title":"Getting started with structured and semi-structured de-identification with Presidio","text":"<p>Presidio-structured is a package built on top of Presidio that provides a simple way to de-identify structured and semi-structured data by detecting and anonymizing personally identifiable information (PII).</p> <p>Presidio-structured supports the detection and anonymization of PII in tables (e.g. Pandas DataFrames or SQL tables) and semi-structured data (e.g. JSON).</p> <p>Warning</p> <p>Alpha: This package is currently in alpha, meaning it is in its early stages of development. Features and functionality may change as the project evolves.</p>"},{"location":"getting_started/getting_started_structured/#simple-flow-structured-data","title":"Simple flow - structured data","text":"<pre><code>import pandas as pd\nfrom presidio_structured import StructuredEngine, PandasAnalysisBuilder\nfrom presidio_anonymizer.entities import OperatorConfig\nfrom faker import Faker # optionally using faker as an example\n\n# Initialize the engine with a Pandas data processor (default)\npandas_engine = StructuredEngine()\n\n# Create a sample DataFrame\nsample_df = pd.DataFrame({'name': ['John Doe', 'Jane Smith'], 'email': ['john.doe@example.com', 'jane.smith@example.com']})\n\n# Generate a tabular analysis which detects the PII entities in the DataFrame.\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df)\n\n# Define anonymization operators\nfake = Faker()\noperators = {\n    \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"REDACTED\"}),\n    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.safe_email()})\n}\n\n# Anonymize DataFrame\nanonymized_df = pandas_engine.anonymize(sample_df, tabular_analysis, operators=operators)\nprint(anonymized_df)\n</code></pre>"},{"location":"getting_started/getting_started_structured/#read-more","title":"Read more","text":"<ul> <li>Presidio structured documentation</li> <li>Presidio structured sample notebook</li> </ul>"},{"location":"getting_started/getting_started_text/","title":"Getting started with text de-identification with Presidio","text":"<p>Presidio provides a simple way to de-identify text data by detecting and anonymizing personally identifiable information (PII). This guide shows you how to get started with text de-identification using Presidio's Python packages.</p> <p>Note that Presidio can leverage different NLP packages to analyze text data. The default engine is based on <code>spaCy</code>, but you can also use others. This guide shows two examples: one using <code>spaCy</code> and the other using <code>transformers</code>.</p>"},{"location":"getting_started/getting_started_text/#simple-flow-python-package","title":"Simple flow - Python package","text":"<p>Using Presidio's modules as Python packages to get started:</p> Anonymize PII in text (Default spaCy model)Anonymize PII in text (transformers) <ol> <li> <p>Install Presidio</p> <pre><code>pip install presidio-analyzer\npip install presidio-anonymizer\npython -m spacy download en_core_web_lg\n</code></pre> </li> <li> <p>Analyze + Anonymize</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\ntext=\"My phone number is 212-555-5555\"\n\n# Set up the engine, loads the NLP module (spaCy model by default) \n# and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text=text,\n                           entities=[\"PHONE_NUMBER\"],\n                           language='en')\nprint(results)\n\n# Analyzer results are passed to the AnonymizerEngine for anonymization\n\nanonymizer = AnonymizerEngine()\n\nanonymized_text = anonymizer.anonymize(text=text,analyzer_results=results)\n\nprint(anonymized_text)\n</code></pre> </li> </ol> <ol> <li> <p>Install Presidio</p> <pre><code>pip install \"presidio-analyzer[transformers]\"\npip install presidio-anonymizer\npython -m spacy download en_core_web_sm\n</code></pre> </li> <li> <p>Analyze + Anonymize</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import TransformersNlpEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\ntext = \"My name is Don and my phone number is 212-555-5555\"\n\n# Define which transformers model to use\nmodel_config = [{\"lang_code\": \"en\", \"model_name\": {\n    \"spacy\": \"en_core_web_sm\",  # use a small spaCy model for lemmas, tokens etc.\n    \"transformers\": \"dslim/bert-base-NER\"\n    }\n}]\n\nnlp_engine = TransformersNlpEngine(models=model_config)\n\n# Set up the engine, loads the NLP module (spaCy model by default) \n# and other PII recognizers\nanalyzer = AnalyzerEngine(nlp_engine=nlp_engine)\n\n# Call analyzer to get results\nresults = analyzer.analyze(text=text, language='en')\nprint(results)\n\n# Analyzer results are passed to the AnonymizerEngine for anonymization\n\nanonymizer = AnonymizerEngine()\n\nanonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n\nprint(anonymized_text)\n</code></pre> <p>Tip: Downloading models</p> <p>If not available, the transformers model and the spacy model would be downloaded on the first call to the <code>AnalyzerEngine</code>. To pre-download, see this doc.</p> </li> </ol>"},{"location":"getting_started/getting_started_text/#simple-flow-docker-container","title":"Simple flow - Docker container","text":"<p>Presidio provides Docker containers that you can use to de-identify text data. Each module, analyzer, and anonymizer, has its own Docker container. The containers are available on Docker Hub.</p> <ol> <li>Download Docker images</li> </ol> <pre><code>docker pull mcr.microsoft.com/presidio-analyzer\ndocker pull mcr.microsoft.com/presidio-anonymizer\n</code></pre> <ol> <li>Run containers</li> </ol> <pre><code>docker run -d -p 5002:3000 mcr.microsoft.com/presidio-analyzer:latest\n\ndocker run -d -p 5001:3000 mcr.microsoft.com/presidio-anonymizer:latest\n</code></pre> <ol> <li>Use the API</li> </ol> <pre><code>curl -X POST http://localhost:5002/analyze \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"text\": \"My phone number is 555-123-4567.\",\n  \"language\": \"en\"\n}'\n\n\ncurl -X POST http://localhost:5001/anonymize -H \"Content-Type: application/json\"  -d '\n    {\n        \"text\": \"My phone number is 555-123-4567\",\n        \"anonymizers\": {\n            \"PHONE_NUMBER\": {\n            \"type\": \"replace\",\n            \"new_value\": \"--Redacted phone number--\"\n            }\n        },\n        \"analyzer_results\": [\n        {\n            \"start\": 19,\n            \"end\": 31,\n            \"score\": 0.95,\n            \"entity_type\": \"PHONE_NUMBER\"\n        }\n    ]}'\n</code></pre>"},{"location":"getting_started/getting_started_text/#read-more","title":"Read more","text":"<ul> <li>Installing Presidio</li> <li>PII detection in text</li> <li>PII anonymization in text</li> <li>Tutorial</li> <li>Samples</li> <li>Python API reference - Analyzer</li> <li>Python API reference - Anonymizer</li> <li>REST API reference</li> </ul>"},{"location":"image-redactor/","title":"Presidio Image Redactor","text":"<p>Please notice, this package is still in beta and not production ready.</p>"},{"location":"image-redactor/#description","title":"Description","text":"<p>The Presidio Image Redactor is a Python based module for detecting and redacting PII text entities in images. </p> <p>This module may also be used on medical DICOM images. The <code>DicomImageRedactorEngine</code> class may be used to redact text PII present as pixels in DICOM images. </p> <p>Note</p> <p>This class only redacts pixel data and does not scrub text PII which may exist in the DICOM metadata.  We highly recommend using the DICOM image redactor engine to redact text from images BEFORE scrubbing metadata PII.*</p>"},{"location":"image-redactor/#installation","title":"Installation","text":"<p>Pre-requisites:</p> <ul> <li>Install Tesseract OCR by following the   instructions on how to install it for your operating system.</li> </ul> <p>Attention</p> <p>For best performance, please use the most up-to-date version of Tesseract OCR. Presidio was tested with v5.2.0.</p> Using pipUsing DockerFrom source <p>Note</p> <p>Consider installing the Presidio python packages on a virtual environment like venv or conda.</p> <p>To get started with Presidio-image-redactor, download the package and the <code>en_core_web_lg</code> spaCy model:</p> <pre><code>pip install presidio-image-redactor\npython -m spacy download en_core_web_lg\n</code></pre> <p>Note</p> <p>This requires Docker to be installed. Download Docker.</p> <pre><code># Download image from Dockerhub\ndocker pull mcr.microsoft.com/presidio-image-redactor\n\n# Run the container with the default port\ndocker run -d -p 5003:3000 mcr.microsoft.com/presidio-image-redactor:latest\n</code></pre> <p>First, clone the Presidio repo. See here for instructions.</p> <p>Then, build the presidio-image-redactor container:</p> <pre><code>cd presidio-image-redactor\ndocker build . -t presidio/presidio-image-redactor\n</code></pre>"},{"location":"image-redactor/#getting-started-standard-image-types","title":"Getting started (standard image types)","text":"PythonAs an HTTP server <p>Once the Presidio-image-redactor package is installed, run this simple script:</p> <pre><code>from PIL import Image\nfrom presidio_image_redactor import ImageRedactorEngine\n\n# Get the image to redact using PIL lib (pillow)\nimage = Image.open(\"./docs/image-redactor/ocr_text.png\")\n\n# Initialize the engine\nengine = ImageRedactorEngine()\n\n# Redact the image with pink color\nredacted_image = engine.redact(image, (255, 192, 203))\n\n# save the redacted image \nredacted_image.save(\"new_image.png\")\n# uncomment to open the image for viewing\n# redacted_image.show()\n</code></pre> <p>You can run presidio image redactor as an http server using either python runtime or using a docker container.</p> <p>Python script example can be found under: /presidio/e2e-tests/tests/test_image_redactor.py</p>"},{"location":"image-redactor/#using-docker-container","title":"Using docker container","text":"<pre><code>cd presidio-image-redactor\ndocker run -p 5003:3000 presidio-image-redactor \n</code></pre>"},{"location":"image-redactor/#using-python-runtime","title":"Using python runtime","text":"<p>Note</p> <p>This requires the Presidio Github repository to be cloned.</p> <pre><code>cd presidio-image-redactor\npython app.py\n# use ocr_test.png as the image to redact, and 255 as the color fill. \n# out.png is the new redacted image received from the server.\ncurl -XPOST \"http://localhost:3000/redact\" -H \"content-type: multipart/form-data\" -F \"image=@ocr_test.png\" -F \"data=\\\"{'color_fill':'255'}\\\"\" &gt; out.png\n</code></pre>"},{"location":"image-redactor/#getting-started-dicom-images","title":"Getting started (DICOM images)","text":"Python <p>Once the Presidio-image-redactor package is installed, run this simple script:</p> <pre><code>import pydicom\nfrom presidio_image_redactor import DicomImageRedactorEngine\n\n# Set input and output paths\ninput_path = \"path/to/your/dicom/file.dcm\"\noutput_dir = \"./output\"\n\n# Initialize the engine\nengine = DicomImageRedactorEngine()\n\n# Option 1: Redact from a loaded DICOM image\ndicom_image = pydicom.dcmread(input_path)\nredacted_dicom_image = engine.redact(dicom_image, fill=\"contrast\")\n\n# Option 2: Redact from a loaded DICOM image and return redacted regions\nredacted_dicom_image, bboxes = engine.redact_and_return_bbox(dicom_image, fill=\"contrast\")\n\n# Option 3: Redact from DICOM file and save redacted regions as json file\nengine.redact_from_file(input_path, output_dir, padding_width=25, fill=\"contrast\", save_bboxes=True)\n\n# Option 4: Redact from directory and save redacted regions as json files\nocr_kwargs = {\"ocr_threshold\": 50}\nengine.redact_from_directory(\"path/to/your/dicom\", output_dir, fill=\"background\", save_bboxes=True, ocr_kwargs=ocr_kwargs)\n</code></pre>"},{"location":"image-redactor/#getting-started-using-the-document-intelligence-ocr-engine","title":"Getting started using the document intelligence OCR engine","text":"<p>Presidio offers two engines for OCR based PII removal. The first is the default engine which uses Tesseract OCR. The second is the Document Intelligence OCR engine which uses Azure's Document Intelligence service, which requires an Azure subscription. The following sections describe how to setup and use the Document Intelligence OCR engine.</p> <p>You will need to register with Azure to get an API key and endpoint.  Perform the steps in the \"Prerequisites\" section of this page.  Once your resource deploys, copy your endpoint and key values and save them for the next step.</p> <p>The most basic usage of the engine can be setup like the following in python</p> <pre><code>diOCR = DocumentIntelligenceOCR(endpoint=\"&lt;your_endpoint&gt;\", key=\"&lt;your_key&gt;\")\n</code></pre> <p>The DocumentIntelligenceOCR can also attempt to pull your endpoint and key values from environment variables.  </p> <pre><code>export DOCUMENT_INTELLIGENCE_ENDPOINT=&lt;your_endpoint&gt;\nexport DOCUMENT_INTELLIGENCE_KEY=&lt;your_key&gt;\n</code></pre>"},{"location":"image-redactor/#document-intelligence-model-support","title":"Document Intelligence Model Support","text":"<p>There are numerous document processing models available, and currently we only support the most basic usage of the model.  For an overview of the functionalities offered by Document Intelligence, see this page. Presidio offers only word-level processing on the result for PII redaction purposes, as all prebuilt document models support this interface. Different models support additional structured support for tables, paragraphs, key-value pairs, fields and other types of metadata in the response.</p> <p>Additional metadata can be sent to the Document Intelligence API call, such as pages, locale, and features, which are documented here. You are encouraged to test each model to see which fits best to your use case.</p>"},{"location":"image-redactor/#creating-an-image-redactor-engine-in-python","title":"Creating an image redactor engine in Python","text":"<pre><code>diOCR = DocumentIntelligenceOCR()\nia_engine = ImageAnalyzerEngine(ocr=di_ocr)\nmy_engine = ImageRedactorEngine(image_analyzer_engine=ia_engine)\n</code></pre>"},{"location":"image-redactor/#testing-document-intelligence","title":"Testing Document Intelligence","text":"<p>Follow the steps of running the tests</p> <p>The test suite has a series of tests which are only exercised when the appropriate environment variables are populated.  To run the test suite, to test the DocumentIntelligenceOCR engine, call the tests like this:</p> <pre><code>export DOCUMENT_INTELLIGENCE_ENDPOINT=&lt;your_endpoint&gt;\nexport DOCUMENT_INTELLIGENCE_KEY=&lt;your_key&gt;\npytest\n</code></pre>"},{"location":"image-redactor/#evaluating-de-identification-performance","title":"Evaluating de-identification performance","text":"<p>If you are interested in evaluating the performance of the DICOM de-identification against ground truth labels, please see the evaluating DICOM de-identification page.</p>"},{"location":"image-redactor/#side-note-for-windows","title":"Side note for Windows","text":"<p>If you are using a Windows machine, you may run into issues if file paths are too long. Unfortunately, this is not rare when working with DICOM images that are often nested in directories with descriptive names.</p> <p>To avoid errors where the code may not recognize a path as existing due to the length of the characters in the file path, please enable long paths on your system.</p>"},{"location":"image-redactor/#dicom-data-citation","title":"DICOM Data Citation","text":"<p>The DICOM data used for unit and integration testing for <code>DicomImageRedactorEngine</code> are stored in this repository with permission from the original dataset owners. Please see the dataset information as follows:</p> <p>Rutherford, M., Mun, S.K., Levine, B., Bennett, W.C., Smith, K., Farmer, P., Jarosz, J., Wagner, U., Farahani, K., Prior, F. (2021). A DICOM dataset for evaluation of medical image de-identification (Pseudo-PHI-DICOM-Data) [Data set]. The Cancer Imaging Archive. DOI: https://doi.org/10.7937/s17z-r072</p>"},{"location":"image-redactor/#api-reference","title":"API reference","text":"<p>the API Spec for the Image Redactor REST API reference details and Image Redactor Python API for Python API reference</p>"},{"location":"image-redactor/evaluating_dicom_redaction/","title":"Evaluating DICOM de-identification","text":""},{"location":"image-redactor/evaluating_dicom_redaction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Ground truth format</li> <li>Creating ground truth files</li> <li>Evaluating de-identification performance</li> </ul>"},{"location":"image-redactor/evaluating_dicom_redaction/#introduction","title":"Introduction","text":"<p>We can evaluate the performance of the <code>DicomImageRedactorEngine</code> DICOM de-identification by using the <code>DicomImagePiiVerifyEngine</code>. The evaluation results consist of:</p> <ul> <li>Image with bounding boxes identifying detected Personal Health Information (PHI)</li> <li>All positives (True Positives and False Positives)</li> <li>Precision</li> <li>Recall</li> </ul>"},{"location":"image-redactor/evaluating_dicom_redaction/#ground-truth-format","title":"Ground truth format","text":"<p>Ground truth labels are stored as <code>.json</code> files containing filename as the highest level keys. Each filename object consists of an item for each individual entity.</p> <pre><code>{\n    \"your/dicom/dir/file_0.dcm\": [\n        {\n            \"label\": \"DAVIDSON\",\n            \"left\": 25,\n            \"top\": 25,\n            \"width\": 241,\n            \"height\": 37\n        },\n        {\n            \"label\": \"DOUGLAS\",\n            \"left\": 287,\n            \"top\": 25,\n            \"width\": 230,\n            \"height\": 36\n        },\n        {\n            \"label\": \"[M]\",\n            \"left\": 535,\n            \"top\": 25,\n            \"width\": 60,\n            \"height\": 45\n        },\n        {\n            \"label\": \"01.09.2012\",\n            \"left\": 613,\n            \"top\": 26,\n            \"width\": 226,\n            \"height\": 35\n        },\n        {\n            \"label\": \"06.16.1976\",\n            \"left\": 170,\n            \"top\": 72,\n            \"width\": 218,\n            \"height\": 35\n        }\n    ],\n    \"your/dicom/dir/file_1.dcm\": [\n        ...\n    ]\n}\n</code></pre> <p>Return to the Table of Contents</p>"},{"location":"image-redactor/evaluating_dicom_redaction/#creating-ground-truth-files","title":"Creating ground truth files","text":"<p>The <code>DicomImagePiiVerifyEngine</code> class can be used to assist in ground truth label generation. Use the following code snippet to generate the verification image, OCR results, and NER (analyzer) results.</p> <pre><code>import pydicom\nfrom presidio_image_redactor import DicomImagePiiVerifyEngine\n\n# Initialize engine\ndicom_engine = DicomImagePiiVerifyEngine()\n\n# Choose your file to create ground truth for\nfilename = \"path/to/your/file.dcm\"\ninstance = pydicom.dcmread(filename)\npadding_width = 25\n\n# Get OCR and NER results\nverification_image, ocr_results, analyzer_results = dicom_engine.verify_dicom_instance(instance, padding_width)\n</code></pre> <p>By looking at the output of <code>verify_dicom_instance</code>, we can create a ground truth labels json.</p> <p>Save <code>analyzer_results</code> as a json file and then perform the following</p> <ol> <li>Group the results into a new item with the file name set as the key.</li> <li>For each item in this group:     a. Remove the \"entity_type\" field and value.     b. Add a new \"label\" field with the value set to the ground truth text PHI with matching coordinate as you can see in the formatted OCR results and verification image.</li> </ol> <p>Then check that your ground truth json contains all the text PHI you can visually confirm in the DICOM image. If something is not detected by the OCR or NER, you will need to manually add the item yourself.</p> <p>Pixel position and size data can be obtained using any labeling software or imaging processing software (e.g., MS Paint) on the verification image.</p> <p>Note: When manually specifying pixel position, make sure to account for any padding introduced in the OCR process (default padding added is 25 pixels).</p>"},{"location":"image-redactor/evaluating_dicom_redaction/#example","title":"Example","text":"<p>Let's say we ran the above code block and see the following for <code>ocr_results</code> and <code>analyzer_results</code>.</p> <pre><code>// OCR Results\n[\n    {\n        \"left\": 25,\n        \"top\": 25,\n        \"width\": 241,\n        \"height\": 37,\n        \"conf\": 95.833916,\n        \"label\": \"DAVIDSON\"\n    },\n    {\n        \"left\": 287,\n        \"top\": 25,\n        \"width\": 230,\n        \"height\": 36,\n        \"conf\": 93.292221,\n        \"label\": \"DOUGLAS\"\n    }\n]\n\n// Analyzer Results\n[\n    {\n        \"entity_type\": \"PERSON\",\n        \"score\": 1.0,\n        \"left\": 25,\n        \"top\": 25,\n        \"width\": 241,\n        \"height\": 37\n    },\n    {\n        \"entity_type\": \"PERSON\",\n        \"score\": 1.0,\n        \"left\": 287,\n        \"top\": 25,\n        \"width\": 230,\n        \"height\": 36\n    }\n]\n</code></pre> <p>Looking at the position and size values of the ground truth and detected text from the analyzer results, we can see that the first item in the analyzer results is likely \"DAVIDSON\" and the second is likely \"DOUGLAS\".</p> <p>With this, we set our ground truth json to the following:</p> <pre><code>// Ground truth json\n{\n    \"path/to/your/file.dcm\": [\n        {\n            \"label\": \"DAVIDSON\",\n            \"left\": 25,\n            \"top\": 25,\n            \"width\": 241,\n            \"height\": 37\n        },\n        {\n            \"label\": \"DOUGLAS\",\n            \"left\": 287,\n            \"top\": 25,\n            \"width\": 230,\n            \"height\": 36\n        }\n    ]\n}\n</code></pre> <p>Return to the Table of Contents</p>"},{"location":"image-redactor/evaluating_dicom_redaction/#evaluating-de-identification-performance","title":"Evaluating de-identification performance","text":"<p>The <code>DicomImagePiiVerifyEngine</code> can be used to evaluate DICOM de-identification performance.</p> <pre><code># Load ground truth for one file\nwith open(gt_path) as json_file:\n    all_ground_truth = json.load(json_file)\nground_truth = all_ground_truth[file_of_interest]\n\n# Select your DICOM instance\ninstance = pydicom.dcmread(file_of_interest)\n\n# Evaluate the DICOM de-identification performance\n_, eval_results = dicom_engine.eval_dicom_instance(instance, ground_truth)\n</code></pre> <p>You can also set optional arguments to see the effect of padding width, ground-truth matching tolerance, and OCR confidence threshold (e.g., <code>ocr_kwargs={\"ocr_threshold\": 50}</code>).</p> <p>For a full demonstration, please see the evaluation notebook.</p> <p>Return to the Table of Contents</p>"},{"location":"learn_presidio/","title":"Learn Presidio","text":"<p>Presidio is a suite of tools for detecting and de-identifying PII in text, images, and structured data.</p> <p>The recommended place to start is to follow the tutorial which will guide you through the process of setting up and using Presidio. To learn about the different concepts in Presidio, visit the concepts page.</p> <p>To go deeper into each component, visit the relevant docs:</p> <ul> <li>For the Presidio Analyzer, visit the Analyzer docs.</li> <li>For Presidio Anonymizer, visit the Anonymizer docs.</li> <li>For Presidio Image Redactor, visit the Image Redactor docs.</li> <li>For Presidio structured, visit the Structured docs.</li> </ul> <p>The following diagrams provide a high level understanding of the Presidio components:</p>"},{"location":"learn_presidio/#analyzer","title":"Analyzer","text":""},{"location":"learn_presidio/#anonymizer","title":"Anonymizer","text":""},{"location":"learn_presidio/#image-redactor","title":"Image Redactor","text":""},{"location":"learn_presidio/#standard-image-types","title":"Standard Image Types","text":""},{"location":"learn_presidio/#dicom-images","title":"DICOM Images","text":""},{"location":"learn_presidio/concepts/","title":"Concepts in Microsoft Presidio","text":""},{"location":"learn_presidio/concepts/#high-level-concepts","title":"High-level concepts","text":"Concept Definition Learn More Entity An entity is a span of text that can be used to directly identify an individual. For example, a phone number, email address, or social security number. In Presidio, an entity is represented by a RecognizerResult object. Analyzer concepts Context Context is defined as the surrounding text of an entity. Context can be used to provide additional information about the entity which can be used to improve the detection accuracy. Analyzer concepts Recognizer A recognizer is an object that is responsible for detecting entities in text. Recognizers can be rule-based, machine learning-based, or a combination of both. The Presidio Analyzer orchestrates multiple recognizers to detect PII entities in text. The main objects in Presidio that implement PII detection logic are the EntityRecognizer and PatternRecognizer. Analyzer concepts Analyzer The Presidio <code>AnalyzerEngine</code> is responsible for orchestrating the PII detection using various recognizers. Analyzer concepts Predefined recognizer A recognizer that already exists in Presidio Predefined recognizers Custom recognizer A recognizer that is added by the user Adding recognizers ad-hoc recognizer A recognizer that is added to the request itself, rather than to the list of recognizers loaded within Presidio ad-hoc recognizers Deny list A list of terms that should always be identified as PII denylist tutorial Allow list A list of terms that should not be identified as PII allowlist tutorial"},{"location":"learn_presidio/concepts/#main-objects-in-presidio","title":"Main objects in Presidio","text":"Concept Definition Learn More EntityRecognizer An EntityRecognizer is an object in Presidio that is responsible for detecting entities in text. An entity recognizer can be rule-based, a machine learning model, or a combination of both. Analyzer concepts RecognizerResult A RecognizerResult holds the type and span of a PII entity. Analyzer concepts RecognizerRegistry The RecognizerRegistry is a class in Presidio that is responsible for holding the various recognizers used by the AnalyzerEngine. link NlpEngine The NlpEngine is an interface that defines the methods for processing text. Presidio provides several implementations of the NlpEngine, such as SpacyNlpEngine, TransformersNlpEngine, and StanzaNlpEngine. Analyzer concepts AnalyzerEngine The AnalyzerEngine is the main class in Presidio that is responsible for orchestrating the PII detection in text. It uses an NlpEngine to process the text and a RecognizerRegistry to hold the different recognizers. Analyzer concepts BatchAnalyzerEngine The BatchAnalyzerEngine is a class in Presidio that is responsible for detecting PII entities in a batch of texts. It uses the AnalyzerEngine to process each text in the batch. Batch processing sample AnonymizerEngine The AnonymizerEngine is the main class in Presidio that is responsible for anonymizing PII entities in text. It uses the results from the AnalyzerEngine to perform the anonymization. Anonymizer concepts DeanonymizerEngine The DeanonymizerEngine is a class in Presidio that is responsible for deanonymizing text that has been anonymized by the AnonymizerEngine, given that the operation is reversible (e.g. encryption). Anonymizer concepts Operator An Operator is an object in Presidio that is responsible for performing the anonymization operation on a PII entity. Presidio provides several built-in operators, such as Replace, Redact, and Encrypt, and allows users to create custom operators. Anonymizer concepts BatchAnonymizerEngine The BatchAnonymizerEngine is a class in Presidio that is responsible for anonymizing PII entities in a batch of texts. It uses the AnonymizerEngine to perform the anonymization on each text in the batch. Sample ImageRedactorEngine The ImageRedactorEngine is a class in Presidio that is responsible for redacting PII entities in images. It leverages the AnalyzerEngine to detect PII entities in the text extracted from the images. Image redaction docs StructuredEngine The StructuredEngine is a class in Presidio that is responsible for detecting PII entities in structured data. It uses the AnalyzerEngine to detect PII entities in the text fields of the structured data. Image redaction docs"},{"location":"learn_presidio/concepts/#evaluation-concepts","title":"Evaluation concepts","text":"Concept Definition Learn More Precision Precision is a metric that measures the proportion of true positive results among the positive results. In the context of PII detection, precision measures the proportion of correctly identified PII entities among all the entities identified by the system. Evaluation docs Recall Recall is a metric that measures the proportion of true positive results among the actual positive results. In the context of PII detection, recall measures the proportion of correctly identified PII entities among all the PII entities present in the text. Evaluation docs"},{"location":"samples/","title":"Samples","text":"Topic Data Type Resource Sample Usage Text Python Notebook Presidio Basic Usage Notebook Usage Text Python Notebook Customizing Presidio Analyzer Usage Text Python Notebook Configuring The NLP engine Usage Semi-structured Python Notebook Analyzing structured / semi-structured data in batch Usage Text Python Notebook Encrypting and Decrypting identified entities Usage Text Python Notebook Getting the identified entity value using a custom Operator Usage text Python Notebook Anonymizing known values Usage Images Python Notebook Redacting Text PII from DICOM images Usage Images Python Notebook Using an allow list with image redaction Usage PDF Python Notebook Annotating PII in a PDF Usage Images Python Notebook Plot custom bounding boxes Usage Text Python Notebook Integrating with external services Usage Text Python file Remote Recognizer Usage Structured Python Notebook Presidio Structured Basic Usage Notebook Usage Text Python file Azure AI Language as a Remote Recognizer Usage Text Python file Azure Health Data Services de-identification Service as a Remote Recognizer Usage Text Python file AHDS Surrogate Example Usage CSV Python file Analyze and Anonymize CSV file Usage Text Python Using Flair as an external PII model Usage Text Python file Using Span Marker as an external PII model Usage Text Python file Using Transformers as an external PII model Usage Text Python file Pseudonymization (replace PII values using mappings) Usage Text Python file Passing a lambda as a Presidio anonymizer using Faker Usage Text Python file Synthetic data generation with OpenAI Usage Text Python file Keeping some entities from being anonymized Usage Text LiteLLM Proxy PII Masking LLM calls across Anthropic/Gemini/Bedrock/Azure, etc. Usage Text Python Notebook YAML based no-code configuration Usage Text Python file Using GLiNER within Presidio Usage REST API (postman) Presidio as a REST endpoint Deployment App Service Presidio with App Service Deployment Kubernetes Presidio with Kubernetes Deployment Spark/Azure Databricks Presidio with Spark Deployment Data Protection toolkit for OpenAI Data Protection toolkit for OpenAI Deployment Spark with Microsoft Fabric Presidio with Fabric Deployment Azure Data Factory with App Service ETL for small dataset Deployment Azure Data Factory with Databricks ETL for large datasets ADF Pipeline Azure Data Factory Add Presidio as an HTTP service to your Azure Data Factory ADF Pipeline Azure Data Factory Add Presidio on Databricks to your Azure Data Factory Demo Streamlit app Create a simple demo app using Streamlit"},{"location":"samples/deployments/","title":"Example deployments","text":"<ul> <li>Azure App Service</li> <li>Kubernetes</li> <li>Spark/Azure Databricks</li> <li>Azure Data Factory</li> <li>Data Protection toolkit for OpenAI</li> </ul>"},{"location":"samples/deployments/app-service/","title":"Deploy presidio services to an Azure App Service","text":"<p>Presidio containers can be hosted on an Azure App Service. Azure App Service provides a managed production environment, which supports docker containers and devops optimizations. It is a global scale service with built in security and compliance features that fits multiple cloud workloads. The presidio team uses Azure App Service for both its development environment and the presidio demo website.</p>"},{"location":"samples/deployments/app-service/#deploy-presidio-services-to-azure","title":"Deploy Presidio services to Azure","text":"<p>Use the following button to deploy presidio services to your Azure subscription.</p> <p></p>"},{"location":"samples/deployments/app-service/#deploy-using-command-line-script","title":"Deploy using command-line script","text":"<p>The following script can be used alternatively to the ARM template deployment above. It sets up the same components which are required for each of the presidio services (analyzer and anonymizer) as the template.</p>"},{"location":"samples/deployments/app-service/#basic-setup","title":"Basic setup","text":"<pre><code>RESOURCE_GROUP=&lt;resource group name&gt;\nAPP_SERVICE_NAME=&lt;name of app service&gt;\nLOCATION=&lt;location&gt;\nAPP_SERVICE_SKU=&lt;sku&gt;\n\nIMAGE_NAME=mcr.microsoft.com/presidio-analyzer\n# the following parameters are only required if you build and deploy your own containers from a private registry\nACR_USER_NAME=&lt;user name&gt;\nACR_USER_PASSWORD=&lt;password&gt;\n\n# create the resource group\naz group create --name $RESOURCE_GROUP\n# create the app service plan\naz appservice plan create --name $APP_SERVICE_NAME-plan --resource-group $RESOURCE_GROUP  \\\n--is-linux --location $LOCATION --sku $APP_SERVICE_SKU\n# create the web app using the official presidio images\naz webapp create --name $APP_SERVICE_NAME --plan $APP_SERVICE_NAME-plan \\\n--resource-group $RESOURCE_GROUP -i $IMAGE_NAME\n\n# or alternatively, if building presidio and deploying from a private container registry\naz webapp create --name $APP_SERVICE_NAME --plan $APP_SERVICE_NAME-plan \\\n--resource-group $RESOURCE_GROUP -i $IMAGE_NAME -s $ACR_USER_NAME -w $ACR_USER_PASSWORD\n</code></pre>"},{"location":"samples/deployments/app-service/#blocking-network-access","title":"Blocking network access","text":"<p>Use the following script to restrict network access for a specific ip such as your computer, a front-end website or an API management.</p> <pre><code>FRONT_END_IP_RANGE=[front end ip range]\naz webapp config access-restriction add --resource-group $RESOURCE_GROUP --name $APP_SERVICE_NAME \\\n  --rule-name 'Front-end allow rule' --action Allow --ip-address $FRONT_END_IP_RANGE --priority 100\n</code></pre> <p>Further network isolation, using virtual networks, is possible using an Isolated tier of Azure App Service.</p>"},{"location":"samples/deployments/app-service/#configure-app-service-logging","title":"Configure App Service Logging","text":""},{"location":"samples/deployments/app-service/#logging-to-the-app-service-file-system","title":"Logging to the App Service File System","text":"<pre><code>az webapp log config --name $APP_SERVICE_NAME --resource-group $RESOURCE_GROUP \\\n--application-logging filesystem --detailed-error-messages true \\\n--docker-container-logging filesystem --level information\n</code></pre>"},{"location":"samples/deployments/app-service/#logging-to-log-analytics-workspace","title":"Logging to Log Analytics Workspace","text":"<pre><code>LOG_ANALYTICS_WORKSPACE_RESROUCE_GROUP=&lt;resource group of log analytics&gt;\nLOG_ANALYTICS_WORKSPACE_NAME=&lt;log analytics name&gt;\n\n# create a log analytics workspace\naz monitor log-analytics workspace create --resource-group $LOG_ANALYTICS_WORKSPACE_RESROUCE_GROUP --workspace-name $LOG_ANALYTICS_WORKSPACE_NAME\n\n# query the log analytics workspace id\nLOG_ANALYTICS_WORKSPACE_ID=$(az monitor log-analytics workspace show --resource-group $LOG_ANALYTICS_WORKSPACE_RESROUCE_GROUP --workspace-name $LOG_ANALYTICS_WORKSPACE_NAME --query id -o tsv)\n# query the app service id\nAPP_SERVICE_ID=$(az monitor log-analytics workspace show --resource-group $RESOURCE_GROUP --name $APP_SERVICE_NAME --query id -o tsv)\n\n# create the diagnostics settings\naz monitor diagnostic-settings create --name $APP_SERVICE_NAME-diagnostics --resource /\n$APP_SERVICE_ID --logs   '[{\"category\": \"AppServicePlatformLogs\",\"enabled\": true}, {\"category\": \"AppServiceConsoleLogs\", \"enabled\": true}]' --metrics '[{\"category\": \"AllMetrics\",\"enabled\": true}]' --workspace $LOG_ANALYTICS_WORKSPACE_ID\n</code></pre>"},{"location":"samples/deployments/app-service/#using-an-arm-template","title":"Using an ARM template","text":"<p>Alternatively, you can use the provided ARM template which can deploy either both or any of the presidio services. Note that while Log Analytics integration with Azure App Service is in preview, the ARM template deployment will not create a Log Analytics resource or configure the diagnostics settings from the App Service to a Log Analytics workspace. To deploy the app services using the provided ARM template, fill in the provided values.json file with the required values and run the following script.</p> <pre><code>az deployment group create --resource-group $RESOURCE_GROUP --template-file presidio-services.json --parameters @values.json\n</code></pre>"},{"location":"samples/deployments/data-factory/","title":"Anonymize PII entities with Azure Data Factory","text":"<p>You can build data anonymization ETL pipelines using Azure Data Factory (ADF) and Presidio. This section provides instructions on how to leverage presidio in Azure Data Factory:</p> <ol> <li>Complete samples - Setup Azure Data Factory and Presidio in a single deployment and run the samples.</li> <li>Using Azure Data Factory Template Gallery to anonymize text files - If you already have an instance of Azure Data Factory and would like to use the built in data-anonymization-with-presidio template to anonymize text files.</li> <li>Using Azure Data Factory Template Gallery to anonymize datasets - If you already have an instance of Azure Data Factory and would like to use the built in data-anonymization-with-presidio template to anonymize csv datasets.</li> </ol>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-databricks/","title":"Anonymize PII entities in datasets using Azure Data Factory template and Presidio on Databricks","text":"<p>This sample uses the built in data anonymization template of Azure Data Factory (which is a part of the Template Gallery) to copy a csv dataset from one location to another, while anonymizing PII data from a text column in the dataset. It leverages the code for using Presidio on Azure Databricks to call Presidio as a Databricks notebook job in the Azure Data Factory (ADF) pipeline to transform the input dataset before merging the results to an Azure Blob Storage.</p> <p>Note that this solution is capable of transforming large datasets. For smaller, text based input you may want to work with the Data Anonymization with Presidio as an HTTP service template which offers an easier deployment for Presidio.</p> <p>The sample deploys the following Azure Services:</p> <ul> <li>Azure Storage - The target storage account where data will be persisted.</li> <li>Azure Databricks - Host presidio to anonymize the data.</li> </ul> <p>Additionally you should already have an instance of Azure Data Factory which hosts and orchestrates the transformation pipeline and a storage account which holds the source files.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-databricks/#about-this-solution-template","title":"About this Solution Template","text":"<p>This template gets the files from your source file-based store. It then anonymizes the content and uploads each of them to the destination store.</p> <p>The template contains three activities:</p> <ul> <li>AnonymizeSource runs the presidio notebook on the input file to create an output folder with csv parts.</li> <li>MergeAnonymizedToTarget merges the csv parts from databricks output folder to a single csv on the target storage container.</li> <li>DeleteAnonymized deletes the temporary output folder.</li> </ul> <p>The template defines four parameters:</p> <ul> <li>SourceStore_Location is the container name of your source store where you want to move files from (STORAGE_CONTAINER_NAME).</li> <li>DestinationStore_Name is the container name in the target storage account which is provisioned by the ARM template.</li> <li>SourceFile_Name is the name of the input file.</li> <li>TextColumn_Name is the name of the column to be anonymized.</li> </ul>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-databricks/#how-to-use-this-solution-template","title":"How to use this Solution Template","text":"<p>To use this template you should first setup the required infrastructure for the sample to run, then setup the template in Azure Data Factory.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-databricks/#setup-presidio","title":"Setup Presidio","text":"<p>Provision and setup the databricks cluster by following the Deploy and Setup steps in presidio-spark sample. Take a note of the authentication token and do not follow the \"Running the sample\" steps.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-databricks/#setup-azure-data-factory","title":"Setup Azure Data Factory","text":"<ol> <li> <p>Go to the Data anonymization with Presidio on Databricks template. Select the AnonymizedCSV connection (Azure Storage) and select \"New\" from the drop down menu. </p> </li> <li> <p>Name the service \"PresidioStorage\" and select the storage account that was created in the previous steps from your subscription. Note that Target source was also selected as the sample uses the same storage account for both source and target. </p> </li> <li> <p>Select the Anonymize Source connection (Databricks) and select \"New\" from the drop down menu. </p> </li> <li> <p>Name the service \"PresidioDatabricks\" and select the Azure Databricks workspace that was created in the previous steps from your subscription. Follow through the steps to input the authentication token which was generated in the previous step, or create a new one by following this guide. Select presidio_cluster to run the job. </p> </li> <li> <p>Select Use this template tab</p> </li> <li> <p>You'll see the pipeline, as in the following example: </p> </li> <li> <p>Upload a csv file to the storage container and select Debug, enter the Parameters, and then select Finish. The parameters are the container where you want to move files from, the container name where you want to move the anonymized files to, the csv file name and the name of a text column in the csv file. </p> </li> <li> <p>Review the result. </p> </li> </ol>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-http/","title":"Anonymize PII entities in text using Azure Data Factory template and Presidio as an HTTP service","text":"<p>This sample uses the built in data anonymization template of Azure Data Factory which is a part of the Template Gallery to move a set of text files from one location to another while anonymizing their content. It leverages the code for using Presidio on Azure App Service to call Presidio as an HTTP REST endpoint in the Azure Data Factory (ADF) pipeline while parsing and storing each file as an Azure Blob Storage.</p> <p>Note that given the solution architecture which call presidio services using HTTP, this sample should be used for up to 5000 files, each up to 200KB in size. The restrictions are based on ADF lookup-activity which is used to iterate the files in the storage container (up to 5000 records), and having Presidio as an HTTP endpoint with text being sent over network to be anonymized. For larger sets please work with the Data Anonymization with Presidio on Databricks template.</p> <p>The sample deploys the following Azure Services:</p> <ul> <li>Azure KeyVault - Holds the access keys for Azure Storage to avoid having keys and secrets in the code.</li> <li>Azure Storage - The target storage account where data will be persisted.</li> <li>Azure App Service - Host presidio to anonymize the data.</li> </ul> <p>Additionally you should already have an instance of Azure Data Factory which host and orchestrate the transformation pipeline and a storage account which holds the source files.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-http/#about-this-solution-template","title":"About this Solution Template","text":"<p>This template gets the files from your source file-based store. It then anonymizes the content and uploads each of them to the destination store.</p> <p>The template contains eight activities:</p> <ul> <li>GetMetadata gets the list of objects including the files and subfolders from your folder on source store. It will not retrieve the objects recursively.</li> <li>Filter filter the objects list from GetMetadata activity to select the files only.</li> <li>GetSASToken gets the target storage account SAS token from the Azure Key Vault.</li> <li>ForEach gets the file list from the Filter activity and then iterates over the list and passes each file to the Anonymization activities.</li> <li>LoadFileContent loads the content of a text file to an ADF Lookup.</li> <li>PresidioAnalyze sends the text content to Presidio Analyzer.</li> <li>PresidioAnonymize sends the text and the analysis response to Presidio Anonymizer to get the anonymized text.</li> <li>UploadBlob uses Azure Blob Storage REST API to upload the anonymized text to the target container.</li> </ul> <p>The template defines four parameters:</p> <ul> <li>SourceStore_Location is the container name of your source store where you want to move files from.</li> <li>DestinationStore_Name is the name of the target storage account which is provisioned by the ARM template.</li> <li>DestinationStore_Location is the container name of your destination store where you want to move files to. it has a default value of a container which was created during provisioning of the ARM template (presidio).</li> <li>KeyVault_Name is the name of the Azure Key Vault which is provisioned by the ARM template.</li> <li>Analyzer_Url is the URL for the Analyzer App Service which is provisioned by the ARM template.</li> <li>Anonymizer_Url is the URL for the Anonymizer App Service which is provisioned by the ARM template.</li> </ul>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-http/#how-to-use-this-solution-template","title":"How to use this Solution Template","text":"<p>To use this template you should first setup the required infrastructure for the sample to run, then setup the template in Azure Data Factory.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-http/#setup-presidio","title":"Setup Presidio","text":"<p>Create the Azure App Service, the storage accounts and an Azure Key Vault by clicking the Deploy-to-Azure button, or by running the following script to provision the provided ARM template.</p> <p></p> <pre><code>RESOURCE_GROUP=[Name of resource group]\nLOCATION=[location of resources]\n\naz group create --name $RESOURCE_GROUP --location $LOCATION\naz deployment group create -g $RESOURCE_GROUP --template-file ./arm-templates/azure-deploy-adf-template-gallery-http.json\n</code></pre> <p>Note that:</p> <ul> <li>A SAS token keys is created and read from Azure Storage and then imported to Azure Key Vault. Using ARM template built in functions: listAccountSas. This token is time limited.</li> <li>An access policy grants the Azure Data Factory managed identity access to the Azure Key Vault. You should provide your ADF client principal ID by following this guide.</li> </ul>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-http/#setup-azure-data-factory","title":"Setup Azure Data Factory","text":"<ol> <li> <p>Go to the Data anonymization with Presidio as an HTTP service template. Select existing connection or create a New connection to your source file store where you want to move files from. Be aware that DataSource_Folder and DataSource_File are reference to the same connection of your source file store. </p> </li> <li> <p>Select Use this template tab</p> </li> <li> <p>You'll see the pipeline, as in the following example: </p> </li> <li> <p>Select Debug, enter the Parameters, and then select Finish. The parameters are the container where you want to move files from and the container path where you want to move the anonymized files to. </p> </li> <li> <p>Review the result. </p> </li> </ol>"},{"location":"samples/deployments/data-factory/presidio-data-factory/","title":"Anonymize PII entities in an Azure Data Factory ETL Pipeline","text":"<p>The following samples showcase two scenarios which use Azure Data Factory (ADF) to move a set of JSON objects from an online location to an Azure Storage while anonymizing their content. The first sample leverages the code for using Presidio on Azure App Service to call Presidio as an HTTP REST endpoint in the ADF pipeline while parsing and storing each file as an Azure Blob Storage. The second sample leverage the code for using Presidio on spark to run over a set of files on an Azure Blob Storage to anonymize their content, in the case of having a large data set that requires the scale of databricks.</p> <p>The samples deploy and use the following Azure Services:</p> <ul> <li>Azure Data Factory - Host and orchestrate the transformation pipeline.</li> <li>Azure KeyVault - Holds the access keys for Azure Storage to avoid having keys and secrets in the code.</li> <li>Azure Storage - Persistence layer of this sample.</li> <li>Azure Databricks/ Azure App Service - Host presidio to anonymize the data.</li> </ul> <p>The input file used by the samples is hosted on presidio-research repository. It is setup as a variable on the provided ARM template and used by Azure Data Factory as the input source.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#option-1-presidio-as-an-http-rest-endpoint","title":"Option 1: Presidio as an HTTP REST endpoint","text":"<p>By using Presidio as an HTTP endpoint, the user can select which infrastructure best suits their requirements. in this sample, Presidio is deployed to an Azure App Service, but other deployment targets can be used, such as kubernetes.</p> <p></p>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#deploy-the-arm-template","title":"Deploy the ARM template","text":"<p>Create the Azure App Service and the ADF pipeline by clicking the Deploy-to-Azure button, or by running the following script to provision the provided ARM template.</p> <p></p> <pre><code>RESOURCE_GROUP=[Name of resource group]\nLOCATION=[location of resources]\n\naz group create --name $RESOURCE_GROUP --location $LOCATION\naz deployment group create -g $RESOURCE_GROUP --template-file ./arm-templates/azure-deploy-adf-app-service.json\n</code></pre> <p>Note that:</p> <ul> <li>A SAS token keys is created and read from Azure Storage and then imported to Azure Key Vault. Using ARM template built in functions: listAccountSas.</li> <li>An access policy grants the Azure Data Factory managed identity access to the Azure Key Vault by using ARM template reference function to the Data Factory object and acquire its identity.principalId property. This is enabled by setting the data factory ARM resource's identity attribute to managed identity (SystemAssigned).</li> </ul>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#about-this-solution-template","title":"About this Solution Template","text":"<p>This template gets a collection of JSON documents from a file on GitHub. It then extracts one of the text fields of the document, anonymizes the content and uploads it as a text file to the destination store.</p> <p>The template contains seven activities:</p> <ul> <li>GetDataSet-\u200aCopy the dataset from GitHub to the first folder on the Azure Storage blob container (/dataset).</li> <li>LoadSet-\u200aLoads the dataset into the Azure Data Factory memory for processing in a for-each loop.</li> <li>GetSASToken\u200a-\u200aGet the SAS token from Azure Key Vault. This will be used later for writing to the blob container.</li> <li>SaveBlobs\u200a-\u200aIs a For-Each loop activity. It includes a clause which is executed for each document in the array.</li> <li>PresidioAnalyze\u200a-\u200aSends the text to presidio analyzer endpoint.</li> <li>PresidioAnonymize\u200a-\u200aSends the response from presidio analyzer to presidio anonymizer endpoint.</li> <li>UploadBlob\u200a-\u200aSaves the anonymized response from presidio to a randomly named text file on the target Azure Blob Storage.</li> </ul>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#option-2-presidio-on-azure-databricks","title":"Option 2: Presidio on Azure Databricks","text":"<p>By using Presidio as a Notebook step in ADF, we allow Databricks to scale presidio according to the cluster capabilities and the input dataset. Using presidio as a native python package in pyspark can unlock more analysis and de-identification scenarios.</p> <p></p>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#pre-requisite-deploy-azure-databricks","title":"Pre-requisite - Deploy Azure Databricks","text":"<p>Provision and setup the databricks cluster by following the steps in presidio-spark sample. Note the output key and export it as DATABRICKS_TOKEN environment variable.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#deploy-the-arm-template_1","title":"Deploy the ARM template","text":"<p>Create the rest of the services by running the following script which uses the provided ARM template.</p> <pre><code>RESOURCE_GROUP=[Name of resource group]\nLOCATION=[location of resources]\nDATABRICKS_HOST=https://$DATABRICKS_WORKSPACE_URL\nDATABRICKS_CLUSTER_ID=$(databricks clusters get --cluster-name presidio_cluster | jq -r .cluster_id)\nDATABRICKS_NOTEBOOK_LOCATION=\"/notebooks/01_transform_presidio\"\n\naz deployment group create -g $RESOURCE_GROUP --template-file ./arm-templates/azure-deploy-adf-databricks.json --parameters Databricks_accessToken=$DATABRICKS_TOKEN Databricks_clusterId=$DATABRICKS_CLUSTER_ID Databricks_notebookLocation=$DATABRICKS_NOTEBOOK_LOCATION Databricks_workSpaceUrl=$DATABRICKS_HOST AzureBlobStorage_accountName=$STORAGE_ACCOUNT_NAME AzureBlobStorage_cotainerName=$STORAGE_CONTAINER_NAME\n</code></pre> <p>Note that: Two keys are read from Azure Storage and imported to Azure Key Vault, the account Access Token and a SAS token, using ARM template built in functions: listAccountSas and listKeys.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#about-this-solution-template_1","title":"About this Solution Template","text":"<p>This template gets a collection of JSON documents from a file on GitHub. It then extracts one of the text fields of the document and saves it to a text file on a temporary folder in the storage account (un-anonymized content). It then runs a spark notebook job that anonymizes the content of the files in that folder and saves the result as csv files on the destination store.</p> <p>The template contains seven activities:</p> <ul> <li>GetDataSet\u200a-\u200aCopy the dataset from GitHub to the first folder on the Azure Storage blob container (/dataset).</li> <li>GetSASToken\u200a-\u200aGet the SAS token from Azure Key Vault. This will be used later for writing to the blob container.</li> <li>LoadSet\u200a-\u200a Loads the dataset into the Azure Data Factory memory for processing in a for-each loop.</li> <li>SaveBlobs\u200a-\u200aIs a For-Each loop activity. It includes a clause which is executed for each document in the array.</li> <li>UploadBlob\u200a-\u200aSaves the text file on a temporary container on the target Azure Blob Storage</li> <li>GetSecret\u200a-\u200aGet the storage account secret from Azure Key Vault. This will be used later for accessing the blob container from databricks</li> <li>Presidio-Anonymize\u200a-\u200aIs a databricks spark job which runs presidio on the temporary storage container. the result of this job is a new container (/output) with csv files that contain the anonymized text.</li> </ul>"},{"location":"samples/deployments/k8s/","title":"Deploy presidio to Kubernetes","text":"<p>You can install Presidio locally using KIND, as a service in Kubernetes or AKS.</p> <ul> <li>Deploy locally using KIND</li> <li>Deploy with Kubernetes</li> <li>Prerequisites</li> <li>Step by Step Deployment with customizable parameters</li> </ul>"},{"location":"samples/deployments/k8s/#deploy-locally-with-kind","title":"Deploy locally with KIND","text":"<p>KIND (Kubernetes IN Docker).</p> <ol> <li> <p>Install Docker.</p> </li> <li> <p>Clone Presidio.</p> </li> <li> <p>Run the following script, which will use KIND (Kubernetes emulation in Docker)</p> </li> </ol> <pre><code>cd docs/samples/deployments/k8s/deployment/\n./run-with-kind.sh\n</code></pre> <ol> <li>Wait and verify all pods are running:</li> </ol> <pre><code>kubectl get pod -n presidio\n</code></pre> <ol> <li>Port forwarding of HTTP requests to the API micro-service will be done automatically. In order to run manual:</li> </ol> <pre><code>kubectl port-forward &lt;presidio-analyzer-pod-name&gt; 8080:8080 -n presidio\n</code></pre>"},{"location":"samples/deployments/k8s/#presidio-as-a-service-with-kubernetes","title":"Presidio As a Service with Kubernetes","text":""},{"location":"samples/deployments/k8s/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Kubernetes 1.18+ cluster with RBAC enabled. If you are using AKS RBAC is enabled by default.</li> </ol> <p>!!! note: Note       Note the pod's resource requirements (CPU and memory) and plan the cluster accordingly.</p> <ol> <li> <p>kubectl installed. Verify you can communicate with the cluster by running:</p> <pre><code>kubectl version\n</code></pre> </li> <li> <p>Local helm client.</p> </li> <li>Optional - Container Registry - such as ACR. Only needed if you are using your own presidio images and not the default ones from from Microsoft syndicates container catalog</li> <li>Recent presidio repo is cloned on your local machine.</li> </ol>"},{"location":"samples/deployments/k8s/#step-by-step-deployment-with-customizable-parameters","title":"Step by step deployment with customizable parameters","text":"<ol> <li> <p>Install Helm with RBAC.</p> </li> <li> <p>Optional - Ingress controller for presidio API, e.g., NGINX.</p> </li> </ol> <p>NOTE: Presidio is deployed with an ingress controller by default, and uses <code>nginx</code> as <code>ingress.class</code>.  To change this behavior, deploy the helm chart with <code>ingress.enabled=false</code>.</p> <ol> <li>Deploy from <code>/docs/samples/deployments/k8s/charts/presidio</code></li> </ol> <pre><code># Choose a namespace and ensure it is created\nNAMESPACE=presidio\n\n# Choose the tag, from mcr.microsoft.com, e.g. `latest`\nTAG=latest\n\n# Choose a name for the deployment\nNAME=&lt;name&gt;\n\n# Use Helm to install all required components\nhelm install $NAME . --set tag=$PRESIDIO_LABEL --namespace $NAMESPACE\n\n# If you have your own images in a separate ACR, run\nDOCKER_REGISTRY=&lt;your_registry&gt;\nhelm install $NAME . --set registry=$DOCKER_REGISTRY,tag=$PRESIDIO_LABEL . --namespace $NAMESPACE\n</code></pre>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/","title":"Invisio - Data Protection toolkit for OpenAI","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/#sample-overview","title":"Sample Overview","text":"<p>In this sample, we will demonstrate best practices for creating meaningful conversations with OpenAI while ensuring that all Personally Identifiable Information (PII) is anonymized before being sent to the OpenAI API. Additionally, we will cover the process of de-anonymizing the data upon receiving the response.</p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/#problem-statement","title":"Problem Statement","text":"<p>The primary challenge we aim to address is: how can we ensure that the responses processed by OpenAI, which are also anonymized, remain meaningful to the user? </p> <p>To tackle this issue, we provide a reversible and repeatable AI Design Win which handles the anonymization process for sensitive data. In other words, we implemented a de-anonymization middleware that maps the anonymized entities, allowing us to de-anonymize the data effectively upon receiving it back.</p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/#core-concept","title":"Core Concept","text":"<p>The core concept of the toolkit revolves around sessions, which encapsulate the anonymization context. This context defines how each Personally Identifiable Information (PII) entity is anonymized. By maintaining consistency within the same session, the toolkit ensures that the same PII is always anonymized in the same manner. Moreover, it facilitates the de-anonymization of data as needed.</p> <p>This repository serves as a comprehensive sample that you can use as a foundation and extend according to your specific requirements.</p> <p>The toolkit is particularly useful in \"Chat with your data\" scenarios within enterprises, ensuring that personally identifiable information (PII) present in corporate documents is not exposed to users during queries.</p> <p>The following sections will describe the required infrastructure and code to achieve the above functionality.</p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/#workflow-example","title":"Workflow Example","text":"<p>The following is the desired output. We can input PII data annoymized to the LLM input and return the replaced entities back to the original in the response back to the user.</p> <ul> <li>User Input: \"Hello world, my name is Jane Doe. My number is: 034453334\"</li> <li>Anonymized LLM Input: \"Hello world, my name is [PERSON]. My number is: [PHONE_NUMBER]\"</li> <li>LLM Response: \"Hey [PERSON], nice to meet you!\"</li> <li>De-anonymized User Response: \"Hey Jane Doe, nice to meet you!\"</li> </ul>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/#demo-client-application","title":"Demo Client Application","text":"<p>The demo client application showcases the capabilities of the Anonymization Toolkit within the context of an \"Anonymous Chat with a Large Language Model (LLM)\" use case.</p> <p>In this scenario, the toolkit anonymizes Personally Identifiable Information (PII) in the messages sent to the LLM and subsequently de-anonymizes the responses received from it. This approach ensures that the LLM never has access to actual PII, while still facilitating meaningful conversations.</p> <p></p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/#architecture-and-infrastructure","title":"Architecture and Infrastructure","text":"<p>The solution consists of the following components:</p> <ul> <li>Api (Deployed in AKS)</li> <li>Client</li> <li>Redis</li> </ul> <p></p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/#code","title":"Code","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/#anonymization","title":"Anonymization","text":"<p>First we need to setup our <code>Presidio</code> engines to aid with analysis, anonoymization and deanonymization. The following code can exist as part of a service available to the appropriate api endpoints.</p> <pre><code>        configuration = {\n            \"nlp_engine_name\": \"spacy\",\n            \"models\": [\n                {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n                {\"lang_code\": \"nl\", \"model_name\": \"nl_core_news_sm\"},\n                {\"lang_code\": \"es\", \"model_name\": \"es_core_news_sm\"},\n            ],\n        }\n        provider = NlpEngineProvider(nlp_configuration=configuration)\n        nlp_engine = provider.create_engine()\n\n        self.analyzer = AnalyzerEngine(\n            nlp_engine=nlp_engine,\n            supported_languages=[\"en\", \"nl\", \"es\"]\n        )\n        self.anonymizer = AnonymizerEngine()\n        self.anonymizer.add_anonymizer(InstanceCounterAnonymizer)\n        self.deanonymizer = DeanonymizeEngine()\n        self.deanonymizer.add_deanonymizer(InstanceCounterDeanonymizer)\n</code></pre> <p><code>InstanceCounterAnonymizer</code> and <code>InstanceCounterDeanonymizer</code> taken from these examples.</p> <p>We anonymize data with the following:</p> <pre><code>    def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -&gt; Tuple[str, List[OperatorResult], dict] :\n        \"\"\" Anonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Anonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            results = self.analyzer.analyze(text=text, language=language)\n            logger.info(f\"Analyze took {timer() - start_time:.3f} seconds for session_id: {session_id}\")\n\n            anonymizer_start_time = timer()\n            anonymizer_entity_mapping = entity_mappings.copy() if entity_mappings is not None else dict()\n            anonymized_result = self.anonymizer.anonymize(\n                text=text,\n                analyzer_results=results,\n                operators={\n                    \"DEFAULT\": OperatorConfig(\n                        \"entity_counter\", {\"entity_mapping\": anonymizer_entity_mapping}\n                    )\n                },\n            )\n            logger.info(f\"Anonymize took {timer() - anonymizer_start_time:.3f} seconds for session_id: {session_id}\")\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n\n            return anonymized_result.text, anonymizer_entity_mapping\n        except Exception as e:\n            logger.exception(f\"Error in anonymize_text for session_id {session_id}\")\n            raise\n</code></pre> <p>and deanonymize with:</p> <pre><code>    def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -&gt; str:\n        \"\"\" Deanonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Deanonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            entities = self.get_entities(entity_mappings, text)\n\n            deanonymized = self.deanonymizer.deanonymize(\n                text=text,\n                entities=entities,\n                operators=\n                {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\", \n                                        params={\"entity_mapping\": entity_mappings})}\n            )\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n\n            return deanonymized.text\n        except Exception as e:\n            logger.exception(f\"Error in deanonymize_text for session_id {session_id}\")\n            raise\n\n    def get_entities(self, entity_mappings: dict, text: str) -&gt; List[OperatorResult]:\n        \"\"\" Get the entities from the entity mappings \"\"\"\n\n        entities = []\n        for entity_type, entity_mapping in entity_mappings.items():\n            for entity_value, entity_id in entity_mapping.items():\n                start_index = 0\n                while True:\n                    start_index = text.find(entity_id, start_index)\n                    if start_index == -1:\n                        break\n                    end_index = start_index + len(entity_id)\n                    entities.append(OperatorResult(start_index, end_index, entity_type, entity_value, entity_id))\n                    start_index += len(entity_id)\n        return entities\n</code></pre> <p>The above code would be called with:</p> <pre><code>    # anonymize example\n    anonymized_text, new_entity_mappings = presidio_service.anonymize_text(\n            request.session_id, request.text, request.language, entity_mappings\n        )\n\n    # deanonymize example\n    deanonymized_text = presidio_service.deanonymize_text(\n            request.session_id, request.text, entity_mappings\n        )\n</code></pre> <p>Where:</p> <ul> <li><code>request.session_id</code></li> <li>A unique session identifier, e.g. guid.</li> <li><code>request.text</code></li> <li>The text to be anonymized or deanonymized.</li> <li><code>request.language</code></li> <li>The language code to analyze with. Note this requires the appropriate language model to be setup during the configuration. See code above. Example codes, <code>en</code>, <code>es</code>, <code>nl</code>.</li> <li><code>entity_mappings</code></li> <li> <p>A dictionary with the structure of:</p> <pre><code>    {'LOCATION': {'Redmond': '&lt;LOCATION_0&gt;'}, 'PERSON': {'Joe Bloggs': '&lt;PERSON_0&gt;'}}\n</code></pre> </li> <li> <p>Each item is the entity type and the value contains each instance of the entity and its replacement.</p> </li> <li>This dictionary is optional. On the first request of a session we wont have any mappings, subsequent requests can then build upon this dictionary to maintain knowledge of previous request mappings.</li> </ul>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/#repository-structure","title":"Repository structure","text":"<ul> <li>src/api - The toolkit API, which is responsible for performing anonymization and deanonymization, as well as managing user sessions in a persistent storage.</li> <li>src/client_app/ - The demo client app, which enables users to engage in an anonymous chat with an LLM. The app utilizes the toolkit API to hide sensitive data from the LLM. It can be run either in a terminal or as a website.</li> <li>infrastructure/ - Bicep definition for Azure infrastructure required to support deployment to a Kubernetes (k8s) cluster.</li> <li>deployments/ - The manifests necessary to deploy the Toolkit API and the Demo to a k8s cluster.</li> <li>spikes/ - Experimental code, such as a Streamlit chat application that can interact with the toolkit API in a similar manner to the demo client app.</li> </ul>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/docs/sample_for_presidio_pr/anonymization_toolkit_sample/","title":"Anonymization Toolkit Sample","text":"<p>This sample introduces a toolkit designed to: </p> <ul> <li>Anonymize sensitive data before sending it to large language models (LLMs)</li> <li>De-anonymize the received data while maintaining confidentiality.</li> </ul> <p>The toolkit is particularly useful in \"Chat with your data\" scenarios within enterprises, ensuring that personally identifiable information (PII) present in corporate documents is not exposed to users during queries.</p> <p>The following sections will describe the required infrastructure and code to achieve the above functionality.</p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/docs/sample_for_presidio_pr/anonymization_toolkit_sample/#target-audience","title":"Target Audience","text":"<p>The toolkit serves enterprises that: Require PII to remain internal while leveraging external OpenAI APIs, and Aim to safeguard PII from being displayed post-OpenAI processing.</p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/docs/sample_for_presidio_pr/anonymization_toolkit_sample/#workflow-example","title":"Workflow Example","text":"<p>The following is the desired output. We can input PII data annoymized to the LLM input and return the replaced entities back to the original in the response back to the user.</p> <ul> <li>User Input: \"Hello world, my name is Jane Doe. My number is: 034453334\"</li> <li>Anonymized LLM Input: \"Hello world, my name is [PERSON]. My number is: [PHONE_NUMBER]\"</li> <li>LLM Response: \"Hey [PERSON], nice to meet you!\"</li> <li>De-anonymized User Response: \"Hey Jane Doe, nice to meet you!\"</li> </ul>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/docs/sample_for_presidio_pr/anonymization_toolkit_sample/#infrastructure","title":"Infrastructure","text":"<p>The solution consists of the following components:</p> <ul> <li>Api</li> <li>Client</li> <li>Redis</li> </ul> <p>TODO complete this section</p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/docs/sample_for_presidio_pr/anonymization_toolkit_sample/#code","title":"Code","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/docs/sample_for_presidio_pr/anonymization_toolkit_sample/#anonymization","title":"Anonymization","text":"<p>First we need to setup our <code>Presidio</code> engines to aid with analysis, anonoymization and deanonymization. The following code can exist as part of a service available to the appropriate api endpoints.</p> <pre><code>        configuration = {\n            \"nlp_engine_name\": \"spacy\",\n            \"models\": [\n                {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n                {\"lang_code\": \"nl\", \"model_name\": \"nl_core_news_sm\"},\n                {\"lang_code\": \"es\", \"model_name\": \"es_core_news_sm\"},\n            ],\n        }\n        provider = NlpEngineProvider(nlp_configuration=configuration)\n        nlp_engine = provider.create_engine()\n\n        self.analyzer = AnalyzerEngine(\n            nlp_engine=nlp_engine,\n            supported_languages=[\"en\", \"nl\", \"es\"]\n        )\n        self.anonymizer = AnonymizerEngine()\n        self.anonymizer.add_anonymizer(InstanceCounterAnonymizer)\n        self.deanonymizer = DeanonymizeEngine()\n        self.deanonymizer.add_deanonymizer(InstanceCounterDeanonymizer)\n</code></pre> <p><code>InstanceCounterAnonymizer</code> and <code>InstanceCounterDeanonymizer</code> taken from these examples.</p> <p>We anonymize data with the following:</p> <pre><code>    def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -&gt; Tuple[str, List[OperatorResult], dict] :\n        \"\"\" Anonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Anonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            results = self.analyzer.analyze(text=text, language=language)\n            logger.info(f\"Analyze took {timer() - start_time:.3f} seconds for session_id: {session_id}\")\n\n            anonymizer_start_time = timer()\n            anonymizer_entity_mapping = entity_mappings.copy() if entity_mappings is not None else dict()\n            anonymized_result = self.anonymizer.anonymize(\n                text=text,\n                analyzer_results=results,\n                operators={\n                    \"DEFAULT\": OperatorConfig(\n                        \"entity_counter\", {\"entity_mapping\": anonymizer_entity_mapping}\n                    )\n                },\n            )\n            logger.info(f\"Anonymize took {timer() - anonymizer_start_time:.3f} seconds for session_id: {session_id}\")\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n\n            return anonymized_result.text, anonymizer_entity_mapping\n        except Exception as e:\n            logger.exception(f\"Error in anonymize_text for session_id {session_id}\")\n            raise\n</code></pre> <p>and deanonymize with:</p> <pre><code>    def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -&gt; str:\n        \"\"\" Deanonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Deanonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            entities = self.get_entities(entity_mappings, text)\n\n            deanonymized = self.deanonymizer.deanonymize(\n                text=text,\n                entities=entities,\n                operators=\n                {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\", \n                                        params={\"entity_mapping\": entity_mappings})}\n            )\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n\n            return deanonymized.text\n        except Exception as e:\n            logger.exception(f\"Error in deanonymize_text for session_id {session_id}\")\n            raise\n\n    def get_entities(self, entity_mappings: dict, text: str) -&gt; List[OperatorResult]:\n        \"\"\" Get the entities from the entity mappings \"\"\"\n\n        entities = []\n        for entity_type, entity_mapping in entity_mappings.items():\n            for entity_value, entity_id in entity_mapping.items():\n                start_index = 0\n                while True:\n                    start_index = text.find(entity_id, start_index)\n                    if start_index == -1:\n                        break\n                    end_index = start_index + len(entity_id)\n                    entities.append(OperatorResult(start_index, end_index, entity_type, entity_value, entity_id))\n                    start_index += len(entity_id)\n        return entities\n</code></pre> <p>The above code would be called with:</p> <pre><code>    # anonymize example\n    anonymized_text, new_entity_mappings = presidio_service.anonymize_text(\n            request.session_id, request.text, request.language, entity_mappings\n        )\n\n    # deanonymize example\n    deanonymized_text = presidio_service.deanonymize_text(\n            request.session_id, request.text, entity_mappings\n        )\n</code></pre> <p>Where:</p> <ul> <li><code>request.session_id</code></li> <li>A unique session identifier, e.g. guid.</li> <li><code>request.text</code></li> <li>The text to be anonymized or deanonymized.</li> <li><code>request.language</code></li> <li>The language code to analyze with. Note this requires the appropriate language model to be setup during the configuration. See code above. Example codes, <code>en</code>, <code>es</code>, <code>nl</code>.</li> <li><code>entity_mappings</code></li> <li> <p>A dictionary with the structure of:</p> <pre><code>    {'LOCATION': {'Redmond': '&lt;LOCATION_0&gt;'}, 'PERSON': {'Joe Bloggs': '&lt;PERSON_0&gt;'}}\n</code></pre> </li> <li> <p>Each item is the entity type and the value contains each instance of the entity and its replacement.</p> </li> <li>This dictionary is optional. On the first request of a session we wont have any mappings, subsequent requests can then build upon this dictionary to maintain knowledge of previous request mappings.</li> </ul>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/","title":"API Readme","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/#install-requirements","title":"Install requirements","text":"<pre><code>cd src/api\npip install -r requirements.txt\n</code></pre>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/#set-environment-variables","title":"Set environment variables","text":"<p>Add <code>.env</code> file in the <code>/src/api</code> folder:</p> <pre><code>cp .env.sample .env\n</code></pre> <p>Then edit this configuration according to your setup.</p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/#run-locally","title":"Run locally","text":"<pre><code>uvicorn main:app --host 0.0.0.0 --port 8080\n</code></pre>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/#run-as-a-docker-container","title":"Run as a docker container","text":"<pre><code>docker build -t api .\ndocker run -d -p 8080:80 --env-file .env api\n</code></pre>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/__init__/","title":"init","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/main/","title":"Main","text":"In\u00a0[\u00a0]: Copied! <pre>import logging\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Any, Optional\n</pre> import logging from fastapi import FastAPI, Depends, HTTPException from pydantic import BaseModel from typing import Any, Optional In\u00a0[\u00a0]: Copied! <pre>from services.presidio.python_presidio_service import PythonPresidioService\nfrom services.presidio.hybrid_presidio_service import HybridPresidioService\nfrom services.presidio.http_presidio_service import HttpPresidioService\nfrom services.toolkit_service import ToolkitService\nfrom services.state.redis_state_service import RedisStateService\n</pre> from services.presidio.python_presidio_service import PythonPresidioService from services.presidio.hybrid_presidio_service import HybridPresidioService from services.presidio.http_presidio_service import HttpPresidioService from services.toolkit_service import ToolkitService from services.state.redis_state_service import RedisStateService In\u00a0[\u00a0]: Copied! <pre>app = FastAPI()\n</pre> app = FastAPI() In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n</pre> logger = logging.getLogger(__name__) logging.basicConfig(level=logging.INFO) In\u00a0[\u00a0]: Copied! <pre>class AnonymizeRequest(BaseModel):\n    text: str\n    session_id: Optional[str] = None\n    language: Optional[str] = \"en\"\n</pre> class AnonymizeRequest(BaseModel):     text: str     session_id: Optional[str] = None     language: Optional[str] = \"en\" In\u00a0[\u00a0]: Copied! <pre>class AnonymizeResponse(BaseModel):\n    session_id: str\n    text: str\n</pre> class AnonymizeResponse(BaseModel):     session_id: str     text: str In\u00a0[\u00a0]: Copied! <pre>class DeanonymizeRequest(BaseModel):\n    text: str\n    session_id: str\n</pre> class DeanonymizeRequest(BaseModel):     text: str     session_id: str In\u00a0[\u00a0]: Copied! <pre>class DeanonymizeResponse(BaseModel):\n    text: str\n</pre> class DeanonymizeResponse(BaseModel):     text: str In\u00a0[\u00a0]: Copied! <pre>presidio_service = PythonPresidioService()\n# presidio_service = HttpPresidioService()\n# presidio_service = HybridPresidioService()\nstate_service = RedisStateService()\ntoolkit_service = ToolkitService(presidio_service, state_service)\n</pre> presidio_service = PythonPresidioService() # presidio_service = HttpPresidioService() # presidio_service = HybridPresidioService() state_service = RedisStateService() toolkit_service = ToolkitService(presidio_service, state_service) In\u00a0[\u00a0]: Copied! <pre>def get_toolkit_service() -&gt; ToolkitService:\n    return toolkit_service\n</pre> def get_toolkit_service() -&gt; ToolkitService:     return toolkit_service In\u00a0[\u00a0]: Copied! <pre>@app.post(\"/anonymize\", response_model=AnonymizeResponse)\nasync def anonymize_endpoint(\n    request: AnonymizeRequest,\n    toolkit_service: ToolkitService = Depends(get_toolkit_service)\n) -&gt; Any:\n    \"\"\"Anonymize the given text using Toolkit service\"\"\"\n\n    logger.info(f\"Anonymize endpoint called with session_id: '{request.session_id}'\")\n\n    try:\n        result = toolkit_service.anonymize(\n            text=request.text, session_id=request.session_id, language=request.language\n        )\n        return result\n    except Exception as e:\n        logger.exception(f\"Error during anonymization: {e}\")\n        raise HTTPException(\n            status_code=500, detail=\"An error occurred during anonymization\"\n        )\n</pre> @app.post(\"/anonymize\", response_model=AnonymizeResponse) async def anonymize_endpoint(     request: AnonymizeRequest,     toolkit_service: ToolkitService = Depends(get_toolkit_service) ) -&gt; Any:     \"\"\"Anonymize the given text using Toolkit service\"\"\"      logger.info(f\"Anonymize endpoint called with session_id: '{request.session_id}'\")      try:         result = toolkit_service.anonymize(             text=request.text, session_id=request.session_id, language=request.language         )         return result     except Exception as e:         logger.exception(f\"Error during anonymization: {e}\")         raise HTTPException(             status_code=500, detail=\"An error occurred during anonymization\"         ) In\u00a0[\u00a0]: Copied! <pre>@app.post(\"/deanonymize\", response_model=DeanonymizeResponse)\nasync def deanonymize_endpoint(\n    request: DeanonymizeRequest,\n    toolkit_service: ToolkitService = Depends(get_toolkit_service)\n):\n    \"\"\"Deanonymize the given text using Toolkit service\"\"\"\n\n    logger.info(f\"Deanonymize endpoint called with session_id: '{request.session_id}'\")\n\n    try:\n        result = toolkit_service.deanonymize(text=request.text, session_id=request.session_id)\n        return result\n    except ValueError as ve:\n        logger.error(f\"Deanonymization value error: {ve}\")\n        raise HTTPException(status_code=404, detail=str(ve))\n    except Exception as e:\n        logger.exception(f\"Error during deanonymization: {e}\")\n        raise HTTPException(\n            status_code=500, detail=\"An error occurred during deanonymization\"\n        )\n</pre> @app.post(\"/deanonymize\", response_model=DeanonymizeResponse) async def deanonymize_endpoint(     request: DeanonymizeRequest,     toolkit_service: ToolkitService = Depends(get_toolkit_service) ):     \"\"\"Deanonymize the given text using Toolkit service\"\"\"      logger.info(f\"Deanonymize endpoint called with session_id: '{request.session_id}'\")      try:         result = toolkit_service.deanonymize(text=request.text, session_id=request.session_id)         return result     except ValueError as ve:         logger.error(f\"Deanonymization value error: {ve}\")         raise HTTPException(status_code=404, detail=str(ve))     except Exception as e:         logger.exception(f\"Error during deanonymization: {e}\")         raise HTTPException(             status_code=500, detail=\"An error occurred during deanonymization\"         )"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/config/config/","title":"Config","text":"In\u00a0[\u00a0]: Copied! <pre>from dotenv import load_dotenv\nimport os\n</pre> from dotenv import load_dotenv import os In\u00a0[\u00a0]: Copied! <pre>load_dotenv()\n</pre> load_dotenv() In\u00a0[\u00a0]: Copied! <pre>class Config:\n    class Redis:\n        hostname = os.getenv('REDIS_HOSTNAME')\n        port = int(os.getenv('REDIS_PORT'))\n        key = os.getenv('REDIS_KEY')\n        ssl = os.getenv('REDIS_SSL')\n\n    class Presidio:\n        analyzer_url = os.getenv('PRESIDIO_ANALYZER_URL')\n        anonymizer_url = os.getenv('PRESIDIO_ANONYMIZER_URL')\n</pre> class Config:     class Redis:         hostname = os.getenv('REDIS_HOSTNAME')         port = int(os.getenv('REDIS_PORT'))         key = os.getenv('REDIS_KEY')         ssl = os.getenv('REDIS_SSL')      class Presidio:         analyzer_url = os.getenv('PRESIDIO_ANALYZER_URL')         anonymizer_url = os.getenv('PRESIDIO_ANONYMIZER_URL') In\u00a0[\u00a0]: Copied! <pre>config = Config()\n</pre> config = Config()"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/__init__/","title":"init","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/toolkit_service/","title":"Toolkit service","text":"In\u00a0[\u00a0]: Copied! <pre>import logging\nimport uuid\nfrom typing import Optional\n</pre> import logging import uuid from typing import Optional In\u00a0[\u00a0]: Copied! <pre>from services.state.state_service import StateService\nfrom services.presidio.presidio_service import PresidioService\n</pre> from services.state.state_service import StateService from services.presidio.presidio_service import PresidioService In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(__name__)\n</pre> logger = logging.getLogger(__name__) In\u00a0[\u00a0]: Copied! <pre>class ToolkitService:\n    def __init__(self, presidio_service: PresidioService, state_service: StateService):\n        self.presidio_service = presidio_service\n        self.state_service = state_service\n\n    def anonymize(self, text: str, session_id: Optional[str] = None, language: Optional[str] = \"en\") -&gt; dict:\n        \"\"\"Anonymize the given text using Presidio service\"\"\"\n\n        entity_mappings = None\n        if not session_id:\n            logger.info(f\"Anonymize called without session_id\")\n            session_id = str(uuid.uuid4())\n            logger.info(f\"Generated new session_id: {session_id}\")\n        else:\n            logger.info(f\"Anonymize called with session_id: {session_id}\")\n            entity_mappings = self.state_service.get_state(session_id)\n\n        try:\n            anonymized_text, new_entity_mappings = self.presidio_service.anonymize_text(\n                session_id, text, language, entity_mappings\n            )\n            \n            # Save the state in the state service\n            self.state_service.set_state(session_id, new_entity_mappings)\n\n            return {\"session_id\": session_id, \"text\": anonymized_text}\n        except Exception as e:\n            logger.exception(f\"Error during anonymization for session_id {session_id} {e}\")\n            raise e\n\n\n    def deanonymize(self, text: str, session_id: str) -&gt; dict:\n        \"\"\"Deanonymize the given text using Presidio service\"\"\"\n\n        logger.info(f\"Deanonymize endpoint called with session_id: {session_id}\")\n\n        entity_mappings = self.state_service.get_state(session_id)\n        if entity_mappings is None:\n            raise ValueError(\"Deanonymization is not possible because the session is not found\")\n\n        try:\n            deanonymized_text = self.presidio_service.deanonymize_text(\n                session_id, text, entity_mappings\n            )\n            return {\"text\": deanonymized_text}\n        except Exception as e:\n            logger.exception(f\"Error during deanonymization for session_id {session_id}  {e}\")\n            raise e\n</pre> class ToolkitService:     def __init__(self, presidio_service: PresidioService, state_service: StateService):         self.presidio_service = presidio_service         self.state_service = state_service      def anonymize(self, text: str, session_id: Optional[str] = None, language: Optional[str] = \"en\") -&gt; dict:         \"\"\"Anonymize the given text using Presidio service\"\"\"          entity_mappings = None         if not session_id:             logger.info(f\"Anonymize called without session_id\")             session_id = str(uuid.uuid4())             logger.info(f\"Generated new session_id: {session_id}\")         else:             logger.info(f\"Anonymize called with session_id: {session_id}\")             entity_mappings = self.state_service.get_state(session_id)          try:             anonymized_text, new_entity_mappings = self.presidio_service.anonymize_text(                 session_id, text, language, entity_mappings             )                          # Save the state in the state service             self.state_service.set_state(session_id, new_entity_mappings)              return {\"session_id\": session_id, \"text\": anonymized_text}         except Exception as e:             logger.exception(f\"Error during anonymization for session_id {session_id} {e}\")             raise e       def deanonymize(self, text: str, session_id: str) -&gt; dict:         \"\"\"Deanonymize the given text using Presidio service\"\"\"          logger.info(f\"Deanonymize endpoint called with session_id: {session_id}\")          entity_mappings = self.state_service.get_state(session_id)         if entity_mappings is None:             raise ValueError(\"Deanonymization is not possible because the session is not found\")          try:             deanonymized_text = self.presidio_service.deanonymize_text(                 session_id, text, entity_mappings             )             return {\"text\": deanonymized_text}         except Exception as e:             logger.exception(f\"Error during deanonymization for session_id {session_id}  {e}\")             raise e"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/anonymizers/__init__/","title":"init","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/anonymizers/instance_counter_anonymizer/","title":"Instance counter anonymizer","text":"In\u00a0[\u00a0]: Copied! <pre>from presidio_anonymizer.operators import Operator, OperatorType\n</pre> from presidio_anonymizer.operators import Operator, OperatorType In\u00a0[\u00a0]: Copied! <pre>from typing import Dict\n</pre> from typing import Dict In\u00a0[\u00a0]: Copied! <pre>class InstanceCounterAnonymizer(Operator):\n    \"\"\"\n    Anonymizer which replaces the entity value\n    with an instance counter per entity.\n    \"\"\"\n\n    REPLACING_FORMAT = \"&lt;{entity_type}_{index}&gt;\"\n\n    def operate(self, text: str, params: Dict = None) -&gt; str:\n        \"\"\"Anonymize the input text.\"\"\"\n\n        entity_type: str = params[\"entity_type\"]\n\n        # entity_mapping is a dict of dicts containing mappings per entity type\n        entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]\n\n        entity_mapping_for_type = entity_mapping.get(entity_type)\n        if not entity_mapping_for_type:\n            new_text = self.REPLACING_FORMAT.format(\n                entity_type=entity_type, index=0\n            )\n            entity_mapping[entity_type] = {}\n\n        else:\n            if text in entity_mapping_for_type:\n                return entity_mapping_for_type[text]\n\n            previous_index = self._get_last_index(entity_mapping_for_type)\n            new_text = self.REPLACING_FORMAT.format(\n                entity_type=entity_type, index=previous_index + 1\n            )\n\n        entity_mapping[entity_type][text] = new_text\n        return new_text\n\n    @staticmethod\n    def _get_last_index(entity_mapping_for_type: Dict) -&gt; int:\n        \"\"\"Get the last index for a given entity type.\"\"\"\n\n        def get_index(value: str) -&gt; int:\n            return int(value.split(\"_\")[-1][:-1])\n\n        indices = [get_index(v) for v in entity_mapping_for_type.values()]\n        return max(indices)\n\n    def validate(self, params: Dict = None) -&gt; None:\n        \"\"\"Validate operator parameters.\"\"\"\n\n        if \"entity_mapping\" not in params:\n            raise ValueError(\"An input Dict called `entity_mapping` is required.\")\n        if \"entity_type\" not in params:\n            raise ValueError(\"An entity_type param is required.\")\n\n    def operator_name(self) -&gt; str:\n        return \"entity_counter\"\n\n    def operator_type(self) -&gt; OperatorType:\n        return OperatorType.Anonymize\n</pre> class InstanceCounterAnonymizer(Operator):     \"\"\"     Anonymizer which replaces the entity value     with an instance counter per entity.     \"\"\"      REPLACING_FORMAT = \"&lt;{entity_type}_{index}&gt;\"      def operate(self, text: str, params: Dict = None) -&gt; str:         \"\"\"Anonymize the input text.\"\"\"          entity_type: str = params[\"entity_type\"]          # entity_mapping is a dict of dicts containing mappings per entity type         entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]          entity_mapping_for_type = entity_mapping.get(entity_type)         if not entity_mapping_for_type:             new_text = self.REPLACING_FORMAT.format(                 entity_type=entity_type, index=0             )             entity_mapping[entity_type] = {}          else:             if text in entity_mapping_for_type:                 return entity_mapping_for_type[text]              previous_index = self._get_last_index(entity_mapping_for_type)             new_text = self.REPLACING_FORMAT.format(                 entity_type=entity_type, index=previous_index + 1             )          entity_mapping[entity_type][text] = new_text         return new_text      @staticmethod     def _get_last_index(entity_mapping_for_type: Dict) -&gt; int:         \"\"\"Get the last index for a given entity type.\"\"\"          def get_index(value: str) -&gt; int:             return int(value.split(\"_\")[-1][:-1])          indices = [get_index(v) for v in entity_mapping_for_type.values()]         return max(indices)      def validate(self, params: Dict = None) -&gt; None:         \"\"\"Validate operator parameters.\"\"\"          if \"entity_mapping\" not in params:             raise ValueError(\"An input Dict called `entity_mapping` is required.\")         if \"entity_type\" not in params:             raise ValueError(\"An entity_type param is required.\")      def operator_name(self) -&gt; str:         return \"entity_counter\"      def operator_type(self) -&gt; OperatorType:         return OperatorType.Anonymize"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/anonymizers/instance_counter_deanonymizer/","title":"Instance counter deanonymizer","text":"In\u00a0[\u00a0]: Copied! <pre>from presidio_anonymizer.operators import Operator, OperatorType\n</pre> from presidio_anonymizer.operators import Operator, OperatorType In\u00a0[\u00a0]: Copied! <pre>from typing import Dict\n</pre> from typing import Dict In\u00a0[\u00a0]: Copied! <pre>class InstanceCounterDeanonymizer(Operator):\n    \"\"\"\n    Deanonymizer which replaces the unique identifier \n    with the original text.\n    \"\"\"\n\n    def operate(self, text: str, params: Dict = None) -&gt; str:\n        \"\"\"Anonymize the input text.\"\"\"\n\n        entity_type: str = params[\"entity_type\"]\n\n        # entity_mapping is a dict of dicts containing mappings per entity type\n        entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]\n\n        if entity_type not in entity_mapping:\n            raise ValueError(f\"Entity type {entity_type} not found in entity mapping!\")\n        if text not in entity_mapping[entity_type].values():\n            raise ValueError(f\"Text {text} not found in entity mapping for entity type {entity_type}!\")\n\n        return self._find_key_by_value(entity_mapping[entity_type], text)\n\n    @staticmethod\n    def _find_key_by_value(entity_mapping, value):\n        for key, val in entity_mapping.items():\n            if val == value:\n                return key\n        return None\n    \n    def validate(self, params: Dict = None) -&gt; None:\n        \"\"\"Validate operator parameters.\"\"\"\n\n        if \"entity_mapping\" not in params:\n            raise ValueError(\"An input Dict called `entity_mapping` is required.\")\n        if \"entity_type\" not in params:\n            raise ValueError(\"An entity_type param is required.\")\n\n    def operator_name(self) -&gt; str:\n        return \"entity_counter_deanonymizer\"\n\n    def operator_type(self) -&gt; OperatorType:\n        return OperatorType.Deanonymize\n</pre> class InstanceCounterDeanonymizer(Operator):     \"\"\"     Deanonymizer which replaces the unique identifier      with the original text.     \"\"\"      def operate(self, text: str, params: Dict = None) -&gt; str:         \"\"\"Anonymize the input text.\"\"\"          entity_type: str = params[\"entity_type\"]          # entity_mapping is a dict of dicts containing mappings per entity type         entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]          if entity_type not in entity_mapping:             raise ValueError(f\"Entity type {entity_type} not found in entity mapping!\")         if text not in entity_mapping[entity_type].values():             raise ValueError(f\"Text {text} not found in entity mapping for entity type {entity_type}!\")          return self._find_key_by_value(entity_mapping[entity_type], text)      @staticmethod     def _find_key_by_value(entity_mapping, value):         for key, val in entity_mapping.items():             if val == value:                 return key         return None          def validate(self, params: Dict = None) -&gt; None:         \"\"\"Validate operator parameters.\"\"\"          if \"entity_mapping\" not in params:             raise ValueError(\"An input Dict called `entity_mapping` is required.\")         if \"entity_type\" not in params:             raise ValueError(\"An entity_type param is required.\")      def operator_name(self) -&gt; str:         return \"entity_counter_deanonymizer\"      def operator_type(self) -&gt; OperatorType:         return OperatorType.Deanonymize"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/presidio/__init__/","title":"init","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/presidio/http_presidio_service/","title":"Http presidio service","text":"In\u00a0[\u00a0]: Copied! <pre>from collections import defaultdict\nimport logging\nfrom presidio_anonymizer import OperatorResult\nimport requests\nfrom timeit import default_timer as timer\nfrom typing import List, Tuple\n</pre> from collections import defaultdict import logging from presidio_anonymizer import OperatorResult import requests from timeit import default_timer as timer from typing import List, Tuple In\u00a0[\u00a0]: Copied! <pre>from services.presidio.presidio_service import PresidioService\nfrom config.config import config\n</pre> from services.presidio.presidio_service import PresidioService from config.config import config In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(__name__)\n</pre> logger = logging.getLogger(__name__) In\u00a0[\u00a0]: Copied! <pre>class HttpPresidioService(PresidioService):\n    \"\"\" Presidio Service class that uses both Presidio Analyzer and Anonymizer via HTTP \"\"\"\n\n    def __init__(self):\n        self.analyzer_base_url = config.Presidio.analyzer_url\n        self.anonymizer_base_url = config.Presidio.anonymizer_url        \n\n    def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -&gt; Tuple[str, dict] :\n        \"\"\" Anonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Anonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            analyze_url = f\"{self.analyzer_base_url}/analyze\"\n            analyze_payload = {\n                \"text\": text,\n                \"language\": language\n            }\n            analyze_response = requests.post(analyze_url, json=analyze_payload)\n            analyzer_result = analyze_response.json()\n            logger.info(f\"Analyze took {timer() - start_time:.3f} seconds for session_id: {session_id}\")\n\n            analyzer_result_with_id, entities = self.add_id_to_analyzer_result(text, analyzer_result, entity_mappings)\n        \n            anonymizer_start_time = timer()\n            anonymize_url = f\"{self.anonymizer_base_url}/anonymize\"\n            anonymize_payload = {\n                \"text\": text,\n                \"analyzer_results\": analyzer_result_with_id\n            }\n            anonymize_response = requests.post(anonymize_url, json=anonymize_payload)\n            anonymizer_result = anonymize_response.json()            \n            logger.info(f\"Anonymize took {timer() - anonymizer_start_time:.3f} seconds for session_id: {session_id}\")\n\n            anonymizer_text = anonymizer_result[\"text\"]\n            anonymizer_results = anonymizer_result[\"items\"]\n            new_entity_mappings = self.build_entity_mappgings(entity_mappings, entities, anonymizer_results)\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")            \n\n            return anonymizer_text, new_entity_mappings\n        except Exception as e:\n            logger.exception(f\"Error in anonymize_text for session_id {session_id}\")\n            raise\n\n    def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -&gt; str:\n        \"\"\" Deanonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Deanonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        deanonymized_text = text\n        try:\n            for _, entities in entity_mappings.items():\n                for entity_value, entity_id in entities.items():\n                    deanonymized_text = deanonymized_text.replace(entity_id, entity_value)\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n            \n            return deanonymized_text\n        except Exception as e:\n            logger.exception(f\"Error in deanonymize_text for session_id {session_id}\")\n            raise\n\n    def build_entity_mappgings(self, entity_mappings: dict, entities: dict, anonymizer_results: List) -&gt; dict:\n        \"\"\" Build the entity mappings based on the previous mappings and the anonymizer results \"\"\"\n\n        new_entity_mappings = entity_mappings.copy() if entity_mappings is not None else dict()\n\n        for entity in anonymizer_results:\n            entity_text = entity[\"text\"]\n            entity_id = entity_text.strip('&lt;&gt;')\n            entity_type = entity_id.rsplit('_', 1)[0]\n            entity_value = entities[entity_id]\n\n            if entity_type not in new_entity_mappings:\n                new_entity_mappings[entity_type] = {}\n\n            new_entity_mappings[entity_type][entity_value] = entity_text\n\n        return new_entity_mappings\n    \n    def add_id_to_analyzer_result(self, text: str, analyzer_result: List[OperatorResult], entity_mappings: dict) -&gt; Tuple[List[OperatorResult], dict]:\n        \"\"\" Add an ID to each entity type in the analyzer_result based on the entity_mappings \"\"\"\n\n        entity_type_counts = {}\n        processed_entities = {}\n        new_entity_mappings = entity_mappings.copy() if entity_mappings is not None else dict()\n\n        # Initialize counts from the mapping\n        for entity_type, entities in new_entity_mappings.items():\n            # Extract indices from the IDs in the mapping\n            indices = [\n                int(id.strip('&lt;&gt;').split('_')[-1]) for id in entities.values()\n                if id.strip('&lt;&gt;').startswith(entity_type)\n            ]\n            entity_type_counts[entity_type] = max(indices) + 1 if indices else 0\n\n        # Now process each item in analyzer_result\n        for item in analyzer_result:\n            entity_type = item['entity_type']\n            start = item['start']\n            end = item['end']\n            entity_text = text[start:end]\n\n            if entity_type not in entity_type_counts:\n                entity_type_counts[entity_type] = 0\n\n            if entity_type not in new_entity_mappings:\n                new_entity_mappings[entity_type] = {}\n\n            if entity_text in new_entity_mappings[entity_type]:\n                # Reuse the existing ID from entity_mappings (strip angle brackets)\n                entity_id = new_entity_mappings[entity_type][entity_text].strip('&lt;&gt;')\n            else:\n                # Assign a new ID\n                count = entity_type_counts[entity_type]\n                entity_id = f\"{entity_type}_{count}\"\n                new_entity_mappings[entity_type][entity_text] = f\"&lt;{entity_id}&gt;\"\n                entity_type_counts[entity_type] = count + 1\n\n            # Update the entity_type in the item\n            item['entity_type'] = entity_id\n            processed_entities[entity_id] = entity_text\n\n        return analyzer_result, processed_entities\n</pre> class HttpPresidioService(PresidioService):     \"\"\" Presidio Service class that uses both Presidio Analyzer and Anonymizer via HTTP \"\"\"      def __init__(self):         self.analyzer_base_url = config.Presidio.analyzer_url         self.anonymizer_base_url = config.Presidio.anonymizer_url              def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -&gt; Tuple[str, dict] :         \"\"\" Anonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"          logger.info(f\"Anonymize text called with session_id: {session_id}\")         start_time = timer()          try:             analyze_url = f\"{self.analyzer_base_url}/analyze\"             analyze_payload = {                 \"text\": text,                 \"language\": language             }             analyze_response = requests.post(analyze_url, json=analyze_payload)             analyzer_result = analyze_response.json()             logger.info(f\"Analyze took {timer() - start_time:.3f} seconds for session_id: {session_id}\")              analyzer_result_with_id, entities = self.add_id_to_analyzer_result(text, analyzer_result, entity_mappings)                      anonymizer_start_time = timer()             anonymize_url = f\"{self.anonymizer_base_url}/anonymize\"             anonymize_payload = {                 \"text\": text,                 \"analyzer_results\": analyzer_result_with_id             }             anonymize_response = requests.post(anonymize_url, json=anonymize_payload)             anonymizer_result = anonymize_response.json()                         logger.info(f\"Anonymize took {timer() - anonymizer_start_time:.3f} seconds for session_id: {session_id}\")              anonymizer_text = anonymizer_result[\"text\"]             anonymizer_results = anonymizer_result[\"items\"]             new_entity_mappings = self.build_entity_mappgings(entity_mappings, entities, anonymizer_results)              total_time = timer() - start_time             logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")                          return anonymizer_text, new_entity_mappings         except Exception as e:             logger.exception(f\"Error in anonymize_text for session_id {session_id}\")             raise      def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -&gt; str:         \"\"\" Deanonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"          logger.info(f\"Deanonymize text called with session_id: {session_id}\")         start_time = timer()          deanonymized_text = text         try:             for _, entities in entity_mappings.items():                 for entity_value, entity_id in entities.items():                     deanonymized_text = deanonymized_text.replace(entity_id, entity_value)              total_time = timer() - start_time             logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")                          return deanonymized_text         except Exception as e:             logger.exception(f\"Error in deanonymize_text for session_id {session_id}\")             raise      def build_entity_mappgings(self, entity_mappings: dict, entities: dict, anonymizer_results: List) -&gt; dict:         \"\"\" Build the entity mappings based on the previous mappings and the anonymizer results \"\"\"          new_entity_mappings = entity_mappings.copy() if entity_mappings is not None else dict()          for entity in anonymizer_results:             entity_text = entity[\"text\"]             entity_id = entity_text.strip('&lt;&gt;')             entity_type = entity_id.rsplit('_', 1)[0]             entity_value = entities[entity_id]              if entity_type not in new_entity_mappings:                 new_entity_mappings[entity_type] = {}              new_entity_mappings[entity_type][entity_value] = entity_text          return new_entity_mappings          def add_id_to_analyzer_result(self, text: str, analyzer_result: List[OperatorResult], entity_mappings: dict) -&gt; Tuple[List[OperatorResult], dict]:         \"\"\" Add an ID to each entity type in the analyzer_result based on the entity_mappings \"\"\"          entity_type_counts = {}         processed_entities = {}         new_entity_mappings = entity_mappings.copy() if entity_mappings is not None else dict()          # Initialize counts from the mapping         for entity_type, entities in new_entity_mappings.items():             # Extract indices from the IDs in the mapping             indices = [                 int(id.strip('&lt;&gt;').split('_')[-1]) for id in entities.values()                 if id.strip('&lt;&gt;').startswith(entity_type)             ]             entity_type_counts[entity_type] = max(indices) + 1 if indices else 0          # Now process each item in analyzer_result         for item in analyzer_result:             entity_type = item['entity_type']             start = item['start']             end = item['end']             entity_text = text[start:end]              if entity_type not in entity_type_counts:                 entity_type_counts[entity_type] = 0              if entity_type not in new_entity_mappings:                 new_entity_mappings[entity_type] = {}              if entity_text in new_entity_mappings[entity_type]:                 # Reuse the existing ID from entity_mappings (strip angle brackets)                 entity_id = new_entity_mappings[entity_type][entity_text].strip('&lt;&gt;')             else:                 # Assign a new ID                 count = entity_type_counts[entity_type]                 entity_id = f\"{entity_type}_{count}\"                 new_entity_mappings[entity_type][entity_text] = f\"&lt;{entity_id}&gt;\"                 entity_type_counts[entity_type] = count + 1              # Update the entity_type in the item             item['entity_type'] = entity_id             processed_entities[entity_id] = entity_text          return analyzer_result, processed_entities"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/presidio/hybrid_presidio_service/","title":"Hybrid presidio service","text":"In\u00a0[\u00a0]: Copied! <pre>import logging\nfrom timeit import default_timer as timer\nfrom typing import List, Tuple\nfrom presidio_analyzer import AnalyzerEngine, RecognizerResult\nfrom presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine, OperatorConfig, OperatorResult\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nimport requests\n</pre> import logging from timeit import default_timer as timer from typing import List, Tuple from presidio_analyzer import AnalyzerEngine, RecognizerResult from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine, OperatorConfig, OperatorResult from presidio_analyzer.nlp_engine import NlpEngineProvider import requests In\u00a0[\u00a0]: Copied! <pre>from services.presidio.presidio_service import PresidioService\nfrom services.anonymizers.instance_counter_anonymizer import InstanceCounterAnonymizer\nfrom services.anonymizers.instance_counter_deanonymizer import InstanceCounterDeanonymizer\nfrom config.config import config\n</pre> from services.presidio.presidio_service import PresidioService from services.anonymizers.instance_counter_anonymizer import InstanceCounterAnonymizer from services.anonymizers.instance_counter_deanonymizer import InstanceCounterDeanonymizer from config.config import config In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(__name__)\n</pre> logger = logging.getLogger(__name__) In\u00a0[\u00a0]: Copied! <pre>class HybridPresidioService(PresidioService):\n    \"\"\" Presidio Service class that uses Presidio Analyzer via HTTP and Anonymizer as a Python library \"\"\"\n    \n    def __init__(self):\n        self.analyzer_base_url = config.Presidio.analyzer_url\n\n        self.anonymizer = AnonymizerEngine()\n        self.anonymizer.add_anonymizer(InstanceCounterAnonymizer)\n        self.deanonymizer = DeanonymizeEngine()\n        self.deanonymizer.add_deanonymizer(InstanceCounterDeanonymizer)\n\n    def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -&gt; Tuple[str, dict] :\n        \"\"\" Anonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Anonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            analyze_url = f\"{self.analyzer_base_url}/analyze\"\n            analyze_payload = {\n                \"text\": text,\n                \"language\": language\n            }\n            analyze_response = requests.post(analyze_url, json=analyze_payload)\n            analyzer_result_json = analyze_response.json()\n            analyzer_result = [RecognizerResult.from_json(item) for item in analyzer_result_json]\n            logger.info(f\"Analyze took {timer() - start_time:.3f} seconds for session_id: {session_id}\")\n\n            anonymizer_start_time = timer()\n            anonymizer_entity_mapping = entity_mappings.copy() if entity_mappings is not None else dict()\n            anonymized_result = self.anonymizer.anonymize(\n                text=text,\n                analyzer_results=analyzer_result,\n                operators={\n                    \"DEFAULT\": OperatorConfig(\n                        \"entity_counter\", {\"entity_mapping\": anonymizer_entity_mapping}\n                    )\n                },\n            )\n            logger.info(f\"Anonymize took {timer() - anonymizer_start_time:.3f} seconds for session_id: {session_id}\")\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n\n            return anonymized_result.text, anonymizer_entity_mapping\n        except Exception as e:\n            logger.exception(f\"Error in anonymize_text for session_id {session_id}\")\n            raise\n\n    def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -&gt; str:\n        \"\"\" Deanonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Deanonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            entities = self.get_entities(entity_mappings, text)\n\n            deanonymized = self.deanonymizer.deanonymize(\n                text=text,\n                entities=entities,\n                operators=\n                {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\", \n                                        params={\"entity_mapping\": entity_mappings})}\n            )\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n            \n            return deanonymized.text\n        except Exception as e:\n            logger.exception(f\"Error in deanonymize_text for session_id {session_id}\")\n            raise\n\n    def get_entities(self, entity_mappings: dict, text: str) -&gt; List[OperatorResult]:\n        \"\"\" Get the entities from the entity mappings \"\"\"\n\n        entities = []\n        for entity_type, entity_mapping in entity_mappings.items():\n            for entity_value, entity_id in entity_mapping.items():\n                start_index = 0\n                while True:\n                    start_index = text.find(entity_id, start_index)\n                    if start_index == -1:\n                        break\n                    end_index = start_index + len(entity_id)\n                    entities.append(OperatorResult(start_index, end_index, entity_type, entity_value, entity_id))\n                    start_index += len(entity_id)\n        return entities\n</pre> class HybridPresidioService(PresidioService):     \"\"\" Presidio Service class that uses Presidio Analyzer via HTTP and Anonymizer as a Python library \"\"\"          def __init__(self):         self.analyzer_base_url = config.Presidio.analyzer_url          self.anonymizer = AnonymizerEngine()         self.anonymizer.add_anonymizer(InstanceCounterAnonymizer)         self.deanonymizer = DeanonymizeEngine()         self.deanonymizer.add_deanonymizer(InstanceCounterDeanonymizer)      def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -&gt; Tuple[str, dict] :         \"\"\" Anonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"          logger.info(f\"Anonymize text called with session_id: {session_id}\")         start_time = timer()          try:             analyze_url = f\"{self.analyzer_base_url}/analyze\"             analyze_payload = {                 \"text\": text,                 \"language\": language             }             analyze_response = requests.post(analyze_url, json=analyze_payload)             analyzer_result_json = analyze_response.json()             analyzer_result = [RecognizerResult.from_json(item) for item in analyzer_result_json]             logger.info(f\"Analyze took {timer() - start_time:.3f} seconds for session_id: {session_id}\")              anonymizer_start_time = timer()             anonymizer_entity_mapping = entity_mappings.copy() if entity_mappings is not None else dict()             anonymized_result = self.anonymizer.anonymize(                 text=text,                 analyzer_results=analyzer_result,                 operators={                     \"DEFAULT\": OperatorConfig(                         \"entity_counter\", {\"entity_mapping\": anonymizer_entity_mapping}                     )                 },             )             logger.info(f\"Anonymize took {timer() - anonymizer_start_time:.3f} seconds for session_id: {session_id}\")              total_time = timer() - start_time             logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")              return anonymized_result.text, anonymizer_entity_mapping         except Exception as e:             logger.exception(f\"Error in anonymize_text for session_id {session_id}\")             raise      def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -&gt; str:         \"\"\" Deanonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"          logger.info(f\"Deanonymize text called with session_id: {session_id}\")         start_time = timer()          try:             entities = self.get_entities(entity_mappings, text)              deanonymized = self.deanonymizer.deanonymize(                 text=text,                 entities=entities,                 operators=                 {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\",                                          params={\"entity_mapping\": entity_mappings})}             )              total_time = timer() - start_time             logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")                          return deanonymized.text         except Exception as e:             logger.exception(f\"Error in deanonymize_text for session_id {session_id}\")             raise      def get_entities(self, entity_mappings: dict, text: str) -&gt; List[OperatorResult]:         \"\"\" Get the entities from the entity mappings \"\"\"          entities = []         for entity_type, entity_mapping in entity_mappings.items():             for entity_value, entity_id in entity_mapping.items():                 start_index = 0                 while True:                     start_index = text.find(entity_id, start_index)                     if start_index == -1:                         break                     end_index = start_index + len(entity_id)                     entities.append(OperatorResult(start_index, end_index, entity_type, entity_value, entity_id))                     start_index += len(entity_id)         return entities"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/presidio/presidio_service/","title":"Presidio service","text":"In\u00a0[\u00a0]: Copied! <pre>from abc import ABC, abstractmethod\nfrom typing import List, Tuple\n</pre> from abc import ABC, abstractmethod from typing import List, Tuple In\u00a0[\u00a0]: Copied! <pre>from presidio_anonymizer import OperatorResult\n</pre> from presidio_anonymizer import OperatorResult In\u00a0[\u00a0]: Copied! <pre>class PresidioService(ABC):\n    @abstractmethod\n    def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -&gt; Tuple[str, dict]:\n        pass\n    \n    @abstractmethod\n    def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -&gt; str:\n        pass\n</pre> class PresidioService(ABC):     @abstractmethod     def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -&gt; Tuple[str, dict]:         pass          @abstractmethod     def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -&gt; str:         pass"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/presidio/python_presidio_service/","title":"Python presidio service","text":"In\u00a0[\u00a0]: Copied! <pre>import logging\nfrom timeit import default_timer as timer\nfrom typing import List, Tuple\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine, OperatorConfig, OperatorResult\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n</pre> import logging from timeit import default_timer as timer from typing import List, Tuple from presidio_analyzer import AnalyzerEngine from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine, OperatorConfig, OperatorResult from presidio_analyzer.nlp_engine import NlpEngineProvider In\u00a0[\u00a0]: Copied! <pre>from services.presidio.presidio_service import PresidioService\nfrom services.anonymizers.instance_counter_anonymizer import InstanceCounterAnonymizer\nfrom services.anonymizers.instance_counter_deanonymizer import InstanceCounterDeanonymizer\n</pre> from services.presidio.presidio_service import PresidioService from services.anonymizers.instance_counter_anonymizer import InstanceCounterAnonymizer from services.anonymizers.instance_counter_deanonymizer import InstanceCounterDeanonymizer In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(__name__)\n</pre> logger = logging.getLogger(__name__) In\u00a0[\u00a0]: Copied! <pre>class PythonPresidioService(PresidioService):\n    \"\"\" Presidio Service class that uses both Presidio Analyzer and Anonymizer as Python libraries \"\"\"\n    \n    def __init__(self):\n        configuration = {\n            \"nlp_engine_name\": \"spacy\",\n            \"models\": [\n                {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n                {\"lang_code\": \"nl\", \"model_name\": \"nl_core_news_sm\"},\n                {\"lang_code\": \"es\", \"model_name\": \"es_core_news_sm\"},\n            ],\n        }\n        provider = NlpEngineProvider(nlp_configuration=configuration)\n        nlp_engine = provider.create_engine()\n\n        self.analyzer = AnalyzerEngine(\n            nlp_engine=nlp_engine,\n            supported_languages=[\"en\", \"nl\", \"es\"]\n        )\n        self.anonymizer = AnonymizerEngine()\n        self.anonymizer.add_anonymizer(InstanceCounterAnonymizer)\n        self.deanonymizer = DeanonymizeEngine()\n        self.deanonymizer.add_deanonymizer(InstanceCounterDeanonymizer)\n\n    def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -&gt; Tuple[str, dict] :\n        \"\"\" Anonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Anonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            results = self.analyzer.analyze(text=text, language=language)\n            logger.info(f\"Analyze took {timer() - start_time:.3f} seconds for session_id: {session_id}\")\n\n            anonymizer_start_time = timer()\n            anonymizer_entity_mapping = entity_mappings.copy() if entity_mappings is not None else dict()\n            anonymized_result = self.anonymizer.anonymize(\n                text=text,\n                analyzer_results=results,\n                operators={\n                    \"DEFAULT\": OperatorConfig(\n                        \"entity_counter\", {\"entity_mapping\": anonymizer_entity_mapping}\n                    )\n                },\n            )\n            logger.info(f\"Anonymize took {timer() - anonymizer_start_time:.3f} seconds for session_id: {session_id}\")\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n\n            return anonymized_result.text, anonymizer_entity_mapping\n        except Exception as e:\n            logger.exception(f\"Error in anonymize_text for session_id {session_id}\")\n            raise\n\n    def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -&gt; str:\n        \"\"\" Deanonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"\n\n        logger.info(f\"Deanonymize text called with session_id: {session_id}\")\n        start_time = timer()\n\n        try:\n            entities = self.get_entities(entity_mappings, text)\n\n            deanonymized = self.deanonymizer.deanonymize(\n                text=text,\n                entities=entities,\n                operators=\n                {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\", \n                                        params={\"entity_mapping\": entity_mappings})}\n            )\n\n            total_time = timer() - start_time\n            logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")\n            \n            return deanonymized.text\n        except Exception as e:\n            logger.exception(f\"Error in deanonymize_text for session_id {session_id}\")\n            raise\n\n    def get_entities(self, entity_mappings: dict, text: str) -&gt; List[OperatorResult]:\n        \"\"\" Get the entities from the entity mappings \"\"\"\n\n        entities = []\n        for entity_type, entity_mapping in entity_mappings.items():\n            for entity_value, entity_id in entity_mapping.items():\n                start_index = 0\n                while True:\n                    start_index = text.find(entity_id, start_index)\n                    if start_index == -1:\n                        break\n                    end_index = start_index + len(entity_id)\n                    entities.append(OperatorResult(start_index, end_index, entity_type, entity_value, entity_id))\n                    start_index += len(entity_id)\n        return entities\n</pre> class PythonPresidioService(PresidioService):     \"\"\" Presidio Service class that uses both Presidio Analyzer and Anonymizer as Python libraries \"\"\"          def __init__(self):         configuration = {             \"nlp_engine_name\": \"spacy\",             \"models\": [                 {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},                 {\"lang_code\": \"nl\", \"model_name\": \"nl_core_news_sm\"},                 {\"lang_code\": \"es\", \"model_name\": \"es_core_news_sm\"},             ],         }         provider = NlpEngineProvider(nlp_configuration=configuration)         nlp_engine = provider.create_engine()          self.analyzer = AnalyzerEngine(             nlp_engine=nlp_engine,             supported_languages=[\"en\", \"nl\", \"es\"]         )         self.anonymizer = AnonymizerEngine()         self.anonymizer.add_anonymizer(InstanceCounterAnonymizer)         self.deanonymizer = DeanonymizeEngine()         self.deanonymizer.add_deanonymizer(InstanceCounterDeanonymizer)      def anonymize_text(self, session_id: str, text: str, language: str, entity_mappings: dict) -&gt; Tuple[str, dict] :         \"\"\" Anonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"          logger.info(f\"Anonymize text called with session_id: {session_id}\")         start_time = timer()          try:             results = self.analyzer.analyze(text=text, language=language)             logger.info(f\"Analyze took {timer() - start_time:.3f} seconds for session_id: {session_id}\")              anonymizer_start_time = timer()             anonymizer_entity_mapping = entity_mappings.copy() if entity_mappings is not None else dict()             anonymized_result = self.anonymizer.anonymize(                 text=text,                 analyzer_results=results,                 operators={                     \"DEFAULT\": OperatorConfig(                         \"entity_counter\", {\"entity_mapping\": anonymizer_entity_mapping}                     )                 },             )             logger.info(f\"Anonymize took {timer() - anonymizer_start_time:.3f} seconds for session_id: {session_id}\")              total_time = timer() - start_time             logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")              return anonymized_result.text, anonymizer_entity_mapping         except Exception as e:             logger.exception(f\"Error in anonymize_text for session_id {session_id}\")             raise      def deanonymize_text(self, session_id: str, text: str, entity_mappings: dict) -&gt; str:         \"\"\" Deanonymize the given text using Presidio Analyzer and Anonymizer engines \"\"\"          logger.info(f\"Deanonymize text called with session_id: {session_id}\")         start_time = timer()          try:             entities = self.get_entities(entity_mappings, text)              deanonymized = self.deanonymizer.deanonymize(                 text=text,                 entities=entities,                 operators=                 {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\",                                          params={\"entity_mapping\": entity_mappings})}             )              total_time = timer() - start_time             logger.info(f\"Total processing time: {total_time:.3f} seconds for session_id: {session_id}\")                          return deanonymized.text         except Exception as e:             logger.exception(f\"Error in deanonymize_text for session_id {session_id}\")             raise      def get_entities(self, entity_mappings: dict, text: str) -&gt; List[OperatorResult]:         \"\"\" Get the entities from the entity mappings \"\"\"          entities = []         for entity_type, entity_mapping in entity_mappings.items():             for entity_value, entity_id in entity_mapping.items():                 start_index = 0                 while True:                     start_index = text.find(entity_id, start_index)                     if start_index == -1:                         break                     end_index = start_index + len(entity_id)                     entities.append(OperatorResult(start_index, end_index, entity_type, entity_value, entity_id))                     start_index += len(entity_id)         return entities"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/state/__init__/","title":"init","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/state/inmemory_state_service/","title":"Inmemory state service","text":"In\u00a0[\u00a0]: Copied! <pre>import logging\n</pre> import logging In\u00a0[\u00a0]: Copied! <pre>from api.services.state.state_service import StateService\n</pre> from api.services.state.state_service import StateService In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(__name__)\n</pre> logger = logging.getLogger(__name__) In\u00a0[\u00a0]: Copied! <pre>class InMemoryStateService(StateService):\n    def __init__(self):\n        self.store = {}\n\n    def get_state(self, session_id):\n        \"\"\"Get the state for the given session_id\"\"\"\n\n        logger.info(f\"Get state for session_id: {session_id}\")\n        entity_mappings = self.store.get(session_id)\n        if entity_mappings is None:\n            logger.info(f\"No state found for session_id: {session_id}\")\n            return None\n        \n        return entity_mappings\n\n    def set_state(self, session_id, entity_mappings):\n        \"\"\"Set the state for the given session_id\"\"\"\n        \n        logger.info(f\"Save state for session_id: {session_id}\")\n        self.store[session_id] = entity_mappings\n</pre> class InMemoryStateService(StateService):     def __init__(self):         self.store = {}      def get_state(self, session_id):         \"\"\"Get the state for the given session_id\"\"\"          logger.info(f\"Get state for session_id: {session_id}\")         entity_mappings = self.store.get(session_id)         if entity_mappings is None:             logger.info(f\"No state found for session_id: {session_id}\")             return None                  return entity_mappings      def set_state(self, session_id, entity_mappings):         \"\"\"Set the state for the given session_id\"\"\"                  logger.info(f\"Save state for session_id: {session_id}\")         self.store[session_id] = entity_mappings"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/state/redis_state_service/","title":"Redis state service","text":"In\u00a0[\u00a0]: Copied! <pre>import json\nimport logging\nfrom presidio_anonymizer import OperatorResult\nimport redis\n</pre> import json import logging from presidio_anonymizer import OperatorResult import redis In\u00a0[\u00a0]: Copied! <pre>from services.state.state_service import StateService\nfrom config.config import config\n</pre> from services.state.state_service import StateService from config.config import config In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(__name__)\n</pre> logger = logging.getLogger(__name__) In\u00a0[\u00a0]: Copied! <pre>class RedisStateService(StateService):\n    def __init__(self):\n        self.redis = redis.Redis(\n            host=config.Redis.hostname,\n            port=config.Redis.port,\n            db=0,\n            password=config.Redis.key,\n            ssl=config.Redis.ssl)\n\n    def get_state(self, session_id):\n        \"\"\"Get the state for the given session_id\"\"\"\n\n        logger.info(f\"Get state for session_id: {session_id}\")\n        json_data = self.redis.get(session_id)\n        if json_data is None:\n            logger.info(f\"No state found for session_id: {session_id}\")\n            return None\n        \n        entity_mappings = json.loads(json_data) \n        return entity_mappings\n\n    def set_state(self, session_id, entity_mappings):\n        \"\"\"Set the state for the given session_id\"\"\"\n\n        logger.info(f\"Seve state for session_id: {session_id}\")\n        self.redis.set(session_id, json.dumps(entity_mappings))\n</pre> class RedisStateService(StateService):     def __init__(self):         self.redis = redis.Redis(             host=config.Redis.hostname,             port=config.Redis.port,             db=0,             password=config.Redis.key,             ssl=config.Redis.ssl)      def get_state(self, session_id):         \"\"\"Get the state for the given session_id\"\"\"          logger.info(f\"Get state for session_id: {session_id}\")         json_data = self.redis.get(session_id)         if json_data is None:             logger.info(f\"No state found for session_id: {session_id}\")             return None                  entity_mappings = json.loads(json_data)          return entity_mappings      def set_state(self, session_id, entity_mappings):         \"\"\"Set the state for the given session_id\"\"\"          logger.info(f\"Seve state for session_id: {session_id}\")         self.redis.set(session_id, json.dumps(entity_mappings))"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/api/services/state/state_service/","title":"State service","text":"In\u00a0[\u00a0]: Copied! <pre>from abc import ABC, abstractmethod\n</pre> from abc import ABC, abstractmethod In\u00a0[\u00a0]: Copied! <pre>class StateService(ABC):\n    @abstractmethod\n    def get_state(self, session_id):\n        pass\n    \n    @abstractmethod\n    def set_state(self, session_id, entity_mappings):\n        pass\n</pre> class StateService(ABC):     @abstractmethod     def get_state(self, session_id):         pass          @abstractmethod     def set_state(self, session_id, entity_mappings):         pass"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/","title":"Demo client for Anonymizer API","text":"<p>Have an anonymous chat with the LLM right in your terminal or on a web page.</p> <p></p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/#prerequisites","title":"Prerequisites","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/#install-requirements","title":"Install requirements","text":"<pre><code>cd src/client_app\npip install -r requirements.txt\n</code></pre>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/#create-env-file","title":"Create .env file","text":"<pre><code>cp .env.sample .env\n</code></pre> <p>Then edit the configuration in the <code>.env</code> file, for example: - add your OpenAI API settings if you'd like to chat with an LLM - add anonymizer/deanonymizer API endpoints</p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/#running-the-chat","title":"Running the chat","text":""},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/#chat-with-llm-in-the-terminal","title":"Chat with LLM in the terminal","text":"<pre><code>python client.py\n</code></pre>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/#chat-with-yourself-in-the-terminal","title":"Chat with yourself in the terminal","text":"<pre><code>python client.py --mode manual\n</code></pre>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/#chat-with-llm-on-a-web-page","title":"Chat with LLM on a web page","text":"<pre><code>python serve.py\n</code></pre>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/#dockerize-chatting-on-a-web-page","title":"Dockerize chatting on a web page","text":"<p>Build container:</p> <pre><code>docker build -t client .\n</code></pre> <p>And run it, e.g like this:</p> <pre><code>docker run -p 8081:8081 --env-file .env client\n</code></pre> <p>Note: anonymizer API (API_URL env variable) should not point to localhost, otherwise the docker container won't be able to access it.</p>"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/client/","title":"Client","text":"In\u00a0[\u00a0]: Copied! <pre>from dotenv import load_dotenv\n</pre> from dotenv import load_dotenv In\u00a0[\u00a0]: Copied! <pre>load_dotenv()\n</pre> load_dotenv() In\u00a0[\u00a0]: Copied! <pre>import argparse\nimport os\nimport requests\nfrom openai import AzureOpenAI\nfrom textual import on\nfrom textual.app import App, ComposeResult\nfrom textual.widgets import Input, Label, RichLog\nfrom textual.containers import Horizontal, Vertical\n</pre> import argparse import os import requests from openai import AzureOpenAI from textual import on from textual.app import App, ComposeResult from textual.widgets import Input, Label, RichLog from textual.containers import Horizontal, Vertical In\u00a0[\u00a0]: Copied! <pre>def anonymize(text: str, language: str, session_id: str = None):\n    url = os.getenv(\"API_URL\").rstrip(\"/\") + \"/anonymize\"\n\n    body = {\n        \"text\": text,\n        \"language\": language,\n    }\n    if session_id is not None:\n        body[\"session_id\"] = session_id\n    response = requests.post(url, json=body)\n    response.raise_for_status()\n    return response.json()\n</pre> def anonymize(text: str, language: str, session_id: str = None):     url = os.getenv(\"API_URL\").rstrip(\"/\") + \"/anonymize\"      body = {         \"text\": text,         \"language\": language,     }     if session_id is not None:         body[\"session_id\"] = session_id     response = requests.post(url, json=body)     response.raise_for_status()     return response.json() In\u00a0[\u00a0]: Copied! <pre>def deanonymize(text: str, session_id: str):\n    url = os.getenv(\"API_URL\").rstrip(\"/\") + \"/deanonymize\"\n\n    response = requests.post(\n        url,\n        json={\n            \"text\": text,\n            \"session_id\": session_id,\n        },\n    )\n    response.raise_for_status()\n    return response.json()\n</pre> def deanonymize(text: str, session_id: str):     url = os.getenv(\"API_URL\").rstrip(\"/\") + \"/deanonymize\"      response = requests.post(         url,         json={             \"text\": text,             \"session_id\": session_id,         },     )     response.raise_for_status()     return response.json() In\u00a0[\u00a0]: Copied! <pre>def get_llm_response(messages):\n    client = AzureOpenAI(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        api_version=os.getenv(\"OPENAI_API_VERSION\"),\n        azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),\n    )\n    deployment_name = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n    response = client.chat.completions.create(\n        model=deployment_name,\n        messages=messages,\n        max_tokens=100,\n    )\n    return response.choices[0].message.content\n</pre> def get_llm_response(messages):     client = AzureOpenAI(         api_key=os.getenv(\"OPENAI_API_KEY\"),         api_version=os.getenv(\"OPENAI_API_VERSION\"),         azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),     )     deployment_name = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")     response = client.chat.completions.create(         model=deployment_name,         messages=messages,         max_tokens=100,     )     return response.choices[0].message.content In\u00a0[\u00a0]: Copied! <pre>class InputApp(App):\n    CSS = \"\"\"\n    Input {\n        border: red 60%;\n    }\n    Input:focus {\n        border: tall $success;\n    }\n    Label {\n        margin: 1 2;\n    }\n    RichLog {\n        margin-top: 1;\n        margin-left: 1;\n    }\n    \"\"\"\n\n    def __init__(self, mode: str, language: str) -&gt; None:\n        super().__init__()\n        self._mode = mode\n        self._lang = language\n        self._llm_message_history = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You're a friendly assistant\",\n            },  # system prompt\n        ]\n        self._session_id = None\n\n    def compose(self) -&gt; ComposeResult:\n        with Horizontal():\n            with Vertical(classes=\"column\"):\n                yield Label(\"Human view\")\n                yield Input(\n                    placeholder=\"Enter text...\",\n                    id=\"person_input\",\n                )\n                yield RichLog(id=\"person_text\", highlight=True, markup=True, wrap=True)\n\n            with Vertical(classes=\"column\"):\n                yield Label(\"LLM view\")\n                yield Input(\n                    placeholder=\"Enter text...\" if self._mode == 'manual' else \"use --mode manual to chat on behalf of the LLM\",\n                    id=\"llm_input\",\n                    disabled=True,\n                )\n                yield RichLog(id=\"llm_text\", highlight=True, markup=True, wrap=True)\n\n    @on(Input.Submitted)\n    def handle(self, event: Input.Submitted) -&gt; None:\n        person_input = self.query_one(\"#person_input\")\n        llm_input = self.query_one(\"#llm_input\")\n\n        llm_text = self.query_one(\"#llm_text\", RichLog)\n        person_text = self.query_one(\"#person_text\", RichLog)\n\n        if event.input.id == \"person_input\":\n            person_text.write(\"[bold magenta]  You:[/] \" + event.value)\n\n            anonymizer_response = anonymize(\n                text=event.value, language=self._lang, session_id=self._session_id\n            )\n            text_for_llm = anonymizer_response[\"text\"]\n            self._session_id = anonymizer_response[\"session_id\"]\n\n            llm_text.write(\"[bold magenta]Input:[/] \" + text_for_llm)\n            self._llm_message_history.append({\"role\": \"user\", \"content\": text_for_llm})\n\n            if self._mode == \"llm\":\n                # let LLM generate response...\n                llm_response = get_llm_response(self._llm_message_history)\n                self._llm_message_history.append(\n                    {\"role\": \"assistant\", \"content\": llm_response}\n                )\n\n                deanonymizer_response = deanonymize(\n                    text=llm_response, session_id=self._session_id\n                )\n                text_for_person = deanonymizer_response[\"text\"]\n\n                person_text.write(\"[bold cyan]Agent:[/] \" + text_for_person)\n                llm_text.write(\"[bold cyan]  LLM:[/] \" + llm_response)\n            else:\n                # ...or let the person enter the response manually\n                llm_input.disabled = False\n                person_input.disabled = True\n                llm_input.focus()\n\n        if event.input.id == \"llm_input\":\n            deanonymizer_response = deanonymize(\n                text=event.value, session_id=self._session_id\n            )\n            text_for_person = deanonymizer_response[\"text\"]\n\n            person_text.write(\"[bold cyan]Agent:[/] \" + text_for_person)\n            llm_text.write(\"[bold cyan]  LLM:[/] \" + event.value)\n            llm_input.disabled = True\n            person_input.disabled = False\n            person_input.focus()\n\n        event.input.clear()\n</pre> class InputApp(App):     CSS = \"\"\"     Input {         border: red 60%;     }     Input:focus {         border: tall $success;     }     Label {         margin: 1 2;     }     RichLog {         margin-top: 1;         margin-left: 1;     }     \"\"\"      def __init__(self, mode: str, language: str) -&gt; None:         super().__init__()         self._mode = mode         self._lang = language         self._llm_message_history = [             {                 \"role\": \"system\",                 \"content\": \"You're a friendly assistant\",             },  # system prompt         ]         self._session_id = None      def compose(self) -&gt; ComposeResult:         with Horizontal():             with Vertical(classes=\"column\"):                 yield Label(\"Human view\")                 yield Input(                     placeholder=\"Enter text...\",                     id=\"person_input\",                 )                 yield RichLog(id=\"person_text\", highlight=True, markup=True, wrap=True)              with Vertical(classes=\"column\"):                 yield Label(\"LLM view\")                 yield Input(                     placeholder=\"Enter text...\" if self._mode == 'manual' else \"use --mode manual to chat on behalf of the LLM\",                     id=\"llm_input\",                     disabled=True,                 )                 yield RichLog(id=\"llm_text\", highlight=True, markup=True, wrap=True)      @on(Input.Submitted)     def handle(self, event: Input.Submitted) -&gt; None:         person_input = self.query_one(\"#person_input\")         llm_input = self.query_one(\"#llm_input\")          llm_text = self.query_one(\"#llm_text\", RichLog)         person_text = self.query_one(\"#person_text\", RichLog)          if event.input.id == \"person_input\":             person_text.write(\"[bold magenta]  You:[/] \" + event.value)              anonymizer_response = anonymize(                 text=event.value, language=self._lang, session_id=self._session_id             )             text_for_llm = anonymizer_response[\"text\"]             self._session_id = anonymizer_response[\"session_id\"]              llm_text.write(\"[bold magenta]Input:[/] \" + text_for_llm)             self._llm_message_history.append({\"role\": \"user\", \"content\": text_for_llm})              if self._mode == \"llm\":                 # let LLM generate response...                 llm_response = get_llm_response(self._llm_message_history)                 self._llm_message_history.append(                     {\"role\": \"assistant\", \"content\": llm_response}                 )                  deanonymizer_response = deanonymize(                     text=llm_response, session_id=self._session_id                 )                 text_for_person = deanonymizer_response[\"text\"]                  person_text.write(\"[bold cyan]Agent:[/] \" + text_for_person)                 llm_text.write(\"[bold cyan]  LLM:[/] \" + llm_response)             else:                 # ...or let the person enter the response manually                 llm_input.disabled = False                 person_input.disabled = True                 llm_input.focus()          if event.input.id == \"llm_input\":             deanonymizer_response = deanonymize(                 text=event.value, session_id=self._session_id             )             text_for_person = deanonymizer_response[\"text\"]              person_text.write(\"[bold cyan]Agent:[/] \" + text_for_person)             llm_text.write(\"[bold cyan]  LLM:[/] \" + event.value)             llm_input.disabled = True             person_input.disabled = False             person_input.focus()          event.input.clear() In\u00a0[\u00a0]: Copied! <pre>def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--mode\",\n        choices=[\"manual\", \"llm\"],\n        default=\"llm\",\n        help=\"Chat mode: manual or llm\",\n    )\n    parser.add_argument(\n        \"--language\",\n        default=\"en\",\n        help=\"Chat language: one of the languages supported by the API service\",\n    )\n    args = parser.parse_args()\n\n    app = InputApp(mode=args.mode, language=args.language)\n    app.run()\n</pre> def main():     parser = argparse.ArgumentParser()     parser.add_argument(         \"--mode\",         choices=[\"manual\", \"llm\"],         default=\"llm\",         help=\"Chat mode: manual or llm\",     )     parser.add_argument(         \"--language\",         default=\"en\",         help=\"Chat language: one of the languages supported by the API service\",     )     args = parser.parse_args()      app = InputApp(mode=args.mode, language=args.language)     app.run() In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"samples/deployments/openai-anonymaztion-and-deanonymaztion-best-practices/src/client_app/serve/","title":"Serve","text":"In\u00a0[\u00a0]: Copied! <pre>import configargparse\nfrom textual_serve.server import Server\n</pre> import configargparse from textual_serve.server import Server In\u00a0[\u00a0]: Copied! <pre>def main():\n    parser = configargparse.ArgParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\", env_var='TEXTUAL_HOST')\n    parser.add_argument(\"--port\", type=int, default=8000, env_var='TEXTUAL_PORT')\n    parser.add_argument(\"--public_url\", type=str, default=None, env_var='TEXTUAL_PUBLIC_URL')\n    args = parser.parse_args()\n\n    server = Server(\"python client.py --mode llm\", host=args.host, port=args.port, public_url=args.public_url)\n    server.serve()\n</pre> def main():     parser = configargparse.ArgParser()     parser.add_argument(\"--host\", type=str, default=\"localhost\", env_var='TEXTUAL_HOST')     parser.add_argument(\"--port\", type=int, default=8000, env_var='TEXTUAL_PORT')     parser.add_argument(\"--public_url\", type=str, default=None, env_var='TEXTUAL_PUBLIC_URL')     args = parser.parse_args()      server = Server(\"python client.py --mode llm\", host=args.host, port=args.port, public_url=args.public_url)     server.serve() In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"samples/deployments/spark/","title":"Anonymize PII using Presidio on Spark","text":"<p>You can leverages presidio to perform data anonymization as part of spark notebooks.</p> <p>The following sample uses Azure Databricks and simple text files hosted on Azure Blob Storage. However, it can easily change to fit any other scenario which requires PII analysis or anonymization as part of spark jobs.</p> <p>Note that this code works for Databricks runtime 8.1 (spark 3.1.1) and the libraries described here.</p>"},{"location":"samples/deployments/spark/#the-basics-of-working-with-presidio-in-spark","title":"The basics of working with Presidio in Spark","text":"<p>A typical use case of Presidio in Spark is transforming a text column in a data frame, by anonymizing its content. The following code sample, a part of transform presidio notebook, is the basis of the e2e sample which uses Azure Databricks as the Spark environment.</p> <pre><code>anonymized_column = \"value\" # name of column to anonymize\nanalyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n\n# broadcast the engines to the cluster nodes\nbroadcasted_analyzer = sc.broadcast(analyzer)\nbroadcasted_anonymizer = sc.broadcast(anonymizer)\n\n# define a pandas UDF function and a series function over it.\ndef anonymize_text(text: str) -&gt; str:\n    analyzer = broadcasted_analyzer.value\n    anonymizer = broadcasted_anonymizer.value\n    analyzer_results = analyzer.analyze(text=text, language=\"en\")\n    anonymized_results = anonymizer.anonymize(\n        text=text,\n        analyzer_results=analyzer_results,\n        operators={\n            \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"&lt;ANONYMIZED&gt;\"})\n        },\n    )\n    return anonymized_results.text\n\n\ndef anonymize_series(s: pd.Series) -&gt; pd.Series:\n    return s.apply(anonymize_text)\n\n\n# define a the function as pandas UDF\nanonymize = pandas_udf(anonymize_series, returnType=StringType())\n\n# apply the udf\nanonymized_df = input_df.withColumn(\n    anonymized_column, anonymize(col(anonymized_column))\n)\n</code></pre>"},{"location":"samples/deployments/spark/#pre-requisites","title":"Pre-requisites","text":"<p>If you do not have an instance of Azure Databricks, follow through with the following steps to provision and setup the required infrastructure.</p> <p>If you do have a Databricks workspace and a cluster you wish to configure to run Presidio, jump over to the Configure an existing cluster section.</p>"},{"location":"samples/deployments/spark/#deploy-infrastructure","title":"Deploy Infrastructure","text":"<p>Provision the Azure resources by running the following script.</p> <pre><code>export RESOURCE_GROUP=[resource group name]\nexport STORAGE_ACCOUNT_NAME=[storage account name]\nexport STORAGE_CONTAINER_NAME=[blob container name]\nexport DATABRICKS_WORKSPACE_NAME=[databricks workspace name]\nexport DATABRICKS_SKU=[basic/standard/premium]\nexport LOCATION=[location]\n\n# Create the resource group\naz group create --name $RESOURCE_GROUP --location $LOCATION\n\n# Use ARM template to build the resources and get back the workspace URL\ndeployment_response=$(az deployment group create -g $RESOURCE_GROUP --template-file ./docs/samples/deployments/spark/arm-template/databricks.json  --parameters location=$LOCATION workspaceName=$DATABRICKS_WORKSPACE_NAME storageAccountName=$STORAGE_ACCOUNT_NAME containerName=$STORAGE_CONTAINER_NAME)\n\nexport DATABRICKS_WORKSPACE_URL=$(echo $deployment_response | jq -r \".properties.outputs.workspaceUrl.value\")\nexport DATABRICKS_WORKSPACE_ID=$(echo $deployment_response | jq -r \".properties.outputs.workspaceId.value\")\n</code></pre>"},{"location":"samples/deployments/spark/#setup-databricks","title":"Setup Databricks","text":"<p>The following script will setup a new cluster in the databricks workspace and prepare it to run presidio anonymization jobs. Once finished, the script will output an access key which you can use when working with databricks cli.</p> <pre><code>sh ./scripts/configure_databricks.sh\n</code></pre>"},{"location":"samples/deployments/spark/#configure-an-existing-cluster","title":"Configure an existing cluster","text":"<p>Only follow through with the steps in this section if you have an existing databricks workspace and clsuter you wish to configure to run presidio. If you've followed through with the \"Deploy Infrastructure\" and \"Setup Databricks\" sections you do not have to run the script in this section.</p>"},{"location":"samples/deployments/spark/#set-up-secret-scope-and-secrets-for-storage-account","title":"Set up secret scope and secrets for storage account","text":"<p>Add an Azure Storage account key to secret scope.</p> <pre><code>STORAGE_PRIMARY_KEY=[Primary key of storage account]\n\ndatabricks secrets create-scope --scope storage_scope --initial-manage-principal users\ndatabricks secrets put --scope storage_scope --key storage_account_access_key --string-value \"$STORAGE_PRIMARY_KEY\"\n</code></pre>"},{"location":"samples/deployments/spark/#upload-or-update-cluster-init-scripts","title":"Upload or update cluster init scripts","text":"<p>Presidio libraries are loaded to the cluster on init. Upload the cluster setup script or add its content to the existing cluster's init script.</p> <pre><code>databricks fs cp \"./setup/startup.sh\" \"dbfs:/FileStore/dependencies/startup.sh\"\n</code></pre> <p>Setup the cluster to run the init script.</p>"},{"location":"samples/deployments/spark/#upload-presidio-notebooks","title":"Upload presidio notebooks","text":"<pre><code>databricks workspace import_dir \"./notebooks\" \"/notebooks\" --overwrite\n</code></pre>"},{"location":"samples/deployments/spark/#update-cluster-environment","title":"Update cluster environment","text":"<p>Add the following environment variables to your databricks cluster:</p> <pre><code>\"STORAGE_MOUNT_NAME\": \"/mnt/files\"\n\"STORAGE_CONTAINER_NAME\": [Blob container name]\n\"STORAGE_ACCOUNT_NAME\": [Storage account name]\n</code></pre>"},{"location":"samples/deployments/spark/#mount-the-storage-container","title":"Mount the storage container","text":"<p>Run the notebook 00_setup to mount the storage account to databricks.</p>"},{"location":"samples/deployments/spark/#running-the-sample","title":"Running the sample","text":""},{"location":"samples/deployments/spark/#configure-presidio-transformation-notebook","title":"Configure Presidio transformation notebook","text":"<p>From Databricks workspace, under notebooks folder, open the provided 01_transform_presidio notebook and attach it to the cluster preisidio_cluster. Run the first code-cell and note the following parameters on the top end of the notebook (notebook widgets) and set them accordingly</p> <ul> <li>Input File Format - text (selected).</li> <li>Input path - a folder on the container where input files are found.</li> <li>Output Folder - a folder on the container where output files will be written to.</li> <li>Column to Anonymize - value (selected).</li> </ul>"},{"location":"samples/deployments/spark/#run-the-notebook","title":"Run the notebook","text":"<p>Upload a text file to the blob storage input folder, using any preferred method (Azure Portal, Azure Storage Explorer, Azure CLI).</p> <pre><code>az storage blob upload --account-name $STORAGE_ACCOUNT_NAME  --container $STORAGE_CONTAINER_NAME --file ./[file name] --name input/[file name]\n</code></pre> <p>Run the notebook cells, the output should be csv files which contain two columns, the original file name, and the anonymized content of that file.</p>"},{"location":"samples/deployments/spark/notebooks/00_setup/","title":"00 setup","text":"<p>Databricks notebook source MAGIC %md MAGIC # Mount Azure Storage blob container MAGIC MAGIC Mount an Azure Storage blob container to a databricks cluster. MAGIC MAGIC This script requires the following environment variables to be set. MAGIC MAGIC <ol> MAGIC <li>STORAGE_MOUNT_NAME - Name of mount which will be used by notebooks accessing the mount point.</li> MAGIC <li>STORAGE_ACCOUNT_NAME - Azure Storage account name.</li> MAGIC <li>STORAGE_CONTAINER_NAME - Blob container name</li> MAGIC </ol> MAGIC MAGIC Additionally, the following secrets are used. MAGIC MAGIC <ol> MAGIC <li>storage_account_access_key under scope storage_scope - storage account key.</li> MAGIC </ol></p> <p>COMMAND ----------</p> In\u00a0[\u00a0]: Copied! <pre>import os\n</pre> import os In\u00a0[\u00a0]: Copied! <pre># Environment Variables\nstorage_mount_name = os.environ[\"STORAGE_MOUNT_NAME\"]\nstorage_account_name = os.environ[\"STORAGE_ACCOUNT_NAME\"]\nstorage_container_name = os.environ[\"STORAGE_CONTAINER_NAME\"]\n</pre> # Environment Variables storage_mount_name = os.environ[\"STORAGE_MOUNT_NAME\"] storage_account_name = os.environ[\"STORAGE_ACCOUNT_NAME\"] storage_container_name = os.environ[\"STORAGE_CONTAINER_NAME\"] <p>COMMAND ----------</p> In\u00a0[\u00a0]: Copied! <pre># Retrieve storage credentials\nstorage_account_access_key = dbutils.secrets.get(\n    scope=\"storage_scope\", key=\"storage_account_access_key\"\n)\n</pre> # Retrieve storage credentials storage_account_access_key = dbutils.secrets.get(     scope=\"storage_scope\", key=\"storage_account_access_key\" ) In\u00a0[\u00a0]: Copied! <pre># unmount container if previously mounted\ndef sub_unmount(str_path):\n    if any(mount.mountPoint == str_path for mount in dbutils.fs.mounts()):\n        dbutils.fs.unmount(str_path)\n</pre> # unmount container if previously mounted def sub_unmount(str_path):     if any(mount.mountPoint == str_path for mount in dbutils.fs.mounts()):         dbutils.fs.unmount(str_path) In\u00a0[\u00a0]: Copied! <pre>sub_unmount(storage_mount_name)\n# Refresh mounts\ndbutils.fs.refreshMounts()\n</pre> sub_unmount(storage_mount_name) # Refresh mounts dbutils.fs.refreshMounts() In\u00a0[\u00a0]: Copied! <pre># mount the container\ndbutils.fs.mount(\n    source=\"wasbs://\"\n    + storage_container_name\n    + \"@\"\n    + storage_account_name\n    + \".blob.core.windows.net\",\n    mount_point=storage_mount_name,\n    extra_configs={\n        \"fs.azure.account.key.\"\n        + storage_account_name\n        + \".blob.core.windows.net\": storage_account_access_key\n    },\n)\n</pre> # mount the container dbutils.fs.mount(     source=\"wasbs://\"     + storage_container_name     + \"@\"     + storage_account_name     + \".blob.core.windows.net\",     mount_point=storage_mount_name,     extra_configs={         \"fs.azure.account.key.\"         + storage_account_name         + \".blob.core.windows.net\": storage_account_access_key     }, )"},{"location":"samples/deployments/spark/notebooks/01_transform_presidio/","title":"01 transform presidio","text":"<p>Databricks notebook source MAGIC %md MAGIC # Anonymize PII Entities with Presidio MAGIC MAGIC Using Presidio, anonymize PII content in text or csv files. MAGIC MAGIC The following code sample: MAGIC <ol> MAGIC <li>Imports the content of a single csv file, or a collection of text files, from a mounted folder</li> MAGIC <li>Anonymizes the content of the text files, or a single column in the csv dataset, using Presidio</li> MAGIC <li>Writes the anonymized content back to the mounted folder, as csv set, under the output folder. MAGIC   The output set from text files anonymization includes a column with the original file path</li> MAGIC </ol> MAGIC MAGIC Input Parameters (widgets): MAGIC <ol> MAGIC <li>Input File Format (file_format) - Input file format, can be either csv or text.</li> MAGIC <li>Input path (storage_input_path) - Folder name in case of text file, a path to a single file in case of csv.</li> MAGIC <li>Output Folder Name (storage_output_folder) - Output folder name</li> MAGIC <li>Column to Anonymize (anonymized_column) - Name of column to anonymize in case of csv. NA for text.</li> MAGIC </ol></p> <p>COMMAND ----------</p> In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import input_file_name, regexp_replace\nfrom pyspark.sql.functions import col, pandas_udf\nimport pandas as pd\nimport os\n</pre> from presidio_analyzer import AnalyzerEngine from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import OperatorConfig from pyspark.sql.types import StringType from pyspark.sql.functions import input_file_name, regexp_replace from pyspark.sql.functions import col, pandas_udf import pandas as pd import os In\u00a0[\u00a0]: Copied! <pre>dbutils.widgets.dropdown(\n    \"file_format\", \"text\", [\"text\", \"csv\"], \"Input File Format (csv/text)\"\n)\ndbutils.widgets.text(\"storage_input_path\", \"input\", \"Input path (file or folder)\")\ndbutils.widgets.text(\"storage_output_folder\", \"output\", \"Output Folder Name\")\ndbutils.widgets.text(\"anonymized_column\", \"value\", \"Column to Anonymize\")\n</pre> dbutils.widgets.dropdown(     \"file_format\", \"text\", [\"text\", \"csv\"], \"Input File Format (csv/text)\" ) dbutils.widgets.text(\"storage_input_path\", \"input\", \"Input path (file or folder)\") dbutils.widgets.text(\"storage_output_folder\", \"output\", \"Output Folder Name\") dbutils.widgets.text(\"anonymized_column\", \"value\", \"Column to Anonymize\") <p>COMMAND ----------</p> <p>MAGIC %md MAGIC # Import the text files from mounted folder</p> <p>COMMAND ----------</p> In\u00a0[\u00a0]: Copied! <pre>storage_mount_name = os.environ[\"STORAGE_MOUNT_NAME\"]\nstorage_input_path = dbutils.widgets.get(\"storage_input_path\")\nstorage_output_folder = dbutils.widgets.get(\"storage_output_folder\")\nfile_format = dbutils.widgets.get(\"file_format\")\nanonymized_column = dbutils.widgets.get(\"anonymized_column\")\n</pre> storage_mount_name = os.environ[\"STORAGE_MOUNT_NAME\"] storage_input_path = dbutils.widgets.get(\"storage_input_path\") storage_output_folder = dbutils.widgets.get(\"storage_output_folder\") file_format = dbutils.widgets.get(\"file_format\") anonymized_column = dbutils.widgets.get(\"anonymized_column\") In\u00a0[\u00a0]: Copied! <pre>if file_format == \"csv\":\n    input_df = spark.read.option(\"header\", \"true\").csv(\n        storage_mount_name + \"/\" + storage_input_path\n    )\nelif file_format == \"text\":\n    input_df = (\n        spark.read.text(storage_mount_name + \"/\" + storage_input_path + \"/*\")\n        .withColumn(\"filename\", input_file_name())\n        .withColumn(\n            \"filename\",\n            regexp_replace(\"filename\", \"^.*(\" + storage_mount_name + \"/)\", \"\"),\n        )\n    )\n</pre> if file_format == \"csv\":     input_df = spark.read.option(\"header\", \"true\").csv(         storage_mount_name + \"/\" + storage_input_path     ) elif file_format == \"text\":     input_df = (         spark.read.text(storage_mount_name + \"/\" + storage_input_path + \"/*\")         .withColumn(\"filename\", input_file_name())         .withColumn(             \"filename\",             regexp_replace(\"filename\", \"^.*(\" + storage_mount_name + \"/)\", \"\"),         )     ) In\u00a0[\u00a0]: Copied! <pre># load the files\ndisplay(input_df)\n</pre> # load the files display(input_df) <p>COMMAND ----------</p> <p>MAGIC %md MAGIC # Anonymize text using Presidio</p> <p>COMMAND ----------</p> In\u00a0[\u00a0]: Copied! <pre>analyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\nbroadcasted_analyzer = sc.broadcast(analyzer)\nbroadcasted_anonymizer = sc.broadcast(anonymizer)\n</pre> analyzer = AnalyzerEngine() anonymizer = AnonymizerEngine() broadcasted_analyzer = sc.broadcast(analyzer) broadcasted_anonymizer = sc.broadcast(anonymizer) <p>define a pandas UDF function and a series function over it. Note that analyzer and anonymizer are broadcasted.</p> In\u00a0[\u00a0]: Copied! <pre>def anonymize_text(text: str) -&gt; str:\n    analyzer = broadcasted_analyzer.value\n    anonymizer = broadcasted_anonymizer.value\n    analyzer_results = analyzer.analyze(text=text, language=\"en\")\n    anonymized_results = anonymizer.anonymize(\n        text=text,\n        analyzer_results=analyzer_results,\n        operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"&lt;ANONYMIZED&gt;\"})},\n    )\n    return anonymized_results.text\n</pre> def anonymize_text(text: str) -&gt; str:     analyzer = broadcasted_analyzer.value     anonymizer = broadcasted_anonymizer.value     analyzer_results = analyzer.analyze(text=text, language=\"en\")     anonymized_results = anonymizer.anonymize(         text=text,         analyzer_results=analyzer_results,         operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"\"})},     )     return anonymized_results.text In\u00a0[\u00a0]: Copied! <pre>def anonymize_series(s: pd.Series) -&gt; pd.Series:\n    return s.apply(anonymize_text)\n</pre> def anonymize_series(s: pd.Series) -&gt; pd.Series:     return s.apply(anonymize_text) In\u00a0[\u00a0]: Copied! <pre># define a the function as pandas UDF\nanonymize = pandas_udf(anonymize_series, returnType=StringType())\n</pre> # define a the function as pandas UDF anonymize = pandas_udf(anonymize_series, returnType=StringType()) In\u00a0[\u00a0]: Copied! <pre># apply the udf\nanonymized_df = input_df.withColumn(\n    anonymized_column, anonymize(col(anonymized_column))\n)\ndisplay(anonymized_df)\n</pre> # apply the udf anonymized_df = input_df.withColumn(     anonymized_column, anonymize(col(anonymized_column)) ) display(anonymized_df) <p>COMMAND ----------</p> <p>MAGIC %md MAGIC # Write the anonymized content back to mounted folder</p> <p>COMMAND ----------</p> <p>write the dataset to output folder</p> In\u00a0[\u00a0]: Copied! <pre>anonymized_df.write.option(\"header\", \"true\").csv(\n    storage_mount_name + \"/\" + storage_output_folder\n)\n</pre> anonymized_df.write.option(\"header\", \"true\").csv(     storage_mount_name + \"/\" + storage_output_folder ) <p>COMMAND ----------</p>"},{"location":"samples/docker/","title":"Using Presidio in Docker","text":""},{"location":"samples/docker/#description","title":"Description","text":"<p>Presidio can expose REST endpoints for each service using Flask and Docker. Follow the installation guide to learn how to install and run presidio-analyzer and presidio-anonymizer using docker.</p>"},{"location":"samples/docker/#postman-collection","title":"Postman collection","text":"<p>This repository contains a postman collection with sample REST API request for each service. Follow this tutorial to learn how to export the sample requests into postman</p> <ol> <li>Download Presidio Analyzer postman requests</li> <li>Download Presidio Anonymizer postman requests</li> </ol>"},{"location":"samples/docker/#sample-api-calls","title":"Sample API Calls","text":""},{"location":"samples/docker/#simple-text-analysis","title":"Simple Text Analysis","text":"<pre><code>curl -X POST http://localhost:5002/analyze -H \"Content-type: application/json\" --data \"{ \\\"text\\\": \\\"John Smith drivers license is AC432223\\\", \\\"language\\\" : \\\"en\\\"}\"\n</code></pre>"},{"location":"samples/docker/#simple-text-anonymization","title":"Simple Text Anonymization","text":"<pre><code>curl -X POST http://localhost:5001/anonymize -H \"Content-type: application/json\" --data \"{\\\"text\\\": \\\"hello world, my name is Jane Doe. My number is: 034453334\\\", \\\"analyzer_results\\\": [{\\\"start\\\": 24, \\\"end\\\": 32, \\\"score\\\": 0.8, \\\"entity_type\\\": \\\"NAME\\\"}, { \\\"start\\\": 48, \\\"end\\\": 57,  \\\"score\\\": 0.95,\\\"entity_type\\\": \\\"PHONE_NUMBER\\\" }],  \\\"anonymizers\\\": {\\\"DEFAULT\\\": { \\\"type\\\": \\\"replace\\\", \\\"new_value\\\": \\\"ANONYMIZED\\\" },\\\"PHONE_NUMBER\\\": { \\\"type\\\": \\\"mask\\\", \\\"masking_char\\\": \\\"*\\\", \\\"chars_to_mask\\\": 4, \\\"from_end\\\": true }}}\"\n</code></pre>"},{"location":"samples/docker/litellm/","title":"LiteLLM (OpenAI Proxy) with Presidio","text":"<p>Run Presidio PII Masking across Anthropic/Gemini/Bedrock/etc. calls with LiteLLM</p> <p>\ud83d\udc49 Refer to LiteLLM Docs for detailed guide</p> <p>Flow: App &lt;-&gt; <code>LiteLLM Proxy + Presidio PII Masking</code> &lt;-&gt; LLM Provider</p>"},{"location":"samples/docker/litellm/#pre-requiesites","title":"Pre-Requiesites","text":"<ul> <li>Run <code>pip install 'litellm[proxy]'</code> Docs</li> <li>Setup Presidio Docker</li> </ul>"},{"location":"samples/docker/litellm/#quick-start","title":"Quick Start","text":""},{"location":"samples/docker/litellm/#step-1-add-to-env","title":"Step 1. Add to env","text":"<pre><code>export PRESIDIO_ANALYZER_API_BASE=\"http://localhost:5002\"\nexport PRESIDIO_ANONYMIZER_API_BASE=\"http://localhost:5001\"\nexport OPENAI_API_KEY=\"sk-...\"\n</code></pre>"},{"location":"samples/docker/litellm/#step-2-set-presidio-as-a-callback-in-configyaml","title":"Step 2. Set Presidio as a callback in config.yaml","text":"<pre><code>model_list:\n  - model_name: my-openai-model ### RECEIVED MODEL NAME ###\n    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input\n      model: gpt-3.5-turbo ### MODEL NAME sent to `litellm.completion()` ###\n\nlitellm_settings: \n    callbacks = [\"presidio\"]\n</code></pre>"},{"location":"samples/docker/litellm/#step-3-start-proxy","title":"Step 3. Start proxy","text":"<pre><code>litellm --config /path/to/config.yaml\n</code></pre> <p>This will mask the input going to the llm provider</p>"},{"location":"samples/docker/litellm/#output-parsing","title":"Output parsing","text":"<p>LLM responses can sometimes contain the masked tokens. </p> <p>For presidio 'replace' operations, LiteLLM can check the LLM response and replace the masked token with the user-submitted values. </p> <p>Just set <code>litellm.output_parse_pii = True</code>, to enable this. </p> <pre><code>litellm_settings:\n    output_parse_pii: true\n</code></pre> <p>**Expected Flow: **</p> <ol> <li> <p>User Input: \"hello world, my name is Jane Doe. My number is: 034453334\"</p> </li> <li> <p>LLM Input: \"hello world, my name is [PERSON]. My number is: [PHONE_NUMBER]\"</p> </li> <li> <p>LLM Response: \"Hey [PERSON], nice to meet you!\"</p> </li> <li> <p>User Response: \"Hey Jane Doe, nice to meet you!\"</p> </li> </ol>"},{"location":"samples/docker/litellm/#ad-hoc-recognizers","title":"Ad-hoc recognizers","text":"<p>Send ad-hoc recognizers to presidio <code>/analyze</code> by passing a json file to the proxy </p> <p>Example ad-hoc recognizer</p> <pre><code>litellm_settings: \n  callbacks: [\"presidio\"]\n  presidio_ad_hoc_recognizers: \"./hooks/example_presidio_ad_hoc_recognizer.json\"\n</code></pre> <p>You can see this working, when you run the proxy: </p> <pre><code>litellm --config /path/to/config.yaml --debug\n</code></pre> <p>Make a chat completions request, example:</p> <pre><code>{\n  \"model\": \"azure-gpt-3.5\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"John Smith AHV number is 756.3026.0705.92. Zip code: 1334023\"}]\n}\n</code></pre> <p>And search for any log starting with <code>Presidio PII Masking</code>, example: <pre><code>Presidio PII Masking: Redacted pii message: &lt;PERSON&gt; AHV number is &lt;AHV_NUMBER&gt;. Zip code: &lt;US_DRIVER_LICENSE&gt;\n</code></pre></p>"},{"location":"samples/docker/litellm/#turn-onoff-per-key","title":"Turn on/off per key","text":"<p>LiteLLM lets you create virtual keys for calling the proxy. You can use these to control model access, set budgets, track usage, etc. </p> <p>Turn off PII masking for a given key. </p> <p>Do this by setting <code>permissions: {\"pii\": false}</code>, when generating a key. </p> <pre><code>curl --location 'http://0.0.0.0:4000/key/generate' \\\n--header 'Authorization: Bearer sk-1234' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"permissions\": {\"pii\": false}\n}'\n</code></pre>"},{"location":"samples/docker/litellm/#turn-onoff-per-request","title":"Turn on/off per request","text":"<p>The proxy supports 2 request-level PII controls:</p> <ul> <li>no-pii: Optional(bool) - Allow user to turn off pii masking per request.</li> <li>output_parse_pii: Optional(bool) - Allow user to turn off pii output parsing per request. Output Parsing</li> </ul>"},{"location":"samples/docker/litellm/#usage","title":"Usage","text":"<p>Step 1. Create key with pii permissions</p> <p>Set <code>allow_pii_controls</code> to true for a given key. This will allow the user to set request-level PII controls.</p> <pre><code>curl --location 'http://0.0.0.0:4000/key/generate' \\\n--header 'Authorization: Bearer my-master-key' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"permissions\": {\"allow_pii_controls\": true}\n}'\n</code></pre> <p>Step 2. Turn off pii output parsing</p> <pre><code>import os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n        base_url=\"http://0.0.0.0:4000\"\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"My name is Jane Doe, my number is 8382043839\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n    extra_body={\n        \"content_safety\": {\"output_parse_pii\": False} \n    }\n)\n</code></pre> <p>Step 3: See response</p> <pre><code>{\n  \"id\": \"chatcmpl-8c5qbGTILZa1S4CK3b31yj5N40hFN\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"Hi [PERSON], what can I help you with?\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1704089632,\n  \"model\": \"gpt-35-turbo\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 47,\n    \"prompt_tokens\": 12,\n    \"total_tokens\": 59\n  },\n  \"_response_ms\": 1753.426\n}\n</code></pre>"},{"location":"samples/docker/litellm/#turn-on-for-logging-only","title":"Turn on for logging only","text":"<p>Only apply PII Masking before logging to Langfuse, etc.</p> <p>Not on the actual llm api request / response.</p> <p>This is currently only applied for  - <code>/chat/completion</code> requests - on 'success' logging</p> <ol> <li> <p>Setup config.yaml <pre><code>litellm_settings:\n  presidio_logging_only: true \n\nmodel_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: gpt-3.5-turbo\n      api_key: os.environ/OPENAI_API_KEY\n</code></pre></p> </li> <li> <p>Start proxy</p> </li> </ol> <pre><code>litellm --config /path/to/config.yaml\n</code></pre> <ol> <li>Test it! </li> </ol> <pre><code>curl -X POST 'http://0.0.0.0:4000/chat/completions' \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer sk-1234' \\\n-D '{\n  \"model\": \"gpt-3.5-turbo\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Hi, my name is Jane!\"\n    }\n  ]\n  }'\n</code></pre> <p>Expected Logged Response</p> <pre><code>Hi, my name is &lt;PERSON&gt;!\n</code></pre>"},{"location":"samples/fabric/","title":"Presidio with Fabric","text":"<p>This folder contains guides and samples for running Presidio in Microsoft Fabric notebooks with Spark for scalable PII detection and anonymization.</p>"},{"location":"samples/fabric/#contents","title":"Contents","text":"<ol> <li>Environment Setup - How to set up your Fabric environment with the required dependencies.</li> <li>Presidio and Spark Notebook - The sample notebook demonstrating PII detection and anonymization using Presidio with Spark.</li> <li>Sample Data - Example CSV data for testing the PII detection and anonymization workflow.</li> </ol>"},{"location":"samples/fabric/#overview","title":"Overview","text":"<p>Microsoft Fabric provides a unified analytics platform, and these samples demonstrate how to leverage Presidio's PII detection and anonymization capabilities at scale using Fabric's Spark processing. The integration enables data engineers and analysts to:</p> <ul> <li>Detect PII in large datasets using distributed Spark processing</li> <li>Anonymize sensitive information in a scalable manner</li> <li>Enhance data privacy compliance for analytics workloads</li> </ul>"},{"location":"samples/fabric/#what-youll-accomplish","title":"What You'll Accomplish","text":"<p>By the end of this guide, you'll have a working Presidio implementation in Fabric that can:</p> <ul> <li>Automatically detect various types of PII in your data at scale</li> <li>Transform sensitive data while preserving its analytical value</li> <li>Scale to handle larger datasets through Spark's distributed processing</li> <li>Integrate with your existing Fabric data workflows</li> </ul> <p>You can easily incorporate this solution into your Fabric pipelines for scheduled runs and integrated workflows, or extend it with custom detection rules and anonymization methods.</p>"},{"location":"samples/fabric/#results-preview","title":"Results Preview","text":"<p>Here's what you'll achieve when running the sample notebook:</p>"},{"location":"samples/fabric/#pii-detection-results","title":"PII Detection Results","text":"<p>Quickly identify what types of sensitive information exist in your data, helping you understand your privacy exposure:</p> <p></p>"},{"location":"samples/fabric/#anonymized-data","title":"Anonymized Data","text":"<p>Transform your data by masking or replacing sensitive information while keeping its structure intact:</p> <p></p>"},{"location":"samples/fabric/#scale-testing","title":"Scale Testing","text":"<p>See how the solution performs with larger datasets\u2014 for understanding how it will handle your production workloads:</p> <p></p>"},{"location":"samples/fabric/#delta-table-output","title":"Delta Table Output","text":"<p>Save your anonymized data directly to a Delta table, making it immediately available for downstream analytics while maintaining privacy:</p> <p></p>"},{"location":"samples/fabric/env_setup/","title":"Env setup","text":""},{"location":"samples/fabric/env_setup/#environment-setup-for-presidio-in-fabric","title":"Environment Setup for Presidio in Fabric","text":"<p>The spaCy model can be downloaded from here: English \u00b7 spaCy Models Documentation</p>"},{"location":"samples/fabric/env_setup/#1-requirements","title":"1. Requirements","text":"<ul> <li>Fabric workspace with sufficient permissions to create and manage custom environments.</li> <li>Lakehouse access for uploading large models or data files.</li> </ul>"},{"location":"samples/fabric/env_setup/#2-configure-spark-pool","title":"2. Configure Spark Pool","text":"<p>Make sure to create (or select) a valid Spark pool that you can attach to your Fabric environment.</p> <p></p>"},{"location":"samples/fabric/env_setup/#3-create-a-new-environment","title":"3. Create a New Environment","text":"<ol> <li>In your Fabric workspace, go to Settings and select New Environment.</li> <li>Provide a name (e.g., <code>presidio-env</code>) and choose the appropriate Python version.</li> <li>Configure any required settings (e.g., pinned versions, advanced options).</li> </ol>"},{"location":"samples/fabric/env_setup/#4-add-dependencies","title":"4. Add Dependencies","text":"<ol> <li>Under Public Library, add the essential libraries:</li> <li><code>presidio-analyzer</code></li> <li><code>presidio-anonymizer</code></li> <li><code>spacy</code></li> </ol> <ol> <li>For smaller SpaCy models (like <code>en_core_web_md</code> &lt; 300MB), you can include them directly in this environment.</li> </ol>"},{"location":"samples/fabric/env_setup/#5-upload-a-large-spacy-model","title":"5. Upload a Large SpaCy Model","text":"<p>If you want to use <code>en_core_web_lg</code> (which typically exceeds 300MB): 1. Upload the <code>.whl</code> file to your Lakehouse (or any location accessible by Spark). 2. You will install it within the notebook rather than from this environment.</p> <p></p>"},{"location":"samples/fabric/env_setup/#6-compute","title":"6. Compute","text":"<p>Configure your compute, make sure to use the pool configured before</p> <p></p>"},{"location":"samples/fabric/env_setup/#7-review-save","title":"7. Review &amp; Save","text":"<ul> <li>Confirm your chosen libraries appear under the Custom Library or Public Library tabs.</li> <li>Click Save to finalize your environment setup.</li> </ul>"},{"location":"samples/fabric/env_setup/#8-run-the-sample-notebook","title":"8. Run the Sample Notebook","text":"<ol> <li>Open the <code>presidio_and_spark.ipynb</code> notebook.</li> <li>When opening your notebook, ensure you pick the custom environment you created.</li> <li>Confirm you have selected the valid Spark pool you configured earlier.</li> </ol> <ol> <li>Within the notebook, if you're using a large SpaCy model, install it using:    ```python    # Please update the path to your model path.    %pip install /lakehouse/default/Files/presidio/models/en_core_web_lg-3.8.0-py3-none-any.whl</li> </ol>"},{"location":"samples/fabric/artifacts/presidio_and_spark/","title":"Presidio and spark","text":"In\u00a0[1]: Copied! <pre>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"PresidioInFabric\").getOrCreate()\n</pre> from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"PresidioInFabric\").getOrCreate() <pre>StatementMeta(, 1c67db7b-90ca-4222-8845-9aade21d8c79, 5, Finished, Available, Finished)</pre> In\u00a0[\u00a0]: Copied! <pre>%pip install /lakehouse/default/Files/presidio/models/en_core_web_lg-3.8.0-py3-none-any.whl  \n# Installing the large model from the lakehouse as it exceeds the size limit for custom libraries in the Fabric environment.\n</pre> %pip install /lakehouse/default/Files/presidio/models/en_core_web_lg-3.8.0-py3-none-any.whl   # Installing the large model from the lakehouse as it exceeds the size limit for custom libraries in the Fabric environment. In\u00a0[3]: Copied! <pre>from pyspark.sql.functions import (\n    array, lit, explode, col, monotonically_increasing_id, concat\n)\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nfrom presidio_anonymizer import AnonymizerEngine\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom presidio_anonymizer.entities import OperatorConfig\nimport pandas as pd\n</pre> from pyspark.sql.functions import (     array, lit, explode, col, monotonically_increasing_id, concat ) from presidio_analyzer import AnalyzerEngine from presidio_analyzer.nlp_engine import NlpEngineProvider from presidio_anonymizer import AnonymizerEngine from pyspark.sql.functions import udf, col from pyspark.sql.types import ArrayType, StringType from pyspark.sql.functions import pandas_udf, PandasUDFType from presidio_anonymizer.entities import OperatorConfig import pandas as pd <pre>StatementMeta(, 1c67db7b-90ca-4222-8845-9aade21d8c79, 13, Finished, Available, Finished)</pre> In\u00a0[4]: parameters Copied! <pre>num_duplicates = 5000 # for the scale part\ncsv_path = \"Files/presidio/fabric_sample_data.csv\"\nis_write_to_delta = True\ntable_namne = \"presidio_demo_table\"\npartitions_number = 100\n</pre> num_duplicates = 5000 # for the scale part csv_path = \"Files/presidio/fabric_sample_data.csv\" is_write_to_delta = True table_namne = \"presidio_demo_table\" partitions_number = 100 <pre>StatementMeta(, 1c67db7b-90ca-4222-8845-9aade21d8c79, 14, Finished, Available, Finished)</pre> In\u00a0[5]: Copied! <pre>configuration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [\n        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_md\"},\n    ]\n}\n\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine = provider.create_engine()\n\nsmall_analyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine, supported_languages=[\"en\"]\n)\n\ntext_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\"\nanalyzer_results = small_analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en')\nprint(analyzer_results)\n</pre> configuration = {     \"nlp_engine_name\": \"spacy\",     \"models\": [         {\"lang_code\": \"en\", \"model_name\": \"en_core_web_md\"},     ] }  provider = NlpEngineProvider(nlp_configuration=configuration) nlp_engine = provider.create_engine()  small_analyzer = AnalyzerEngine(     nlp_engine=nlp_engine, supported_languages=[\"en\"] )  text_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\" analyzer_results = small_analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en') print(analyzer_results) <pre>StatementMeta(, 1c67db7b-90ca-4222-8845-9aade21d8c79, 15, Finished, Available, Finished)</pre> <pre>model_to_presidio_entity_mapping is missing from configuration, using default\nlow_score_entity_names is missing from configuration, using default\nlabels_to_ignore is missing from configuration, using default\nRecognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n[type: PHONE_NUMBER, start: 46, end: 58, score: 0.75]\n</pre> In\u00a0[6]: Copied! <pre>configuration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [\n        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n    ]\n}\n\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine = provider.create_engine()\n\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine, supported_languages=[\"en\"]\n)\n\ntext_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\"\nanalyzer_results = analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en')\n\nprint(analyzer_results)\n</pre> configuration = {     \"nlp_engine_name\": \"spacy\",     \"models\": [         {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},     ] }  provider = NlpEngineProvider(nlp_configuration=configuration) nlp_engine = provider.create_engine()  analyzer = AnalyzerEngine(     nlp_engine=nlp_engine, supported_languages=[\"en\"] )  text_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\" analyzer_results = analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en')  print(analyzer_results) <pre>StatementMeta(, 1c67db7b-90ca-4222-8845-9aade21d8c79, 16, Finished, Available, Finished)</pre> <pre>model_to_presidio_entity_mapping is missing from configuration, using default\nlow_score_entity_names is missing from configuration, using default\nlabels_to_ignore is missing from configuration, using default\nRecognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\nRecognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n[type: PHONE_NUMBER, start: 46, end: 58, score: 0.75]\n</pre> In\u00a0[\u00a0]: Copied! <pre>anonymizer = AnonymizerEngine()\nbroadcasted_analyzer = spark.sparkContext.broadcast(analyzer)\nbroadcasted_anonymizer = spark.sparkContext.broadcast(anonymizer)\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_path)\ndisplay(df)\n# df.show()\n</pre> anonymizer = AnonymizerEngine() broadcasted_analyzer = spark.sparkContext.broadcast(analyzer) broadcasted_anonymizer = spark.sparkContext.broadcast(anonymizer) df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_path) display(df) # df.show() <pre>StatementMeta(, 1c67db7b-90ca-4222-8845-9aade21d8c79, 17, Finished, Available, Finished)</pre> <pre>SynapseWidget(Synapse.DataFrame, b5c48139-db77-44e4-b332-d90c99345f2d)</pre> <pre>+------------+--------------------+------------+-------------+-----+--------+--------------------+\n|        name|               email|      street|         city|state| non_pii|          user_query|\n+------------+--------------------+------------+-------------+-----+--------+--------------------+\n|    John Doe|john.doe@example.com|  123 Elm St|       Dallas|   TX|  abc123|My phone number i...|\n|  Jane Smith|jane.smith@exampl...|  456 Oak Rd|        Miami|   FL|  xyz789|Please call me at...|\n| Alice Brown|alice.brown@examp...|  99 Pine Dr|      Seattle|   WA|cust-123|SSN is 987-65-432...|\n|   Bob Davis|bob.davis@example...|10 Maple Ave|     New York|   NY| info999|Passport number i...|\n| Carol Jones|carol.jones@examp...|777 Cedar Ln|  Los Angeles|   CA| test111|My phone 333-777-...|\n| David Green|david.green@examp...|321 Birch St|      Chicago|   IL| npii-01|He said credit ca...|\n| Emily Clark|emily.clark@examp...|555 Aspen Rd|       Boston|   MA| npii-02|Passport A9876543...|\n|Frank Wilson|frank.wilson@exam...| 1010 Walnut|      Phoenix|   AZ| npii-03|SSN 666-77-8888 a...|\n|   Grace Lee|grace.lee@example...|2020 Palm St|      Orlando|   FL| npii-04|Use my card 4000-...|\n|  Henry King|henry.king@exampl...|  3030 Peach|      Atlanta|   GA| npii-05|NHSNO 0123456789 ...|\n| Irene Adams|irene.adams@examp...|   4040 Plum|       Denver|   CO| npii-06|SSN 000-11-2222 a...|\n| Jack Miller|jack.miller@examp...|   5050 Lily|      Houston|   TX| npii-07|My phone 713-555-...|\n|  Kate Allen|kate.allen@exampl...| 6060 Poplar| Philadelphia|   PA| npii-08|SSN: 123-00-4567,...|\n|   Leo Clark|leo.clark@example...|   7070 Pine|    Las Vegas|   NV| npii-09|He used card # 44...|\n|   Mona Reed|mona.reed@example...|    8080 Bay|    San Diego|   CA| npii-10|Call me at 619-55...|\n|  Nancy Ward|nancy.ward@exampl...|   9090 Moss|San Francisco|   CA| npii-11|Passport is L1234...|\n|  Oscar Hill|oscar.hill@exampl...|   1111 Hill|     Portland|   OR| npii-12|My card is 4002-1...|\n|   Paula Ray|paula.ray@example...|  2222 Cliff|       Austin|   TX| npii-13|CC 4012-8888-9999...|\n|  Ray Foster|ray.foster@exampl...|   3333 Rock|    Nashville|   TN| npii-14|Card # 5555-6666-...|\n|  Sara White|sara.white@exampl...|  4444 Shell|      Raleigh|   NC| npii-15|SSN 111-22-9999 a...|\n+------------+--------------------+------------+-------------+-----+--------+--------------------+\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>def detect_pii_in_row(*cols):\n    \"\"\"\n    Analyze each column separately so we know which substring (entity text) \n    belongs to which column. Return a dict {col_name: [ 'ENTITY_TYPE: substring', ... ] }.\n    \"\"\"\n    analyzer = broadcasted_analyzer.value\n    col_names = detect_pii_in_row.col_names\n    entities_found = {}\n\n    for idx, val in enumerate(cols):\n        if val is None:\n            continue\n        column_text = str(val)\n        results = analyzer.analyze(text=column_text, language=\"en\")\n\n        if results:\n            # Example: [\"PERSON: John Doe\", \"PHONE_NUMBER: 212-555-1111\", ...]\n            found_entities = []\n            for res in results:\n                substring = column_text[res.start:res.end]  # The actual text recognized\n                entity_str = f\"{res.entity_type}: {substring}\"\n                found_entities.append(entity_str)\n            \n            entities_found[col_names[idx]] = found_entities\n\n    # If no PII was detected at all\n    if not entities_found:\n        return \"No PII\"\n    return str(entities_found)\n\ndetect_pii_in_row.col_names = df.columns\ndetect_pii_udf = udf(detect_pii_in_row, StringType())\n\ndf_with_pii_summary = df.withColumn(\n    \"pii_summary\",\n    detect_pii_udf(*[col(c) for c in df.columns])\n)\n\ndisplay(df_with_pii_summary)\n# df_with_pii_summary.show()\n</pre> def detect_pii_in_row(*cols):     \"\"\"     Analyze each column separately so we know which substring (entity text)      belongs to which column. Return a dict {col_name: [ 'ENTITY_TYPE: substring', ... ] }.     \"\"\"     analyzer = broadcasted_analyzer.value     col_names = detect_pii_in_row.col_names     entities_found = {}      for idx, val in enumerate(cols):         if val is None:             continue         column_text = str(val)         results = analyzer.analyze(text=column_text, language=\"en\")          if results:             # Example: [\"PERSON: John Doe\", \"PHONE_NUMBER: 212-555-1111\", ...]             found_entities = []             for res in results:                 substring = column_text[res.start:res.end]  # The actual text recognized                 entity_str = f\"{res.entity_type}: {substring}\"                 found_entities.append(entity_str)                          entities_found[col_names[idx]] = found_entities      # If no PII was detected at all     if not entities_found:         return \"No PII\"     return str(entities_found)  detect_pii_in_row.col_names = df.columns detect_pii_udf = udf(detect_pii_in_row, StringType())  df_with_pii_summary = df.withColumn(     \"pii_summary\",     detect_pii_udf(*[col(c) for c in df.columns]) )  display(df_with_pii_summary) # df_with_pii_summary.show() <pre>StatementMeta(, 1c67db7b-90ca-4222-8845-9aade21d8c79, 22, Finished, Available, Finished)</pre> <pre>SynapseWidget(Synapse.DataFrame, d1fcab6e-5b73-4aa7-9287-b0f4aac44eea)</pre> <pre>+------------+--------------------+------------+-------------+-----+--------+--------------------+--------------------+\n|        name|               email|      street|         city|state| non_pii|          user_query|         pii_summary|\n+------------+--------------------+------------+-------------+-----+--------+--------------------+--------------------+\n|    John Doe|john.doe@example.com|  123 Elm St|       Dallas|   TX|  abc123|My phone number i...|{'name': ['PERSON...|\n|  Jane Smith|jane.smith@exampl...|  456 Oak Rd|        Miami|   FL|  xyz789|Please call me at...|{'name': ['PERSON...|\n| Alice Brown|alice.brown@examp...|  99 Pine Dr|      Seattle|   WA|cust-123|SSN is 987-65-432...|{'name': ['PERSON...|\n|   Bob Davis|bob.davis@example...|10 Maple Ave|     New York|   NY| info999|Passport number i...|{'name': ['PERSON...|\n| Carol Jones|carol.jones@examp...|777 Cedar Ln|  Los Angeles|   CA| test111|My phone 333-777-...|{'name': ['PERSON...|\n| David Green|david.green@examp...|321 Birch St|      Chicago|   IL| npii-01|He said credit ca...|{'name': ['PERSON...|\n| Emily Clark|emily.clark@examp...|555 Aspen Rd|       Boston|   MA| npii-02|Passport A9876543...|{'name': ['PERSON...|\n|Frank Wilson|frank.wilson@exam...| 1010 Walnut|      Phoenix|   AZ| npii-03|SSN 666-77-8888 a...|{'name': ['PERSON...|\n|   Grace Lee|grace.lee@example...|2020 Palm St|      Orlando|   FL| npii-04|Use my card 4000-...|{'name': ['PERSON...|\n|  Henry King|henry.king@exampl...|  3030 Peach|      Atlanta|   GA| npii-05|NHSNO 0123456789 ...|{'name': ['PERSON...|\n| Irene Adams|irene.adams@examp...|   4040 Plum|       Denver|   CO| npii-06|SSN 000-11-2222 a...|{'name': ['PERSON...|\n| Jack Miller|jack.miller@examp...|   5050 Lily|      Houston|   TX| npii-07|My phone 713-555-...|{'name': ['PERSON...|\n|  Kate Allen|kate.allen@exampl...| 6060 Poplar| Philadelphia|   PA| npii-08|SSN: 123-00-4567,...|{'name': ['PERSON...|\n|   Leo Clark|leo.clark@example...|   7070 Pine|    Las Vegas|   NV| npii-09|He used card # 44...|{'name': ['PERSON...|\n|   Mona Reed|mona.reed@example...|    8080 Bay|    San Diego|   CA| npii-10|Call me at 619-55...|{'name': ['PERSON...|\n|  Nancy Ward|nancy.ward@exampl...|   9090 Moss|San Francisco|   CA| npii-11|Passport is L1234...|{'name': ['PERSON...|\n|  Oscar Hill|oscar.hill@exampl...|   1111 Hill|     Portland|   OR| npii-12|My card is 4002-1...|{'name': ['PERSON...|\n|   Paula Ray|paula.ray@example...|  2222 Cliff|       Austin|   TX| npii-13|CC 4012-8888-9999...|{'name': ['PERSON...|\n|  Ray Foster|ray.foster@exampl...|   3333 Rock|    Nashville|   TN| npii-14|Card # 5555-6666-...|{'name': ['PERSON...|\n|  Sara White|sara.white@exampl...|  4444 Shell|      Raleigh|   NC| npii-15|SSN 111-22-9999 a...|{'name': ['PERSON...|\n+------------+--------------------+------------+-------------+-----+--------+--------------------+--------------------+\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\ndef anonymize_text(text: str) -&gt; str:\n    \"\"\"\n    Detect PII in the given text using the large model and replace it with an empty string.\n    \"\"\"\n    if text is None:\n        return None\n\n    analyzer = broadcasted_analyzer.value\n    anonymizer = broadcasted_anonymizer.value\n    analyze_results = analyzer.analyze(text=text, language=\"en\")\n    anonymized_result = anonymizer.anonymize(\n        text=text,\n        analyzer_results=analyze_results,\n        operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"\"})}\n)\n    return anonymized_result.text\n\n# Registering as a regular PySpark UDF\nanon_udf = udf(anonymize_text, StringType())\n\ndf_final = df_with_pii_summary.withColumn(\"anon_user_query\", anon_udf(col(\"user_query\")))\n\ndisplay(df_final)\n# df_final.show()\n</pre> from pyspark.sql.functions import udf from pyspark.sql.types import StringType  def anonymize_text(text: str) -&gt; str:     \"\"\"     Detect PII in the given text using the large model and replace it with an empty string.     \"\"\"     if text is None:         return None      analyzer = broadcasted_analyzer.value     anonymizer = broadcasted_anonymizer.value     analyze_results = analyzer.analyze(text=text, language=\"en\")     anonymized_result = anonymizer.anonymize(         text=text,         analyzer_results=analyze_results,         operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"\"})} )     return anonymized_result.text  # Registering as a regular PySpark UDF anon_udf = udf(anonymize_text, StringType())  df_final = df_with_pii_summary.withColumn(\"anon_user_query\", anon_udf(col(\"user_query\")))  display(df_final) # df_final.show() <pre>StatementMeta(, 1c67db7b-90ca-4222-8845-9aade21d8c79, 23, Finished, Available, Finished)</pre> <pre>SynapseWidget(Synapse.DataFrame, 4053ba15-5b54-4ba8-bc55-889fc129cee4)</pre> <pre>+------------+--------------------+------------+-------------+-----+--------+--------------------+--------------------+--------------------+\n|        name|               email|      street|         city|state| non_pii|          user_query|         pii_summary|     anon_user_query|\n+------------+--------------------+------------+-------------+-----+--------+--------------------+--------------------+--------------------+\n|    John Doe|john.doe@example.com|  123 Elm St|       Dallas|   TX|  abc123|My phone number i...|{'name': ['PERSON...|My phone number i...|\n|  Jane Smith|jane.smith@exampl...|  456 Oak Rd|        Miami|   FL|  xyz789|Please call me at...|{'name': ['PERSON...|Please call me at...|\n| Alice Brown|alice.brown@examp...|  99 Pine Dr|      Seattle|   WA|cust-123|SSN is 987-65-432...|{'name': ['PERSON...|SSN is &lt;US_ITIN&gt;;...|\n|   Bob Davis|bob.davis@example...|10 Maple Ave|     New York|   NY| info999|Passport number i...|{'name': ['PERSON...|Passport number i...|\n| Carol Jones|carol.jones@examp...|777 Cedar Ln|  Los Angeles|   CA| test111|My phone 333-777-...|{'name': ['PERSON...|My phone &lt;PHONE_N...|\n| David Green|david.green@examp...|321 Birch St|      Chicago|   IL| npii-01|He said credit ca...|{'name': ['PERSON...|He said credit ca...|\n| Emily Clark|emily.clark@examp...|555 Aspen Rd|       Boston|   MA| npii-02|Passport A9876543...|{'name': ['PERSON...|Passport &lt;US_DRIV...|\n|Frank Wilson|frank.wilson@exam...| 1010 Walnut|      Phoenix|   AZ| npii-03|SSN 666-77-8888 a...|{'name': ['PERSON...|SSN 666-77-8888 a...|\n|   Grace Lee|grace.lee@example...|2020 Palm St|      Orlando|   FL| npii-04|Use my card 4000-...|{'name': ['PERSON...|Use my card &lt;CRED...|\n|  Henry King|henry.king@exampl...|  3030 Peach|      Atlanta|   GA| npii-05|NHSNO 0123456789 ...|{'name': ['PERSON...|NHSNO &lt;UK_NHS&gt; an...|\n| Irene Adams|irene.adams@examp...|   4040 Plum|       Denver|   CO| npii-06|SSN 000-11-2222 a...|{'name': ['PERSON...|SSN 000-11-2222 a...|\n| Jack Miller|jack.miller@examp...|   5050 Lily|      Houston|   TX| npii-07|My phone 713-555-...|{'name': ['PERSON...|My phone &lt;PHONE_N...|\n|  Kate Allen|kate.allen@exampl...| 6060 Poplar| Philadelphia|   PA| npii-08|SSN: 123-00-4567,...|{'name': ['PERSON...|SSN: 123-00-4567,...|\n|   Leo Clark|leo.clark@example...|   7070 Pine|    Las Vegas|   NV| npii-09|He used card # 44...|{'name': ['PERSON...|He used card # &lt;I...|\n|   Mona Reed|mona.reed@example...|    8080 Bay|    San Diego|   CA| npii-10|Call me at 619-55...|{'name': ['PERSON...|Call me at &lt;PHONE...|\n|  Nancy Ward|nancy.ward@exampl...|   9090 Moss|San Francisco|   CA| npii-11|Passport is L1234...|{'name': ['PERSON...|Passport is L1234...|\n|  Oscar Hill|oscar.hill@exampl...|   1111 Hill|     Portland|   OR| npii-12|My card is 4002-1...|{'name': ['PERSON...|My card is &lt;CREDI...|\n|   Paula Ray|paula.ray@example...|  2222 Cliff|       Austin|   TX| npii-13|CC 4012-8888-9999...|{'name': ['PERSON...|CC &lt;IN_PAN&gt;9999-1...|\n|  Ray Foster|ray.foster@exampl...|   3333 Rock|    Nashville|   TN| npii-14|Card # 5555-6666-...|{'name': ['PERSON...|Card # &lt;IN_PAN&gt;77...|\n|  Sara White|sara.white@exampl...|  4444 Shell|      Raleigh|   NC| npii-15|SSN 111-22-9999 a...|{'name': ['PERSON...|SSN &lt;US_SSN&gt; and ...|\n+------------+--------------------+------------+-------------+-----+--------+--------------------+--------------------+--------------------+\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>df_expanded = df.withColumn(\n    \"duplication_array\",\n    array([lit(i) for i in range(num_duplicates)])\n)\n\ndf_test = df_expanded.withColumn(\"duplicate_id\", explode(col(\"duplication_array\")))\n\ndf_test = df_test.withColumn(\"id\", monotonically_increasing_id())\n\ndf_test = df_test.withColumn(\n    \"user_query\",\n    concat(col(\"user_query\"), lit(\" - ID: \"), col(\"id\"))\n)\n\ndf_test = df_test.drop(\"duplication_array\", \"duplicate_id\")\ndf_test = df_test.repartition(partitions_number) # repartition to show parrallel processing -should be remove/modify to allow high scales.\ndf_test = df_test.withColumn(\"anon_user_query\", anon_udf(col(\"user_query\")))\nprint(f\"total row number {df_test.count()}\") # Number of duplicates X number of rows in the DF\ndisplay(df_test.limit(partitions_number))\n# (df_test.limit(partitions_number)).show()\n</pre> df_expanded = df.withColumn(     \"duplication_array\",     array([lit(i) for i in range(num_duplicates)]) )  df_test = df_expanded.withColumn(\"duplicate_id\", explode(col(\"duplication_array\")))  df_test = df_test.withColumn(\"id\", monotonically_increasing_id())  df_test = df_test.withColumn(     \"user_query\",     concat(col(\"user_query\"), lit(\" - ID: \"), col(\"id\")) )  df_test = df_test.drop(\"duplication_array\", \"duplicate_id\") df_test = df_test.repartition(partitions_number) # repartition to show parrallel processing -should be remove/modify to allow high scales. df_test = df_test.withColumn(\"anon_user_query\", anon_udf(col(\"user_query\"))) print(f\"total row number {df_test.count()}\") # Number of duplicates X number of rows in the DF display(df_test.limit(partitions_number)) # (df_test.limit(partitions_number)).show() <pre>StatementMeta(, 1c67db7b-90ca-4222-8845-9aade21d8c79, 24, Finished, Available, Finished)</pre> <pre>total row number 100000\n</pre> <pre>SynapseWidget(Synapse.DataFrame, 055e3b9b-76b6-4d30-a98f-43928f3c8603)</pre> <pre>+------------+--------------------+------------+-----------+-----+-------+--------------------+-----+--------------------+\n|        name|               email|      street|       city|state|non_pii|          user_query|   id|     anon_user_query|\n+------------+--------------------+------------+-----------+-----+-------+--------------------+-----+--------------------+\n|   Leo Clark|leo.clark@example...|   7070 Pine|  Las Vegas|   NV|npii-09|He used card # 44...|65376|He used card # &lt;I...|\n|  Oscar Hill|oscar.hill@exampl...|   1111 Hill|   Portland|   OR|npii-12|My card is 4002-1...|84635|My card is &lt;CREDI...|\n| Emily Clark|emily.clark@examp...|555 Aspen Rd|     Boston|   MA|npii-02|Passport A9876543...|34945|Passport &lt;US_DRIV...|\n|  Oscar Hill|oscar.hill@exampl...|   1111 Hill|   Portland|   OR|npii-12|My card is 4002-1...|81800|My card is &lt;CREDI...|\n| Irene Adams|irene.adams@examp...|   4040 Plum|     Denver|   CO|npii-06|SSN 000-11-2222 a...|51834|SSN 000-11-2222 a...|\n| Jack Miller|jack.miller@examp...|   5050 Lily|    Houston|   TX|npii-07|My phone 713-555-...|55111|My phone &lt;PHONE_N...|\n|Frank Wilson|frank.wilson@exam...| 1010 Walnut|    Phoenix|   AZ|npii-03|SSN 666-77-8888 a...|38326|SSN 666-77-8888 a...|\n| David Green|david.green@examp...|321 Birch St|    Chicago|   IL|npii-01|He said credit ca...|29259|He said credit ca...|\n|  Sara White|sara.white@exampl...|  4444 Shell|    Raleigh|   NC|npii-15|SSN 111-22-9999 a...|96171|SSN &lt;US_SSN&gt; and ...|\n| David Green|david.green@examp...|321 Birch St|    Chicago|   IL|npii-01|He said credit ca...|29983|He said credit ca...|\n|   Grace Lee|grace.lee@example...|2020 Palm St|    Orlando|   FL|npii-04|Use my card 4000-...|44886|Use my card &lt;CRED...|\n|   Bob Davis|bob.davis@example...|10 Maple Ave|   New York|   NY|info999|Passport number i...|19619|Passport number i...|\n|   Bob Davis|bob.davis@example...|10 Maple Ave|   New York|   NY|info999|Passport number i...|17969|Passport number i...|\n|  Sara White|sara.white@exampl...|  4444 Shell|    Raleigh|   NC|npii-15|SSN 111-22-9999 a...|99185|SSN &lt;US_SSN&gt; and ...|\n|  Ray Foster|ray.foster@exampl...|   3333 Rock|  Nashville|   TN|npii-14|Card # 5555-6666-...|93467|Card # &lt;IN_PAN&gt;77...|\n|   Grace Lee|grace.lee@example...|2020 Palm St|    Orlando|   FL|npii-04|Use my card 4000-...|40770|Use my card &lt;CRED...|\n| Jack Miller|jack.miller@examp...|   5050 Lily|    Houston|   TX|npii-07|My phone 713-555-...|56657|My phone &lt;PHONE_N...|\n| Carol Jones|carol.jones@examp...|777 Cedar Ln|Los Angeles|   CA|test111|My phone 333-777-...|24322|My phone &lt;PHONE_N...|\n|   Grace Lee|grace.lee@example...|2020 Palm St|    Orlando|   FL|npii-04|Use my card 4000-...|41338|Use my card &lt;CRED...|\n|    John Doe|john.doe@example.com|  123 Elm St|     Dallas|   TX| abc123|My phone number i...| 3698|My phone number i...|\n+------------+--------------------+------------+-----------+-----+-------+--------------------+-----+--------------------+\nonly showing top 20 rows\n\n</pre> In\u00a0[11]: Copied! <pre>if is_write_to_delta:\n    df_test.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_namne)\n</pre> if is_write_to_delta:     df_test.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_namne) <pre>StatementMeta(, 1c67db7b-90ca-4222-8845-9aade21d8c79, 21, Finished, Available, Finished)</pre>"},{"location":"samples/fabric/artifacts/presidio_and_spark/#presidio-spark-pii-detection-anonymization","title":"Presidio + Spark: PII Detection &amp; Anonymization\u00b6","text":"<p>This notebook demonstrates how to use Presidio for PII detection and anonymization with Spark in Microsoft Fabric, leveraging SpaCy models for accurate entity recognition.</p>"},{"location":"samples/fabric/artifacts/presidio_and_spark/#table-of-contents","title":"Table of Contents\u00b6","text":"<p>1. Small Model Setup &amp; Demo: Initialize Spark session and set up a lightweight SpaCy-based PII detection model (Small models are not recommended as they might impact accuracy) 2. Large Model Setup &amp; Demo: Configure a larger SpaCy model for enhanced PII detection on bigger datasets. 3. Detect PII Summary: Run Presidio's analyzer engine on a Spark DataFrame to extract sensitive information. 4. Anonymize user_query: Apply Presidio's anonymization engine to mask detected PII while maintaining data utility. 5. Scale Data (Duplicate Rows): Expand datasets by duplicating rows, appending unique IDs, and re-anonymizing. 6. Write to Delta Table (Optional): Persist anonymized data to a Delta table for further analysis. 7. Scale-Up Testing: Evaluate performance on larger datasets by duplicating rows and measuring execution time.</p>"},{"location":"samples/fabric/artifacts/presidio_and_spark/#loading-spacy-models","title":"Loading SpaCy Models\u00b6","text":"<p>There are two methods to load SpaCy models:</p> <ol> <li><p>Via the Environment: For smaller models (e.g., <code>en_core_web_md</code> &lt; 300MB) that were included directly in your Fabric environment.</p> <ul> <li>Note: Small models are not recommended as they might impact accuracy.</li> </ul> </li> <li><p>From the Lakehouse: For larger models (e.g., <code>en_core_web_lg</code> &gt; 300MB), install them within the notebook.</p> <ul> <li>First, upload the <code>.whl</code> file to your Lakehouse</li> <li>Then install it using the pip command below</li> </ul> </li> </ol>"},{"location":"samples/fabric/artifacts/presidio_and_spark/#small-model-setup-demo","title":"Small Model Setup &amp; Demo\u00b6","text":""},{"location":"samples/fabric/artifacts/presidio_and_spark/#large-model-setup-demo","title":"Large Model Setup &amp; Demo\u00b6","text":""},{"location":"samples/fabric/artifacts/presidio_and_spark/#load-the-data-and-broadcast","title":"Load the data and broadcast\u00b6","text":"<p>Load your data from the CSV file stored in your Lakehouse (e.g., <code>fabric_sample_data.csv</code>). This sample data contains various types of PII that we'll detect and anonymize.</p> <p>We broadcast the analyzer and anonymizer objects to make them available to all worker nodes in the Spark cluster, which improves performance for distributed processing.</p>"},{"location":"samples/fabric/artifacts/presidio_and_spark/#detect-pii-summary","title":"Detect PII Summary\u00b6","text":"<p>This section demonstrates how to:</p> <ul> <li>Instantiate an AnalyzerEngine with the SpaCy NLP model</li> <li>Run PII detection across each row in your DataFrame</li> <li>Create a summary of detected PII entities by column</li> </ul> <p>The function below analyzes each column separately so we know which substring (entity text) belongs to which column.</p>"},{"location":"samples/fabric/artifacts/presidio_and_spark/#anonymize-user_query","title":"Anonymize user_query\u00b6","text":"<p>This section shows how to:</p> <ul> <li>Use the AnonymizerEngine to mask or replace sensitive information</li> <li>Apply anonymization to specific columns</li> <li>Preserve the DataFrame structure while removing PII</li> </ul> <p>We implement a UDF that detects and anonymizes text in the given column.</p>"},{"location":"samples/fabric/artifacts/presidio_and_spark/#scale-up-testing","title":"Scale-Up Testing\u00b6","text":"<p>This section lets you:</p> <ul> <li>Evaluate performance on larger datasets by duplicating rows</li> <li>Test how well the solution scales with increasing data volume</li> <li>Measure execution time for PII detection and anonymization at scale</li> </ul> <p>We expand the dataset by creating multiple copies of each row, appending unique IDs, and then re-anonymizing the text to simulate processing a larger dataset.</p>"},{"location":"samples/fabric/artifacts/presidio_and_spark/#write-to-a-delta-table-optional","title":"Write to a Delta Table (Optional)\u00b6","text":"<p>Optionally save your anonymized data to a Delta table:</p> <ul> <li>Persist results for downstream processing</li> <li>Make anonymized data available to other Fabric applications</li> <li>Enable further analysis while maintaining privacy compliance</li> </ul> <p>The following code will execute only if <code>is_write_to_delta</code> is set to <code>True</code>.</p>"},{"location":"samples/fabric/artifacts/presidio_and_spark/#conclusion","title":"Conclusion\u00b6","text":"<p>You have successfully executed this notebook in Fabric, leveraging Presidio for PII detection/anonymization and Spark for scalable data processing.</p> <p>You can incorporate it into a Fabric pipeline for scheduled runs and integrated workflows. For further customization\u2014like adding detection rules, additional anonymization methods, or advanced Spark configurations\u2014refer to the official Presidio documentation and your Fabric environment's settings.</p> <p>Enjoy building robust PII compliance workflows!</p>"},{"location":"samples/python/Anonymizing%20known%20values/","title":"Anonymizing known values","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n\n!python -m spacy download en_core_web_lg\n</pre> # download presidio !pip install presidio_analyzer presidio_anonymizer  !python -m spacy download en_core_web_lg In\u00a0[2]: Copied! <pre>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, PatternRecognizer\nfrom presidio_anonymizer import AnonymizerEngine\n</pre> from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, PatternRecognizer from presidio_anonymizer import AnonymizerEngine In\u00a0[3]: Copied! <pre># Get known values as a deny-list\nknown_names_list = [\"George\", \"Abraham\", \"Theodore\", \"Bill\", \"Barack\", \"Donald\", \"Joe\"]\n</pre> # Get known values as a deny-list known_names_list = [\"George\", \"Abraham\", \"Theodore\", \"Bill\", \"Barack\", \"Donald\", \"Joe\"]  In\u00a0[4]: Copied! <pre># Create a PatternRecognizer for the deny list\ndeny_list_recognizer = PatternRecognizer(supported_entity=\"PRESIDENT_FIRST_NAME\", deny_list=known_names_list)\n</pre> # Create a PatternRecognizer for the deny list deny_list_recognizer = PatternRecognizer(supported_entity=\"PRESIDENT_FIRST_NAME\", deny_list=known_names_list) In\u00a0[5]: Copied! <pre>registry = RecognizerRegistry()\nregistry.add_recognizer(deny_list_recognizer)\n\nanalyzer = AnalyzerEngine(registry=registry)\n\nanonymizer = AnonymizerEngine()\n</pre> registry = RecognizerRegistry() registry.add_recognizer(deny_list_recognizer)  analyzer = AnalyzerEngine(registry=registry)  anonymizer = AnonymizerEngine() In\u00a0[6]: Copied! <pre>text=\"George Washington was the first US president\"\n\nresults = analyzer.analyze(text=text, language=\"en\")\n\nprint(\"Identified entities:\")\nprint(results)\nprint(\"\")\nanonymized = anonymizer.anonymize(text=text, analyzer_results=results)\nprint(f\"Anonymized text:\\n{anonymized.text}\")\n</pre> text=\"George Washington was the first US president\"  results = analyzer.analyze(text=text, language=\"en\")  print(\"Identified entities:\") print(results) print(\"\") anonymized = anonymizer.anonymize(text=text, analyzer_results=results) print(f\"Anonymized text:\\n{anonymized.text}\") <pre>Identified entities:\n[type: PRESIDENT_FIRST_NAME, start: 0, end: 6, score: 1.0]\n\nAnonymized text:\n&lt;PRESIDENT_FIRST_NAME&gt; Washington was the first US president\n</pre> In\u00a0[7]: Copied! <pre>person1 = {\"name\": \"Martin Smith\", \n           \"special_value\":\"145A\", \n           \"free_text\": \"Martin Smith, id 145A, likes playing basketball\"}\nperson2 = {\"name\":\"Deb Schmidt\", \n           \"special_value\":\"256B\", \n           \"free_text\": \"Deb Schmidt, id 256B likes playing soccer\"}\nperson3 = {\"name\":\"R2D2\", \n           \"special_value\":\"X1T2\", \n           \"free_text\": \"X1T2 is R2D2's special value\"}\n\ndataset = [person1, person2, person3]\ndataset\n</pre> person1 = {\"name\": \"Martin Smith\",             \"special_value\":\"145A\",             \"free_text\": \"Martin Smith, id 145A, likes playing basketball\"} person2 = {\"name\":\"Deb Schmidt\",             \"special_value\":\"256B\",             \"free_text\": \"Deb Schmidt, id 256B likes playing soccer\"} person3 = {\"name\":\"R2D2\",             \"special_value\":\"X1T2\",             \"free_text\": \"X1T2 is R2D2's special value\"}  dataset = [person1, person2, person3] dataset Out[7]: <pre>[{'name': 'Martin Smith',\n  'special_value': '145A',\n  'free_text': 'Martin Smith, id 145A, likes playing basketball'},\n {'name': 'Deb Schmidt',\n  'special_value': '256B',\n  'free_text': 'Deb Schmidt, id 256B likes playing soccer'},\n {'name': 'R2D2',\n  'special_value': 'X1T2',\n  'free_text': \"X1T2 is R2D2's special value\"}]</pre> <p>We're interested in anonymizing the free text using the values contained in <code>name</code> and <code>special_value</code>. Since these values are only available in the context of one record, we use the ad-hoc recognizer capability in Presidio, instead of a generic deny-list <code>PatternRecognizer</code> added to Presidio's <code>RecognizerRegistry</code>.</p> In\u00a0[8]: Copied! <pre># Go over dataset\nfor person in dataset:\n    \n    # Get the different known values\n    name = person['name']\n    special_val = person['special_value']\n    \n    # Get the free text to anonymize\n    free_text = person['free_text']\n    \n    # Create ad-hoc recognizers\n    ad_hoc_name_recognizer = PatternRecognizer(supported_entity=\"name\", deny_list = [name])\n    ad_hoc_id_recognizer = PatternRecognizer(supported_entity=\"special_value\", deny_list = [special_val])\n    \n    # Run the analyze method with ad_hoc_recognizers:\n    analyzer_results = analyzer.analyze(text=free_text, \n                                        language=\"en\", \n                                        ad_hoc_recognizers=[ad_hoc_name_recognizer, ad_hoc_id_recognizer])\n    \n    # Anonymize results\n    anonymized = anonymizer.anonymize(text=free_text, analyzer_results=analyzer_results)\n    print(anonymized.text)\n    \n    # Store output in original dataset\n    person[\"anonymized_free_text\"] = anonymized.text\n    \n</pre> # Go over dataset for person in dataset:          # Get the different known values     name = person['name']     special_val = person['special_value']          # Get the free text to anonymize     free_text = person['free_text']          # Create ad-hoc recognizers     ad_hoc_name_recognizer = PatternRecognizer(supported_entity=\"name\", deny_list = [name])     ad_hoc_id_recognizer = PatternRecognizer(supported_entity=\"special_value\", deny_list = [special_val])          # Run the analyze method with ad_hoc_recognizers:     analyzer_results = analyzer.analyze(text=free_text,                                          language=\"en\",                                          ad_hoc_recognizers=[ad_hoc_name_recognizer, ad_hoc_id_recognizer])          # Anonymize results     anonymized = anonymizer.anonymize(text=free_text, analyzer_results=analyzer_results)     print(anonymized.text)          # Store output in original dataset     person[\"anonymized_free_text\"] = anonymized.text      <pre>&lt;name&gt;, id &lt;special_value&gt;, likes playing basketball\n&lt;name&gt;, id &lt;special_value&gt; likes playing soccer\n&lt;special_value&gt; is &lt;name&gt;'s special value\n</pre> In\u00a0[9]: Copied! <pre># Dataset now contains the anonymiezd version as well\ndataset\n</pre> # Dataset now contains the anonymiezd version as well dataset Out[9]: <pre>[{'name': 'Martin Smith',\n  'special_value': '145A',\n  'free_text': 'Martin Smith, id 145A, likes playing basketball',\n  'anonymized_free_text': '&lt;name&gt;, id &lt;special_value&gt;, likes playing basketball'},\n {'name': 'Deb Schmidt',\n  'special_value': '256B',\n  'free_text': 'Deb Schmidt, id 256B likes playing soccer',\n  'anonymized_free_text': '&lt;name&gt;, id &lt;special_value&gt; likes playing soccer'},\n {'name': 'R2D2',\n  'special_value': 'X1T2',\n  'free_text': \"X1T2 is R2D2's special value\",\n  'anonymized_free_text': \"&lt;special_value&gt; is &lt;name&gt;'s special value\"}]</pre> <p>Note that in these examples we're only using the custom recognizers we created. We can also add our custom recognizers to the existing recognizers in presidio, by calling <code>registry.load_predefined_recognizers()</code>:</p> In\u00a0[10]: Copied! <pre>registry = RecognizerRegistry()\n\n# Load existing recognizer\nregistry.load_predefined_recognizers()\n\n# Add our custom one\nregistry.add_recognizer(deny_list_recognizer)\n\n# Initialize AnalyzerEngine\nanalyzer = AnalyzerEngine(registry=registry)\n</pre> registry = RecognizerRegistry()  # Load existing recognizer registry.load_predefined_recognizers()  # Add our custom one registry.add_recognizer(deny_list_recognizer)  # Initialize AnalyzerEngine analyzer = AnalyzerEngine(registry=registry) In\u00a0[11]: Copied! <pre>analyzer.analyze(\"George Washington was the first president of the United States\", language=\"en\")\n</pre> analyzer.analyze(\"George Washington was the first president of the United States\", language=\"en\") Out[11]: <pre>[type: PRESIDENT_FIRST_NAME, start: 0, end: 6, score: 1.0,\n type: PERSON, start: 0, end: 17, score: 0.85,\n type: LOCATION, start: 45, end: 62, score: 0.85]</pre> <p>Since George is also a name, it was detected twice, once as a PERSON entity, and once as a custom entity.</p> <p>Read more:</p> <ul> <li>For more info on Presidio Analyzer, see this documentation</li> <li>For more info on Presidio Anonymize, see this documentation</li> <li>To further customize the anonymization type, see this tutorial</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"samples/python/Anonymizing%20known%20values/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonanonymizing20known20valuesipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing%20known%20values.ipynb\u00b6","text":""},{"location":"samples/python/Anonymizing%20known%20values/#anonymizing-known-values","title":"Anonymizing known values\u00b6","text":"<p>In addition to statistical and pattern based approaches, Presidio also supports the identification and anonymization of known values, using the deny-list mechanism. In this example we'll cover two cases:</p> <ol> <li>The known values are known a-priori (e.g., we have a list of names)</li> <li>The known values are only known in the context of a request (e.g., we have the name of a person as the filename)</li> </ol>"},{"location":"samples/python/Anonymizing%20known%20values/#example-1-values-are-known-a-priori","title":"Example 1: values are known a-priori\u00b6","text":"<p>Assume you have a list of potential PII values, you can create a recognizer which would detect them every time they appear in the text. For this case, we can create a deny-list based recognizer, and add it to presidio's <code>RecognizerRegistry</code>:</p>"},{"location":"samples/python/Anonymizing%20known%20values/#example-2-values-are-only-known-in-the-context-of-the-request","title":"Example 2: values are only known in the context of the request\u00b6","text":"<p>In some cases, we know the potential PII values only in the context of a specific text. Examples could be:</p> <ol> <li>Detect PII entities in free text columns in tabular databases, where other columns have entity values we can leverage</li> <li>Detect PII in a file having the filename or other metadata holding potential PII values (e.g. Smith.csv)</li> <li>Anonymize medical images which contain metadata</li> <li>Anonymize financial forms when the actual PII data is known</li> </ol> <p>In this case we can use a functionality called ad-hoc recognizers. Here's a simple example:</p>"},{"location":"samples/python/batch_processing/","title":"Analyzing structured / semi-structured data in batch","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n#!pip install presidio_analyzer presidio_anonymizer\n#!python -m spacy download en_core_web_lg\n#!pip install pandas\n</pre> # download presidio #!pip install presidio_analyzer presidio_anonymizer #!python -m spacy download en_core_web_lg #!pip install pandas In\u00a0[3]: Copied! <pre>from typing import List, Optional, Dict, Union, Iterator, Iterable\nimport collections\nfrom dataclasses import dataclass\nimport pprint\n\nimport pandas as pd\n\nfrom presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult\nfrom presidio_anonymizer import AnonymizerEngine, BatchAnonymizerEngine\nfrom presidio_anonymizer.entities import EngineResult\n</pre> from typing import List, Optional, Dict, Union, Iterator, Iterable import collections from dataclasses import dataclass import pprint  import pandas as pd  from presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult from presidio_anonymizer import AnonymizerEngine, BatchAnonymizerEngine from presidio_anonymizer.entities import EngineResult  In\u00a0[4]: Copied! <pre>columns = [\"name phrase\", \"phone number phrase\", \"integer\", \"boolean\" ]\nsample_data = [\n        ('Charlie likes this', 'Please call 212-555-1234 after 2pm', 1, True),\n        ('You should talk to Mike', 'his number is 978-428-7111', 2, False),\n        ('Mary had a little startup', 'Phone number: 202-342-1234', 3, False)\n]\n</pre> columns = [\"name phrase\", \"phone number phrase\", \"integer\", \"boolean\" ] sample_data = [         ('Charlie likes this', 'Please call 212-555-1234 after 2pm', 1, True),         ('You should talk to Mike', 'his number is 978-428-7111', 2, False),         ('Mary had a little startup', 'Phone number: 202-342-1234', 3, False) ] In\u00a0[5]: Copied! <pre># Create Pandas DataFrame\ndf  = pd.DataFrame(sample_data,columns=columns)\n\ndf\n</pre> # Create Pandas DataFrame df  = pd.DataFrame(sample_data,columns=columns)  df Out[5]: name phrase phone number phrase integer boolean 0 Charlie likes this Please call 212-555-1234 after 2pm 1 True 1 You should talk to Mike his number is 978-428-7111 2 False 2 Mary had a little startup Phone number: 202-342-1234 3 False In\u00a0[6]: Copied! <pre># DataFrame to dict\ndf_dict = df.to_dict(orient=\"list\")\n</pre> # DataFrame to dict df_dict = df.to_dict(orient=\"list\") In\u00a0[7]: Copied! <pre>pprint.pprint(df_dict)\n</pre> pprint.pprint(df_dict) <pre>{'boolean': [True, False, False],\n 'integer': [1, 2, 3],\n 'name phrase': ['Charlie likes this',\n                 'You should talk to Mike',\n                 'Mary had a little startup'],\n 'phone number phrase': ['Please call 212-555-1234 after 2pm',\n                         'his number is 978-428-7111',\n                         'Phone number: 202-342-1234']}\n</pre> In\u00a0[8]: Copied! <pre>analyzer = AnalyzerEngine()\nbatch_analyzer = BatchAnalyzerEngine(analyzer_engine=analyzer)\nbatch_anonymizer = BatchAnonymizerEngine()\n</pre> analyzer = AnalyzerEngine() batch_analyzer = BatchAnalyzerEngine(analyzer_engine=analyzer) batch_anonymizer = BatchAnonymizerEngine() In\u00a0[9]: Copied! <pre>analyzer_results = batch_analyzer.analyze_dict(df_dict, language=\"en\")\nanalyzer_results = list(analyzer_results)\nanalyzer_results\n</pre> analyzer_results = batch_analyzer.analyze_dict(df_dict, language=\"en\") analyzer_results = list(analyzer_results) analyzer_results Out[9]: <pre>[DictAnalyzerResult(key='name phrase', value=['Charlie likes this', 'You should talk to Mike', 'Mary had a little startup'], recognizer_results=[[type: PERSON, start: 0, end: 7, score: 0.85], [type: PERSON, start: 19, end: 23, score: 0.85], [type: PERSON, start: 0, end: 4, score: 0.85]]),\n DictAnalyzerResult(key='phone number phrase', value=['Please call 212-555-1234 after 2pm', 'his number is 978-428-7111', 'Phone number: 202-342-1234'], recognizer_results=[[type: DATE_TIME, start: 31, end: 34, score: 0.85, type: PHONE_NUMBER, start: 12, end: 24, score: 0.75], [type: PHONE_NUMBER, start: 14, end: 26, score: 0.75], [type: PHONE_NUMBER, start: 14, end: 26, score: 0.75]]),\n DictAnalyzerResult(key='integer', value=[1, 2, 3], recognizer_results=[[], [], []]),\n DictAnalyzerResult(key='boolean', value=[True, False, False], recognizer_results=[[], [], []])]</pre> In\u00a0[10]: Copied! <pre>anonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results)\n</pre> anonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results) In\u00a0[11]: Copied! <pre>scrubbed_df = pd.DataFrame(anonymizer_results)\n</pre> scrubbed_df = pd.DataFrame(anonymizer_results) In\u00a0[12]: Copied! <pre>scrubbed_df\n</pre> scrubbed_df Out[12]: name phrase phone number phrase integer boolean 0 &lt;PERSON&gt; likes this Please call &lt;PHONE_NUMBER&gt; after &lt;DATE_TIME&gt; 1 True 1 You should talk to &lt;PERSON&gt; his number is &lt;PHONE_NUMBER&gt; 2 False 2 &lt;PERSON&gt; had a little startup Phone number: &lt;PHONE_NUMBER&gt; 3 False In\u00a0[13]: Copied! <pre>nested_dict = {\n    \"key_a\": {\"key_a1\": \"My phone number is 212-121-1424\"},\n    \"key_b\": {\"www.abc.com\"},\n    \"key_c\": 3,\n    \"names\": [\"James Bond\", \"Clark Kent\", \"Hakeem Olajuwon\", \"No name here!\"]\n}\n\npprint.pprint(nested_dict)\n</pre> nested_dict = {     \"key_a\": {\"key_a1\": \"My phone number is 212-121-1424\"},     \"key_b\": {\"www.abc.com\"},     \"key_c\": 3,     \"names\": [\"James Bond\", \"Clark Kent\", \"Hakeem Olajuwon\", \"No name here!\"] }  pprint.pprint(nested_dict) <pre>{'key_a': {'key_a1': 'My phone number is 212-121-1424'},\n 'key_b': {'www.abc.com'},\n 'key_c': 3,\n 'names': ['James Bond', 'Clark Kent', 'Hakeem Olajuwon', 'No name here!']}\n</pre> In\u00a0[14]: Copied! <pre># Analyze dict\nanalyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\")\n\n# Anonymize dict\nanonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results)\npprint.pprint(anonymizer_results)\n</pre> # Analyze dict analyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\")  # Anonymize dict anonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results) pprint.pprint(anonymizer_results) <pre>{'key_a': {'key_a1': 'My phone number is &lt;PHONE_NUMBER&gt;'},\n 'key_b': ['&lt;URL&gt;'],\n 'key_c': 3,\n 'names': ['&lt;PERSON&gt;', '&lt;PERSON&gt;', '&lt;PERSON&gt;', 'No name here!']}\n</pre> In\u00a0[15]: Copied! <pre>keys_to_skip=[\"key_a1\", \"names\"]\nanalyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\", keys_to_skip=keys_to_skip)\n\n# Anonymize dict\nanonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results)\npprint.pprint(anonymizer_results)\n</pre> keys_to_skip=[\"key_a1\", \"names\"] analyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\", keys_to_skip=keys_to_skip)  # Anonymize dict anonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results) pprint.pprint(anonymizer_results) <pre>{'key_a': {'key_a1': 'My phone number is 212-121-1424'},\n 'key_b': ['&lt;URL&gt;'],\n 'key_c': 3,\n 'names': ['James Bond', 'Clark Kent', 'Hakeem Olajuwon', 'No name here!']}\n</pre> In\u00a0[16]: Copied! <pre>keys_to_skip = [\"key_a.key_a1\"]\n\nanalyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\", keys_to_skip=keys_to_skip)\n\n# Anonymize dict\nanonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results)\npprint.pprint(anonymizer_results)\n</pre> keys_to_skip = [\"key_a.key_a1\"]  analyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\", keys_to_skip=keys_to_skip)  # Anonymize dict anonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results) pprint.pprint(anonymizer_results) <pre>{'key_a': {'key_a1': 'My phone number is 212-121-1424'},\n 'key_b': ['&lt;URL&gt;'],\n 'key_c': 3,\n 'names': ['&lt;PERSON&gt;', '&lt;PERSON&gt;', '&lt;PERSON&gt;', 'No name here!']}\n</pre> In\u00a0[25]: Copied! <pre>import multiprocessing\nimport psutil\nimport time\n\ndef analyze_batch_multiprocess(n_process=12, batch_size=4):\n    \"\"\"Run BatchAnalyzer with `n_process` processes and batch size of `batch_size`.\"\"\"\n    list_of_texts = [\"My name is mike\"]*1000\n\n    results = batch_analyzer.analyze_iterator(\n            texts=list_of_texts, \n            language=\"en\",\n            n_process=n_process, \n            batch_size=batch_size\n        )\n\n    return list(results)\n\n\n\ndef monitor_processes():\n    \"\"\"Monitor all Python processes dynamically.\"\"\"\n    while True:\n        processes = [p for p in psutil.process_iter(attrs=['pid', 'name']) if \"python\" in p.info['name']]\n        print(f\"[Monitor] Active Python processes: {len(processes)} - {[p.info['pid'] for p in processes]}\")\n        time.sleep(1)\n\n\n# Run interactive monitoring\nmonitor_proc = multiprocessing.Process(target=monitor_processes, daemon=True)\nmonitor_proc.start()\n\n# Run the batch analyzer process\nanalyze_batch_multiprocess(n_process=4, batch_size=2)\n\n# Wait for everything to conclude\ntime.sleep(1)  \n\n# Clean up (not needed if daemon=True, but useful if stopping manually)\nmonitor_proc.terminate()\n</pre> import multiprocessing import psutil import time  def analyze_batch_multiprocess(n_process=12, batch_size=4):     \"\"\"Run BatchAnalyzer with `n_process` processes and batch size of `batch_size`.\"\"\"     list_of_texts = [\"My name is mike\"]*1000      results = batch_analyzer.analyze_iterator(             texts=list_of_texts,              language=\"en\",             n_process=n_process,              batch_size=batch_size         )      return list(results)    def monitor_processes():     \"\"\"Monitor all Python processes dynamically.\"\"\"     while True:         processes = [p for p in psutil.process_iter(attrs=['pid', 'name']) if \"python\" in p.info['name']]         print(f\"[Monitor] Active Python processes: {len(processes)} - {[p.info['pid'] for p in processes]}\")         time.sleep(1)   # Run interactive monitoring monitor_proc = multiprocessing.Process(target=monitor_processes, daemon=True) monitor_proc.start()  # Run the batch analyzer process analyze_batch_multiprocess(n_process=4, batch_size=2)  # Wait for everything to conclude time.sleep(1)    # Clean up (not needed if daemon=True, but useful if stopping manually) monitor_proc.terminate()  <pre>[Monitor] Active Python processes: 4 - [38773, 38774, 45860, 109966]\n</pre> <pre>[Monitor] Active Python processes: 8 - [38773, 38774, 45860, 109966, 109973, 109976, 109977, 109978]\n[Monitor] Active Python processes: 8 - [38773, 38774, 45860, 109966, 109973, 109976, 109977, 109978]\n[Monitor] Active Python processes: 8 - [38773, 38774, 45860, 109966, 109973, 109976, 109977, 109978]\n[Monitor] Active Python processes: 8 - [38773, 38774, 45860, 109966, 109973, 109976, 109977, 109978]\n[Monitor] Active Python processes: 4 - [38773, 38774, 45860, 109966]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"samples/python/batch_processing/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonbatch_processingipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/batch_processing.ipynb\u00b6","text":""},{"location":"samples/python/batch_processing/#run-presidio-on-structured-semi-structured-data","title":"Run Presidio on structured / semi-structured data\u00b6","text":"<p>This sample shows how Presidio could be potentially extended to handle the anonymization of a table or data frame. It introduces methods for the analysis and anonymization of both lists and dicts.</p> <p>Note: this sample input here is a Pandas DataFrame and a JSON file, but it can be used in other scenarios such as querying SQL data or using Spark DataFrames.</p>"},{"location":"samples/python/batch_processing/#set-up-imports","title":"Set up imports\u00b6","text":""},{"location":"samples/python/batch_processing/#example-using-sample-tabular-data","title":"Example using sample tabular data\u00b6","text":""},{"location":"samples/python/batch_processing/#example-using-json","title":"Example using JSON\u00b6","text":""},{"location":"samples/python/batch_processing/#ignoring-specific-keys","title":"Ignoring specific keys\u00b6","text":""},{"location":"samples/python/batch_processing/#ignoring-nested-keys","title":"Ignoring nested keys\u00b6","text":""},{"location":"samples/python/batch_processing/#note","title":"Note!\u00b6","text":"<p>JSON files with objects within lists, e.g.:</p> <pre><code>{\n  \"key\": [\n    {\n      \"key2\": \"Peter Parker\"\n    },\n    {\n      \"key3\": \"555-1234\"\n    }\n  ]\n}\n</code></pre> <p>Are not yet supported. Consider breaking the JSON to parts if needed.</p>"},{"location":"samples/python/batch_processing/#multiprocessing","title":"Multiprocessing\u00b6","text":"<p><code>BatchAnalyzerEngine</code> builds upon spaCy's pipelines. For more info about multiprocessing, see https://spacy.io/usage/processing-pipelines#multiprocessing.</p> <p>In Presidio, one can pass the <code>n_process</code> argument and the <code>batch_size</code> parameter to define how processing is done in parallel.</p>"},{"location":"samples/python/custom_presidio/","title":"Custom presidio","text":"In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern\n</pre> from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n\n    analyzer = AnalyzerEngine()\n\n    text1 = \"Professor Plum, in the Dining Room, with the candlestick\"\n\n    titles_list = [\n        \"Sir\",\n        \"Ma'am\",\n        \"Madam\",\n        \"Mr.\",\n        \"Mrs.\",\n        \"Ms.\",\n        \"Miss\",\n        \"Dr.\",\n        \"Professor\",\n    ]\n    titles_recognizer = PatternRecognizer(\n        supported_entity=\"TITLE\", deny_list=titles_list\n    )\n    analyzer.registry.add_recognizer(titles_recognizer)\n\n    result = analyzer.analyze(text=text1, language=\"en\")\n    print(f\"\\nDeny List result:\\n {result}\")\n\n    text2 = \"I live in 510 Broad st.\"\n\n    numbers_pattern = Pattern(name=\"numbers_pattern\", regex=r\"\\d+\", score=0.5)\n    number_recognizer = PatternRecognizer(\n        supported_entity=\"NUMBER\", patterns=[numbers_pattern]\n    )\n\n    result = number_recognizer.analyze(text=text2, entities=[\"NUMBER\"])\n    print(f\"\\nRegex result:\\n {result}\")\n</pre> if __name__ == \"__main__\":      analyzer = AnalyzerEngine()      text1 = \"Professor Plum, in the Dining Room, with the candlestick\"      titles_list = [         \"Sir\",         \"Ma'am\",         \"Madam\",         \"Mr.\",         \"Mrs.\",         \"Ms.\",         \"Miss\",         \"Dr.\",         \"Professor\",     ]     titles_recognizer = PatternRecognizer(         supported_entity=\"TITLE\", deny_list=titles_list     )     analyzer.registry.add_recognizer(titles_recognizer)      result = analyzer.analyze(text=text1, language=\"en\")     print(f\"\\nDeny List result:\\n {result}\")      text2 = \"I live in 510 Broad st.\"      numbers_pattern = Pattern(name=\"numbers_pattern\", regex=r\"\\d+\", score=0.5)     number_recognizer = PatternRecognizer(         supported_entity=\"NUMBER\", patterns=[numbers_pattern]     )      result = number_recognizer.analyze(text=text2, entities=[\"NUMBER\"])     print(f\"\\nRegex result:\\n {result}\")"},{"location":"samples/python/customizing_presidio_analyzer/","title":"Customizing Presidio Analyzer","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</pre> # download presidio !pip install presidio_analyzer presidio_anonymizer !python -m spacy download en_core_web_lg In\u00a0[1]: Copied! <pre>from typing import List\nimport pprint\n\nfrom presidio_analyzer import (\n    AnalyzerEngine,\n    PatternRecognizer,\n    EntityRecognizer,\n    Pattern,\n    RecognizerResult,\n)\nfrom presidio_analyzer.recognizer_registry import RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\nfrom presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n</pre> from typing import List import pprint  from presidio_analyzer import (     AnalyzerEngine,     PatternRecognizer,     EntityRecognizer,     Pattern,     RecognizerResult, ) from presidio_analyzer.recognizer_registry import RecognizerRegistry from presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer In\u00a0[68]: Copied! <pre># Helper method to print results nicely\n\n\ndef print_analyzer_results(results: List[RecognizerResult], text: str):\n    \"\"\"Print the results in a human readable way.\"\"\"\n\n    for i, result in enumerate(results):\n        print(f\"Result {i}:\")\n        print(f\" {result}, text: {text[result.start:result.end]}\")\n\n        if result.analysis_explanation is not None:\n            print(f\" {result.analysis_explanation.textual_explanation}\")\n</pre> # Helper method to print results nicely   def print_analyzer_results(results: List[RecognizerResult], text: str):     \"\"\"Print the results in a human readable way.\"\"\"      for i, result in enumerate(results):         print(f\"Result {i}:\")         print(f\" {result}, text: {text[result.start:result.end]}\")          if result.analysis_explanation is not None:             print(f\" {result.analysis_explanation.textual_explanation}\") In\u00a0[69]: Copied! <pre>titles_list = [\n    \"Sir\",\n    \"Ma'am\",\n    \"Madam\",\n    \"Mr.\",\n    \"Mrs.\",\n    \"Ms.\",\n    \"Miss\",\n    \"Dr.\",\n    \"Professor\",\n]\n</pre> titles_list = [     \"Sir\",     \"Ma'am\",     \"Madam\",     \"Mr.\",     \"Mrs.\",     \"Ms.\",     \"Miss\",     \"Dr.\",     \"Professor\", ] <p>Second, let's create a <code>PatternRecognizer</code> which would scan for those titles, by passing a <code>deny_list</code>:</p> In\u00a0[70]: Copied! <pre>titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\", deny_list=titles_list)\n</pre> titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\", deny_list=titles_list) <p>At this point we can call our recognizer directly:</p> In\u00a0[83]: Copied! <pre>text1 = \"I suspect Professor Plum, in the Dining Room, with the candlestick\"\nresult = titles_recognizer.analyze(text1, entities=[\"TITLE\"])\nprint(f\"Result:\\n {result}\")\n</pre> text1 = \"I suspect Professor Plum, in the Dining Room, with the candlestick\" result = titles_recognizer.analyze(text1, entities=[\"TITLE\"]) print(f\"Result:\\n {result}\") <pre>Result:\n [type: TITLE, start: 10, end: 19, score: 1.0]\n</pre> <p>Finally, let's add this new recognizer to the list of recognizers used by the Presidio <code>AnalyzerEngine</code>:</p> In\u00a0[84]: Copied! <pre>analyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(titles_recognizer)\n</pre> analyzer = AnalyzerEngine() analyzer.registry.add_recognizer(titles_recognizer) <p>When initializing the <code>AnalyzerEngine</code>, Presidio loads all available recognizers, including the <code>NlpEngine</code> used to detect entities, and extract tokens, lemmas and other linguistic features.</p> <p>Let's run the analyzer with the new recognizer in place:</p> In\u00a0[85]: Copied! <pre>results = analyzer.analyze(text=text1, language=\"en\")\n</pre> results = analyzer.analyze(text=text1, language=\"en\") In\u00a0[86]: Copied! <pre>print_analyzer_results(results, text=text1)\n</pre> print_analyzer_results(results, text=text1) <pre>Result 0:\n type: TITLE, start: 10, end: 19, score: 1.0, text: Professor\nResult 1:\n type: PERSON, start: 20, end: 24, score: 0.85, text: Plum\nResult 2:\n type: LOCATION, start: 29, end: 44, score: 0.85, text: the Dining Room\n</pre> <p>As expected, both the name \"Plum\" and the title were identified as PII:</p> In\u00a0[87]: Copied! <pre>print(\"Identified these PII entities:\")\nfor result in results:\n    print(f\"- {text1[result.start:result.end]} as {result.entity_type}\")\n</pre> print(\"Identified these PII entities:\") for result in results:     print(f\"- {text1[result.start:result.end]} as {result.entity_type}\") <pre>Identified these PII entities:\n- Professor as TITLE\n- Plum as PERSON\n- the Dining Room as LOCATION\n</pre> In\u00a0[88]: Copied! <pre># Define the regex pattern in a Presidio `Pattern` object:\nnumbers_pattern = Pattern(name=\"numbers_pattern\", regex=\"\\d+\", score=0.5)\n\n# Define the recognizer with one or more patterns\nnumber_recognizer = PatternRecognizer(\n    supported_entity=\"NUMBER\", patterns=[numbers_pattern]\n)\n</pre> # Define the regex pattern in a Presidio `Pattern` object: numbers_pattern = Pattern(name=\"numbers_pattern\", regex=\"\\d+\", score=0.5)  # Define the recognizer with one or more patterns number_recognizer = PatternRecognizer(     supported_entity=\"NUMBER\", patterns=[numbers_pattern] ) <p>Testing the recognizer itself:</p> In\u00a0[89]: Copied! <pre>text2 = \"I live in 510 Broad st.\"\n\nnumbers_result = number_recognizer.analyze(text=text2, entities=[\"NUMBER\"])\nprint(\"Result:\")\nprint(numbers_result)\n</pre> text2 = \"I live in 510 Broad st.\"  numbers_result = number_recognizer.analyze(text=text2, entities=[\"NUMBER\"]) print(\"Result:\") print(numbers_result) <pre>Result:\n[type: NUMBER, start: 10, end: 13, score: 0.5]\n</pre> <p>It's important to mention that recognizers is likely to have errors, both false-positive and false-negative, which would impact the entire performance of Presidio. Consider testing each recognizer on a representative dataset prior to integrating it into Presidio. For more info, see the best practices for developing recognizers documentation.</p> In\u00a0[90]: Copied! <pre>class MyRecognizer(EntityRecognizer):\n\n    def load(self) -&gt; None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Logic for detecting a specific PII\n        \"\"\"\n        pass\n</pre> class MyRecognizer(EntityRecognizer):      def load(self) -&gt; None:         \"\"\"No loading is required.\"\"\"         pass      def analyze(         self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts     ) -&gt; List[RecognizerResult]:         \"\"\"         Logic for detecting a specific PII         \"\"\"         pass <p>For example, detecting numbers in either numerical or alphabetic (e.g. Forty five) form:</p> In\u00a0[91]: Copied! <pre>class NumbersRecognizer(EntityRecognizer):\n\n    expected_confidence_level = 0.7  # expected confidence level for this recognizer\n\n    def load(self) -&gt; None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyzes test to find tokens which represent numbers (either 123 or One Two Three).\n        \"\"\"\n        results = []\n\n        # iterate over the spaCy tokens, and call `token.like_num`\n        for token in nlp_artifacts.tokens:\n            if token.like_num:\n                result = RecognizerResult(\n                    entity_type=\"NUMBER\",\n                    start=token.idx,\n                    end=token.idx + len(token),\n                    score=self.expected_confidence_level,\n                )\n                results.append(result)\n        return results\n</pre> class NumbersRecognizer(EntityRecognizer):      expected_confidence_level = 0.7  # expected confidence level for this recognizer      def load(self) -&gt; None:         \"\"\"No loading is required.\"\"\"         pass      def analyze(         self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts     ) -&gt; List[RecognizerResult]:         \"\"\"         Analyzes test to find tokens which represent numbers (either 123 or One Two Three).         \"\"\"         results = []          # iterate over the spaCy tokens, and call `token.like_num`         for token in nlp_artifacts.tokens:             if token.like_num:                 result = RecognizerResult(                     entity_type=\"NUMBER\",                     start=token.idx,                     end=token.idx + len(token),                     score=self.expected_confidence_level,                 )                 results.append(result)         return results In\u00a0[92]: Copied! <pre>new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n</pre> new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"]) <p>Since this recognizer requires the <code>NlpArtifacts</code>, we would have to call it as part of the <code>AnalyzerEngine</code> flow:</p> In\u00a0[93]: Copied! <pre>text3 = \"Roberto lives in Five 10 Broad st.\"\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(new_numbers_recognizer)\n\nnumbers_results2 = analyzer.analyze(text=text3, language=\"en\")\nprint_analyzer_results(numbers_results2, text=text3)\n</pre> text3 = \"Roberto lives in Five 10 Broad st.\" analyzer = AnalyzerEngine() analyzer.registry.add_recognizer(new_numbers_recognizer)  numbers_results2 = analyzer.analyze(text=text3, language=\"en\") print_analyzer_results(numbers_results2, text=text3) <pre>Result 0:\n type: PERSON, start: 0, end: 7, score: 0.85, text: Roberto\nResult 1:\n type: LOCATION, start: 25, end: 34, score: 0.85, text: Broad st.\nResult 2:\n type: NUMBER, start: 17, end: 21, score: 0.7, text: Five\nResult 3:\n type: NUMBER, start: 22, end: 24, score: 0.7, text: 10\n</pre> <p>The analyzer was able to pick up both numeric and alphabetical numbers, including other types of PII entities from other recognizers (PERSON in this case).</p> In\u00a0[82]: Copied! <pre>from presidio_analyzer.nlp_engine import NlpEngineProvider\n\n# import spacy\n# spacy.cli.download(\"es_core_news_md\")\n\n# Create configuration containing engine name and models\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [\n        {\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n    ],\n}\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es Morris\", language=\"es\")\nprint(\"Results from Spanish request:\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(\"Results from English request:\")\nprint(results_english)\n</pre> from presidio_analyzer.nlp_engine import NlpEngineProvider  # import spacy # spacy.cli.download(\"es_core_news_md\")  # Create configuration containing engine name and models configuration = {     \"nlp_engine_name\": \"spacy\",     \"models\": [         {\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},         {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},     ], }  # Create NLP engine based on configuration provider = NlpEngineProvider(nlp_configuration=configuration) nlp_engine_with_spanish = provider.create_engine()  # Pass the created NLP engine and supported_languages to the AnalyzerEngine analyzer = AnalyzerEngine(     nlp_engine=nlp_engine_with_spanish, supported_languages=[\"en\", \"es\"] )  # Analyze in different languages results_spanish = analyzer.analyze(text=\"Mi nombre es Morris\", language=\"es\") print(\"Results from Spanish request:\") print(results_spanish)  results_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\") print(\"Results from English request:\") print(results_english) <pre>Results from Spanish request:\n[type: PERSON, start: 13, end: 19, score: 0.85]\nResults from English request:\n[type: PERSON, start: 11, end: 17, score: 0.85]\n</pre> <ul> <li>See this documentation for more details on how to configure Presidio support additional NLP models and languages.</li> <li>See this sample for more implemention examples of various NLP engines and NER models.</li> </ul> In\u00a0[94]: Copied! <pre># Define the regex pattern\nregex = r\"(\\b\\d{5}(?:\\-\\d{4})?\\b)\"  # very weak regex pattern\nzipcode_pattern = Pattern(name=\"zip code (weak)\", regex=regex, score=0.01)\n\n# Define the recognizer with the defined pattern\nzipcode_recognizer = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\", patterns=[zipcode_pattern]\n)\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test\ntext = \"My zip code is 90210\"\nresults = analyzer.analyze(text=text, language=\"en\")\nprint_analyzer_results(results, text=text)\n</pre> # Define the regex pattern regex = r\"(\\b\\d{5}(?:\\-\\d{4})?\\b)\"  # very weak regex pattern zipcode_pattern = Pattern(name=\"zip code (weak)\", regex=regex, score=0.01)  # Define the recognizer with the defined pattern zipcode_recognizer = PatternRecognizer(     supported_entity=\"US_ZIP_CODE\", patterns=[zipcode_pattern] )  registry = RecognizerRegistry() registry.add_recognizer(zipcode_recognizer) analyzer = AnalyzerEngine(registry=registry)  # Test text = \"My zip code is 90210\" results = analyzer.analyze(text=text, language=\"en\") print_analyzer_results(results, text=text) <pre>Result 0:\n type: US_ZIP_CODE, start: 15, end: 20, score: 0.01, text: 90210\n</pre> <p>So this is working, but would catch any 5 digit string. This is why we set the score to 0.01. Let's use context words to increase score:</p> In\u00a0[96]: Copied! <pre># Define the recognizer with the defined pattern and context words\nzipcode_recognizer = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\",\n    patterns=[zipcode_pattern],\n    context=[\"zip\", \"zipcode\"],\n)\n</pre> # Define the recognizer with the defined pattern and context words zipcode_recognizer = PatternRecognizer(     supported_entity=\"US_ZIP_CODE\",     patterns=[zipcode_pattern],     context=[\"zip\", \"zipcode\"], ) <p>When creating an <code>AnalyzerEngine</code> we can provide our own context enhancement logic by passing it to <code>context_aware_enhancer</code> parameter. <code>AnalyzerEngine</code> will create <code>LemmaContextAwareEnhancer</code> by default if not passed, which will enhance score of each matched result if it's recognizer holds context words and those words are found in context of the matched entity.</p> In\u00a0[97]: Copied! <pre>registry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n</pre> registry = RecognizerRegistry() registry.add_recognizer(zipcode_recognizer) analyzer = AnalyzerEngine(registry=registry) In\u00a0[98]: Copied! <pre># Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\nprint(\"Result:\")\nprint_analyzer_results(results, text=text)\n</pre> # Test results = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\") print(\"Result:\") print_analyzer_results(results, text=text) <pre>Result:\nResult 0:\n type: US_ZIP_CODE, start: 15, end: 20, score: 0.4, text: 90210\n</pre> <p>The confidence score is now 0.4, instead of 0.01. because <code>LemmaContextAwareEnhancer</code> default context similarity factor is 0.35 and default minimum score with context similarity is 0.4, we can change that by passing <code>context_similarity_factor</code> and <code>min_score_with_context_similarity</code> parameters of <code>LemmaContextAwareEnhancer</code> to other than values, for example:</p> In\u00a0[99]: Copied! <pre>registry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(\n    registry=registry,\n    context_aware_enhancer=LemmaContextAwareEnhancer(\n        context_similarity_factor=0.45, min_score_with_context_similarity=0.4\n    ),\n)\n</pre> registry = RecognizerRegistry() registry.add_recognizer(zipcode_recognizer) analyzer = AnalyzerEngine(     registry=registry,     context_aware_enhancer=LemmaContextAwareEnhancer(         context_similarity_factor=0.45, min_score_with_context_similarity=0.4     ), ) In\u00a0[100]: Copied! <pre># Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\nprint(\"Result:\")\nprint_analyzer_results(results, text=text)\n</pre> # Test results = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\") print(\"Result:\") print_analyzer_results(results, text=text) <pre>Result:\nResult 0:\n type: US_ZIP_CODE, start: 15, end: 20, score: 0.46, text: 90210\n</pre> <p>The confidence score is now 0.46 because it got enhanced from 0.01 with 0.45 and is more the minimum of 0.4</p> <p>Presidio supports passing a list of outer context in analyzer level, this is useful if the text is coming from a specific column or a specific user input etc. notice how the \"zip\" context word doesn't appear in the text but still enhance the confidence score from 0.01 to 0.4:</p> In\u00a0[102]: Copied! <pre># Define the recognizer with the defined pattern and context words\nzipcode_recognizer = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\",\n    patterns=[zipcode_pattern],\n    context=[\"zip\", \"zipcode\"],\n)\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test\ntext = \"My code is 90210\"\nresult = analyzer.analyze(text=text, language=\"en\", context=[\"zip\"])\nprint(\"Result:\")\nprint_analyzer_results(result, text=text)\n</pre> # Define the recognizer with the defined pattern and context words zipcode_recognizer = PatternRecognizer(     supported_entity=\"US_ZIP_CODE\",     patterns=[zipcode_pattern],     context=[\"zip\", \"zipcode\"], )  registry = RecognizerRegistry() registry.add_recognizer(zipcode_recognizer) analyzer = AnalyzerEngine(registry=registry)  # Test text = \"My code is 90210\" result = analyzer.analyze(text=text, language=\"en\", context=[\"zip\"]) print(\"Result:\") print_analyzer_results(result, text=text) <pre>Result:\nResult 0:\n type: US_ZIP_CODE, start: 11, end: 16, score: 0.4, text: 90210\n</pre> In\u00a0[103]: Copied! <pre>results = analyzer.analyze(\n    text=\"My zip code is 90210\", language=\"en\", return_decision_process=True\n)\ndecision_process = results[0].analysis_explanation\n\npp = pprint.PrettyPrinter()\nprint(\"Decision process output:\\n\")\npp.pprint(decision_process.__dict__)\n</pre> results = analyzer.analyze(     text=\"My zip code is 90210\", language=\"en\", return_decision_process=True ) decision_process = results[0].analysis_explanation  pp = pprint.PrettyPrinter() print(\"Decision process output:\\n\") pp.pprint(decision_process.__dict__) <pre>Decision process output:\n\n{'original_score': 0.01,\n 'pattern': '(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)',\n 'pattern_name': 'zip code (weak)',\n 'recognizer': 'PatternRecognizer',\n 'regex_flags': regex.I|regex.M|regex.S,\n 'score': 0.4,\n 'score_context_improvement': 0.39,\n 'supportive_context_word': 'zip',\n 'textual_explanation': 'Detected by `PatternRecognizer` using pattern `zip '\n                        'code (weak)`',\n 'validation_result': None}\n</pre> <p>When developing new recognizers, one can add information to this explanation and extend it with additional findings.</p> <p>We will use the built in recognizers that include the <code>URLRecognizer</code> and the NLP model <code>EntityRecognizer</code> and see the default functionality if we don't specify any list of words for the detector to allow to keep in the text.</p> In\u00a0[104]: Copied! <pre>websites_list = [\"bing.com\", \"microsoft.com\"]\ntext1 = \"Bill's favorite website is bing.com, David's is microsoft.com\"\nanalyzer = AnalyzerEngine()\nresults = analyzer.analyze(text=text1, language=\"en\", return_decision_process=True)\nprint_analyzer_results(results, text=text1)\n</pre> websites_list = [\"bing.com\", \"microsoft.com\"] text1 = \"Bill's favorite website is bing.com, David's is microsoft.com\" analyzer = AnalyzerEngine() results = analyzer.analyze(text=text1, language=\"en\", return_decision_process=True) print_analyzer_results(results, text=text1) <pre>Result 0:\n type: PERSON, start: 0, end: 4, score: 0.85, text: Bill\n Identified as PERSON by Spacy's Named Entity Recognition\nResult 1:\n type: URL, start: 27, end: 35, score: 0.85, text: bing.com\n Detected by `UrlRecognizer` using pattern `Non schema URL`\nResult 2:\n type: PERSON, start: 37, end: 42, score: 0.85, text: David\n Identified as PERSON by Spacy's Named Entity Recognition\nResult 3:\n type: URL, start: 48, end: 61, score: 0.85, text: microsoft.com\n Detected by `UrlRecognizer` using pattern `Non schema URL`\n</pre> <p>To specify an allow list we just pass a list of values we want to keep as a parameter to call to <code>analyze</code>. Now we can see that in the results, <code>bing.com</code> is no longer being recognized as a PII item, only <code>microsoft.com</code> as well as the named entities are still recognized since we did include it in the allow list.</p> In\u00a0[105]: Copied! <pre>results = analyzer.analyze(\n    text=text1,\n    language=\"en\",\n    allow_list=[\"bing.com\", \"google.com\"],\n    return_decision_process=True,\n)\nprint_analyzer_results(results, text=text1)\n</pre> results = analyzer.analyze(     text=text1,     language=\"en\",     allow_list=[\"bing.com\", \"google.com\"],     return_decision_process=True, ) print_analyzer_results(results, text=text1) <pre>Result 0:\n type: PERSON, start: 0, end: 4, score: 0.85, text: Bill\n Identified as PERSON by Spacy's Named Entity Recognition\nResult 1:\n type: PERSON, start: 37, end: 42, score: 0.85, text: David\n Identified as PERSON by Spacy's Named Entity Recognition\nResult 2:\n type: URL, start: 48, end: 61, score: 0.85, text: microsoft.com\n Detected by `UrlRecognizer` using pattern `Non schema URL`\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"samples/python/customizing_presidio_analyzer/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythoncustomizing_presidio_analyzeripynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb\u00b6","text":""},{"location":"samples/python/customizing_presidio_analyzer/#customizing-the-pii-analysis-process-in-microsoft-presidio","title":"Customizing the PII analysis process in Microsoft Presidio\u00b6","text":"<p>This notebooks covers different customization use cases to:</p> <ol> <li>Adapt Presidio to detect new types of PII entities</li> <li>Adapt Presidio to detect PII entities in a new language</li> <li>Embed new types of detection modules into Presidio, to improve the coverage of the service.</li> </ol>"},{"location":"samples/python/customizing_presidio_analyzer/#installation","title":"Installation\u00b6","text":"<p>First, let's install presidio using <code>pip</code>. For detailed documentation, see the installation docs.</p> <p>Install from PyPI:</p>"},{"location":"samples/python/customizing_presidio_analyzer/#getting-started","title":"Getting started\u00b6","text":"<p>The high level process in Presidio-Analyzer is the following: </p> <p>Load the <code>presidio-analyzer</code> modules. For more information, see the analyzer docs.</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-1-deny-list-based-pii-recognition","title":"Example 1: Deny-list based PII recognition\u00b6","text":"<p>In this example, we will pass a short list of tokens which should be marked as PII if detected. First, let's define the tokens we want to treat as PII. In this case it would be a list of titles:</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-2-regex-based-pii-recognition","title":"Example 2: Regex based PII recognition\u00b6","text":"<p>Another simple recognizer we can add is based on regular expressions. Let's assume we want to be extremely conservative and treat any token which contains a number as PII.</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-3-rule-based-logic-recognizer","title":"Example 3: Rule based logic recognizer\u00b6","text":"<p>Taking the numbers recognizer one step further, let's say we also would like to detect numbers within words, e.g. \"Number One\". We can leverage the underlying spaCy token attributes, or write our own logic to detect such entities.</p> <p>Notes:</p> <ul> <li><p>In this example we would create a new class, which implements <code>EntityRecognizer</code>, the basic recognizer in Presidio. This abstract class requires us to implement the <code>load</code> method and <code>analyze</code> method.</p> </li> <li><p>Each recognizer accepts an object of type <code>NlpArtifacts</code>, which holds pre-computed attributes on the input text.</p> </li> </ul> <p>A new recognizer should have this structure:</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-4-calling-an-external-service-for-pii-detection","title":"Example 4: Calling an external service for PII detection\u00b6","text":"<p>In a similar way to example 3, we can write logic to call external services for PII detection. For a detailed example, see this part of the documentation.</p> <p>This is a sample implementation of such remote recognizer.</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-5-supporting-new-languages","title":"Example 5: Supporting new languages\u00b6","text":"<p>Two main parts in Presidio handle the text, and should be adapted if a new language is required:</p> <ol> <li>The <code>NlpEngine</code> containing the NLP model which performs tokenization, lemmatization, Named Entity Recognition and other NLP tasks.</li> <li>The different PII recognizers (<code>EntityRecognizer</code> objects) should be adapted or created.</li> </ol>"},{"location":"samples/python/customizing_presidio_analyzer/#adapting-the-nlp-engine","title":"Adapting the NLP engine\u00b6","text":"<p>As its internal NLP engine, Presidio supports both spaCy and Stanza. Make sure you download the required models from spacy/stanza prior to using them. More details here. For example, to download the Spanish medium spaCy model: <code>python -m spacy download es_core_news_md</code></p> <p>In this example we will configure Presidio to use spaCy as its underlying NLP framework, with NLP models in English and Spanish:</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-6-using-context-words","title":"Example 6: Using context words\u00b6","text":"<p>Presidio has a internal mechanism for leveraging context words. This mechanism would increse the detection confidence of a PII entity in case a specific word appears before or after it.</p> <p>In this example we would first implement a zip code recognizer without context, and then add context to see how the confidence changes. Zip regex patterns (essentially 5 digits) are very week, so we would want the initial confidence to be low, and increased with the existence of context words.</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-7-tracing-the-decision-process","title":"Example 7: Tracing the decision process\u00b6","text":"<p>Presidio-analyzer's decision process exposes information on why a specific PII was detected. Such information could contain:</p> <ul> <li>Which recognizer detected the entity</li> <li>Which regex pattern was used</li> <li>Interpretability mechanisms in ML models</li> <li>Which context words improved the score</li> <li>Confidence scores before and after each step And more.</li> </ul> <p>For more information, refer to the decision process documentation.</p> <p>Let's use the decision process output to understand how the zip code value was detected:</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-8-passing-a-list-of-words-to-keep","title":"Example 8: passing a list of words to keep\u00b6","text":""},{"location":"samples/python/encrypt_decrypt/","title":"Encrypting and Decrypting identified entities","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</pre> # download presidio !pip install presidio_analyzer presidio_anonymizer !python -m spacy download en_core_web_lg In\u00a0[2]: Copied! <pre>from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine\nfrom presidio_anonymizer.entities import RecognizerResult, OperatorResult, OperatorConfig\nfrom presidio_anonymizer.operators import Decrypt\n</pre> from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine from presidio_anonymizer.entities import RecognizerResult, OperatorResult, OperatorConfig from presidio_anonymizer.operators import Decrypt In\u00a0[3]: Copied! <pre>crypto_key = \"WmZq4t7w!z%C&amp;F)J\"\n</pre> crypto_key = \"WmZq4t7w!z%C&amp;F)J\" In\u00a0[4]: Copied! <pre>engine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text,\n# analyzer results (potentially coming from presidio-analyzer)\n# and an 'encrypt' operator to get an encrypted anonymization output:\nanonymize_result = engine.anonymize(\n    text=\"My name is James Bond\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),\n    ],\n    operators={\"PERSON\": OperatorConfig(\"encrypt\", {\"key\": crypto_key})},\n)\n\nanonymize_result\n</pre> engine = AnonymizerEngine()  # Invoke the anonymize function with the text, # analyzer results (potentially coming from presidio-analyzer) # and an 'encrypt' operator to get an encrypted anonymization output: anonymize_result = engine.anonymize(     text=\"My name is James Bond\",     analyzer_results=[         RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),     ],     operators={\"PERSON\": OperatorConfig(\"encrypt\", {\"key\": crypto_key})}, )  anonymize_result Out[4]: <pre>text: My name is M4lla0kBCzu6SwCONL6Y+ZqsPqhBp1Lhdc3t0FKnUwM=.\nitems:\n[\n    {'start': 11, 'end': 55, 'entity_type': 'PERSON', 'text': 'M4lla0kBCzu6SwCONL6Y+ZqsPqhBp1Lhdc3t0FKnUwM=', 'operator': 'encrypt'}\n]</pre> In\u00a0[5]: Copied! <pre># Fetch the anonymized text from the result.\nanonymized_text = anonymize_result.text\n\n# Fetch the anonynized entities from the result.\nanonymized_entities = anonymize_result.items\n</pre> # Fetch the anonymized text from the result. anonymized_text = anonymize_result.text  # Fetch the anonynized entities from the result. anonymized_entities = anonymize_result.items In\u00a0[8]: Copied! <pre># Initialize the engine:\nengine = DeanonymizeEngine()\n\n# Invoke the deanonymize function with the text, anonymizer results\n# and a 'decrypt' operator to get the original text as output.\ndeanonymized_result = engine.deanonymize(\n    text=anonymized_text,\n    entities=anonymized_entities,\n    operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": crypto_key})},\n)\n\ndeanonymized_result\n</pre> # Initialize the engine: engine = DeanonymizeEngine()  # Invoke the deanonymize function with the text, anonymizer results # and a 'decrypt' operator to get the original text as output. deanonymized_result = engine.deanonymize(     text=anonymized_text,     entities=anonymized_entities,     operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": crypto_key})}, )  deanonymized_result Out[8]: <pre>text: My name is James Bond.\nitems:\n[\n    {'start': 11, 'end': 21, 'entity_type': 'PERSON', 'text': 'James Bond', 'operator': 'decrypt'}\n]</pre> In\u00a0[9]: Copied! <pre># Alternatively, call the Decrypt operator directly:\n\n# Fetch the encrypted entitiy value from the previous stage\nencrypted_entity_value = anonymize_result.items[0].text\n\n# Restore the original entity value\nDecrypt().operate(text=encrypted_entity_value, params={\"key\": crypto_key})\n</pre> # Alternatively, call the Decrypt operator directly:  # Fetch the encrypted entitiy value from the previous stage encrypted_entity_value = anonymize_result.items[0].text  # Restore the original entity value Decrypt().operate(text=encrypted_entity_value, params={\"key\": crypto_key}) Out[9]: <pre>'James Bond'</pre>"},{"location":"samples/python/encrypt_decrypt/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonencrypt_decryptipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/encrypt_decrypt.ipynb\u00b6","text":""},{"location":"samples/python/encrypt_decrypt/#encrypting-and-decrypting-identified-entities","title":"Encrypting and Decrypting identified entities\u00b6","text":"<p>This sample shows how to use Presidio Anonymizer built-in functionality, to encrypt and decrypt identified entities. The encryption is using AES cypher in CBC mode and requires a cryptographic key as an input for both the encryption and the decryption.</p>"},{"location":"samples/python/encrypt_decrypt/#set-up-imports","title":"Set up imports\u00b6","text":""},{"location":"samples/python/encrypt_decrypt/#define-a-cryptographic-key-for-both-encryption-and-decryption","title":"Define a cryptographic key (for both encryption and decryption)\u00b6","text":""},{"location":"samples/python/encrypt_decrypt/#presidio-anonymizer-encrypt","title":"Presidio Anonymizer: Encrypt\u00b6","text":""},{"location":"samples/python/encrypt_decrypt/#presidio-anonymizer-decrypt","title":"Presidio Anonymizer: Decrypt\u00b6","text":""},{"location":"samples/python/example_custom_lambda_anonymizer/","title":"Example custom lambda anonymizer","text":"In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\nfrom faker import Faker\nfrom faker.providers import internet\n</pre> from presidio_analyzer import AnalyzerEngine from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import OperatorConfig from faker import Faker from faker.providers import internet In\u00a0[\u00a0]: Copied! <pre>def reverse_string(x):\n    \"\"\"Return string in reverse order.\"\"\"\n    return x[::-1]\n</pre> def reverse_string(x):     \"\"\"Return string in reverse order.\"\"\"     return x[::-1] In\u00a0[\u00a0]: Copied! <pre>def anonymize_reverse_lambda(analyzer_results, text_to_anonymize):\n    \"\"\"Anonymize using an example lambda.\"\"\"\n    anonymized_results = anonymizer.anonymize(\n        text=text_to_anonymize,\n        analyzer_results=analyzer_results,\n        operators={\n            \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x[::-1]})\n        },\n    )\n\n    return anonymized_results\n</pre> def anonymize_reverse_lambda(analyzer_results, text_to_anonymize):     \"\"\"Anonymize using an example lambda.\"\"\"     anonymized_results = anonymizer.anonymize(         text=text_to_anonymize,         analyzer_results=analyzer_results,         operators={             \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x[::-1]})         },     )      return anonymized_results In\u00a0[\u00a0]: Copied! <pre>def anonymize_faker_lambda(analyzer_results, text_to_anonymize):\n    \"\"\"Anonymize using a faker provider.\"\"\"\n\n    anonymized_results = anonymizer.anonymize(\n        text=text_to_anonymize,\n        analyzer_results=analyzer_results,\n        operators={\n            \"EMAIL_ADDRESS\": OperatorConfig(\n                \"custom\", {\"lambda\": lambda x: fake.safe_email()}\n            )\n        },\n    )\n\n    return anonymized_results\n</pre> def anonymize_faker_lambda(analyzer_results, text_to_anonymize):     \"\"\"Anonymize using a faker provider.\"\"\"      anonymized_results = anonymizer.anonymize(         text=text_to_anonymize,         analyzer_results=analyzer_results,         operators={             \"EMAIL_ADDRESS\": OperatorConfig(                 \"custom\", {\"lambda\": lambda x: fake.safe_email()}             )         },     )      return anonymized_results In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    fake = Faker(\"en_US\")\n    fake.add_provider(internet)\n\n    analyzer = AnalyzerEngine()\n    anonymizer = AnonymizerEngine()\n\n    text = \"The user has the following two emails: email1@contoso.com and email2@contoso.com\"  # noqa E501\n    analyzer_results = analyzer.analyze(\n        text=text, entities=[\"EMAIL_ADDRESS\"], language=\"en\"\n    )\n    print(f\"Original Text: {text}\")\n    print(f\"Analyzer result: {analyzer_results}\\n\")\n\n    print(\n        f\"Reverse lambda result: {anonymize_reverse_lambda(analyzer_results, text).text}\"  # noqa E501\n    )\n    print(f\"Faker lambda result: {anonymize_faker_lambda(analyzer_results, text).text}\")\n</pre> if __name__ == \"__main__\":     fake = Faker(\"en_US\")     fake.add_provider(internet)      analyzer = AnalyzerEngine()     anonymizer = AnonymizerEngine()      text = \"The user has the following two emails: email1@contoso.com and email2@contoso.com\"  # noqa E501     analyzer_results = analyzer.analyze(         text=text, entities=[\"EMAIL_ADDRESS\"], language=\"en\"     )     print(f\"Original Text: {text}\")     print(f\"Analyzer result: {analyzer_results}\\n\")      print(         f\"Reverse lambda result: {anonymize_reverse_lambda(analyzer_results, text).text}\"  # noqa E501     )     print(f\"Faker lambda result: {anonymize_faker_lambda(analyzer_results, text).text}\")"},{"location":"samples/python/example_dicom_image_redactor/","title":"Redacting Text PII from DICOM images","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install presidio_analyzer presidio_anonymizer presidio_image_redactor\n!python -m spacy download en_core_web_lg\n</pre> !pip install presidio_analyzer presidio_anonymizer presidio_image_redactor !python -m spacy download en_core_web_lg In\u00a0[1]: Copied! <pre>import glob\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport pydicom\nfrom presidio_image_redactor import DicomImageRedactorEngine\n</pre> import glob from pathlib import Path import matplotlib.pyplot as plt import pydicom from presidio_image_redactor import DicomImageRedactorEngine In\u00a0[2]: Copied! <pre>def compare_dicom_images(\n    instance_original: pydicom.dataset.FileDataset,\n    instance_redacted: pydicom.dataset.FileDataset,\n    figsize: tuple = (11, 11)\n) -&gt; None:\n    \"\"\"Display the DICOM pixel arrays of both original and redacted as images.\n\n    Args:\n        instance_original (pydicom.dataset.FileDataset): A single DICOM instance (with text PHI).\n        instance_redacted (pydicom.dataset.FileDataset): A single DICOM instance (redacted PHI).\n        figsize (tuple): Figure size in inches (width, height).\n    \"\"\"\n    _, ax = plt.subplots(1, 2, figsize=figsize)\n    ax[0].imshow(instance_original.pixel_array, cmap=\"gray\")\n    ax[0].set_title('Original')\n    ax[1].imshow(instance_redacted.pixel_array, cmap=\"gray\")\n    ax[1].set_title('Redacted')\n</pre> def compare_dicom_images(     instance_original: pydicom.dataset.FileDataset,     instance_redacted: pydicom.dataset.FileDataset,     figsize: tuple = (11, 11) ) -&gt; None:     \"\"\"Display the DICOM pixel arrays of both original and redacted as images.      Args:         instance_original (pydicom.dataset.FileDataset): A single DICOM instance (with text PHI).         instance_redacted (pydicom.dataset.FileDataset): A single DICOM instance (redacted PHI).         figsize (tuple): Figure size in inches (width, height).     \"\"\"     _, ax = plt.subplots(1, 2, figsize=figsize)     ax[0].imshow(instance_original.pixel_array, cmap=\"gray\")     ax[0].set_title('Original')     ax[1].imshow(instance_redacted.pixel_array, cmap=\"gray\")     ax[1].set_title('Redacted') <p>Instantiate the DICOM image redactor engine object.</p> <p>Note: The <code>DicomImageRedactorEngine</code> object can initialized with a custom <code>ImageAnalyzerEngine</code>, which may be useful in cases where DICOM metadata is insufficient.</p> In\u00a0[3]: Copied! <pre>engine = DicomImageRedactorEngine()\n</pre> engine = DicomImageRedactorEngine() <p>In cases where you already working with loaded DICOM data, the <code>.redact()</code> function is most appropriate.</p> In\u00a0[4]: Copied! <pre># Load in and process your DICOM file as needed\ndicom_instance = pydicom.dcmread('sample_data/0_ORIGINAL.dcm')\n</pre> # Load in and process your DICOM file as needed dicom_instance = pydicom.dcmread('sample_data/0_ORIGINAL.dcm') In\u00a0[5]: Copied! <pre># Redact\nredacted_dicom_instance = engine.redact(dicom_instance, fill=\"contrast\")\n</pre> # Redact redacted_dicom_instance = engine.redact(dicom_instance, fill=\"contrast\") In\u00a0[6]: Copied! <pre>compare_dicom_images(dicom_instance, redacted_dicom_instance)\n</pre> compare_dicom_images(dicom_instance, redacted_dicom_instance) <p>We can also set the \"fill\" to match the background color to blend in more with the image.</p> In\u00a0[7]: Copied! <pre>redacted_dicom_instance_2 = engine.redact(dicom_instance, fill=\"background\")\ncompare_dicom_images(dicom_instance, redacted_dicom_instance_2)\n</pre> redacted_dicom_instance_2 = engine.redact(dicom_instance, fill=\"background\") compare_dicom_images(dicom_instance, redacted_dicom_instance_2) In\u00a0[8]: Copied! <pre>redacted_dicom_instance = engine.redact(dicom_instance, use_metadata=False) # default is use_metadata=True\ncompare_dicom_images(dicom_instance, redacted_dicom_instance)\n</pre> redacted_dicom_instance = engine.redact(dicom_instance, use_metadata=False) # default is use_metadata=True compare_dicom_images(dicom_instance, redacted_dicom_instance) <p>We can also return the bounding box information for the pixel regions that were redacted.</p> In\u00a0[9]: Copied! <pre>redacted_dicom_instance, bbox = engine.redact_and_return_bbox(dicom_instance)\ncompare_dicom_images(dicom_instance, redacted_dicom_instance)\nprint(f\"Number of redacted regions: {len(bbox)}\")\nprint(bbox)\n</pre> redacted_dicom_instance, bbox = engine.redact_and_return_bbox(dicom_instance) compare_dicom_images(dicom_instance, redacted_dicom_instance) print(f\"Number of redacted regions: {len(bbox)}\") print(bbox) <pre>Number of redacted regions: 4\n[{'top': 0, 'left': 0, 'width': 241, 'height': 37}, {'top': 0, 'left': 262, 'width': 230, 'height': 36}, {'top': 1, 'left': 588, 'width': 226, 'height': 35}, {'top': 47, 'left': 145, 'width': 218, 'height': 35}]\n</pre> In\u00a0[10]: Copied! <pre># Single DICOM (.dcm) file or directory containing DICOM files\ninput_path = 'sample_data/'\n\n# Directory where the output will be written\noutput_parent_dir = 'output/'\n</pre> # Single DICOM (.dcm) file or directory containing DICOM files input_path = 'sample_data/'  # Directory where the output will be written output_parent_dir = 'output/' In\u00a0[11]: Copied! <pre># Redact text PHI from DICOM images\nengine.redact_from_directory(\n    input_dicom_path = input_path,\n    output_dir = output_parent_dir,\n    fill=\"contrast\",\n    save_bboxes=True # if True, saves the redacted region bounding box info to .json files in the output dir\n)\n</pre> # Redact text PHI from DICOM images engine.redact_from_directory(     input_dicom_path = input_path,     output_dir = output_parent_dir,     fill=\"contrast\",     save_bboxes=True # if True, saves the redacted region bounding box info to .json files in the output dir ) <pre>Output written to output\\sample_data\n</pre> <p>Get file paths</p> In\u00a0[12]: Copied! <pre># Original DICOM images\np = Path(input_path).glob(\"**/*.dcm\")\noriginal_files = [x for x in p if x.is_file()]\n\n# Redacted DICOM images\np = Path(output_parent_dir).glob(\"**/*.dcm\")\nredacted_files = [x for x in p if x.is_file()]\n</pre> # Original DICOM images p = Path(input_path).glob(\"**/*.dcm\") original_files = [x for x in p if x.is_file()]  # Redacted DICOM images p = Path(output_parent_dir).glob(\"**/*.dcm\") redacted_files = [x for x in p if x.is_file()] <p>Preview images</p> In\u00a0[13]: Copied! <pre>for i in range(0, len(original_files)):\n    original_file = pydicom.dcmread(original_files[i])\n    redacted_file = pydicom.dcmread(redacted_files[i])\n    \n    compare_dicom_images(original_file, redacted_file)\n</pre> for i in range(0, len(original_files)):     original_file = pydicom.dcmread(original_files[i])     redacted_file = pydicom.dcmread(redacted_files[i])          compare_dicom_images(original_file, redacted_file)"},{"location":"samples/python/example_dicom_image_redactor/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonexample_dicom_image_redactoripynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/example_dicom_image_redactor.ipynb\u00b6","text":""},{"location":"samples/python/example_dicom_image_redactor/#de-identifying-sensitive-burnt-in-text-in-dicom-images","title":"De-identifying sensitive burnt-in text in DICOM images\u00b6","text":"<p>This notebook covers how to:</p> <ol> <li>Redact text Personal Health Information (PHI) present as pixels in DICOM images</li> <li>Visually compare original DICOM images with their redacted versions</li> </ol> <p>This module only redacts pixel data and does not scrub text PHI which may exist in the DICOM metadata. To redact sensitive information from metadata, consider using another package such as the Tools for Health Data Anonymization.</p>"},{"location":"samples/python/example_dicom_image_redactor/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before getting started, make sure presidio and the latest version of Tesseract OCR are installed. For detailed documentation, see the installation docs.</p>"},{"location":"samples/python/example_dicom_image_redactor/#dataset","title":"Dataset\u00b6","text":"<p>Sample DICOM files are available for use in this notebook in <code>./sample_data</code>. Copies of the original DICOM data were saved into the folder with permission from the dataset owners. Please see the original dataset information below:</p> <p>Rutherford, M., Mun, S.K., Levine, B., Bennett, W.C., Smith, K., Farmer, P., Jarosz, J., Wagner, U., Farahani, K., Prior, F. (2021). A DICOM dataset for evaluation of medical image de-identification (Pseudo-PHI-DICOM-Data) [Data set]. The Cancer Imaging Archive. DOI: https://doi.org/10.7937/s17z-r072</p>"},{"location":"samples/python/example_dicom_image_redactor/#1-setup","title":"1. Setup\u00b6","text":""},{"location":"samples/python/example_dicom_image_redactor/#2-redacting-from-loaded-dicom-image-data","title":"2. Redacting from loaded DICOM image data\u00b6","text":""},{"location":"samples/python/example_dicom_image_redactor/#22-verify-performance","title":"2.2 Verify performance\u00b6","text":"<p>Let's look at the original input and compare against the de-identified output.</p>"},{"location":"samples/python/example_dicom_image_redactor/#23-adjust-parameters","title":"2.3 Adjust parameters\u00b6","text":"<p>With the <code>use_metadata</code> parameter, we can toggle whether the DICOM metadata is used to augment the analyzer which determines which text to redact.</p>"},{"location":"samples/python/example_dicom_image_redactor/#3-redacting-from-dicom-files","title":"3. Redacting from DICOM files\u00b6","text":"<p>Before instantiating your <code>DicomImageRedactorEngine</code> class, determine where you want your input to come from and where you want your output to be written to.</p> <p>Note: The output will mimic the folder structure of the input if the input is a directory. The redact method will operate on all DICOM (.dcm) files in the input directory and all its subdirectories.</p> <p>To protect against overwriting the original DICOM files, the <code>redact_from_file()</code> and <code>redact_from_directory()</code> methods will not run if the <code>output_dir</code> is a directory which already contains any content.</p>"},{"location":"samples/python/example_dicom_image_redactor/#31-run-de-identification","title":"3.1. Run de-identification\u00b6","text":"<p>Use the <code>DicomImageRedactorEngine</code> class to process your DICOM images. If you have only one image to process and want to specify that directly instead of a directory, use <code>.redact_from_file()</code> instead of <code>.redact_from_directory()</code>.</p>"},{"location":"samples/python/example_dicom_image_redactor/#32-verify-performance","title":"3.2. Verify performance\u00b6","text":"<p>Let's look at the original input and compare against the de-identified output.</p>"},{"location":"samples/python/example_dicom_image_redactor/#conclusion","title":"Conclusion\u00b6","text":"<p>As seen in the DICOM image previews above, we see that our <code>DicomImageRedactorEngine</code> is able to successfully mask out text PHI present in the DICOM images without compromising image quality.</p> <p>Note: Performance is best when the burnt-in text is also present within the DICOM metadata. We recommend not scrubbing metadata until after performing image de-identification.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/","title":"Example DICOM redaction evaluation","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install presidio_analyzer presidio_anonymizer presidio_image_redactor\n!python -m spacy download en_core_web_lg\n</pre> !pip install presidio_analyzer presidio_anonymizer presidio_image_redactor !python -m spacy download en_core_web_lg In\u00a0[1]: Copied! <pre>import os\nimport json\nimport pandas as pd\nimport pydicom\n\nfrom presidio_image_redactor import DicomImagePiiVerifyEngine\n</pre> import os import json import pandas as pd import pydicom  from presidio_image_redactor import DicomImagePiiVerifyEngine In\u00a0[2]: Copied! <pre># Set paths\ndata_dir = \"sample_data\"\ngt_path = \"sample_data/ground_truth.json\"\n</pre> # Set paths data_dir = \"sample_data\" gt_path = \"sample_data/ground_truth.json\" In\u00a0[3]: Copied! <pre># Load ground truth JSON\nwith open(gt_path) as json_file:\n    gt = json.load(json_file)\n\n# Get list of files\ngt_dicom_files = list(gt.keys())\ngt_dicom_files\n</pre> # Load ground truth JSON with open(gt_path) as json_file:     gt = json.load(json_file)  # Get list of files gt_dicom_files = list(gt.keys()) gt_dicom_files Out[3]: <pre>['sample_data/0_ORIGINAL.dcm',\n 'sample_data/1_ORIGINAL.dcm',\n 'sample_data/2_ORIGINAL.dcm',\n 'sample_data/3_ORIGINAL.dcm']</pre> In\u00a0[4]: Copied! <pre>dicom_engine = DicomImagePiiVerifyEngine()\n</pre> dicom_engine = DicomImagePiiVerifyEngine() In\u00a0[5]: Copied! <pre># Select one file to work with\nfile_of_interest = gt_dicom_files[0]\ngt_file_of_interest = gt[file_of_interest]\n</pre> # Select one file to work with file_of_interest = gt_dicom_files[0] gt_file_of_interest = gt[file_of_interest] In\u00a0[6]: Copied! <pre># Return image to visually inspect\ninstance = pydicom.dcmread(file_of_interest)\nverify_image, ocr_results, analyzer_results = dicom_engine.verify_dicom_instance(instance)\n</pre> # Return image to visually inspect instance = pydicom.dcmread(file_of_interest) verify_image, ocr_results, analyzer_results = dicom_engine.verify_dicom_instance(instance) In\u00a0[7]: Copied! <pre>def get_PHI_list(PHI: list) -&gt; list:\n    \"\"\"Get list of PHI from ground truth for a single file.\n    \n    Args:\n        PHI_dict (list): List of ground truth or detected text PHI.\n    \n    Return:\n        PHI_list (list): List of PHI (just text).\n    \"\"\"\n    PHI_list = []\n    for item in PHI:\n        PHI_list.append(item['label'])\n    \n    return PHI_list\n</pre> def get_PHI_list(PHI: list) -&gt; list:     \"\"\"Get list of PHI from ground truth for a single file.          Args:         PHI_dict (list): List of ground truth or detected text PHI.          Return:         PHI_list (list): List of PHI (just text).     \"\"\"     PHI_list = []     for item in PHI:         PHI_list.append(item['label'])          return PHI_list In\u00a0[\u00a0]: Copied! <pre>_, eval_results = dicom_engine.eval_dicom_instance(instance, gt_file_of_interest)\n</pre> _, eval_results = dicom_engine.eval_dicom_instance(instance, gt_file_of_interest) <p>Results</p> In\u00a0[9]: Copied! <pre>print(f\"Precision: {eval_results['precision']}\")\nprint(f\"Recall: {eval_results['recall']}\")\nprint(f\"All Positives: {get_PHI_list(eval_results['all_positives'])}\")\nprint(f\"Ground Truth: {get_PHI_list(eval_results['ground_truth'])}\")\n</pre> print(f\"Precision: {eval_results['precision']}\") print(f\"Recall: {eval_results['recall']}\") print(f\"All Positives: {get_PHI_list(eval_results['all_positives'])}\") print(f\"Ground Truth: {get_PHI_list(eval_results['ground_truth'])}\") <pre>Precision: 1.0\nRecall: 1.0\nAll Positives: ['DAVIDSON', 'DOUGLAS', '[M]', '01.09.2012', '06.16.1976']\nGround Truth: ['DAVIDSON', 'DOUGLAS', '[M]', '01.09.2012', '06.16.1976']\n</pre> In\u00a0[10]: Copied! <pre># Initialize lists to turn into results table\nlist_of_files = gt_dicom_files\nlist_of_gt = []\nlist_of_pos = []\nlist_of_recall = []\nlist_of_precision = []\n</pre> # Initialize lists to turn into results table list_of_files = gt_dicom_files list_of_gt = [] list_of_pos = [] list_of_recall = [] list_of_precision = [] <p>Loop through all the files</p> In\u00a0[\u00a0]: Copied! <pre>for file in gt_dicom_files:\n    # Setup\n    ground_truth = gt[file]\n    instance = pydicom.dcmread(file)\n    \n    # Evaluate\n    _, eval_results = dicom_engine.eval_dicom_instance(instance, ground_truth)\n    \n    # Save results\n    list_of_gt.append(get_PHI_list(eval_results[\"ground_truth\"]))\n    list_of_pos.append(get_PHI_list(eval_results[\"all_positives\"]))\n    list_of_recall.append(eval_results[\"recall\"])\n    list_of_precision.append(eval_results[\"precision\"])\n</pre> for file in gt_dicom_files:     # Setup     ground_truth = gt[file]     instance = pydicom.dcmread(file)          # Evaluate     _, eval_results = dicom_engine.eval_dicom_instance(instance, ground_truth)          # Save results     list_of_gt.append(get_PHI_list(eval_results[\"ground_truth\"]))     list_of_pos.append(get_PHI_list(eval_results[\"all_positives\"]))     list_of_recall.append(eval_results[\"recall\"])     list_of_precision.append(eval_results[\"precision\"]) <p>Create a summary results table</p> In\u00a0[12]: Copied! <pre># Organize results into a table\nall_results_dict = {\n    \"file\": list_of_files,\n    \"ground_truth\": list_of_gt,\n    \"all_positives\": list_of_pos,\n    \"recall\": list_of_recall,\n    \"precision\": list_of_precision\n}\n\ndf_results = pd.DataFrame(all_results_dict)\ndf_results\n</pre> # Organize results into a table all_results_dict = {     \"file\": list_of_files,     \"ground_truth\": list_of_gt,     \"all_positives\": list_of_pos,     \"recall\": list_of_recall,     \"precision\": list_of_precision }  df_results = pd.DataFrame(all_results_dict) df_results Out[12]: file ground_truth all_positives recall precision 0 sample_data/0_ORIGINAL.dcm [DAVIDSON, DOUGLAS, [M], 01.09.2012, 06.16.1976] [DAVIDSON, DOUGLAS, [M], 01.09.2012, 06.16.1976] 1.0 1.0 1 sample_data/1_ORIGINAL.dcm [MARTIN, CHAD, [U], 01.01.2000] [MARTIN, CHAD, [U], 01.01.2000] 1.0 1.0 2 sample_data/2_ORIGINAL.dcm [KAUFMAN, SCOTT, [M], 03.09.2012, 07.22.1943] [KAUFMAN, 07.22.1943, SCOTT, [M], 03.09.2012] 1.0 1.0 3 sample_data/3_ORIGINAL.dcm [MEYER, STEPHANIE, [F], 02.25.2012, 07.16.1953] [MEYER, STEPHANIE, [F], 02.25.2012, 07.16.1953] 1.0 1.0 <p>For example, if we set <code>padding_width=1</code>, this can negatively impact the OCR step which identifies all text regardless of PHI status in an image if text is bordering the edges of the image. When the OCR fails to return all text, we cannot reliably detect PHI.</p> In\u00a0[\u00a0]: Copied! <pre># Select file\nfile_of_interest = gt_dicom_files[3]\ngt_file_of_interest = gt[file_of_interest]\ninstance = pydicom.dcmread(file_of_interest)\n\n# Run evaluation with minimal padding (0 padding not allowed)\n_, eval_results = dicom_engine.eval_dicom_instance(instance, gt_file_of_interest, padding_width=1)\n</pre> # Select file file_of_interest = gt_dicom_files[3] gt_file_of_interest = gt[file_of_interest] instance = pydicom.dcmread(file_of_interest)  # Run evaluation with minimal padding (0 padding not allowed) _, eval_results = dicom_engine.eval_dicom_instance(instance, gt_file_of_interest, padding_width=1) <p>Notice how low the recall is and how different the detected PHI list is here than in the summary table above which ran de-identification and evaluation with the default  <code>padding_width=25</code>.</p> In\u00a0[14]: Copied! <pre>print(f\"Precision: {eval_results['precision']}\")\nprint(f\"Recall: {eval_results['recall']}\")\nprint(f\"All Positives: {get_PHI_list(eval_results['all_positives'])}\")\nprint(f\"Ground Truth: {get_PHI_list(eval_results['ground_truth'])}\")\n</pre> print(f\"Precision: {eval_results['precision']}\") print(f\"Recall: {eval_results['recall']}\") print(f\"All Positives: {get_PHI_list(eval_results['all_positives'])}\") print(f\"Ground Truth: {get_PHI_list(eval_results['ground_truth'])}\") <pre>Precision: 1.0\nRecall: 0.2\nAll Positives: ['07.16.1953']\nGround Truth: ['MEYER', 'STEPHANIE', '[F]', '02.25.2012', '07.16.1953']\n</pre>"},{"location":"samples/python/example_dicom_redactor_evaluation/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonexample_dicom_redactor_evaluationipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/example_dicom_redactor_evaluation.ipynb\u00b6","text":""},{"location":"samples/python/example_dicom_redactor_evaluation/#evaluate-dicom-de-identification-performance","title":"Evaluate DICOM de-identification performance\u00b6","text":"<p>This notebook demonstrates how to use the <code>DicomImagePiiVerifyEngine</code> to evaluate how well the <code>DicomImageRedactorEngine</code> de-identifies text Personal Health Information (PHI) from DICOM images when ground truth labels are available.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before getting started, make sure presidio and the latest version of Tesseract OCR are installed. For detailed documentation, see the installation docs.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#dataset","title":"Dataset\u00b6","text":"<p>Sample DICOM files are available for use in this notebook in <code>./sample_data</code>. Copies of the original DICOM data were saved into the folder with permission from the dataset owners. Please see the original dataset information below:</p> <p>Rutherford, M., Mun, S.K., Levine, B., Bennett, W.C., Smith, K., Farmer, P., Jarosz, J., Wagner, U., Farahani, K., Prior, F. (2021). A DICOM dataset for evaluation of medical image de-identification (Pseudo-PHI-DICOM-Data) [Data set]. The Cancer Imaging Archive. DOI: https://doi.org/10.7937/s17z-r072</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#load-ground-truth","title":"Load ground truth\u00b6","text":"<p>Load the ground truth labels. For more information on the ground truth format, please see the evaluating DICOM de-identification page.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#initialize-the-verification-engine","title":"Initialize the verification engine\u00b6","text":"<p>This engine will be used for both verification and evaluation.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#verify-detected-phi-for-one-dicom-image","title":"Verify detected PHI for one DICOM image\u00b6","text":"<p>To visually identify what text is being detected as PHI on a DICOM image, use the <code>.verify_dicom_instance()</code> method.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#evaluate-de-identification-performance","title":"Evaluate de-identification performance\u00b6","text":"<p>To evaluate how well the actual sensitive text (specified in the ground truth) are identified and redacted, use the <code>.evaluate_dicom_instance()</code> method.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#for-one-image","title":"For one image\u00b6","text":"<p>Display the DICOM image with bounding boxes identifying the detected PHI.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#for-multiple-images","title":"For multiple images\u00b6","text":""},{"location":"samples/python/example_dicom_redactor_evaluation/#experiment-with-settings","title":"Experiment with settings\u00b6","text":"<p>You can experiment with different settings such as <code>padding_width</code>, <code>tolerance</code>, and any additional arguments to feed into the image analyzer in your DICOM verification engine and see the effect on performance.</p> <p>Changing tolerance does not affect the de-identification logic nor image redaction. Tolerance is only used for matching analyzer results to ground truth labels.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#conclusion","title":"Conclusion\u00b6","text":"<p>The <code>DicomImagePiiVerifyEngine</code> allows us to easily do a visual inspection on the identified PHI and also evaluate how well the de-identification worked compared to a provided ground truth.</p> <p>In the case of these sample images, the precision and recall of the Presidio <code>DicomImageRedactorEngine</code> redact function is 1.0 when we use the default values <code>padding_width=25</code> and <code>tolerance=50</code>.</p>"},{"location":"samples/python/example_pdf_annotation/","title":"Annotating PII in a PDF","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install presidio_analyzer\n!pip install presidio_anonymizer\n!python -m spacy download en_core_web_lg\n!pip install pdfminer.six\n!pip install pikepdf\n</pre> !pip install presidio_analyzer !pip install presidio_anonymizer !python -m spacy download en_core_web_lg !pip install pdfminer.six !pip install pikepdf In\u00a0[4]: Copied! <pre># For Presidio\nfrom presidio_analyzer import AnalyzerEngine, PatternRecognizer\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\n\n# For console output\nfrom pprint import pprint\n\n# For extracting text\nfrom pdfminer.high_level import extract_text, extract_pages\nfrom pdfminer.layout import LTTextContainer, LTChar, LTTextLine\n\n# For updating the PDF\nfrom pikepdf import Pdf, AttachedFileSpec, Name, Dictionary, Array\n</pre> # For Presidio from presidio_analyzer import AnalyzerEngine, PatternRecognizer from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import OperatorConfig  # For console output from pprint import pprint  # For extracting text from pdfminer.high_level import extract_text, extract_pages from pdfminer.layout import LTTextContainer, LTChar, LTTextLine  # For updating the PDF from pikepdf import Pdf, AttachedFileSpec, Name, Dictionary, Array In\u00a0[5]: Copied! <pre>analyzer = AnalyzerEngine()\n\nanalyzed_character_sets = []\n\nfor page_layout in extract_pages(\"./sample_data/sample.pdf\"):\n    for text_container in page_layout:\n        if isinstance(text_container, LTTextContainer):\n\n            # The element is a LTTextContainer, containing a paragraph of text.\n            text_to_anonymize = text_container.get_text()\n\n            # Analyze the text using the analyzer engine\n            analyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')\n \n            if text_to_anonymize.isspace() == False:\n                print(text_to_anonymize)\n                print(analyzer_results)\n\n            characters = list([])\n\n            # Grab the characters from the PDF\n            for text_line in filter(lambda t: isinstance(t, LTTextLine), text_container):\n                    for character in filter(lambda t: isinstance(t, LTChar), text_line):\n                            characters.append(character)\n\n\n            # Slice out the characters that match the analyzer results.\n            for result in analyzer_results:\n                start = result.start\n                end = result.end\n                analyzed_character_sets.append({\"characters\": characters[start:end], \"result\": result})\n</pre> analyzer = AnalyzerEngine()  analyzed_character_sets = []  for page_layout in extract_pages(\"./sample_data/sample.pdf\"):     for text_container in page_layout:         if isinstance(text_container, LTTextContainer):              # The element is a LTTextContainer, containing a paragraph of text.             text_to_anonymize = text_container.get_text()              # Analyze the text using the analyzer engine             analyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')               if text_to_anonymize.isspace() == False:                 print(text_to_anonymize)                 print(analyzer_results)              characters = list([])              # Grab the characters from the PDF             for text_line in filter(lambda t: isinstance(t, LTTextLine), text_container):                     for character in filter(lambda t: isinstance(t, LTChar), text_line):                             characters.append(character)               # Slice out the characters that match the analyzer results.             for result in analyzer_results:                 start = result.start                 end = result.end                 analyzed_character_sets.append({\"characters\": characters[start:end], \"result\": result}) <pre>This is a test PDF, created by Microsoft Word. \n\n[]\nHi my name is Charles Darwin and my email is cdarwin@hmsbeagle.org \n\n[type: EMAIL_ADDRESS, start: 45, end: 66, score: 1.0, type: PERSON, start: 14, end: 28, score: 0.85, type: URL, start: 53, end: 66, score: 0.5]\nYou can contact me on 01234 567890. \n\n[type: PHONE_NUMBER, start: 22, end: 34, score: 0.4, type: US_DRIVER_LICENSE, start: 28, end: 34, score: 0.01]\n</pre> In\u00a0[6]: Copied! <pre># Combine the bounding boxes into a single bounding box.\ndef combine_rect(rectA, rectB):\n    a, b = rectA, rectB\n    startX = min( a[0], b[0] )\n    startY = min( a[1], b[1] )\n    endX = max( a[2], b[2] )\n    endY = max( a[3], b[3] )\n    return (startX, startY, endX, endY)\n\nanalyzed_bounding_boxes = []\n\n# For each character set, combine the bounding boxes into a single bounding box.\nfor analyzed_character_set in analyzed_character_sets:\n    completeBoundingBox = analyzed_character_set[\"characters\"][0].bbox\n    \n    for character in analyzed_character_set[\"characters\"]:\n        completeBoundingBox = combine_rect(completeBoundingBox, character.bbox)\n    \n    analyzed_bounding_boxes.append({\"boundingBox\": completeBoundingBox, \"result\": analyzed_character_set[\"result\"]})\n</pre> # Combine the bounding boxes into a single bounding box. def combine_rect(rectA, rectB):     a, b = rectA, rectB     startX = min( a[0], b[0] )     startY = min( a[1], b[1] )     endX = max( a[2], b[2] )     endY = max( a[3], b[3] )     return (startX, startY, endX, endY)  analyzed_bounding_boxes = []  # For each character set, combine the bounding boxes into a single bounding box. for analyzed_character_set in analyzed_character_sets:     completeBoundingBox = analyzed_character_set[\"characters\"][0].bbox          for character in analyzed_character_set[\"characters\"]:         completeBoundingBox = combine_rect(completeBoundingBox, character.bbox)          analyzed_bounding_boxes.append({\"boundingBox\": completeBoundingBox, \"result\": analyzed_character_set[\"result\"]}) In\u00a0[7]: Copied! <pre>pdf = Pdf.open(\"./sample_data/sample.pdf\")\n\nannotations = []\n\n# Create a highlight annotation for each bounding box.\nfor analyzed_bounding_box in analyzed_bounding_boxes:\n\n    boundingBox = analyzed_bounding_box[\"boundingBox\"]\n\n    # Create the annotation. \n    # We could also create a redaction annotation if the ongoing workflows supports them.\n    highlight = Dictionary(\n        Type=Name.Annot,\n        Subtype=Name.Highlight,\n        QuadPoints=[boundingBox[0], boundingBox[3],\n                    boundingBox[2], boundingBox[3],\n                    boundingBox[0], boundingBox[1],\n                    boundingBox[2], boundingBox[1]],\n        Rect=[boundingBox[0], boundingBox[1], boundingBox[2], boundingBox[3]],\n        C=[1, 0, 0],\n        CA=0.5,\n        T=analyzed_bounding_box[\"result\"].entity_type,\n    )\n    \n    annotations.append(highlight)\n\n# Add the annotations to the PDF.\npdf.pages[0].Annots = pdf.make_indirect(annotations)\n\n# And save.\npdf.save(\"./sample_data/sample_annotated.pdf\")\n</pre> pdf = Pdf.open(\"./sample_data/sample.pdf\")  annotations = []  # Create a highlight annotation for each bounding box. for analyzed_bounding_box in analyzed_bounding_boxes:      boundingBox = analyzed_bounding_box[\"boundingBox\"]      # Create the annotation.      # We could also create a redaction annotation if the ongoing workflows supports them.     highlight = Dictionary(         Type=Name.Annot,         Subtype=Name.Highlight,         QuadPoints=[boundingBox[0], boundingBox[3],                     boundingBox[2], boundingBox[3],                     boundingBox[0], boundingBox[1],                     boundingBox[2], boundingBox[1]],         Rect=[boundingBox[0], boundingBox[1], boundingBox[2], boundingBox[3]],         C=[1, 0, 0],         CA=0.5,         T=analyzed_bounding_box[\"result\"].entity_type,     )          annotations.append(highlight)  # Add the annotations to the PDF. pdf.pages[0].Annots = pdf.make_indirect(annotations)  # And save. pdf.save(\"./sample_data/sample_annotated.pdf\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"samples/python/example_pdf_annotation/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonexample_pdf_annotationipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/example_pdf_annotation.ipynb\u00b6","text":""},{"location":"samples/python/example_pdf_annotation/#annotating-pii-in-a-pdf","title":"Annotating PII in a PDF\u00b6","text":"<p>This sample takes a PDF as an input, extracts the text, identifies PII using Presidio and annotates the PII using highlight annotations.</p>"},{"location":"samples/python/example_pdf_annotation/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before getting started, make sure the following packages are installed. For detailed documentation, see the installation docs.</p> <p>Install from PyPI:</p>"},{"location":"samples/python/example_pdf_annotation/#analyze-the-text-in-the-pdf","title":"Analyze the text in the PDF\u00b6","text":"<p>To extract the text from the PDF, we use the pdf miner library. We extract the text from the PDF at the text container level. This is roughly equivalent to a paragraph.</p> <p>We then use Presidio Analyzer to identify the PII and it's location in the text.</p> <p>The Presidio analyzer is using pre-defined entity recognizers, and offers the option to create custom recognizers.</p>"},{"location":"samples/python/example_pdf_annotation/#create-phrase-bounding-boxes","title":"Create phrase bounding boxes\u00b6","text":"<p>The next task is to take the character data, and inflate it into full phrase bounding boxes.</p> <p>For example, for an email address, we'll turn the bounding boxes for each character in the email address into one single bounding box.</p>"},{"location":"samples/python/example_pdf_annotation/#add-highlight-annotations","title":"Add highlight annotations\u00b6","text":"<p>We finally iterate through all the analyzed bounding boxes and create highlight annotations for all of them.</p>"},{"location":"samples/python/example_pdf_annotation/#result","title":"Result\u00b6","text":"<p>The output from the samples above creates a new PDF. This contains the original content, with text highlight annotations where the PII has been found.</p> <p>Each text annotation contains the name of the entity found.</p>"},{"location":"samples/python/example_pdf_annotation/#note","title":"Note\u00b6","text":"<p>Before relying on this methodology to detect or markup PII from a PDF, please be aware of the following:</p>"},{"location":"samples/python/example_pdf_annotation/#text-extraction","title":"Text extraction\u00b6","text":"<p>We purposely use a different library specifically for extracting text from the PDF. This is because text extraction is hard to get right, and it's worth using a library specifically developed with the purpose in mind.</p> <p>For more details, see:</p> <p>https://pdfminersix.readthedocs.io/en/latest/topic/converting_pdf_to_text.html</p> <p>That said, even with a purpose built library, there may be occasions where PII is present and visible in a PDF, but it is not detected by the sample code.</p> <p>This includes, but is not limited to:</p> <ul> <li>Text that cannot be reliable extracted to be analyzed. (e.g. incorrect spacing, wrong reading order)</li> <li>Text present in previous iterations of the PDF which is hidden from text extraction. (See incremental editing)</li> <li>Text present in images. (requires OCRing)</li> <li>Text present in annotations.</li> </ul>"},{"location":"samples/python/example_remote_recognizer/","title":"Example remote recognizer","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nExample implementation of a RemoteRecognizer.\n\nRemote recognizers call external APIs\nto get additional PII identification capabilities.\nThese results are added to all the other results\ngathered by the different recognizers.\n\nThe actual call logic (e.g., HTTP or gRPC)\nshould be implemented within this class.\nIn this example, we use the `requests` package to perform a POST request.\nFlow:\n1. During `load`, call `supported_entities`\nto get a list of what this service can detect\n2. Translate the response coming from the `supported_entities` endpoint\n3. During `analyze`, perform a POST request to the PII detector endpoint\n4. Translate the response coming from the\nPII detector endpoint into a List[RecognizerResult]\n5. Return results\n\nNote: In this example we mimic an external PII detector\nby using Presidio Analyzer itself.\n\n\"\"\"\n</pre> \"\"\" Example implementation of a RemoteRecognizer.  Remote recognizers call external APIs to get additional PII identification capabilities. These results are added to all the other results gathered by the different recognizers.  The actual call logic (e.g., HTTP or gRPC) should be implemented within this class. In this example, we use the `requests` package to perform a POST request. Flow: 1. During `load`, call `supported_entities` to get a list of what this service can detect 2. Translate the response coming from the `supported_entities` endpoint 3. During `analyze`, perform a POST request to the PII detector endpoint 4. Translate the response coming from the PII detector endpoint into a List[RecognizerResult] 5. Return results  Note: In this example we mimic an external PII detector by using Presidio Analyzer itself.  \"\"\" In\u00a0[\u00a0]: Copied! <pre>import json\nimport logging\nfrom typing import List\n</pre> import json import logging from typing import List In\u00a0[\u00a0]: Copied! <pre>import requests\n</pre> import requests In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import RemoteRecognizer, RecognizerResult\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n</pre> from presidio_analyzer import RemoteRecognizer, RecognizerResult from presidio_analyzer.nlp_engine import NlpArtifacts In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(\"presidio-analyzer\")\n</pre> logger = logging.getLogger(\"presidio-analyzer\") In\u00a0[\u00a0]: Copied! <pre>class ExampleRemoteRecognizer(RemoteRecognizer):\n    \"\"\"\n    A reference implementation of a remote recognizer.\n\n    Calls Presidio analyzer as if it was an external remote PII detector\n    :param pii_identification_url: Service URL for detecting PII\n    :param supported_entities_url: Service URL for getting the supported entities\n    by this service\n    \"\"\"\n\n    def __init__(\n        self,\n        pii_identification_url: str = \"https://MYPIISERVICE_URL/detect\",\n        supported_entities_url: str = \"https://MYPIISERVICE_URL/supported_entities\",\n    ):\n        self.pii_identification_url = pii_identification_url\n        self.supported_entities_url = supported_entities_url\n\n        super().__init__(\n            supported_entities=[], name=None, supported_language=\"en\", version=\"1.0\"\n        )\n\n    def load(self) -&gt; None:\n        \"\"\"Call the get_supported_entities API of the external service.\"\"\"\n        try:\n            response = requests.get(\n                self.supported_entities_url,\n                params={\"language\": self.supported_language},\n            )\n            self.supported_entities = self._supported_entities_from_response(response)\n\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Failed to get supported entities from external service. {e}\")\n            self.supported_language = []\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Call an external service for PII detection.\"\"\"\n\n        payload = {\"text\": text, \"language\": self.supported_language}\n\n        response = requests.post(\n            self.pii_identification_url,\n            json=payload,\n            timeout=200,\n        )\n\n        results = self._recognizer_results_from_response(response)\n\n        return results\n\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"Return the list of supported entities.\"\"\"\n        return self.supported_entities\n\n    @staticmethod\n    def _recognizer_results_from_response(\n        response: requests.Response,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Translate the service's response to a list of RecognizerResult.\"\"\"\n        results = json.loads(response.text)\n        recognizer_results = [RecognizerResult(**result) for result in results]\n\n        return recognizer_results\n\n    @staticmethod\n    def _supported_entities_from_response(response: requests.Response) -&gt; List[str]:\n        \"\"\"Translate the service's supported entities list to Presidio's.\"\"\"\n        return json.loads(response.text)\n</pre> class ExampleRemoteRecognizer(RemoteRecognizer):     \"\"\"     A reference implementation of a remote recognizer.      Calls Presidio analyzer as if it was an external remote PII detector     :param pii_identification_url: Service URL for detecting PII     :param supported_entities_url: Service URL for getting the supported entities     by this service     \"\"\"      def __init__(         self,         pii_identification_url: str = \"https://MYPIISERVICE_URL/detect\",         supported_entities_url: str = \"https://MYPIISERVICE_URL/supported_entities\",     ):         self.pii_identification_url = pii_identification_url         self.supported_entities_url = supported_entities_url          super().__init__(             supported_entities=[], name=None, supported_language=\"en\", version=\"1.0\"         )      def load(self) -&gt; None:         \"\"\"Call the get_supported_entities API of the external service.\"\"\"         try:             response = requests.get(                 self.supported_entities_url,                 params={\"language\": self.supported_language},             )             self.supported_entities = self._supported_entities_from_response(response)          except requests.exceptions.RequestException as e:             logger.error(f\"Failed to get supported entities from external service. {e}\")             self.supported_language = []      def analyze(         self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts     ) -&gt; List[RecognizerResult]:         \"\"\"Call an external service for PII detection.\"\"\"          payload = {\"text\": text, \"language\": self.supported_language}          response = requests.post(             self.pii_identification_url,             json=payload,             timeout=200,         )          results = self._recognizer_results_from_response(response)          return results      def get_supported_entities(self) -&gt; List[str]:         \"\"\"Return the list of supported entities.\"\"\"         return self.supported_entities      @staticmethod     def _recognizer_results_from_response(         response: requests.Response,     ) -&gt; List[RecognizerResult]:         \"\"\"Translate the service's response to a list of RecognizerResult.\"\"\"         results = json.loads(response.text)         recognizer_results = [RecognizerResult(**result) for result in results]          return recognizer_results      @staticmethod     def _supported_entities_from_response(response: requests.Response) -&gt; List[str]:         \"\"\"Translate the service's supported entities list to Presidio's.\"\"\"         return json.loads(response.text) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n\n    # Illustrative example only: Run Presidio analyzer\n    # as if it was an external PII detection mechanism.\n    rec = ExampleRemoteRecognizer(\n        pii_identification_url=\"http://localhost:5002/analyze\",\n        supported_entities_url=\"http://localhost:5002/supportedentities\",\n    )\n\n    remote_results = rec.analyze(\n        text=\"My name is Morris\", entities=[\"PERSON\"], nlp_artifacts=None\n    )\n    print(remote_results)\n</pre> if __name__ == \"__main__\":      # Illustrative example only: Run Presidio analyzer     # as if it was an external PII detection mechanism.     rec = ExampleRemoteRecognizer(         pii_identification_url=\"http://localhost:5002/analyze\",         supported_entities_url=\"http://localhost:5002/supportedentities\",     )      remote_results = rec.analyze(         text=\"My name is Morris\", entities=[\"PERSON\"], nlp_artifacts=None     )     print(remote_results)"},{"location":"samples/python/example_structured/","title":"Presidio Structured Basic Usage Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>from presidio_structured import StructuredEngine, JsonAnalysisBuilder, PandasAnalysisBuilder, StructuredAnalysis, CsvReader, JsonReader, JsonDataProcessor, PandasDataProcessor\n</pre> from presidio_structured import StructuredEngine, JsonAnalysisBuilder, PandasAnalysisBuilder, StructuredAnalysis, CsvReader, JsonReader, JsonDataProcessor, PandasDataProcessor <p>This sample showcases presidio-structured on structured and semi-structured data containing sensitive data like names, emails, and addresses. It differs from the sample for the batch analyzer/anonymizer engines example, which includes narrative phrases that might contain sensitive data. The presence of personal data embedded in these phrases requires to analyze and to anonymize the text inside the cells, which is not the case for our structured sample, where the sensitive data is already separated into columns.</p> In\u00a0[13]: Copied! <pre>sample_df = CsvReader().read(\"./csv_sample_data/test_structured.csv\")\nsample_df\n</pre> sample_df = CsvReader().read(\"./csv_sample_data/test_structured.csv\") sample_df Out[13]: id name email street city state non_pii 0 1 John Doe john.doe@example.com 123 Main St Anytown CA reallynotpii 1 2 Jane Smith jane.smith@example.com 456 Elm St Somewhere TX reallynotapii 2 3 Alice Johnson alice.johnson@example.com 789 Pine St Elsewhere NY reallynotapiiatall In\u00a0[14]: Copied! <pre>sample_json = JsonReader().read(\"./sample_data/test_structured.json\")\nsample_json\n</pre> sample_json = JsonReader().read(\"./sample_data/test_structured.json\") sample_json Out[14]: <pre>{'id': 1,\n 'name': 'John Doe',\n 'email': 'john.doe@example.com',\n 'address': {'street': '123 Main St',\n  'city': 'Anytown',\n  'state': 'CA',\n  'non_pii': 'reallynotapiiatall'}}</pre> In\u00a0[15]: Copied! <pre># contains nested objects in lists\nsample_complex_json = JsonReader().read(\"./sample_data/test_structured_complex.json\")\nsample_complex_json\n</pre> # contains nested objects in lists sample_complex_json = JsonReader().read(\"./sample_data/test_structured_complex.json\") sample_complex_json Out[15]: <pre>{'users': [{'id': 1,\n   'name': 'John Doe',\n   'email': 'john.doe@example.com',\n   'address': {'street': '123 Main St',\n    'city': 'Anytown',\n    'state': 'CA',\n    'non_pii': 'reallynotpii'}},\n  {'id': 2,\n   'name': 'Jane Smith',\n   'email': 'jane.smith@example.com',\n   'address': {'street': '456 Elm St',\n    'city': 'Somewhere',\n    'state': 'TX',\n    'non_pii': 'reallynotapii'}},\n  {'id': 3,\n   'name': 'Alice Johnson',\n   'email': 'alice.johnson@example.com',\n   'address': {'street': '789 Pine St',\n    'city': 'Elsewhere',\n    'state': 'NY',\n    'non_pii': 'reallynotapiiatall'}}]}</pre> In\u00a0[16]: Copied! <pre># Automatically detect the entity for the columns\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df)\ntabular_analysis\n</pre> # Automatically detect the entity for the columns tabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df) tabular_analysis Out[16]: <pre>StructuredAnalysis(entity_mapping={'name': 'PERSON', 'email': 'URL', 'city': 'LOCATION', 'state': 'LOCATION'})</pre> In\u00a0[17]: Copied! <pre># anonymized data defaults to be replaced with None, unless operators is specified\n\npandas_engine = StructuredEngine(data_processor=PandasDataProcessor())\ndf_to_be_anonymized = sample_df.copy() # in-place anonymization\nanonymized_df = pandas_engine.anonymize(df_to_be_anonymized, tabular_analysis, operators=None) # explicit None for clarity\nanonymized_df\n</pre> # anonymized data defaults to be replaced with None, unless operators is specified  pandas_engine = StructuredEngine(data_processor=PandasDataProcessor()) df_to_be_anonymized = sample_df.copy() # in-place anonymization anonymized_df = pandas_engine.anonymize(df_to_be_anonymized, tabular_analysis, operators=None) # explicit None for clarity anonymized_df Out[17]: id name email street city state non_pii 0 1 &lt;None&gt; &lt;None&gt; 123 Main St &lt;None&gt; &lt;None&gt; reallynotpii 1 2 &lt;None&gt; &lt;None&gt; 456 Elm St &lt;None&gt; &lt;None&gt; reallynotapii 2 3 &lt;None&gt; &lt;None&gt; 789 Pine St &lt;None&gt; &lt;None&gt; reallynotapiiatall In\u00a0[18]: Copied! <pre>from presidio_anonymizer.entities.engine import OperatorConfig\nfrom faker import Faker\nfake = Faker()\n\noperators = {\n    \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"person...\"}),\n    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.safe_email()})\n    # etc...\n    }\nanonymized_df = pandas_engine.anonymize(sample_df, tabular_analysis, operators=operators)\nanonymized_df\n</pre> from presidio_anonymizer.entities.engine import OperatorConfig from faker import Faker fake = Faker()  operators = {     \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"person...\"}),     \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.safe_email()})     # etc...     } anonymized_df = pandas_engine.anonymize(sample_df, tabular_analysis, operators=operators) anonymized_df Out[18]: id name email street city state non_pii 0 1 person... &lt;None&gt; 123 Main St &lt;None&gt; &lt;None&gt; reallynotpii 1 2 person... &lt;None&gt; 456 Elm St &lt;None&gt; &lt;None&gt; reallynotapii 2 3 person... &lt;None&gt; 789 Pine St &lt;None&gt; &lt;None&gt; reallynotapiiatall In\u00a0[19]: Copied! <pre>json_analysis = JsonAnalysisBuilder().generate_analysis(sample_json)\njson_analysis\n</pre> json_analysis = JsonAnalysisBuilder().generate_analysis(sample_json) json_analysis Out[19]: <pre>StructuredAnalysis(entity_mapping={'name': 'PERSON', 'email': 'EMAIL_ADDRESS', 'address.city': 'LOCATION'})</pre> In\u00a0[20]: Copied! <pre># Currently does not support nested objects in lists\ntry:\n    json_complex_analysis = JsonAnalysisBuilder().generate_analysis(sample_complex_json)\nexcept ValueError as e:\n    print(e)\n\n# however, we can define it manually:\njson_complex_analysis = StructuredAnalysis(entity_mapping={\n    \"users.name\":\"PERSON\",\n    \"users.address.street\":\"LOCATION\",\n    \"users.address.city\":\"LOCATION\",\n    \"users.address.state\":\"LOCATION\",\n    \"users.email\": \"EMAIL_ADDRESS\",\n})\n</pre> # Currently does not support nested objects in lists try:     json_complex_analysis = JsonAnalysisBuilder().generate_analysis(sample_complex_json) except ValueError as e:     print(e)  # however, we can define it manually: json_complex_analysis = StructuredAnalysis(entity_mapping={     \"users.name\":\"PERSON\",     \"users.address.street\":\"LOCATION\",     \"users.address.city\":\"LOCATION\",     \"users.address.state\":\"LOCATION\",     \"users.email\": \"EMAIL_ADDRESS\", }) <pre>Analyzer.analyze_iterator only works on primitive types (int, float, bool, str). Lists of objects are not yet supported.\n</pre> In\u00a0[21]: Copied! <pre># anonymizing simple data\njson_engine = StructuredEngine(data_processor=JsonDataProcessor())\nanonymized_json = json_engine.anonymize(sample_json, json_analysis, operators=operators)\nanonymized_json\n</pre> # anonymizing simple data json_engine = StructuredEngine(data_processor=JsonDataProcessor()) anonymized_json = json_engine.anonymize(sample_json, json_analysis, operators=operators) anonymized_json Out[21]: <pre>{'id': 1,\n 'name': 'person...',\n 'email': 'duarteangela@example.org',\n 'address': {'street': '123 Main St',\n  'city': '&lt;None&gt;',\n  'state': 'CA',\n  'non_pii': 'reallynotapiiatall'}}</pre> In\u00a0[22]: Copied! <pre>anonymized_complex_json = json_engine.anonymize(sample_complex_json, json_complex_analysis, operators=operators)\nanonymized_complex_json\n</pre> anonymized_complex_json = json_engine.anonymize(sample_complex_json, json_complex_analysis, operators=operators) anonymized_complex_json Out[22]: <pre>{'users': [{'id': 1,\n   'name': 'person...',\n   'email': 'bmcfarland@example.org',\n   'address': {'street': '&lt;None&gt;',\n    'city': '&lt;None&gt;',\n    'state': '&lt;None&gt;',\n    'non_pii': 'reallynotpii'}},\n  {'id': 2,\n   'name': 'person...',\n   'email': 'bmcfarland@example.org',\n   'address': {'street': '&lt;None&gt;',\n    'city': '&lt;None&gt;',\n    'state': '&lt;None&gt;',\n    'non_pii': 'reallynotapii'}},\n  {'id': 3,\n   'name': 'person...',\n   'email': 'bmcfarland@example.org',\n   'address': {'street': '&lt;None&gt;',\n    'city': '&lt;None&gt;',\n    'state': '&lt;None&gt;',\n    'non_pii': 'reallynotapiiatall'}}]}</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"samples/python/example_structured/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonexample_structuredipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb\u00b6","text":""},{"location":"samples/python/example_structured/#loading-in-data","title":"Loading in data\u00b6","text":""},{"location":"samples/python/example_structured/#tabular-csv-data-defining-generating-tabular-analysis-anonymization","title":"Tabular (csv) data: defining &amp; generating tabular analysis, anonymization.\u00b6","text":""},{"location":"samples/python/example_structured/#we-can-also-define-operators-using-operatorconfig-similar-as-to-the-anonymizerengine","title":"We can also define operators using OperatorConfig similar as to the AnonymizerEngine:\u00b6","text":""},{"location":"samples/python/example_structured/#semi-structured-json-data-simple-and-complex-analysis-anonymization","title":"Semi-structured (JSON) data: simple and complex analysis, anonymization\u00b6","text":""},{"location":"samples/python/flair_recognizer/","title":"Flair recognizer","text":"In\u00a0[\u00a0]: Copied! <pre>import logging\nfrom typing import Optional, List, Tuple, Set\n</pre> import logging from typing import Optional, List, Tuple, Set In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import (\n    RecognizerResult,\n    EntityRecognizer,\n    AnalysisExplanation,\n)\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n</pre> from presidio_analyzer import (     RecognizerResult,     EntityRecognizer,     AnalysisExplanation, ) from presidio_analyzer.nlp_engine import NlpArtifacts In\u00a0[\u00a0]: Copied! <pre>try:\n    from flair.data import Sentence\n    from flair.models import SequenceTagger\nexcept ImportError:\n    print(\"Flair is not installed\")\n</pre> try:     from flair.data import Sentence     from flair.models import SequenceTagger except ImportError:     print(\"Flair is not installed\") In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(\"presidio-analyzer\")\n</pre> logger = logging.getLogger(\"presidio-analyzer\") In\u00a0[\u00a0]: Copied! <pre>class FlairRecognizer(EntityRecognizer):\n    \"\"\"\n    Wrapper for a flair model, if needed to be used within Presidio Analyzer.\n\n    :example:\n    &gt;from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\n    &gt;flair_recognizer = FlairRecognizer()\n\n    &gt;registry = RecognizerRegistry()\n    &gt;registry.add_recognizer(flair_recognizer)\n\n    &gt;analyzer = AnalyzerEngine(registry=registry)\n\n    &gt;results = analyzer.analyze(\n    &gt;    \"My name is Christopher and I live in Irbid.\",\n    &gt;    language=\"en\",\n    &gt;    return_decision_process=True,\n    &gt;)\n    &gt;for result in results:\n    &gt;    print(result)\n    &gt;    print(result.analysis_explanation)\n\n\n    \"\"\"\n\n    ENTITIES = [\n        \"LOCATION\",\n        \"PERSON\",\n        \"ORGANIZATION\",\n        # \"MISCELLANEOUS\"   # - There are no direct correlation with Presidio entities.\n    ]\n\n    DEFAULT_EXPLANATION = \"Identified as {} by Flair's Named Entity Recognition\"\n\n    CHECK_LABEL_GROUPS = [\n        ({\"LOCATION\"}, {\"LOC\", \"LOCATION\"}),\n        ({\"PERSON\"}, {\"PER\", \"PERSON\"}),\n        ({\"ORGANIZATION\"}, {\"ORG\"}),\n        # ({\"MISCELLANEOUS\"}, {\"MISC\"}), # Probably not PII\n    ]\n\n    MODEL_LANGUAGES = {\n        \"en\": \"flair/ner-english-large\",\n        \"es\": \"flair/ner-spanish-large\",\n        \"de\": \"flair/ner-german-large\",\n        \"nl\": \"flair/ner-dutch-large\",\n    }\n\n    PRESIDIO_EQUIVALENCES = {\n        \"PER\": \"PERSON\",\n        \"LOC\": \"LOCATION\",\n        \"ORG\": \"ORGANIZATION\",\n        # 'MISC': 'MISCELLANEOUS'   # - Probably not PII\n    }\n\n    def __init__(\n        self,\n        supported_language: str = \"en\",\n        supported_entities: Optional[List[str]] = None,\n        check_label_groups: Optional[Tuple[Set, Set]] = None,\n        model: SequenceTagger = None,\n    ):\n        self.check_label_groups = (\n            check_label_groups if check_label_groups else self.CHECK_LABEL_GROUPS\n        )\n\n        supported_entities = supported_entities if supported_entities else self.ENTITIES\n        self.model = (\n            model\n            if model\n            else SequenceTagger.load(self.MODEL_LANGUAGES.get(supported_language))\n        )\n\n        super().__init__(\n            supported_entities=supported_entities,\n            supported_language=supported_language,\n            name=\"Flair Analytics\",\n        )\n\n    def load(self) -&gt; None:\n        \"\"\"Load the model, not used. Model is loaded during initialization.\"\"\"\n        pass\n\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"\n        Return supported entities by this model.\n\n        :return: List of the supported entities.\n        \"\"\"\n        return self.supported_entities\n\n    # Class to use Flair with Presidio as an external recognizer.\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyze text using Text Analytics.\n\n        :param text: The text for analysis.\n        :param entities: Not working properly for this recognizer.\n        :param nlp_artifacts: Not used by this recognizer.\n        :param language: Text language. Supported languages in MODEL_LANGUAGES\n        :return: The list of Presidio RecognizerResult constructed from the recognized\n            Flair detections.\n        \"\"\"\n\n        results = []\n\n        sentences = Sentence(text)\n        self.model.predict(sentences)\n\n        # If there are no specific list of entities, we will look for all of it.\n        if not entities:\n            entities = self.supported_entities\n\n        for entity in entities:\n            if entity not in self.supported_entities:\n                continue\n\n            for ent in sentences.get_spans(\"ner\"):\n                if not self.__check_label(\n                    entity, ent.labels[0].value, self.check_label_groups\n                ):\n                    continue\n                textual_explanation = self.DEFAULT_EXPLANATION.format(\n                    ent.labels[0].value\n                )\n                explanation = self.build_flair_explanation(\n                    round(ent.score, 2), textual_explanation\n                )\n                flair_result = self._convert_to_recognizer_result(ent, explanation)\n\n                results.append(flair_result)\n\n        return results\n\n    def _convert_to_recognizer_result(self, entity, explanation) -&gt; RecognizerResult:\n\n        entity_type = self.PRESIDIO_EQUIVALENCES.get(entity.tag, entity.tag)\n        flair_score = round(entity.score, 2)\n\n        flair_results = RecognizerResult(\n            entity_type=entity_type,\n            start=entity.start_position,\n            end=entity.end_position,\n            score=flair_score,\n            analysis_explanation=explanation,\n        )\n\n        return flair_results\n\n    def build_flair_explanation(\n        self, original_score: float, explanation: str\n    ) -&gt; AnalysisExplanation:\n        \"\"\"\n        Create explanation for why this result was detected.\n\n        :param original_score: Score given by this recognizer\n        :param explanation: Explanation string\n        :return:\n        \"\"\"\n        explanation = AnalysisExplanation(\n            recognizer=self.__class__.__name__,\n            original_score=original_score,\n            textual_explanation=explanation,\n        )\n        return explanation\n\n    @staticmethod\n    def __check_label(\n        entity: str, label: str, check_label_groups: Tuple[Set, Set]\n    ) -&gt; bool:\n        return any(\n            [entity in egrp and label in lgrp for egrp, lgrp in check_label_groups]\n        )\n</pre> class FlairRecognizer(EntityRecognizer):     \"\"\"     Wrapper for a flair model, if needed to be used within Presidio Analyzer.      :example:     &gt;from presidio_analyzer import AnalyzerEngine, RecognizerRegistry      &gt;flair_recognizer = FlairRecognizer()      &gt;registry = RecognizerRegistry()     &gt;registry.add_recognizer(flair_recognizer)      &gt;analyzer = AnalyzerEngine(registry=registry)      &gt;results = analyzer.analyze(     &gt;    \"My name is Christopher and I live in Irbid.\",     &gt;    language=\"en\",     &gt;    return_decision_process=True,     &gt;)     &gt;for result in results:     &gt;    print(result)     &gt;    print(result.analysis_explanation)       \"\"\"      ENTITIES = [         \"LOCATION\",         \"PERSON\",         \"ORGANIZATION\",         # \"MISCELLANEOUS\"   # - There are no direct correlation with Presidio entities.     ]      DEFAULT_EXPLANATION = \"Identified as {} by Flair's Named Entity Recognition\"      CHECK_LABEL_GROUPS = [         ({\"LOCATION\"}, {\"LOC\", \"LOCATION\"}),         ({\"PERSON\"}, {\"PER\", \"PERSON\"}),         ({\"ORGANIZATION\"}, {\"ORG\"}),         # ({\"MISCELLANEOUS\"}, {\"MISC\"}), # Probably not PII     ]      MODEL_LANGUAGES = {         \"en\": \"flair/ner-english-large\",         \"es\": \"flair/ner-spanish-large\",         \"de\": \"flair/ner-german-large\",         \"nl\": \"flair/ner-dutch-large\",     }      PRESIDIO_EQUIVALENCES = {         \"PER\": \"PERSON\",         \"LOC\": \"LOCATION\",         \"ORG\": \"ORGANIZATION\",         # 'MISC': 'MISCELLANEOUS'   # - Probably not PII     }      def __init__(         self,         supported_language: str = \"en\",         supported_entities: Optional[List[str]] = None,         check_label_groups: Optional[Tuple[Set, Set]] = None,         model: SequenceTagger = None,     ):         self.check_label_groups = (             check_label_groups if check_label_groups else self.CHECK_LABEL_GROUPS         )          supported_entities = supported_entities if supported_entities else self.ENTITIES         self.model = (             model             if model             else SequenceTagger.load(self.MODEL_LANGUAGES.get(supported_language))         )          super().__init__(             supported_entities=supported_entities,             supported_language=supported_language,             name=\"Flair Analytics\",         )      def load(self) -&gt; None:         \"\"\"Load the model, not used. Model is loaded during initialization.\"\"\"         pass      def get_supported_entities(self) -&gt; List[str]:         \"\"\"         Return supported entities by this model.          :return: List of the supported entities.         \"\"\"         return self.supported_entities      # Class to use Flair with Presidio as an external recognizer.     def analyze(         self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None     ) -&gt; List[RecognizerResult]:         \"\"\"         Analyze text using Text Analytics.          :param text: The text for analysis.         :param entities: Not working properly for this recognizer.         :param nlp_artifacts: Not used by this recognizer.         :param language: Text language. Supported languages in MODEL_LANGUAGES         :return: The list of Presidio RecognizerResult constructed from the recognized             Flair detections.         \"\"\"          results = []          sentences = Sentence(text)         self.model.predict(sentences)          # If there are no specific list of entities, we will look for all of it.         if not entities:             entities = self.supported_entities          for entity in entities:             if entity not in self.supported_entities:                 continue              for ent in sentences.get_spans(\"ner\"):                 if not self.__check_label(                     entity, ent.labels[0].value, self.check_label_groups                 ):                     continue                 textual_explanation = self.DEFAULT_EXPLANATION.format(                     ent.labels[0].value                 )                 explanation = self.build_flair_explanation(                     round(ent.score, 2), textual_explanation                 )                 flair_result = self._convert_to_recognizer_result(ent, explanation)                  results.append(flair_result)          return results      def _convert_to_recognizer_result(self, entity, explanation) -&gt; RecognizerResult:          entity_type = self.PRESIDIO_EQUIVALENCES.get(entity.tag, entity.tag)         flair_score = round(entity.score, 2)          flair_results = RecognizerResult(             entity_type=entity_type,             start=entity.start_position,             end=entity.end_position,             score=flair_score,             analysis_explanation=explanation,         )          return flair_results      def build_flair_explanation(         self, original_score: float, explanation: str     ) -&gt; AnalysisExplanation:         \"\"\"         Create explanation for why this result was detected.          :param original_score: Score given by this recognizer         :param explanation: Explanation string         :return:         \"\"\"         explanation = AnalysisExplanation(             recognizer=self.__class__.__name__,             original_score=original_score,             textual_explanation=explanation,         )         return explanation      @staticmethod     def __check_label(         entity: str, label: str, check_label_groups: Tuple[Set, Set]     ) -&gt; bool:         return any(             [entity in egrp and label in lgrp for egrp, lgrp in check_label_groups]         ) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n\n    from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\n    flair_recognizer = (\n        FlairRecognizer()\n    )  # This would download a very large (+2GB) model on the first run\n\n    registry = RecognizerRegistry()\n    registry.add_recognizer(flair_recognizer)\n\n    analyzer = AnalyzerEngine(registry=registry)\n\n    results = analyzer.analyze(\n        \"My name is Christopher and I live in Irbid.\",\n        language=\"en\",\n        return_decision_process=True,\n    )\n    for result in results:\n        print(result)\n        print(result.analysis_explanation)\n</pre> if __name__ == \"__main__\":      from presidio_analyzer import AnalyzerEngine, RecognizerRegistry      flair_recognizer = (         FlairRecognizer()     )  # This would download a very large (+2GB) model on the first run      registry = RecognizerRegistry()     registry.add_recognizer(flair_recognizer)      analyzer = AnalyzerEngine(registry=registry)      results = analyzer.analyze(         \"My name is Christopher and I live in Irbid.\",         language=\"en\",         return_decision_process=True,     )     for result in results:         print(result)         print(result.analysis_explanation)"},{"location":"samples/python/getting_entity_values/","title":"Getting the identified entity value using a custom Operator","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</pre> # download presidio !pip install presidio_analyzer presidio_anonymizer !python -m spacy download en_core_web_lg In\u00a0[1]: Copied! <pre>from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\n</pre> from presidio_analyzer import AnalyzerEngine from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import OperatorConfig In\u00a0[2]: Copied! <pre>analyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n</pre> analyzer = AnalyzerEngine() anonymizer = AnonymizerEngine() In\u00a0[3]: Copied! <pre>text_to_analyze = \"Hi my name is Charles Darwin and my email is cdarwin@hmsbeagle.org\"\nanalyzer_results = analyzer.analyze(text_to_analyze, language=\"en\")\n</pre> text_to_analyze = \"Hi my name is Charles Darwin and my email is cdarwin@hmsbeagle.org\" analyzer_results = analyzer.analyze(text_to_analyze, language=\"en\")  <p>A naive approach for getting the text values:</p> In\u00a0[4]: Copied! <pre>[(text_to_analyze[res.start:res.end], res.start, res.end) for res in analyzer_results]\n</pre> [(text_to_analyze[res.start:res.end], res.start, res.end) for res in analyzer_results] Out[4]: <pre>[('cdarwin@hmsbeagle.org', 45, 66),\n ('Charles Darwin', 14, 28),\n ('hmsbeagle.org', 53, 66)]</pre> <p>Another option is to set up a custom operator* which runs an identity function (<code>lambda x: x</code>). This operator doesn't really anonymize, but replaces the identified value with itself. This is useful as the Anonymizer handles the overlaps automatically.</p> <p>In this example, the URL (hmsbeagle.org) is contained in the email address, so it's ommitted from the final result.</p> <p>* an <code>Operator</code> is usually either an <code>Anonymizer</code> or <code>Deanonymizer</code> on the presidio-anonymizer library/</p> In\u00a0[7]: Copied! <pre>anonymized_results = anonymizer.anonymize(\n        text=text_to_analyze,\n        analyzer_results=analyzer_results,            \n        operators={\"DEFAULT\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x})}        \n    )\n</pre> anonymized_results = anonymizer.anonymize(         text=text_to_analyze,         analyzer_results=analyzer_results,                     operators={\"DEFAULT\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x})}             ) <p>The operator defined here is <code>DEFAULT</code>, meaning it will be used for all entities. The <code>OperatorConfig</code> is a custom one and the labmda is the identity function.</p> <p>Output text, start and end locations for each detected entity</p> In\u00a0[8]: Copied! <pre>[(item.text, item.start, item.end) for item in anonymized_results.items]\n</pre> [(item.text, item.start, item.end) for item in anonymized_results.items] Out[8]: <pre>[('cdarwin@hmsbeagle.org', 45, 66), ('Charles Darwin', 14, 28)]</pre> <p>A third option would be to use the <code>keep</code> operator:</p> In\u00a0[9]: Copied! <pre>anonymized_results_with_keep = anonymizer.anonymize(\n        text=text_to_analyze,\n        analyzer_results=analyzer_results,            \n        operators={\"DEFAULT\": OperatorConfig(\"keep\")}        \n    )\n[(item.text, item.start, item.end) for item in anonymized_results_with_keep.items]\n</pre> anonymized_results_with_keep = anonymizer.anonymize(         text=text_to_analyze,         analyzer_results=analyzer_results,                     operators={\"DEFAULT\": OperatorConfig(\"keep\")}             ) [(item.text, item.start, item.end) for item in anonymized_results_with_keep.items] Out[9]: <pre>[('cdarwin@hmsbeagle.org', 45, 66), ('Charles Darwin', 14, 28)]</pre>"},{"location":"samples/python/getting_entity_values/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythongetting_entity_valuesipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/getting_entity_values.ipynb\u00b6","text":""},{"location":"samples/python/getting_entity_values/#getting-a-list-of-all-identified-texts","title":"Getting a list of all identified texts\u00b6","text":"<p>This sample illustrates how to get a list of all the identified PII entities using Presidio Analyzer for detection and a custom Presidio Anonymizer operator.</p>"},{"location":"samples/python/gliner/","title":"Using GLiNER within Presidio","text":""},{"location":"samples/python/gliner/#what-is-gliner","title":"What is GLiNER","text":"<p>GLiNER is a Named Entity Recognition (NER) model capable of identifying any entity type using a bidirectional transformer encoder (BERT-like). It provides a practical alternative to traditional NER models, which are limited to predefined entities, and Large Language Models (LLMs) that, despite their flexibility, are costly and large for resource-constrained scenarios.</p> <p>Paper: GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer</p> <p>Since GLiNER takes as input both the sentence/text and entity types, it can be used for zero-shot named entity recognition. This means that it can recognize entities that were not seen during training.</p>"},{"location":"samples/python/gliner/#pii-detection-with-gliner","title":"PII Detection with GLiNER","text":"<p>GLiNER has a trained PII detection model: \ud83d\udd0d <code>urchade/gliner_multi_pii-v1</code> (Apache 2.0)</p> <p>This model is capable of recognizing various types of personally identifiable information (PII), including but not limited to these entity types: <code>person</code>, <code>organization</code>, <code>phone number</code>, <code>address</code>, <code>passport number</code>, <code>email</code>, <code>credit card number</code>, <code>social security number</code>, <code>health insurance id number</code>, <code>date of birth</code>, <code>mobile phone number</code>, <code>bank account number</code>, <code>medication</code>, <code>cpf</code>, <code>driver's license number</code>, <code>tax identification number</code>, <code>medical condition</code>, <code>identity card number</code>, <code>national id number</code>, <code>ip address</code>, <code>email address</code>, <code>iban</code>, <code>credit card expiration date</code>, <code>username</code>, <code>health insurance number</code>, <code>registration number</code>, <code>student id number</code>, <code>insurance number</code>, <code>flight number</code>, <code>landline phone number</code>, <code>blood type</code>, <code>cvv</code>, <code>reservation number</code>, <code>digital signature</code>, <code>social media handle</code>, <code>license plate number</code>, <code>cnpj</code>, <code>postal code</code>, <code>passport_number</code>, <code>serial number</code>, <code>vehicle registration number</code>, <code>credit card brand</code>, <code>fax number</code>, <code>visa number</code>, <code>insurance company</code>, <code>identity document number</code>, <code>transaction number</code>, <code>national health insurance number</code>, <code>cvc</code>, <code>birth certificate number</code>, <code>train ticket number</code>, <code>passport expiration date</code>, and <code>social_security_number</code>.</p>"},{"location":"samples/python/gliner/#using-gliner-with-presidio","title":"Using GLiNER with Presidio","text":"<p>Presidio has a built-in <code>EntityRecognizer</code> for GLiNER: <code>GLiNERRecognizer</code>. This recognizer can be used to detect PII entities in text using the GLiNER model.</p>"},{"location":"samples/python/gliner/#installation","title":"Installation","text":"<p>To use GLiNER with Presidio, you need to install the <code>presidio-analyzer</code> with the <code>gliner</code> extra:</p> <pre><code>pip install 'presidio-analyzer[gliner]'\n</code></pre> <p>Note</p> <p>GLiNER only supports python 3.10 and above, while Presidio supports version 3.9 and above.</p>"},{"location":"samples/python/gliner/#example","title":"Example","text":"<pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nfrom presidio_analyzer.predefined_recognizers import GLiNERRecognizer\n\n\n# Load a small spaCy model as we don't need spaCy's NER\nnlp_engine = NlpEngineProvider(\n    nlp_configuration={\n        \"nlp_engine_name\": \"spacy\",\n        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n    }\n)\n\n# Create an analyzer engine \nanalyzer_engine = AnalyzerEngine()\n\n# Define and create the GLiNER recognizer\nentity_mapping = {\n    \"person\": \"PERSON\",\n    \"name\": \"PERSON\",\n    \"organization\": \"ORGANIZATION\",\n    \"location\": \"LOCATION\"\n}\n\ngliner_recognizer = GLiNERRecognizer(\n    model_name=\"urchade/gliner_multi_pii-v1\",\n    entity_mapping=entity_mapping,\n    flat_ner=False,\n    multi_label=True,\n    map_location=\"cpu\",\n)\n\n# Add the GLiNER recognizer to the registry\nanalyzer_engine.registry.add_recognizer(gliner_recognizer)\n\n# Remove the spaCy recognizer to avoid NER coming from spaCy\nanalyzer_engine.registry.remove_recognizer(\"SpacyRecognizer\")\n\n# Analyze text\nresults = analyzer_engine.analyze(\n    text=\"Hello, my name is Rafi Mor, I'm from Binyamina and I work at Microsoft. \", language=\"en\"\n)\n\nprint(results)\n</code></pre>"},{"location":"samples/python/image_redaction_allow_list_approach/","title":"Using an allow list with image redaction","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install presidio_analyzer presidio_anonymizer presidio_image_redactor\n!python -m spacy download en_core_web_lg\n</pre> !pip install presidio_analyzer presidio_anonymizer presidio_image_redactor !python -m spacy download en_core_web_lg In\u00a0[1]: Copied! <pre>from PIL import Image\nimport pydicom\nfrom presidio_analyzer import Pattern, PatternRecognizer\nfrom presidio_image_redactor import ImageRedactorEngine, DicomImageRedactorEngine\nimport matplotlib.pyplot as plt\n</pre> from PIL import Image import pydicom from presidio_analyzer import Pattern, PatternRecognizer from presidio_image_redactor import ImageRedactorEngine, DicomImageRedactorEngine import matplotlib.pyplot as plt <p>Initialize engines used for image redaction</p> In\u00a0[2]: Copied! <pre># Standard images\nengine = ImageRedactorEngine()\n\n# DICOM images\ndicom_engine = DicomImageRedactorEngine()\npadding_width = 3\nfill = \"background\"\n</pre> # Standard images engine = ImageRedactorEngine()  # DICOM images dicom_engine = DicomImageRedactorEngine() padding_width = 3 fill = \"background\" In\u00a0[3]: Copied! <pre>image = Image.open(\"../../image-redactor/ocr_text.png\")\ndisplay(image)\n</pre> image = Image.open(\"../../image-redactor/ocr_text.png\") display(image) <p>And this is what the image looks like with the standard, default behavior redaction.</p> In\u00a0[4]: Copied! <pre>redacted_image = engine.redact(image, (255, 192, 203))\ndisplay(redacted_image)\n</pre> redacted_image = engine.redact(image, (255, 192, 203)) display(redacted_image) In\u00a0[5]: Copied! <pre>instance = pydicom.dcmread(\"./sample_data/0_ORIGINAL.dcm\")\nplt.imshow(instance.pixel_array, cmap=\"gray\")\n</pre> instance = pydicom.dcmread(\"./sample_data/0_ORIGINAL.dcm\") plt.imshow(instance.pixel_array, cmap=\"gray\") Out[5]: <pre>&lt;matplotlib.image.AxesImage at 0x1edb761bd00&gt;</pre> <p>And this is what the image looks like with the standard, default behavior redaction.</p> In\u00a0[6]: Copied! <pre>results = dicom_engine.redact(instance, padding_width=padding_width, fill=fill)\nplt.imshow(results.pixel_array, cmap=\"gray\")\n</pre> results = dicom_engine.redact(instance, padding_width=padding_width, fill=fill) plt.imshow(results.pixel_array, cmap=\"gray\") Out[6]: <pre>&lt;matplotlib.image.AxesImage at 0x1edb7da4610&gt;</pre> <p>Redacted image when using the allow list approach</p> In\u00a0[7]: Copied! <pre>redacted_image = engine.redact(image, (255, 192, 203), allow_list=[\"David\", \"(212) 555-1234\"])\ndisplay(redacted_image)\n</pre> redacted_image = engine.redact(image, (255, 192, 203), allow_list=[\"David\", \"(212) 555-1234\"]) display(redacted_image) <p>Redacted DICOM image when using the allow list approach</p> In\u00a0[8]: Copied! <pre>results = dicom_engine.redact(instance, padding_width=padding_width, fill=fill, allow_list=[\"DAVIDSON\"])\nplt.imshow(results.pixel_array, cmap=\"gray\")\n</pre> results = dicom_engine.redact(instance, padding_width=padding_width, fill=fill, allow_list=[\"DAVIDSON\"]) plt.imshow(results.pixel_array, cmap=\"gray\") Out[8]: <pre>&lt;matplotlib.image.AxesImage at 0x1edaedf6340&gt;</pre> <p>Create a custom recognizer to mark all text as sensitive</p> In\u00a0[9]: Copied! <pre>pattern_all_text = Pattern(name=\"any_text\", regex=r\"(?s).*\", score=0.5)\ncustom_recognizer = PatternRecognizer(\n    supported_entity=\"TEXT\",\n    patterns=[pattern_all_text],\n)\n</pre> pattern_all_text = Pattern(name=\"any_text\", regex=r\"(?s).*\", score=0.5) custom_recognizer = PatternRecognizer(     supported_entity=\"TEXT\",     patterns=[pattern_all_text], ) <p>Then pass that custom recognizer into your redactor engine as an ad-hoc recognizer</p> In\u00a0[10]: Copied! <pre># Standard image\nredacted_image = engine.redact(\n    image,\n    (255, 192, 203),\n    ad_hoc_recognizers = [custom_recognizer], # you can pass in multiple ad-hoc recognizers\n    allow_list=[\"This\", \"project\",]\n)\ndisplay(redacted_image)\n</pre> # Standard image redacted_image = engine.redact(     image,     (255, 192, 203),     ad_hoc_recognizers = [custom_recognizer], # you can pass in multiple ad-hoc recognizers     allow_list=[\"This\", \"project\",] ) display(redacted_image) In\u00a0[11]: Copied! <pre># DICOM image\nresults = dicom_engine.redact(\n    instance,\n    padding_width = padding_width,\n    fill = fill,\n    ad_hoc_recognizers = [custom_recognizer], # you can pass in multiple ad-hoc recognizers\n    allow_list = [\"DAVIDSON\", \"L\"]\n)\nplt.imshow(results.pixel_array, cmap=\"gray\")\n</pre> # DICOM image results = dicom_engine.redact(     instance,     padding_width = padding_width,     fill = fill,     ad_hoc_recognizers = [custom_recognizer], # you can pass in multiple ad-hoc recognizers     allow_list = [\"DAVIDSON\", \"L\"] ) plt.imshow(results.pixel_array, cmap=\"gray\") Out[11]: <pre>&lt;matplotlib.image.AxesImage at 0x1edaee5f160&gt;</pre> <p>Create a custom recognizer that marks all text as sensitive</p> In\u00a0[12]: Copied! <pre>pattern_all_text = Pattern(name=\"any_text\", regex=r\"(?s).*\", score=0.5)\ncustom_recognizer = PatternRecognizer(\n    supported_entity=\"TEXT\",\n    patterns=[pattern_all_text],\n)\n</pre> pattern_all_text = Pattern(name=\"any_text\", regex=r\"(?s).*\", score=0.5) custom_recognizer = PatternRecognizer(     supported_entity=\"TEXT\",     patterns=[pattern_all_text], ) <p>Specify an empty allow list such that no text is allowed</p> In\u00a0[13]: Copied! <pre># Standard image\nredacted_image = engine.redact(\n    image,\n    (255, 192, 203),\n    ad_hoc_recognizers = [custom_recognizer],\n    allow_list=[]\n)\ndisplay(redacted_image)\n</pre> # Standard image redacted_image = engine.redact(     image,     (255, 192, 203),     ad_hoc_recognizers = [custom_recognizer],     allow_list=[] ) display(redacted_image) In\u00a0[14]: Copied! <pre># DICOM image\nresults = dicom_engine.redact(\n    instance,\n    padding_width = padding_width,\n    fill = fill,\n    ad_hoc_recognizers = [custom_recognizer],\n    allow_list = []\n)\nplt.imshow(results.pixel_array, cmap=\"gray\")\n</pre> # DICOM image results = dicom_engine.redact(     instance,     padding_width = padding_width,     fill = fill,     ad_hoc_recognizers = [custom_recognizer],     allow_list = [] ) plt.imshow(results.pixel_array, cmap=\"gray\") Out[14]: <pre>&lt;matplotlib.image.AxesImage at 0x1edaeee7580&gt;</pre> <p>In this case, we see that all text picked up by the OCR is redacted. The \"L\" on the right side of the DICOM image and the single \"a\" in the standard image are still visible because they were not detected by the OCR.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonimage_redaction_allow_list_approachipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/image_redaction_allow_list_approach.ipynb\u00b6","text":""},{"location":"samples/python/image_redaction_allow_list_approach/#allow-list-approach-with-image-redaction","title":"Allow list approach with image redaction\u00b6","text":"<p>This notebook covers how to use the <code>allow_list</code> argument to prevent certain words from being redacted from images and explains how you can use this to implement a strict redact all text approach.</p> <p>Note: Always place the <code>allow_list</code> argument last in your redact call as this is considered a text analyzer kwarg.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before getting started, make sure presidio and the latest version of Tesseract OCR are installed. For detailed documentation, see the installation docs.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#0-imports-and-initializations","title":"0. Imports and initializations\u00b6","text":""},{"location":"samples/python/image_redaction_allow_list_approach/#1-example-images","title":"1. Example images\u00b6","text":"<p>In this notebook, we will use the following examples images.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#11-standard-example-image","title":"1.1 Standard example image\u00b6","text":""},{"location":"samples/python/image_redaction_allow_list_approach/#12-dicom-medical-image","title":"1.2 DICOM medical image\u00b6","text":"<p>For more information on DICOM image redaction, please see example_dicom_image_redactor.ipynb and the Image redactor module documentation.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#2-scenario-prevent-some-words-from-being-redacted","title":"2. Scenario: Prevent some words from being redacted\u00b6","text":"<p>Whether using the default recognizer, registering your own custom recognizer, or using ad-hoc recognizers to identify sensitive entities, there may be times where you do not want certain words redacted.</p> <p>In these cases, we can use the <code>allow_list</code> argument passed into the <code>ImageAnalyzerEngine</code> via our redact engine to preserve specified strings.</p> <p>Note: The <code>allow_list</code> argument should be positioned as the last argument in the redact call as it is considered a text analyzer kwarg.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#3-scenario-only-allow-specific-words-while-redacting-all-other-text","title":"3. Scenario : Only allow specific words while redacting all other text\u00b6","text":"<p>In some cases, we want to preserve certain words and redact all other text in the image. We can create an ad-hoc recognizer that considers all text as sensitive and couple that with the allow list.</p> <p>Note: The <code>allow_list</code> argument should be positioned as the last argument in the redact call as it is considered a text analyzer kwarg.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#4-scenario-redact-all-text-on-the-image","title":"4. Scenario: Redact all text on the image\u00b6","text":"<p>When it is critical to minimize False Negatives during the redaction process, we recommend using a \"redact all\" approach to redact all detected text.</p> <p>As with the other scenarios, good OCR performance is critical in ensuring the analyzer can pick up on all text in the image. False Negatives may still occur with images if the OCR fails to pick up on all the text.</p> <p>Note: The <code>allow_list</code> argument should be positioned as the last argument in the redact call as it is considered a text analyzer kwarg.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#conclusion","title":"Conclusion\u00b6","text":"<p>The <code>allow_list</code> argument can be used in both standard and DICOM image redaction to allow specified words to avoid redaction. This can also be used to redact all detected text.</p> <p>While this approach allows for the greatest recall in terms of redacting sensitive text, it is dependent on the performance of the text detection which comes before analysis.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#tips-for-improved-performance","title":"Tips for improved performance\u00b6","text":"<ol> <li>To avoid False Negative redaction, we recommend applying preprocessing techniques or experimenting with parameters to improve OCR performance or use an alternative approach to improve text detection. We are actively working on adding a preprocessing module to allow for easy application of image preprocessing methods.</li> <li>We recommend augmenting your allowlist to consider various casing and punctuation.</li> </ol>"},{"location":"samples/python/integrating_with_external_services/","title":"Integrating with external services","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</pre> # download presidio !pip install presidio_analyzer presidio_anonymizer !python -m spacy download en_core_web_lg <p>####### Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/integrating_with_external_services.ipynb</p> In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import AnalyzerEngine\nfrom text_analytics.example_text_analytics_recognizer import TextAnalyticsEntityCategory, TextAnalyticsRecognizer\n</pre> from presidio_analyzer import AnalyzerEngine from text_analytics.example_text_analytics_recognizer import TextAnalyticsEntityCategory, TextAnalyticsRecognizer <ol> <li>Define which entities to get from Text Analytics</li> </ol> In\u00a0[\u00a0]: Copied! <pre>ta_entities = [\n    TextAnalyticsEntityCategory(name=\"Person\",\n                                entity_type=\"NAME\",\n                                supported_languages=[\"en\"]),\n    TextAnalyticsEntityCategory(name=\"Age\",\n                                entity_type=\"AGE\",\n                                subcategory = \"Age\", \n                                supported_languages=[\"en\"]),\n    TextAnalyticsEntityCategory(name=\"InternationlBankingAccountNumber\",\n                                entity_type=\"IBAN\",\n                                supported_languages=[\"en\"])]\n</pre> ta_entities = [     TextAnalyticsEntityCategory(name=\"Person\",                                 entity_type=\"NAME\",                                 supported_languages=[\"en\"]),     TextAnalyticsEntityCategory(name=\"Age\",                                 entity_type=\"AGE\",                                 subcategory = \"Age\",                                  supported_languages=[\"en\"]),     TextAnalyticsEntityCategory(name=\"InternationlBankingAccountNumber\",                                 entity_type=\"IBAN\",                                 supported_languages=[\"en\"])] <p>For a full list of entities: https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/named-entity-types?tabs=personal</p> <ol> <li>Instantiate the remote recognizer object (In this case <code>TextAnalyticsRecognizer</code>)</li> </ol> In\u00a0[\u00a0]: Copied! <pre>text_analytics_recognizer = TextAnalyticsRecognizer(\n        text_analytics_key=\"&lt;YOUR_TEXT_ANALYTICS_KEY&gt;\",\n        text_analytics_endpoint=\"&lt;YOUR_TEXT_ANALYTICS_ENDPOINT&gt;\",\n        text_analytics_categories = ta_entities)\n</pre> text_analytics_recognizer = TextAnalyticsRecognizer(         text_analytics_key=\"\",         text_analytics_endpoint=\"\",         text_analytics_categories = ta_entities) <ol> <li>Add the new recognizer to the list of recognizers and run the <code>PresidioAnalyzer</code></li> </ol> In\u00a0[\u00a0]: Copied! <pre>analyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(text_analytics_recognizer)\n\nresults = analyzer.analyze(\n    text=\"David is 30 years old. His IBAN: IL150120690000003111111\", language=\"en\"\n)\nprint(results)\n</pre> analyzer = AnalyzerEngine() analyzer.registry.add_recognizer(text_analytics_recognizer)  results = analyzer.analyze(     text=\"David is 30 years old. His IBAN: IL150120690000003111111\", language=\"en\" ) print(results)"},{"location":"samples/python/integrating_with_external_services/#integrating-external-modelsservices-with-presidio","title":"Integrating external models/services with Presidio\u00b6","text":"<p>Presidio analyzer is comprised of a set of PII recognizers which can run local or remotely. In this notebook we'll give an example of integrating an external service into Presidio-Analyzer.</p>"},{"location":"samples/python/integrating_with_external_services/#azure-text-analytics","title":"Azure Text Analytics\u00b6","text":"<p>Azure Text Analytics is a cloud-based service that provides advanced natural language processing over raw text. One of its main functions includes Named Entity Recognition (NER), which has the ability to identify different entities in text and categorize them into pre-defined classes or types.</p>"},{"location":"samples/python/integrating_with_external_services/#supported-entity-categories-in-the-text-analytics-api","title":"Supported entity categories in the Text Analytics API\u00b6","text":"<p>Text Analytics supports multiple PII entity categories. The Text Analytics service runs a predictive model to identify and categorize named entities from an input document. The service's latest version includes the ability to detect personal (PII) and health (PHI) information. A list of all supported entities can be found in the official documentation.</p>"},{"location":"samples/python/integrating_with_external_services/#prerequisites","title":"Prerequisites\u00b6","text":"<p>To use Text Analytics with Preisido, an Azure Text Analytics resource should first be created under an Azure subscription. Follow the official documentation for instructions. The key and endpoint, generated once the resource is created, should replace the placeholders <code>&lt;YOUR_TEXT_ANALYTICS_KEY&gt;</code> and <code>&lt;YOUR_TEXT_ANALYTICS_ENDPOINT&gt;</code> in this notebook, respectively.</p>"},{"location":"samples/python/integrating_with_external_services/#text-analytics-recognizer","title":"Text Analytics Recognizer\u00b6","text":"<p>In this example we will use the <code>TextAnalyticsRecognizer</code> sample implementation. This class extends Presidio's Remote Recognizer for calling the Text Analytics service REST API. For additional information of a remote recognizer, see the ExampleRemoteRecognizer sample.</p>"},{"location":"samples/python/keep_entities/","title":"Keeping some entities from being anonymized","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</pre> # download presidio !pip install presidio_analyzer presidio_anonymizer !python -m spacy download en_core_web_lg In\u00a0[1]: Copied! <pre>from presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n</pre> from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import RecognizerResult, OperatorConfig In\u00a0[2]: Copied! <pre>engine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text,\n# analyzer results (potentially coming from presidio-analyzer)\n# and 'keep' operator on &lt;PERSON&gt; PIIs\nanonymize_result = engine.anonymize(\n    text=\"My name is James Bond, I live in London\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),\n        RecognizerResult(entity_type=\"LOCATION\", start=33, end=39, score=0.8),\n    ],\n    operators={\n        \"PERSON\": OperatorConfig(\"keep\"),\n        \"DEFAULT\": OperatorConfig(\"replace\"),\n    },\n)\n</pre> engine = AnonymizerEngine()  # Invoke the anonymize function with the text, # analyzer results (potentially coming from presidio-analyzer) # and 'keep' operator on  PIIs anonymize_result = engine.anonymize(     text=\"My name is James Bond, I live in London\",     analyzer_results=[         RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),         RecognizerResult(entity_type=\"LOCATION\", start=33, end=39, score=0.8),     ],     operators={         \"PERSON\": OperatorConfig(\"keep\"),         \"DEFAULT\": OperatorConfig(\"replace\"),     }, ) In\u00a0[3]: Copied! <pre>anonymize_result\n</pre> anonymize_result Out[3]: <pre>text: My name is James Bond, I live in &lt;LOCATION&gt;\nitems:\n[\n    {'start': 33, 'end': 43, 'entity_type': 'LOCATION', 'text': '&lt;LOCATION&gt;', 'operator': 'replace'},\n    {'start': 11, 'end': 21, 'entity_type': 'PERSON', 'text': 'James Bond', 'operator': 'keep'}\n]</pre>"},{"location":"samples/python/keep_entities/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonkeep_entitiesipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/keep_entities.ipynb\u00b6","text":""},{"location":"samples/python/keep_entities/#keeping-some-piis-from-being-anonymized","title":"Keeping some PIIs from being anonymized\u00b6","text":"<p>This sample shows how to use Presidio's <code>keep</code> anonymizer to keep some of the identified PIIs in the output string</p>"},{"location":"samples/python/keep_entities/#set-up-imports","title":"Set up imports\u00b6","text":""},{"location":"samples/python/keep_entities/#presidio-anonymizer-keep-person-names","title":"Presidio Anonymizer: Keep person names\u00b6","text":"<p>This example input has 2 PIIs, an person name and a location. We configure the anonymizer to replace the location name with a placeholder, but keep the person name unmodified.</p>"},{"location":"samples/python/keep_entities/#result-name-unmodified-but-tracked","title":"Result: Name unmodified, but tracked\u00b6","text":"<p>The person name is preserved in the result text, but remains tracked in the items list.</p>"},{"location":"samples/python/ner_model_configuration/","title":"Configuring The NLP engine","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n</pre> # download presidio !pip install presidio_analyzer presidio_anonymizer In\u00a0[15]: Copied! <pre>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NerModelConfiguration\n</pre> from presidio_analyzer import AnalyzerEngine from presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NerModelConfiguration In\u00a0[16]: Copied! <pre># Define which model to use\nmodel_config = [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}]\n\n# Define which entities the model returns and how they map to Presidio's\nentity_mapping = dict(\n    PER=\"PERSON\",\n    LOC= \"LOCATION\",\n    GPE=\"LOCATION\",\n    ORG=\"ORGANIZATION\"\n)\n\nner_model_configuration = NerModelConfiguration(default_score = 0.6, \n                                                model_to_presidio_entity_mapping=entity_mapping)\n\n# Create the NLP Engine based on this configuration\nspacy_nlp_engine = SpacyNlpEngine(models= model_config, ner_model_configuration=ner_model_configuration)\n</pre> # Define which model to use model_config = [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}]  # Define which entities the model returns and how they map to Presidio's entity_mapping = dict(     PER=\"PERSON\",     LOC= \"LOCATION\",     GPE=\"LOCATION\",     ORG=\"ORGANIZATION\" )  ner_model_configuration = NerModelConfiguration(default_score = 0.6,                                                  model_to_presidio_entity_mapping=entity_mapping)  # Create the NLP Engine based on this configuration spacy_nlp_engine = SpacyNlpEngine(models= model_config, ner_model_configuration=ner_model_configuration)  In\u00a0[17]: Copied! <pre># Helper method to use the NLP Engine as part of Presidio Analyzer, and print configuration+results\n\ndef call_analyzer_and_print_results(nlp_engine: NlpEngine,\n                                    language: str = \"en\",\n                                    text: str = \"Bill Clinton used to be the president of the United States\") -&gt; None:\n    \"\"\"\n    Instantiate the AnalyzerEngine with the provided nlp_engine and return output.\n\n    This method creates an AnalyzerEngine instance with the provided NlpEngine, and three supported languages (en, es, de)\n    Then, it calls the analyze method to return identified PII.\n\n    :param nlp_engine: The NlpEngine instance as configured by the user\n    :param language: the language the request should support (in contrast to the AnalyzerEngine which can support multiple)\n    :param text: The text to look for PII entities in.\n\n    \"\"\"\n    \n    print(f\"Input text:\\n\\t{text}\\n\")\n    \n    # Initialize the AnalyzerEngine with the configured Nlp Engine:\n    analyzer = AnalyzerEngine(nlp_engine=nlp_engine, \n                              supported_languages=[\"en\", \"de\", \"es\"])\n\n    # Print the NLP Engine's configuration\n    print(f\"NLP Engine configuration:\\n\\tLoaded NLP engine: {analyzer.nlp_engine.__class__.__name__}\")\n    print(f\"\\tSupported entities: {analyzer.nlp_engine.get_supported_entities()}\")\n    print(f\"\\tSupported languages: {analyzer.nlp_engine.get_supported_languages()}\")\n    print()\n    \n    # Call the analyzer.analyze to detect PII entities (from the NLP engine + all other recognizers)\n    results = analyzer.analyze(text=text, \n                               language=language, \n                               return_decision_process=True)\n\n    # sort results\n    results = sorted(results, key= lambda x: x.start)\n    \n    # Print results\n    print(\"Returning full results, including the decision process:\")\n    for i, result in enumerate(results):\n        print(f\"\\tResult {i}: {result}\")\n        print(f\"\\tDetected text: {text[result.start: result.end]}\")\n        print(f\"\\t{result.analysis_explanation.textual_explanation}\")\n        print(\"\")\n</pre> # Helper method to use the NLP Engine as part of Presidio Analyzer, and print configuration+results  def call_analyzer_and_print_results(nlp_engine: NlpEngine,                                     language: str = \"en\",                                     text: str = \"Bill Clinton used to be the president of the United States\") -&gt; None:     \"\"\"     Instantiate the AnalyzerEngine with the provided nlp_engine and return output.      This method creates an AnalyzerEngine instance with the provided NlpEngine, and three supported languages (en, es, de)     Then, it calls the analyze method to return identified PII.      :param nlp_engine: The NlpEngine instance as configured by the user     :param language: the language the request should support (in contrast to the AnalyzerEngine which can support multiple)     :param text: The text to look for PII entities in.      \"\"\"          print(f\"Input text:\\n\\t{text}\\n\")          # Initialize the AnalyzerEngine with the configured Nlp Engine:     analyzer = AnalyzerEngine(nlp_engine=nlp_engine,                                supported_languages=[\"en\", \"de\", \"es\"])      # Print the NLP Engine's configuration     print(f\"NLP Engine configuration:\\n\\tLoaded NLP engine: {analyzer.nlp_engine.__class__.__name__}\")     print(f\"\\tSupported entities: {analyzer.nlp_engine.get_supported_entities()}\")     print(f\"\\tSupported languages: {analyzer.nlp_engine.get_supported_languages()}\")     print()          # Call the analyzer.analyze to detect PII entities (from the NLP engine + all other recognizers)     results = analyzer.analyze(text=text,                                 language=language,                                 return_decision_process=True)      # sort results     results = sorted(results, key= lambda x: x.start)          # Print results     print(\"Returning full results, including the decision process:\")     for i, result in enumerate(results):         print(f\"\\tResult {i}: {result}\")         print(f\"\\tDetected text: {text[result.start: result.end]}\")         print(f\"\\t{result.analysis_explanation.textual_explanation}\")         print(\"\") In\u00a0[18]: Copied! <pre># Run it as part of Presidio's AnalyzerEngine\ncall_analyzer_and_print_results(spacy_nlp_engine)\n</pre> # Run it as part of Presidio's AnalyzerEngine call_analyzer_and_print_results(spacy_nlp_engine) <pre>Input text:\n\tBill Clinton used to be the president of the United States\n\nNLP Engine configuration:\n\tLoaded NLP engine: SpacyNlpEngine\n\tSupported entities: ['LOCATION', 'PERSON', 'ORGANIZATION']\n\tSupported languages: ['en']\n\nReturning full results, including the decision process:\n\tResult 0: type: PERSON, start: 0, end: 12, score: 0.6\n\tDetected text: Bill Clinton\n\tIdentified as PERSON by Spacy's Named Entity Recognition\n\n\tResult 1: type: LOCATION, start: 41, end: 58, score: 0.6\n\tDetected text: the United States\n\tIdentified as LOCATION by Spacy's Named Entity Recognition\n\n</pre> <p>Stanza is an NLP package by Stanford. More details on Stanza can be found here: https://stanfordnlp.github.io/stanza/ Loading Stanza instead of spaCy is straightforward. Just use <code>StanzaNlpEngine</code> instead of <code>SpacyNlpEngine</code> and define a model name supported by stanza (for example, <code>en</code> instead of <code>en_core_web_lg</code>)</p> In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer.nlp_engine import StanzaNlpEngine, NerModelConfiguration\n</pre> from presidio_analyzer.nlp_engine import StanzaNlpEngine, NerModelConfiguration In\u00a0[\u00a0]: Copied! <pre># Define which model to use\nmodel_config = [{\"lang_code\": \"en\", \"model_name\": \"en\"}]\n\n# Define which entities the model returns and how they map to Presidio's\nentity_mapping = dict(\n    PER=\"PERSON\",\n    LOC= \"LOCATION\",\n    GPE=\"LOCATION\",\n    ORG=\"ORGANIZATION\"\n)\n\nner_model_configuration = NerModelConfiguration(model_to_presidio_entity_mapping=entity_mapping)\n\n# Create the Stanza NLP Engine based on this configuration\nstanza_nlp_engine = StanzaNlpEngine(models= model_config, ner_model_configuration=ner_model_configuration)\n\n# Run it as part of Presidio's AnalyzerEngine\ncall_analyzer_and_print_results(stanza_nlp_engine)\n</pre> # Define which model to use model_config = [{\"lang_code\": \"en\", \"model_name\": \"en\"}]  # Define which entities the model returns and how they map to Presidio's entity_mapping = dict(     PER=\"PERSON\",     LOC= \"LOCATION\",     GPE=\"LOCATION\",     ORG=\"ORGANIZATION\" )  ner_model_configuration = NerModelConfiguration(model_to_presidio_entity_mapping=entity_mapping)  # Create the Stanza NLP Engine based on this configuration stanza_nlp_engine = StanzaNlpEngine(models= model_config, ner_model_configuration=ner_model_configuration)  # Run it as part of Presidio's AnalyzerEngine call_analyzer_and_print_results(stanza_nlp_engine) <p>A third option is to use a model based on the <code>transformers</code> package. Note that in this case, we use both spaCy and transformers. The actual PII entities are detected using a transformers model, but additional text features such as lemmas and others, are extracted from a spaCy pipeline. We use a small spaCy model as it's faster and more memory efficient.</p> In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer.nlp_engine import TransformersNlpEngine, NerModelConfiguration\n</pre> from presidio_analyzer.nlp_engine import TransformersNlpEngine, NerModelConfiguration In\u00a0[\u00a0]: Copied! <pre># Define which model to use\nmodel_config = [{\n   \"lang_code\":\"en\",\n   \"model_name\":{\n      \"spacy\":\"en_core_web_sm\",\n      \"transformers\":\"obi/deid_roberta_i2b2\"\n   }\n}]\n\n# Map transformers model labels to Presidio's\nmodel_to_presidio_entity_mapping = dict(\n    PER=\"PERSON\",\n    PERSON=\"PERSON\",\n    LOC= \"LOCATION\",\n    LOCATION= \"LOCATION\",\n    GPE=\"LOCATION\",\n    ORG=\"ORGANIZATION\",\n    ORGANIZATION=\"ORGANIZATION\",\n    NORP=\"NRP\",\n    AGE=\"AGE\",\n    ID=\"ID\",\n    EMAIL=\"EMAIL\",\n    PATIENT=\"PERSON\",\n    STAFF=\"PERSON\",\n    HOSP=\"ORGANIZATION\",\n    PATORG=\"ORGANIZATION\",\n    DATE=\"DATE_TIME\",\n    TIME=\"DATE_TIME\",\n    PHONE=\"PHONE_NUMBER\",\n    HCW=\"PERSON\",\n    HOSPITAL=\"ORGANIZATION\",\n    FACILITY=\"LOCATION\",\n)\n\nner_model_configuration = NerModelConfiguration(model_to_presidio_entity_mapping=model_to_presidio_entity_mapping, \n                                                aggregation_strategy=\"simple\",\n                                                stride=14)\n\ntransformers_nlp_engine = TransformersNlpEngine(models=model_config,\n                                                ner_model_configuration=ner_model_configuration)\n</pre> # Define which model to use model_config = [{    \"lang_code\":\"en\",    \"model_name\":{       \"spacy\":\"en_core_web_sm\",       \"transformers\":\"obi/deid_roberta_i2b2\"    } }]  # Map transformers model labels to Presidio's model_to_presidio_entity_mapping = dict(     PER=\"PERSON\",     PERSON=\"PERSON\",     LOC= \"LOCATION\",     LOCATION= \"LOCATION\",     GPE=\"LOCATION\",     ORG=\"ORGANIZATION\",     ORGANIZATION=\"ORGANIZATION\",     NORP=\"NRP\",     AGE=\"AGE\",     ID=\"ID\",     EMAIL=\"EMAIL\",     PATIENT=\"PERSON\",     STAFF=\"PERSON\",     HOSP=\"ORGANIZATION\",     PATORG=\"ORGANIZATION\",     DATE=\"DATE_TIME\",     TIME=\"DATE_TIME\",     PHONE=\"PHONE_NUMBER\",     HCW=\"PERSON\",     HOSPITAL=\"ORGANIZATION\",     FACILITY=\"LOCATION\", )  ner_model_configuration = NerModelConfiguration(model_to_presidio_entity_mapping=model_to_presidio_entity_mapping,                                                  aggregation_strategy=\"simple\",                                                 stride=14)  transformers_nlp_engine = TransformersNlpEngine(models=model_config,                                                 ner_model_configuration=ner_model_configuration) In\u00a0[\u00a0]: Copied! <pre># Run it as part of Presidio's AnalyzerEngine\ncall_analyzer_and_print_results(transformers_nlp_engine)\n</pre> # Run it as part of Presidio's AnalyzerEngine call_analyzer_and_print_results(transformers_nlp_engine) In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer.nlp_engine import TransformersNlpEngine, NerModelConfiguration\n</pre> from presidio_analyzer.nlp_engine import TransformersNlpEngine, NerModelConfiguration In\u00a0[\u00a0]: Copied! <pre># Define which model to use\nmodel_config = [{\n   \"lang_code\":\"en\",\n   \"model_name\":{\n      \"spacy\":\"en_core_web_sm\",\n      \"transformers\":\"obi/deid_roberta_i2b2\"\n   }\n},\n{\n    \"lang_code\":\"es\",\n    \"model_name\":{\n      \"spacy\":\"es_core_news_sm\",\n      \"transformers\":\"PlanTL-GOB-ES/roberta-large-bne-capitel-ner\"\n   }\n}]\n\ntransformers_nlp_engine = TransformersNlpEngine(models=model_config,\n                                                ner_model_configuration=ner_model_configuration)\n</pre> # Define which model to use model_config = [{    \"lang_code\":\"en\",    \"model_name\":{       \"spacy\":\"en_core_web_sm\",       \"transformers\":\"obi/deid_roberta_i2b2\"    } }, {     \"lang_code\":\"es\",     \"model_name\":{       \"spacy\":\"es_core_news_sm\",       \"transformers\":\"PlanTL-GOB-ES/roberta-large-bne-capitel-ner\"    } }]  transformers_nlp_engine = TransformersNlpEngine(models=model_config,                                                 ner_model_configuration=ner_model_configuration)   In\u00a0[\u00a0]: Copied! <pre># Call in English\ncall_analyzer_and_print_results(transformers_nlp_engine, \n                                language=\"en\", \n                                text = \"Bill Clinton was the president of the United States\")\n</pre> # Call in English call_analyzer_and_print_results(transformers_nlp_engine,                                  language=\"en\",                                  text = \"Bill Clinton was the president of the United States\") In\u00a0[\u00a0]: Copied! <pre># Call in Spanish\ncall_analyzer_and_print_results(transformers_nlp_engine, \n                                language=\"es\", \n                                text = \"Bill Clinton sol\u00eda ser el presidente de los Estados Unidos.\")\n</pre> # Call in Spanish call_analyzer_and_print_results(transformers_nlp_engine,                                  language=\"es\",                                  text = \"Bill Clinton sol\u00eda ser el presidente de los Estados Unidos.\")"},{"location":"samples/python/ner_model_configuration/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonner_model_configurationipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/ner_model_configuration.ipynb\u00b6","text":""},{"location":"samples/python/ner_model_configuration/#configuring-the-ner-model","title":"Configuring the NER model\u00b6","text":"<p>This notebook contains a few examples to customize and configure the NER model through code. Examples:</p> <ol> <li>Changing the default model's parameters</li> <li>Using Stanza as the NER engine</li> <li>Using transformers as the NER engine</li> <li>Supporting multiple languages</li> </ol> <p>This notebook complements the documentation, which primarily focuses on reading the NER configuration from file</p>"},{"location":"samples/python/ner_model_configuration/#1-changing-the-default-models-parameters","title":"1. Changing the default model's parameters\u00b6","text":"<p>In this example, we'll change the models' default confidence score (spaCy models do not generally output confidence per prediction, so we add a default score(. In addition, we'll change the types of PII entities the model returns.</p>"},{"location":"samples/python/ner_model_configuration/#2-using-stanza","title":"2. Using Stanza\u00b6","text":""},{"location":"samples/python/ner_model_configuration/#3-using-transformers-as-the-nlp-engine","title":"3. Using transformers as the NLP engine\u00b6","text":""},{"location":"samples/python/ner_model_configuration/#4-supporting-multiple-languages","title":"4. Supporting multiple languages\u00b6","text":"<p>Presidio allows the user to create a model per language:</p>"},{"location":"samples/python/no_code_config/","title":"YAML based no-code configuration","text":"<p>No-code configuration can be helpful in three scenarios:</p> <ol> <li>There's an existing set of regular expressions / deny-lists that should be leveraged within Presidio.</li> <li>As a simple way to configure which recognizers to enable and disable, and how to configure the NLP engine.</li> <li>For team members interested in changing the configuration without writing code.</li> </ol> <p>In this example, we'll show how to create a no-code configuration in Presidio. We start by creating YAML configuration files that are based on the default ones. Te default configuration files for Presidio can be found here:</p> <ul> <li>Analyzer configuration</li> <li>Recognizer registry configuration</li> <li>NLP engine configuration</li> </ul> <p>Alternatively, one can create one configuration file for all three components. In this example, we'll tweak the configuration to reduce the number of predefinedrecognizers to only a few, and add a new custom one. We'll also adjust the context words to support the detection of a different language (Spanish).</p> In\u00a0[1]: Copied! <pre>import yaml\nimport json\nimport tempfile\nimport warnings\nfrom pprint import pprint\nfrom presidio_analyzer import AnalyzerEngineProvider\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import yaml import json import tempfile import warnings from pprint import pprint from presidio_analyzer import AnalyzerEngineProvider  warnings.filterwarnings(\"ignore\") <p>In this example we're going to create the yaml as a string for illustration purposes, but the more common scenario is to create these YAML files and load them into the <code>PresidioAnalyzerProvider</code>.</p> In\u00a0[2]: Copied! <pre>analyzer_config_yaml = \"\"\"\nsupported_languages: \n  - en\n  - es\ndefault_score_threshold: 0.4\n\"\"\"\n</pre> analyzer_config_yaml = \"\"\" supported_languages:    - en   - es default_score_threshold: 0.4 \"\"\" In\u00a0[3]: Copied! <pre>recognizer_registry_config_yaml = \"\"\"\nrecognizer_registry:\n  supported_languages: \n  - en\n  - es\n  global_regex_flags: 26\n\n  recognizers:\n  - name: CreditCardRecognizer\n    supported_languages:\n    - language: en\n      context: [credit, card, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    - language: es\n      context: [tarjeta, credito, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    type: predefined\n    \n  - name: DateRecognizer\n    supported_languages:\n    - language: en\n      context: [date, time, birthday, birthdate, dob]\n    - language: es\n      context: [fecha, tiempo, hora, nacimiento, dob]\n    type: predefined\n\n  - name: EmailRecognizer\n    supported_languages:\n    - language: en\n      context: [email, mail, address]\n    - language: es\n      context: [correo, electr\u00f3nico, email]\n    type: predefined\n    \n  - name: PhoneRecognizer\n    type: predefined\n    supported_languages:\n    - language: en\n      context: [phone, number, telephone, fax]\n    - language: es\n      context: [tel\u00e9fono, n\u00famero, fax]\n    \n  - name: \"Titles recognizer (en)\"\n    supported_language: \"en\"\n    supported_entity: \"TITLE\"\n    deny_list:\n      - Mr.\n      - Mrs.\n      - Ms.\n      - Miss\n      - Dr.\n      - Prof.\n      - Doctor\n      - Professor\n  - name: \"Titles recognizer (es)\"\n    supported_language: \"es\"\n    supported_entity: \"TITLE\"\n    deny_list:\n      - Sr.\n      - Se\u00f1or\n      - Sra.\n      - Se\u00f1ora\n      - Srta.\n      - Se\u00f1orita\n      - Dr.\n      - Doctor\n      - Doctora\n      - Prof.\n      - Profesor\n      - Profesora\n\"\"\"\n</pre>  recognizer_registry_config_yaml = \"\"\" recognizer_registry:   supported_languages:    - en   - es   global_regex_flags: 26    recognizers:   - name: CreditCardRecognizer     supported_languages:     - language: en       context: [credit, card, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]     - language: es       context: [tarjeta, credito, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]     type: predefined        - name: DateRecognizer     supported_languages:     - language: en       context: [date, time, birthday, birthdate, dob]     - language: es       context: [fecha, tiempo, hora, nacimiento, dob]     type: predefined    - name: EmailRecognizer     supported_languages:     - language: en       context: [email, mail, address]     - language: es       context: [correo, electr\u00f3nico, email]     type: predefined        - name: PhoneRecognizer     type: predefined     supported_languages:     - language: en       context: [phone, number, telephone, fax]     - language: es       context: [tel\u00e9fono, n\u00famero, fax]        - name: \"Titles recognizer (en)\"     supported_language: \"en\"     supported_entity: \"TITLE\"     deny_list:       - Mr.       - Mrs.       - Ms.       - Miss       - Dr.       - Prof.       - Doctor       - Professor   - name: \"Titles recognizer (es)\"     supported_language: \"es\"     supported_entity: \"TITLE\"     deny_list:       - Sr.       - Se\u00f1or       - Sra.       - Se\u00f1ora       - Srta.       - Se\u00f1orita       - Dr.       - Doctor       - Doctora       - Prof.       - Profesor       - Profesora \"\"\" In\u00a0[4]: Copied! <pre>nlp_engine_yaml = \"\"\"\nnlp_configuration:\n    nlp_engine_name: transformers\n    models:\n      -\n        lang_code: en\n        model_name:\n          spacy: en_core_web_sm\n          transformers: StanfordAIMI/stanford-deidentifier-base\n      -\n        lang_code: es\n        model_name:\n          spacy: es_core_news_sm\n          transformers: MMG/xlm-roberta-large-ner-spanish  \n    ner_model_configuration:\n      labels_to_ignore:\n      - O\n      aggregation_strategy: first # \"simple\", \"first\", \"average\", \"max\"\n      stride: 16\n      alignment_mode: expand # \"strict\", \"contract\", \"expand\"\n      model_to_presidio_entity_mapping:\n        PER: PERSON\n        PERSON: PERSON\n        LOC: LOCATION\n        LOCATION: LOCATION\n        GPE: LOCATION\n        ORG: ORGANIZATION\n        ORGANIZATION: ORGANIZATION\n        NORP: NRP\n        AGE: AGE\n        ID: ID\n        EMAIL: EMAIL\n        PATIENT: PERSON\n        STAFF: PERSON\n        HOSP: ORGANIZATION\n        PATORG: ORGANIZATION\n        DATE: DATE_TIME\n        TIME: DATE_TIME\n        PHONE: PHONE_NUMBER\n        HCW: PERSON\n        HOSPITAL: LOCATION\n        FACILITY: LOCATION\n        VENDOR: ORGANIZATION\n        MISC: ID\n    \n      low_confidence_score_multiplier: 0.4\n      low_score_entity_names:\n      - ID\n\"\"\"\n</pre> nlp_engine_yaml = \"\"\" nlp_configuration:     nlp_engine_name: transformers     models:       -         lang_code: en         model_name:           spacy: en_core_web_sm           transformers: StanfordAIMI/stanford-deidentifier-base       -         lang_code: es         model_name:           spacy: es_core_news_sm           transformers: MMG/xlm-roberta-large-ner-spanish       ner_model_configuration:       labels_to_ignore:       - O       aggregation_strategy: first # \"simple\", \"first\", \"average\", \"max\"       stride: 16       alignment_mode: expand # \"strict\", \"contract\", \"expand\"       model_to_presidio_entity_mapping:         PER: PERSON         PERSON: PERSON         LOC: LOCATION         LOCATION: LOCATION         GPE: LOCATION         ORG: ORGANIZATION         ORGANIZATION: ORGANIZATION         NORP: NRP         AGE: AGE         ID: ID         EMAIL: EMAIL         PATIENT: PERSON         STAFF: PERSON         HOSP: ORGANIZATION         PATORG: ORGANIZATION         DATE: DATE_TIME         TIME: DATE_TIME         PHONE: PHONE_NUMBER         HCW: PERSON         HOSPITAL: LOCATION         FACILITY: LOCATION         VENDOR: ORGANIZATION         MISC: ID            low_confidence_score_multiplier: 0.4       low_score_entity_names:       - ID \"\"\" <p>Create a unified YAML file and save it as a temp file</p> In\u00a0[5]: Copied! <pre>full_config = f\"{analyzer_config_yaml}\\n{recognizer_registry_config_yaml}\\n{nlp_engine_yaml}\"\n\nwith tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.yaml') as temp_file:\n    # Write the YAML string to the temp file\n    temp_file.write(full_config)\n    temp_file_path = temp_file.name\n</pre> full_config = f\"{analyzer_config_yaml}\\n{recognizer_registry_config_yaml}\\n{nlp_engine_yaml}\"  with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.yaml') as temp_file:     # Write the YAML string to the temp file     temp_file.write(full_config)     temp_file_path = temp_file.name   <p>Pass the YAML file to <code>AnalyzerEngineProvider</code> to create an <code>AnalyzerEngine</code> instance</p> In\u00a0[\u00a0]: Copied! <pre>analyzer_engine = AnalyzerEngineProvider(analyzer_engine_conf_file=temp_file_path).create_engine()\n</pre> analyzer_engine = AnalyzerEngineProvider(analyzer_engine_conf_file=temp_file_path).create_engine()  <p>Print the loaded configuration for both languages</p> In\u00a0[\u00a0]: Copied! <pre>for lang in (\"en\", \"es\"):\n    pprint(f\"Supported entities for {lang}:\")\n    print(\"\\n\")\n    pprint(analyzer_engine.get_supported_entities(lang), compact=True)\n    \n    print(f\"\\nLoaded recognizers for {lang}:\")\n    pprint([rec.name for rec in analyzer_engine.registry.get_recognizers(lang, all_fields=True)], compact=True)\n    print(\"\\n\")\n   \nprint(f\"\\nLoaded NER models:\")\npprint(analyzer_engine.nlp_engine.models)\n</pre> for lang in (\"en\", \"es\"):     pprint(f\"Supported entities for {lang}:\")     print(\"\\n\")     pprint(analyzer_engine.get_supported_entities(lang), compact=True)          print(f\"\\nLoaded recognizers for {lang}:\")     pprint([rec.name for rec in analyzer_engine.registry.get_recognizers(lang, all_fields=True)], compact=True)     print(\"\\n\")     print(f\"\\nLoaded NER models:\") pprint(analyzer_engine.nlp_engine.models) In\u00a0[\u00a0]: Copied! <pre>es_text = \"Hola, me llamo David Johnson y soy originalmente de Liverpool. Mi n\u00famero de tarjeta de cr\u00e9dito es 4095260993934932\"\nanalyzer_engine.analyze(es_text, language=\"es\")\n</pre> es_text = \"Hola, me llamo David Johnson y soy originalmente de Liverpool. Mi n\u00famero de tarjeta de cr\u00e9dito es 4095260993934932\" analyzer_engine.analyze(es_text, language=\"es\") In\u00a0[\u00a0]: Copied! <pre>en_text = \"Hi, my name is David Johnson and I'm originally from Liverpool. My credit card number is 4095260993934932\"\nanalyzer_engine.analyze(en_text, language=\"en\")\n</pre> en_text = \"Hi, my name is David Johnson and I'm originally from Liverpool. My credit card number is 4095260993934932\" analyzer_engine.analyze(en_text, language=\"en\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"samples/python/no_code_config/#no-code-configuration","title":"No code configuration\u00b6","text":""},{"location":"samples/python/no_code_config/#general-analyzer-parameters","title":"General Analyzer parameters\u00b6","text":"<p>(default file)</p>"},{"location":"samples/python/no_code_config/#recognizer-registry-parameters","title":"Recognizer Registry parameters\u00b6","text":"<p>(default file)</p>"},{"location":"samples/python/no_code_config/#nlp-engine-parameters","title":"NLP Engine parameters\u00b6","text":"<p>(default file)</p>"},{"location":"samples/python/plot_custom_bboxes/","title":"Plot custom bounding boxes","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install presidio_analyzer presidio_anonymizer presidio_image_redactor\n!python -m spacy download en_core_web_lg\n</pre> !pip install presidio_analyzer presidio_anonymizer presidio_image_redactor !python -m spacy download en_core_web_lg In\u00a0[1]: Copied! <pre>from PIL import Image\nimport pydicom\nfrom presidio_image_redactor import ImageAnalyzerEngine, ImagePiiVerifyEngine, DicomImagePiiVerifyEngine\nimport matplotlib.pyplot as plt\n</pre> from PIL import Image import pydicom from presidio_image_redactor import ImageAnalyzerEngine, ImagePiiVerifyEngine, DicomImagePiiVerifyEngine import matplotlib.pyplot as plt <p>Initialize engines used for image redaction</p> In\u00a0[2]: Copied! <pre># Image analyzer engine\nimage_analyzer_engine = ImageAnalyzerEngine()\n\n# Verification engines\nverify_engine = ImagePiiVerifyEngine() # standard images\ndicom_verify_engine = DicomImagePiiVerifyEngine() # DICOM images\npadding_width = 3\n</pre> # Image analyzer engine image_analyzer_engine = ImageAnalyzerEngine()  # Verification engines verify_engine = ImagePiiVerifyEngine() # standard images dicom_verify_engine = DicomImagePiiVerifyEngine() # DICOM images padding_width = 3 In\u00a0[3]: Copied! <pre>image = Image.open(\"../../image-redactor/ocr_text.png\")\ndisplay(image)\n</pre> image = Image.open(\"../../image-redactor/ocr_text.png\") display(image) <p>And this is what the image looks like with the standard, default behavior of the verification engine.</p> In\u00a0[4]: Copied! <pre>verify_image = verify_engine.verify(image, display_image=True)\n</pre> verify_image = verify_engine.verify(image, display_image=True) In\u00a0[5]: Copied! <pre>instance = pydicom.dcmread(\"./sample_data/0_ORIGINAL.dcm\")\nplt.imshow(instance.pixel_array, cmap=\"gray\")\n</pre> instance = pydicom.dcmread(\"./sample_data/0_ORIGINAL.dcm\") plt.imshow(instance.pixel_array, cmap=\"gray\") Out[5]: <pre>&lt;matplotlib.image.AxesImage at 0x1f2a91512b0&gt;</pre> <p>And this is what the image looks like with the standard, default behavior verification.</p> In\u00a0[6]: Copied! <pre>dicom_image, dicom_ocr_bboxes, dicom_analyzer_bboxes = dicom_verify_engine.verify_dicom_instance(\n    instance,\n    padding_width=padding_width,\n    display_image=True\n)\n</pre> dicom_image, dicom_ocr_bboxes, dicom_analyzer_bboxes = dicom_verify_engine.verify_dicom_instance(     instance,     padding_width=padding_width,     display_image=True ) In\u00a0[7]: Copied! <pre>len(dicom_ocr_bboxes)\n</pre> len(dicom_ocr_bboxes) Out[7]: <pre>9</pre> In\u00a0[8]: Copied! <pre>len(dicom_analyzer_bboxes)\n</pre> len(dicom_analyzer_bboxes) Out[8]: <pre>4</pre> <p>Let's look at the format of the analyzer results bounding boxes returned by the DICOM verification engine. This the general format expected of custom bounding boxes passed into <code>add_custom_bbox()</code>.</p> In\u00a0[9]: Copied! <pre>type(dicom_analyzer_bboxes)\n</pre> type(dicom_analyzer_bboxes) Out[9]: <pre>list</pre> In\u00a0[10]: Copied! <pre>type(dicom_analyzer_bboxes[0])\n</pre> type(dicom_analyzer_bboxes[0]) Out[10]: <pre>dict</pre> In\u00a0[11]: Copied! <pre>dicom_analyzer_bboxes[0]\n</pre> dicom_analyzer_bboxes[0] Out[11]: <pre>{'entity_type': 'PERSON',\n 'score': 0.85,\n 'left': 3,\n 'top': 3,\n 'width': 241,\n 'height': 37,\n 'is_PII': True}</pre> <p>For our custom bounding boxes, the \"entity_type\" and \"is_PII\" fields are optional and \"score\" is not used. However, the \"is_PII\" field is helpful in visually identifying which bounding boxes from your given bounding box list are considered PII.</p> In\u00a0[12]: Copied! <pre># Example provided bounding box list\ngiven_bboxes = [\n    {\n        'entity_type': 'PERSON',\n         'left': 3,\n         'top': 3,\n         'width': 241,\n         'height': 37,\n         'is_PII': True\n    },\n    {\n        'entity_type': 'PERSON',\n         'left': 179,\n         'top': 150,\n         'width': 300,\n         'height': 74,\n         'is_PII': False\n    }\n]\n</pre> # Example provided bounding box list given_bboxes = [     {         'entity_type': 'PERSON',          'left': 3,          'top': 3,          'width': 241,          'height': 37,          'is_PII': True     },     {         'entity_type': 'PERSON',          'left': 179,          'top': 150,          'width': 300,          'height': 74,          'is_PII': False     } ] <p>Let's plot the given bounding boxes.</p> In\u00a0[13]: Copied! <pre># Let's plot with the given bounding boxes\ntest_image = image_analyzer_engine.add_custom_bboxes(image, given_bboxes, show_text_annotation=True)\n</pre> # Let's plot with the given bounding boxes test_image = image_analyzer_engine.add_custom_bboxes(image, given_bboxes, show_text_annotation=True) <p>While the placement of our example bounding boxes here is not ideal, this shows how you can easily visualize if a provided set of bounding boxes match your expectations.</p>"},{"location":"samples/python/plot_custom_bboxes/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonplot_custom_bboxesipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/plot_custom_bboxes.ipynb\u00b6","text":""},{"location":"samples/python/plot_custom_bboxes/#plot-custom-bounding-boxes","title":"Plot custom bounding boxes\u00b6","text":"<p>This notebook covers how to use the image verification engines to plot custom bounding boxes. This can be helpful when you want to verify a provided set of bounding boxes against an image.</p>"},{"location":"samples/python/plot_custom_bboxes/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before getting started, make sure presidio and the latest version of Tesseract OCR are installed. For detailed documentation, see the installation docs.</p>"},{"location":"samples/python/plot_custom_bboxes/#0-imports-and-initializations","title":"0. Imports and initializations\u00b6","text":""},{"location":"samples/python/plot_custom_bboxes/#1-example-images","title":"1. Example images\u00b6","text":"<p>In this notebook, we will use the following examples images.</p>"},{"location":"samples/python/plot_custom_bboxes/#11-standard-example-image","title":"1.1 Standard example image\u00b6","text":""},{"location":"samples/python/plot_custom_bboxes/#12-dicom-medical-image","title":"1.2 DICOM medical image\u00b6","text":"<p>For more information on DICOM image redaction, please see example_dicom_image_redactor.ipynb and the Image redactor module documentation.</p>"},{"location":"samples/python/plot_custom_bboxes/#2-plot-custom-bounding-boxes","title":"2. Plot custom bounding boxes\u00b6","text":"<p>There may be situations where you want to visually validate whether a set of given bounding boxes match your expectations on an image. In these cases, we can call <code>ImageAnalyzerEngine.add_custom_bbox()</code> instead of using the verify methods which include OCR and text analysis.</p>"},{"location":"samples/python/presidio_notebook/","title":"Presidio Basic Usage Notebook","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</pre> # download presidio !pip install presidio_analyzer presidio_anonymizer !python -m spacy download en_core_web_lg In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import AnalyzerEngine, PatternRecognizer\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\nimport json\nfrom pprint import pprint\n</pre> from presidio_analyzer import AnalyzerEngine, PatternRecognizer from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import OperatorConfig import json from pprint import pprint In\u00a0[\u00a0]: Copied! <pre>text_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\"\n</pre> text_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\" In\u00a0[\u00a0]: Copied! <pre>analyzer = AnalyzerEngine()\nanalyzer_results = analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en')\n\nprint(analyzer_results)\n</pre> analyzer = AnalyzerEngine() analyzer_results = analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en')  print(analyzer_results) In\u00a0[\u00a0]: Copied! <pre>titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n                                      deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n\npronoun_recognizer = PatternRecognizer(supported_entity=\"PRONOUN\",\n                                       deny_list=[\"he\", \"He\", \"his\", \"His\", \"she\", \"She\", \"hers\", \"Hers\"])\n\nanalyzer.registry.add_recognizer(titles_recognizer)\nanalyzer.registry.add_recognizer(pronoun_recognizer)\n\nanalyzer_results = analyzer.analyze(text=text_to_anonymize,\n                            entities=[\"TITLE\", \"PRONOUN\"],\n                            language=\"en\")\nprint(analyzer_results)\n</pre> titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",                                       deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])  pronoun_recognizer = PatternRecognizer(supported_entity=\"PRONOUN\",                                        deny_list=[\"he\", \"He\", \"his\", \"His\", \"she\", \"She\", \"hers\", \"Hers\"])  analyzer.registry.add_recognizer(titles_recognizer) analyzer.registry.add_recognizer(pronoun_recognizer)  analyzer_results = analyzer.analyze(text=text_to_anonymize,                             entities=[\"TITLE\", \"PRONOUN\"],                             language=\"en\") print(analyzer_results)  <p>Call Presidio Analyzer and get analyzed results with all the configured recognizers - default and new custom recognizers</p> In\u00a0[\u00a0]: Copied! <pre>analyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')\n\nanalyzer_results\n</pre> analyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')  analyzer_results In\u00a0[\u00a0]: Copied! <pre>anonymizer = AnonymizerEngine()\n\nanonymized_results = anonymizer.anonymize(\n    text=text_to_anonymize,\n    analyzer_results=analyzer_results,    \n    operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"&lt;ANONYMIZED&gt;\"}), \n                        \"PHONE_NUMBER\": OperatorConfig(\"mask\", {\"type\": \"mask\", \"masking_char\" : \"*\", \"chars_to_mask\" : 12, \"from_end\" : True}),\n                        \"TITLE\": OperatorConfig(\"redact\", {})}\n)\n\nprint(f\"text: {anonymized_results.text}\")\nprint(\"detailed response:\")\n\npprint(json.loads(anonymized_results.to_json()))\n</pre> anonymizer = AnonymizerEngine()  anonymized_results = anonymizer.anonymize(     text=text_to_anonymize,     analyzer_results=analyzer_results,         operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"\"}),                          \"PHONE_NUMBER\": OperatorConfig(\"mask\", {\"type\": \"mask\", \"masking_char\" : \"*\", \"chars_to_mask\" : 12, \"from_end\" : True}),                         \"TITLE\": OperatorConfig(\"redact\", {})} )  print(f\"text: {anonymized_results.text}\") print(\"detailed response:\")  pprint(json.loads(anonymized_results.to_json())) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"samples/python/presidio_notebook/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonpresidio_notebookipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/presidio_notebook.ipynb\u00b6","text":""},{"location":"samples/python/presidio_notebook/#analyze-text-for-pii-entities","title":"Analyze Text for PII Entities\u00b6","text":"<p>Using Presidio Analyzer, analyze a text to identify PII entities. The Presidio analyzer is using pre-defined entity recognizers, and offers the option to create custom recognizers.</p> <p>The following code sample will:</p> <ul> <li>Set up the Analyzer engine: load the NLP module (spaCy model by default) and other PII recognizers</li> <li>Call analyzer to get analyzed results for \"PHONE_NUMBER\" entity type</li> </ul>"},{"location":"samples/python/presidio_notebook/#create-custom-pii-entity-recognizers","title":"Create Custom PII Entity Recognizers\u00b6","text":"<p>Presidio Analyzer comes with a pre-defined set of entity recognizers. It also allows adding new recognizers without changing the analyzer base code, by creating custom recognizers. In the following example, we will create two new recognizers of type <code>PatternRecognizer</code> to identify titles and pronouns in the analyzed text. A <code>PatternRecognizer</code> is a PII entity recognizer which uses regular expressions or deny-lists.</p> <p>The following code sample will:</p> <ul> <li>Create custom recognizers</li> <li>Add the new custom recognizers to the analyzer</li> <li>Call analyzer to get results from the new recognizers</li> </ul>"},{"location":"samples/python/presidio_notebook/#anonymize-text-with-identified-pii-entities","title":"Anonymize Text with Identified PII Entities\u00b6","text":"<p>Presidio Anonymizer iterates over the Presidio Analyzer result, and provides anonymization capabilities for the identified text. The anonymizer provides 5 types of anonymizers - replace, redact, mask, hash and encrypt. The default is replace</p> <p>The following code sample will:</p> <ol> <li>Setup the anonymizer engine </li> <li>Create an anonymizer request - text to anonymize, list of anonymizers to apply and the results from the analyzer request</li> <li>Anonymize the text</li> </ol>"},{"location":"samples/python/process_csv_file/","title":"Process csv file","text":"In\u00a0[\u00a0]: Copied! <pre>import csv\nimport pprint\nfrom typing import List, Iterable, Optional\n</pre> import csv import pprint from typing import List, Iterable, Optional In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import BatchAnalyzerEngine, DictAnalyzerResult\nfrom presidio_anonymizer import BatchAnonymizerEngine\n</pre> from presidio_analyzer import BatchAnalyzerEngine, DictAnalyzerResult from presidio_anonymizer import BatchAnonymizerEngine In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nExample implementing a CSV analyzer\n\nThis example shows how to use the Presidio Analyzer and Anonymizer\nto detect and anonymize PII in a CSV file.\nIt uses the BatchAnalyzerEngine to analyze the CSV file, and \nBatchAnonymizerEngine to anonymize the requested columns.\n\nContent of csv file:\nid,name,city,comments\n1,John,New York,called him yesterday to confirm he requested to call back in 2 days\n2,Jill,Los Angeles,accepted the offer license number AC432223\n3,Jack,Chicago,need to call him at phone number 212-555-5555\n\n\"\"\"\n</pre> \"\"\" Example implementing a CSV analyzer  This example shows how to use the Presidio Analyzer and Anonymizer to detect and anonymize PII in a CSV file. It uses the BatchAnalyzerEngine to analyze the CSV file, and  BatchAnonymizerEngine to anonymize the requested columns.  Content of csv file: id,name,city,comments 1,John,New York,called him yesterday to confirm he requested to call back in 2 days 2,Jill,Los Angeles,accepted the offer license number AC432223 3,Jack,Chicago,need to call him at phone number 212-555-5555  \"\"\" In\u00a0[\u00a0]: Copied! <pre>class CSVAnalyzer(BatchAnalyzerEngine):\n\n    def analyze_csv(\n        self,\n        csv_full_path: str,\n        language: str,\n        keys_to_skip: Optional[List[str]] = None,\n        **kwargs,\n    ) -&gt; Iterable[DictAnalyzerResult]:\n\n        with open(csv_full_path, 'r') as csv_file:\n            csv_list = list(csv.reader(csv_file))\n            csv_dict = {header: list(map(str, values)) for header, *values in zip(*csv_list)}\n            analyzer_results = self.analyze_dict(csv_dict, language, keys_to_skip)\n            return list(analyzer_results)\n</pre> class CSVAnalyzer(BatchAnalyzerEngine):      def analyze_csv(         self,         csv_full_path: str,         language: str,         keys_to_skip: Optional[List[str]] = None,         **kwargs,     ) -&gt; Iterable[DictAnalyzerResult]:          with open(csv_full_path, 'r') as csv_file:             csv_list = list(csv.reader(csv_file))             csv_dict = {header: list(map(str, values)) for header, *values in zip(*csv_list)}             analyzer_results = self.analyze_dict(csv_dict, language, keys_to_skip)             return list(analyzer_results) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n\n    analyzer = CSVAnalyzer()\n    analyzer_results = analyzer.analyze_csv('./csv_sample_data/sample_data.csv',\n                                            language=\"en\")\n    pprint.pprint(analyzer_results)\n\n    anonymizer = BatchAnonymizerEngine()\n    anonymized_results = anonymizer.anonymize_dict(analyzer_results)\n    pprint.pprint(anonymized_results)\n</pre> if __name__ == \"__main__\":      analyzer = CSVAnalyzer()     analyzer_results = analyzer.analyze_csv('./csv_sample_data/sample_data.csv',                                             language=\"en\")     pprint.pprint(analyzer_results)      anonymizer = BatchAnonymizerEngine()     anonymized_results = anonymizer.anonymize_dict(analyzer_results)     pprint.pprint(anonymized_results)"},{"location":"samples/python/pseudonymization/","title":"Pseudonymization (replace PII values using mappings)","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</pre> # download presidio !pip install presidio_analyzer presidio_anonymizer !python -m spacy download en_core_web_lg In\u00a0[2]: Copied! <pre>from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine, OperatorConfig\nfrom presidio_anonymizer.operators import Operator, OperatorType\n\nfrom typing import Dict\nfrom pprint import pprint\n</pre> from presidio_analyzer import AnalyzerEngine from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine, OperatorConfig from presidio_anonymizer.operators import Operator, OperatorType  from typing import Dict from pprint import pprint In\u00a0[3]: Copied! <pre>text = \"Peter gave his book to Heidi which later gave it to Nicole. Peter lives in London and Nicole lives in Tashkent.\"\nprint(\"original text:\")\npprint(text)\nanalyzer = AnalyzerEngine()\nanalyzer_results = analyzer.analyze(text=text, language=\"en\")\nprint(\"analyzer results:\")\npprint(analyzer_results)\n</pre> text = \"Peter gave his book to Heidi which later gave it to Nicole. Peter lives in London and Nicole lives in Tashkent.\" print(\"original text:\") pprint(text) analyzer = AnalyzerEngine() analyzer_results = analyzer.analyze(text=text, language=\"en\") print(\"analyzer results:\") pprint(analyzer_results)  <pre>original text:\n('Peter gave his book to Heidi which later gave it to Nicole. Peter lives in '\n 'London and Nicole lives in Tashkent.')\nanalyzer results:\n[type: PERSON, start: 0, end: 5, score: 0.85,\n type: PERSON, start: 23, end: 28, score: 0.85,\n type: PERSON, start: 52, end: 58, score: 0.85,\n type: PERSON, start: 60, end: 65, score: 0.85,\n type: LOCATION, start: 75, end: 81, score: 0.85,\n type: PERSON, start: 86, end: 92, score: 0.85,\n type: LOCATION, start: 102, end: 110, score: 0.85]\n</pre> In\u00a0[4]: Copied! <pre>class InstanceCounterAnonymizer(Operator):\n    \"\"\"\n    Anonymizer which replaces the entity value\n    with an instance counter per entity.\n    \"\"\"\n\n    REPLACING_FORMAT = \"&lt;{entity_type}_{index}&gt;\"\n\n    def operate(self, text: str, params: Dict = None) -&gt; str:\n        \"\"\"Anonymize the input text.\"\"\"\n\n        entity_type: str = params[\"entity_type\"]\n\n        # entity_mapping is a dict of dicts containing mappings per entity type\n        entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]\n\n        entity_mapping_for_type = entity_mapping.get(entity_type)\n        if not entity_mapping_for_type:\n            new_text = self.REPLACING_FORMAT.format(\n                entity_type=entity_type, index=0\n            )\n            entity_mapping[entity_type] = {}\n\n        else:\n            if text in entity_mapping_for_type:\n                return entity_mapping_for_type[text]\n\n            previous_index = self._get_last_index(entity_mapping_for_type)\n            new_text = self.REPLACING_FORMAT.format(\n                entity_type=entity_type, index=previous_index + 1\n            )\n\n        entity_mapping[entity_type][text] = new_text\n        return new_text\n\n    @staticmethod\n    def _get_last_index(entity_mapping_for_type: Dict) -&gt; int:\n        \"\"\"Get the last index for a given entity type.\"\"\"\n        return len(entity_mapping_for_type)\n\n    def validate(self, params: Dict = None) -&gt; None:\n        \"\"\"Validate operator parameters.\"\"\"\n\n        if \"entity_mapping\" not in params:\n            raise ValueError(\"An input Dict called `entity_mapping` is required.\")\n        if \"entity_type\" not in params:\n            raise ValueError(\"An entity_type param is required.\")\n\n    def operator_name(self) -&gt; str:\n        return \"entity_counter\"\n\n    def operator_type(self) -&gt; OperatorType:\n        return OperatorType.Anonymize\n</pre> class InstanceCounterAnonymizer(Operator):     \"\"\"     Anonymizer which replaces the entity value     with an instance counter per entity.     \"\"\"      REPLACING_FORMAT = \"&lt;{entity_type}_{index}&gt;\"      def operate(self, text: str, params: Dict = None) -&gt; str:         \"\"\"Anonymize the input text.\"\"\"          entity_type: str = params[\"entity_type\"]          # entity_mapping is a dict of dicts containing mappings per entity type         entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]          entity_mapping_for_type = entity_mapping.get(entity_type)         if not entity_mapping_for_type:             new_text = self.REPLACING_FORMAT.format(                 entity_type=entity_type, index=0             )             entity_mapping[entity_type] = {}          else:             if text in entity_mapping_for_type:                 return entity_mapping_for_type[text]              previous_index = self._get_last_index(entity_mapping_for_type)             new_text = self.REPLACING_FORMAT.format(                 entity_type=entity_type, index=previous_index + 1             )          entity_mapping[entity_type][text] = new_text         return new_text      @staticmethod     def _get_last_index(entity_mapping_for_type: Dict) -&gt; int:         \"\"\"Get the last index for a given entity type.\"\"\"         return len(entity_mapping_for_type)      def validate(self, params: Dict = None) -&gt; None:         \"\"\"Validate operator parameters.\"\"\"          if \"entity_mapping\" not in params:             raise ValueError(\"An input Dict called `entity_mapping` is required.\")         if \"entity_type\" not in params:             raise ValueError(\"An entity_type param is required.\")      def operator_name(self) -&gt; str:         return \"entity_counter\"      def operator_type(self) -&gt; OperatorType:         return OperatorType.Anonymize In\u00a0[5]: Copied! <pre># Create Anonymizer engine and add the custom anonymizer\nanonymizer_engine = AnonymizerEngine()\nanonymizer_engine.add_anonymizer(InstanceCounterAnonymizer)\n\n# Create a mapping between entity types and counters\nentity_mapping = dict()\n\n# Anonymize the text\n\nanonymized_result = anonymizer_engine.anonymize(\n    text,\n    analyzer_results,\n    {\n        \"DEFAULT\": OperatorConfig(\n            \"entity_counter\", {\"entity_mapping\": entity_mapping}\n        )\n    },\n)\n\nprint(anonymized_result.text)\n</pre> # Create Anonymizer engine and add the custom anonymizer anonymizer_engine = AnonymizerEngine() anonymizer_engine.add_anonymizer(InstanceCounterAnonymizer)  # Create a mapping between entity types and counters entity_mapping = dict()  # Anonymize the text  anonymized_result = anonymizer_engine.anonymize(     text,     analyzer_results,     {         \"DEFAULT\": OperatorConfig(             \"entity_counter\", {\"entity_mapping\": entity_mapping}         )     }, )  print(anonymized_result.text)  <pre>&lt;PERSON_1&gt; gave his book to &lt;PERSON_2&gt; which later gave it to &lt;PERSON_0&gt;. &lt;PERSON_1&gt; lives in &lt;LOCATION_1&gt; and &lt;PERSON_0&gt; lives in &lt;LOCATION_0&gt;.\n</pre> <p>Note that the order is reversed due to the way entities are replaced in Presidio.</p> <p>Since the user/client is holding the entity_mapping, it is possible to use it for de-anonymization as well. First, let's look at its contents.</p> In\u00a0[6]: Copied! <pre>pprint(entity_mapping, indent=2)\n</pre> pprint(entity_mapping, indent=2) <pre>{ 'LOCATION': {'London': '&lt;LOCATION_1&gt;', 'Tashkent': '&lt;LOCATION_0&gt;'},\n  'PERSON': { 'Heidi': '&lt;PERSON_2&gt;',\n              'Nicole': '&lt;PERSON_0&gt;',\n              'Peter': '&lt;PERSON_1&gt;'}}\n</pre> In\u00a0[7]: Copied! <pre>class InstanceCounterDeanonymizer(Operator):\n    \"\"\"\n    Deanonymizer which replaces the unique identifier \n    with the original text.\n    \"\"\"\n\n    def operate(self, text: str, params: Dict = None) -&gt; str:\n        \"\"\"Anonymize the input text.\"\"\"\n\n        entity_type: str = params[\"entity_type\"]\n\n        # entity_mapping is a dict of dicts containing mappings per entity type\n        entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]\n\n        if entity_type not in entity_mapping:\n            raise ValueError(f\"Entity type {entity_type} not found in entity mapping!\")\n        if text not in entity_mapping[entity_type].values():\n            raise ValueError(f\"Text {text} not found in entity mapping for entity type {entity_type}!\")\n\n        return self._find_key_by_value(entity_mapping[entity_type], text)\n\n    @staticmethod\n    def _find_key_by_value(entity_mapping, value):\n        for key, val in entity_mapping.items():\n            if val == value:\n                return key\n        return None\n    \n    def validate(self, params: Dict = None) -&gt; None:\n        \"\"\"Validate operator parameters.\"\"\"\n\n        if \"entity_mapping\" not in params:\n            raise ValueError(\"An input Dict called `entity_mapping` is required.\")\n        if \"entity_type\" not in params:\n            raise ValueError(\"An entity_type param is required.\")\n\n    def operator_name(self) -&gt; str:\n        return \"entity_counter_deanonymizer\"\n\n    def operator_type(self) -&gt; OperatorType:\n        return OperatorType.Deanonymize\n</pre> class InstanceCounterDeanonymizer(Operator):     \"\"\"     Deanonymizer which replaces the unique identifier      with the original text.     \"\"\"      def operate(self, text: str, params: Dict = None) -&gt; str:         \"\"\"Anonymize the input text.\"\"\"          entity_type: str = params[\"entity_type\"]          # entity_mapping is a dict of dicts containing mappings per entity type         entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]          if entity_type not in entity_mapping:             raise ValueError(f\"Entity type {entity_type} not found in entity mapping!\")         if text not in entity_mapping[entity_type].values():             raise ValueError(f\"Text {text} not found in entity mapping for entity type {entity_type}!\")          return self._find_key_by_value(entity_mapping[entity_type], text)      @staticmethod     def _find_key_by_value(entity_mapping, value):         for key, val in entity_mapping.items():             if val == value:                 return key         return None          def validate(self, params: Dict = None) -&gt; None:         \"\"\"Validate operator parameters.\"\"\"          if \"entity_mapping\" not in params:             raise ValueError(\"An input Dict called `entity_mapping` is required.\")         if \"entity_type\" not in params:             raise ValueError(\"An entity_type param is required.\")      def operator_name(self) -&gt; str:         return \"entity_counter_deanonymizer\"      def operator_type(self) -&gt; OperatorType:         return OperatorType.Deanonymize  In\u00a0[8]: Copied! <pre>deanonymizer_engine = DeanonymizeEngine()\ndeanonymizer_engine.add_deanonymizer(InstanceCounterDeanonymizer)\n\ndeanonymized = deanonymizer_engine.deanonymize(\n    anonymized_result.text, \n    anonymized_result.items, \n    {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\", \n                               params={\"entity_mapping\": entity_mapping})}\n)\nprint(\"anonymized text:\")\npprint(anonymized_result.text)\nprint(\"de-anonymized text:\")\npprint(deanonymized.text)\n</pre> deanonymizer_engine = DeanonymizeEngine() deanonymizer_engine.add_deanonymizer(InstanceCounterDeanonymizer)  deanonymized = deanonymizer_engine.deanonymize(     anonymized_result.text,      anonymized_result.items,      {\"DEFAULT\": OperatorConfig(\"entity_counter_deanonymizer\",                                 params={\"entity_mapping\": entity_mapping})} ) print(\"anonymized text:\") pprint(anonymized_result.text) print(\"de-anonymized text:\") pprint(deanonymized.text) <pre>anonymized text:\n('&lt;PERSON_1&gt; gave his book to &lt;PERSON_2&gt; which later gave it to &lt;PERSON_0&gt;. '\n '&lt;PERSON_1&gt; lives in &lt;LOCATION_1&gt; and &lt;PERSON_0&gt; lives in &lt;LOCATION_0&gt;.')\nde-anonymized text:\n('Peter gave his book to Heidi which later gave it to Nicole. Peter lives in '\n 'London and Nicole lives in Tashkent.')\n</pre>"},{"location":"samples/python/pseudonymization/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonpseudonomyzationipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonomyzation.ipynb\u00b6","text":""},{"location":"samples/python/pseudonymization/#use-presidio-anonymizer-for-pseudonymization-of-pii-data","title":"Use Presidio Anonymizer for Pseudonymization of PII data\u00b6","text":"<p>Pseudonymization is a data management and de-identification procedure by which personally identifiable information fields within a data record are replaced by one or more artificial identifiers, or pseudonyms. (https://en.wikipedia.org/wiki/Pseudonymization)</p> <p>In this notebook, we'll show an example of how to use the Presidio Anonymizer library to pseudonymize PII data. In this example, we will replace each value with a unique identifier (e.g. &lt;PERSON_14&gt;). Then, we'll de-anonymize the data by replacing the unique identifiers back with their mapped PII values.</p>"},{"location":"samples/python/pseudonymization/#important-the-following-logic-is-not-thread-safe-and-may-produce-incorrect-results-if-run-concurrently-in-a-multi-threaded-environment-since-the-mapping-has-to-be-shared-between-threadsworkersprocesses","title":"Important: The following logic is not thread-safe and may produce incorrect results if run concurrently in a multi-threaded environment, since the mapping has to be shared between threads/workers/processes.\u00b6","text":""},{"location":"samples/python/pseudonymization/#1-using-the-analyzerengine-to-identify-pii-in-a-text","title":"1. Using the <code>AnalyzerEngine</code> to identify PII in a text\u00b6","text":""},{"location":"samples/python/pseudonymization/#2-creating-a-custom-anonymizer-called-operator-which-replaces-each-text-with-a-unique-identifier","title":"2. Creating a custom Anonymizer (called Operator) which replaces each text with a unique identifier.\u00b6","text":"<p>To create a custom anonymizer, we need to create a class that inherits from <code>Operator</code> and implement the <code>operate</code> method. This method receives the original text and a dictionary called <code>params</code> with the configuration defined by the user. The method should return the anonymized text.</p> <p>In this example we also implement the <code>validate</code> method to check that the input parameters are available, i.e. that the <code>entity_type</code> and <code>entity_mapping</code> parameters are defined, as they are required for this specific anonymizer. <code>entity_mapping</code> is a dictionary that maps each entity value to a unique identifier, for each entity type.</p>"},{"location":"samples/python/pseudonymization/#3-passing-the-new-operator-to-the-anonymizerengine-and-use-it-to-anonymize-the-text","title":"3. Passing the new operator to the <code>AnonymizerEngine</code> and use it to anonymize the text.\u00b6","text":""},{"location":"samples/python/pseudonymization/#4-de-anonymizing-the-text-using-the-entity_mapping","title":"4. De-anonymizing the text using the entity_mapping\u00b6","text":"<p>Similar to the anonymization operator, we need to create a custom de-anonymization operator. This operator will replace the unique identifiers with the original values.</p>"},{"location":"samples/python/simple_anonymization_example/","title":"Simple anonymization example","text":"In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\nfrom pprint import pprint\nimport json\n</pre> from presidio_analyzer import AnalyzerEngine from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import OperatorConfig from pprint import pprint import json In\u00a0[\u00a0]: Copied! <pre>text_to_anonymize = \"His name is Tom and his phone number is 212-555-5555\"\n</pre> text_to_anonymize = \"His name is Tom and his phone number is 212-555-5555\" In\u00a0[\u00a0]: Copied! <pre>analyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n</pre> analyzer = AnalyzerEngine() anonymizer = AnonymizerEngine() In\u00a0[\u00a0]: Copied! <pre>analyzer_results = analyzer.analyze(text=text_to_anonymize, language=\"en\")\nprint(\"PII Detection:\")\nprint(analyzer_results)\n</pre> analyzer_results = analyzer.analyze(text=text_to_anonymize, language=\"en\") print(\"PII Detection:\") print(analyzer_results) In\u00a0[\u00a0]: Copied! <pre>anonymized_results = anonymizer.anonymize(\n    text=text_to_anonymize,\n    analyzer_results=analyzer_results,\n    operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"&lt;ANONYMIZED&gt;\"})},\n)\nprint(\"\\nPII Anonymization:\")\npprint(json.loads(anonymized_results.to_json()))\n</pre> anonymized_results = anonymizer.anonymize(     text=text_to_anonymize,     analyzer_results=analyzer_results,     operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"\"})}, ) print(\"\\nPII Anonymization:\") pprint(json.loads(anonymized_results.to_json()))"},{"location":"samples/python/span_marker_recognizer/","title":"Span marker recognizer","text":"In\u00a0[\u00a0]: Copied! <pre>import logging\nfrom typing import Optional, List, Dict\n</pre> import logging from typing import Optional, List, Dict In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import (\n    RecognizerResult,\n    EntityRecognizer,\n    AnalysisExplanation,\n)\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n</pre> from presidio_analyzer import (     RecognizerResult,     EntityRecognizer,     AnalysisExplanation, ) from presidio_analyzer.nlp_engine import NlpArtifacts In\u00a0[\u00a0]: Copied! <pre>try:\n    from span_marker import SpanMarkerModel\nexcept ImportError:\n    print(\"Span Marker is not installed\")\n</pre> try:     from span_marker import SpanMarkerModel except ImportError:     print(\"Span Marker is not installed\") In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(\"presidio-analyzer\")\n</pre> logger = logging.getLogger(\"presidio-analyzer\") In\u00a0[\u00a0]: Copied! <pre>class SpanMarkerRecognizer(EntityRecognizer):\n    \"\"\"\n    Wrapper for a span marker models, if needed to be used within Presidio Analyzer.\n    :param supported_language: The language supported by the model,\n    default is set to English (en).\n    :param model: A string referencing a Span Marker model name or path.\n    :param supported_entities: A list of entities supported by Presidio.\n    :param presidio_equivalences: Mapping of model-defined entities with\n    Presidio-supported entities.\n    :param ignore_labels: A list of entities specified by the model that\n    should not be extracted.\n\n    :example:\n    &gt;from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\n    &gt;span_marker_recognizer = SpanMarkerRecognizer()\n\n    &gt;registry = RecognizerRegistry()\n    &gt;registry.add_recognizer(span_marker_recognizer)\n\n    &gt;analyzer = AnalyzerEngine(registry=registry)\n\n    &gt;results = analyzer.analyze(\n    &gt;    \"My name is Vijay and I live in Pune.\",\n    &gt;    language=\"en\",\n    &gt;    return_decision_process=True,\n    &gt;)\n    &gt;for result in results:\n    &gt;    print(result)\n    &gt;    print(result.analysis_explanation)\n\n\n    \"\"\"\n\n    ENTITIES = [\n        \"PERSON\",\n        \"LOCATION\",\n        \"ORGANIZATION\",\n        # \"MISCELLANEOUS\"   # - There are no direct correlation with Presidio entities.\n    ]\n\n    DEFAULT_MODEL = \"tomaarsen/span-marker-bert-base-fewnerd-fine-super\"\n\n    DEFAULT_EXPLANATION = \"Identified as {} by Span Marker's Named Entity Recognition\"\n\n    PRESIDIO_EQUIVALENCES = {\n        \"person-other\": \"PERSON\",\n        \"location-GPE\": \"LOCATION\",\n        \"organization-company\": \"ORGANIZATION\",\n        # 'MISC': 'MISCELLANEOUS'   # - Probably not PII\n    }\n\n    IGNORE_LABELS = [\"O\"]\n\n    def __init__(\n        self,\n        supported_language: str = \"en\",\n        model: str = None,\n        supported_entities: Optional[List[str]] = None,\n        presidio_equivalences: Optional[Dict[str, str]] = None,\n        ignore_labels: Optional[List[str]] = None,\n    ):\n        self.model = (\n            model\n            if model\n            else self.DEFAULT_MODEL\n        )\n\n        self.presidio_equivalences = (\n            presidio_equivalences\n            if presidio_equivalences\n            else self.PRESIDIO_EQUIVALENCES\n        )\n\n        supported_entities = (\n            supported_entities if supported_entities else self.ENTITIES\n        )\n\n        self.ignore_labels = (\n            ignore_labels if ignore_labels else self.IGNORE_LABELS\n        )\n\n        labels = list(self.presidio_equivalences.keys())\n        self.span_marker_model = SpanMarkerModel.from_pretrained(\n            self.model,\n            labels=labels\n        )\n\n        super().__init__(\n            supported_entities=supported_entities,\n            supported_language=supported_language,\n            name=\"Span Marker Analytics\",\n        )\n\n    def load(self) -&gt; None:\n        \"\"\"Load the model, not used. Model is loaded during initialization.\"\"\"\n        pass\n\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"\n        Return supported entities by this model.\n\n        :return: List of the supported entities.\n        \"\"\"\n        return self.supported_entities\n\n    # Class to use Span Marker with Presidio as an external recognizer.\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyze text using Text Analytics.\n\n        :param text: The text for analysis.\n        :param entities: Not working properly for this recognizer.\n        :param nlp_artifacts: Not used by this recognizer.\n        :return: The list of Presidio RecognizerResult constructed from the recognized\n            Span Marker detections.\n        \"\"\"\n\n        results = []\n        ner_res = self.span_marker_model.predict(text)\n\n        for res in ner_res:\n            if not self.__check_label(\n                res['label']\n            ):\n                continue\n            textual_explanation = self.DEFAULT_EXPLANATION.format(\n                res['label']\n            )\n            explanation = self.build_span_marker_explanation(\n                round(res['score'], 2), textual_explanation\n            )\n            span_marker_result = self._convert_to_recognizer_result(res, explanation)\n            results.append(span_marker_result)\n\n        return results\n\n    def _convert_to_recognizer_result(self, entity, explanation) -&gt; RecognizerResult:\n\n        entity_type = self.presidio_equivalences.get(entity['label'], entity['label'])\n        span_marker_score = round(entity['score'], 2)\n\n        span_marker_results = RecognizerResult(\n            entity_type=entity_type,\n            start=entity['char_start_index'],\n            end=entity['char_end_index'],\n            score=span_marker_score,\n            analysis_explanation=explanation,\n        )\n\n        return span_marker_results\n\n    def build_span_marker_explanation(\n        self, original_score: float, explanation: str\n    ) -&gt; AnalysisExplanation:\n        \"\"\"\n        Create explanation for why this result was detected.\n\n        :param original_score: Score given by this recognizer\n        :param explanation: Explanation string\n        :return:\n        \"\"\"\n        explanation = AnalysisExplanation(\n            recognizer=self.__class__.__name__,\n            original_score=original_score,\n            textual_explanation=explanation,\n        )\n        return explanation\n\n    def __check_label(\n        self, label: str\n    ) -&gt; bool:\n        entity = self.presidio_equivalences.get(label, None)\n\n        if entity in self.ignore_labels:\n            return None\n\n        if entity is None:\n            logger.warning(f\"Found unrecognized label {label}, returning entity as is\")\n            return label\n\n        if entity not in self.supported_entities:\n            logger.warning(f\"Found entity {entity} which is not supported by Presidio\")\n            return entity\n        return entity\n</pre> class SpanMarkerRecognizer(EntityRecognizer):     \"\"\"     Wrapper for a span marker models, if needed to be used within Presidio Analyzer.     :param supported_language: The language supported by the model,     default is set to English (en).     :param model: A string referencing a Span Marker model name or path.     :param supported_entities: A list of entities supported by Presidio.     :param presidio_equivalences: Mapping of model-defined entities with     Presidio-supported entities.     :param ignore_labels: A list of entities specified by the model that     should not be extracted.      :example:     &gt;from presidio_analyzer import AnalyzerEngine, RecognizerRegistry      &gt;span_marker_recognizer = SpanMarkerRecognizer()      &gt;registry = RecognizerRegistry()     &gt;registry.add_recognizer(span_marker_recognizer)      &gt;analyzer = AnalyzerEngine(registry=registry)      &gt;results = analyzer.analyze(     &gt;    \"My name is Vijay and I live in Pune.\",     &gt;    language=\"en\",     &gt;    return_decision_process=True,     &gt;)     &gt;for result in results:     &gt;    print(result)     &gt;    print(result.analysis_explanation)       \"\"\"      ENTITIES = [         \"PERSON\",         \"LOCATION\",         \"ORGANIZATION\",         # \"MISCELLANEOUS\"   # - There are no direct correlation with Presidio entities.     ]      DEFAULT_MODEL = \"tomaarsen/span-marker-bert-base-fewnerd-fine-super\"      DEFAULT_EXPLANATION = \"Identified as {} by Span Marker's Named Entity Recognition\"      PRESIDIO_EQUIVALENCES = {         \"person-other\": \"PERSON\",         \"location-GPE\": \"LOCATION\",         \"organization-company\": \"ORGANIZATION\",         # 'MISC': 'MISCELLANEOUS'   # - Probably not PII     }      IGNORE_LABELS = [\"O\"]      def __init__(         self,         supported_language: str = \"en\",         model: str = None,         supported_entities: Optional[List[str]] = None,         presidio_equivalences: Optional[Dict[str, str]] = None,         ignore_labels: Optional[List[str]] = None,     ):         self.model = (             model             if model             else self.DEFAULT_MODEL         )          self.presidio_equivalences = (             presidio_equivalences             if presidio_equivalences             else self.PRESIDIO_EQUIVALENCES         )          supported_entities = (             supported_entities if supported_entities else self.ENTITIES         )          self.ignore_labels = (             ignore_labels if ignore_labels else self.IGNORE_LABELS         )          labels = list(self.presidio_equivalences.keys())         self.span_marker_model = SpanMarkerModel.from_pretrained(             self.model,             labels=labels         )          super().__init__(             supported_entities=supported_entities,             supported_language=supported_language,             name=\"Span Marker Analytics\",         )      def load(self) -&gt; None:         \"\"\"Load the model, not used. Model is loaded during initialization.\"\"\"         pass      def get_supported_entities(self) -&gt; List[str]:         \"\"\"         Return supported entities by this model.          :return: List of the supported entities.         \"\"\"         return self.supported_entities      # Class to use Span Marker with Presidio as an external recognizer.     def analyze(         self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None     ) -&gt; List[RecognizerResult]:         \"\"\"         Analyze text using Text Analytics.          :param text: The text for analysis.         :param entities: Not working properly for this recognizer.         :param nlp_artifacts: Not used by this recognizer.         :return: The list of Presidio RecognizerResult constructed from the recognized             Span Marker detections.         \"\"\"          results = []         ner_res = self.span_marker_model.predict(text)          for res in ner_res:             if not self.__check_label(                 res['label']             ):                 continue             textual_explanation = self.DEFAULT_EXPLANATION.format(                 res['label']             )             explanation = self.build_span_marker_explanation(                 round(res['score'], 2), textual_explanation             )             span_marker_result = self._convert_to_recognizer_result(res, explanation)             results.append(span_marker_result)          return results      def _convert_to_recognizer_result(self, entity, explanation) -&gt; RecognizerResult:          entity_type = self.presidio_equivalences.get(entity['label'], entity['label'])         span_marker_score = round(entity['score'], 2)          span_marker_results = RecognizerResult(             entity_type=entity_type,             start=entity['char_start_index'],             end=entity['char_end_index'],             score=span_marker_score,             analysis_explanation=explanation,         )          return span_marker_results      def build_span_marker_explanation(         self, original_score: float, explanation: str     ) -&gt; AnalysisExplanation:         \"\"\"         Create explanation for why this result was detected.          :param original_score: Score given by this recognizer         :param explanation: Explanation string         :return:         \"\"\"         explanation = AnalysisExplanation(             recognizer=self.__class__.__name__,             original_score=original_score,             textual_explanation=explanation,         )         return explanation      def __check_label(         self, label: str     ) -&gt; bool:         entity = self.presidio_equivalences.get(label, None)          if entity in self.ignore_labels:             return None          if entity is None:             logger.warning(f\"Found unrecognized label {label}, returning entity as is\")             return label          if entity not in self.supported_entities:             logger.warning(f\"Found entity {entity} which is not supported by Presidio\")             return entity         return entity In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n\n    from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\n    span_marker_recognizer = (\n        SpanMarkerRecognizer()\n    )\n\n    registry = RecognizerRegistry()\n    registry.add_recognizer(span_marker_recognizer)\n\n    analyzer = AnalyzerEngine(registry=registry)\n\n    results = analyzer.analyze(\n        \"My name is Vijay and I live in Pune.\",\n        language=\"en\",\n        return_decision_process=True,\n    )\n    for result in results:\n        print(result)\n        print(result.analysis_explanation)\n</pre> if __name__ == \"__main__\":      from presidio_analyzer import AnalyzerEngine, RecognizerRegistry      span_marker_recognizer = (         SpanMarkerRecognizer()     )      registry = RecognizerRegistry()     registry.add_recognizer(span_marker_recognizer)      analyzer = AnalyzerEngine(registry=registry)      results = analyzer.analyze(         \"My name is Vijay and I live in Pune.\",         language=\"en\",         return_decision_process=True,     )     for result in results:         print(result)         print(result.analysis_explanation)"},{"location":"samples/python/synth_data_with_openai/","title":"Synthetic data generation with OpenAI","text":"In\u00a0[\u00a0]: Copied! <pre># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!pip install openai pandas\n!python -m spacy download en_core_web_lg\n</pre> # download presidio !pip install presidio_analyzer presidio_anonymizer !pip install openai pandas !python -m spacy download en_core_web_lg In\u00a0[3]: Copied! <pre>import pprint\nfrom dotenv import load_dotenv\nimport os\nimport pandas as pd\nfrom openai import OpenAI\n\nload_dotenv()\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n#Or put explicitly in notebook. Find out more here: https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key\n</pre> import pprint from dotenv import load_dotenv import os import pandas as pd from openai import OpenAI  load_dotenv()  client = OpenAI(     api_key=os.environ.get(\"OPENAI_API_KEY\"), ) #Or put explicitly in notebook. Find out more here: https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key In\u00a0[4]: Copied! <pre>def call_completion_model(prompt:str, model:str=\"gpt-3.5-turbo\", max_tokens:int=512) -&gt;str:\n    \"\"\"Creates a request for the OpenAI Completion service and returns the response.\n    \n    :param prompt: The prompt for the completion model\n    :param model: OpenAI model name\n    :param max_tokens: Model's max tokens parameter\n    \"\"\"\n\n    completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": prompt,\n        }\n    ],\n    model=model,\n)\n\n    return completion.choices[0].message.content\n</pre> def call_completion_model(prompt:str, model:str=\"gpt-3.5-turbo\", max_tokens:int=512) -&gt;str:     \"\"\"Creates a request for the OpenAI Completion service and returns the response.          :param prompt: The prompt for the completion model     :param model: OpenAI model name     :param max_tokens: Model's max tokens parameter     \"\"\"      completion = client.chat.completions.create(     messages=[         {             \"role\": \"user\",             \"content\": prompt,         }     ],     model=model, )      return completion.choices[0].message.content In\u00a0[5]: Copied! <pre>from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\nanalyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n\nsample = \"\"\"\nHello, my name is David Johnson and I live in Maine.\nMy credit card number is 4095-2609-9393-4932 and my crypto wallet id is 16Yeky6GMjeNkAiNcBY7ZhrLoMSgg1BoyZ.\n\nOn September 18 I visited microsoft.com and sent an email to test@presidio.site,  from the IP 192.168.0.1.\n\nMy passport: 191280342 and my phone number: (212) 555-1234.\n\nThis is a valid International Bank Account Number: IL150120690000003111111 . Can you please check the status on bank account 954567876544?\n\nKate's social security number is 078-05-1126.  Her driver license? it is 1234567A.\n\"\"\"\n\nresults = analyzer.analyze(sample, language=\"en\")\nanonymized = anonymizer.anonymize(text=sample, analyzer_results=results)\nanonymized_text = anonymized.text\nprint(anonymized_text)\n</pre> from presidio_analyzer import AnalyzerEngine from presidio_anonymizer import AnonymizerEngine  analyzer = AnalyzerEngine() anonymizer = AnonymizerEngine()  sample = \"\"\" Hello, my name is David Johnson and I live in Maine. My credit card number is 4095-2609-9393-4932 and my crypto wallet id is 16Yeky6GMjeNkAiNcBY7ZhrLoMSgg1BoyZ.  On September 18 I visited microsoft.com and sent an email to test@presidio.site,  from the IP 192.168.0.1.  My passport: 191280342 and my phone number: (212) 555-1234.  This is a valid International Bank Account Number: IL150120690000003111111 . Can you please check the status on bank account 954567876544?  Kate's social security number is 078-05-1126.  Her driver license? it is 1234567A. \"\"\"  results = analyzer.analyze(sample, language=\"en\") anonymized = anonymizer.anonymize(text=sample, analyzer_results=results) anonymized_text = anonymized.text print(anonymized_text)  <pre>\nHello, my name is &lt;PERSON&gt; and I live in &lt;LOCATION&gt;.\nMy credit card number is &lt;CREDIT_CARD&gt; and my crypto wallet id is &lt;CRYPTO&gt;.\n\nOn &lt;DATE_TIME&gt; I visited &lt;URL&gt; and sent an email to &lt;EMAIL_ADDRESS&gt;,  from the IP &lt;IP_ADDRESS&gt;.\n\nMy passport: &lt;US_PASSPORT&gt; and my phone number: &lt;PHONE_NUMBER&gt;.\n\nThis is a valid International Bank Account Number: &lt;IBAN_CODE&gt; . Can you please check the status on bank account &lt;US_BANK_NUMBER&gt;?\n\n&lt;PERSON&gt;'s social security number is &lt;US_SSN&gt;.  Her driver license? it is &lt;US_DRIVER_LICENSE&gt;.\n\n</pre> In\u00a0[6]: Copied! <pre>def create_prompt(anonymized_text: str) -&gt; str:\n    \"\"\"\n    Create the prompt with instructions to GPT-3.\n    \n    :param anonymized_text: Text with placeholders instead of PII values, e.g. My name is &lt;PERSON&gt;.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Your role is to create synthetic text based on de-identified text with placeholders instead of Personally Identifiable Information (PII).\n    Replace the placeholders (e.g. ,&lt;PERSON&gt;, {{DATE}}, {{ip_address}}) with fake values.\n    Instructions:\n    a. Use completely random numbers, so every digit is drawn between 0 and 9.\n    b. Use realistic names that come from diverse genders, ethnicities and countries.\n    c. If there are no placeholders, return the text as is.\n    d. Keep the formatting as close to the original as possible.\n    e. If PII exists in the input, replace it with fake values in the output.\n    f. Remove whitespace before and after the generated text\n    \n    input: [[TEXT STARTS]] How do I change the limit on my credit card {{credit_card_number}}?[[TEXT ENDS]]\n    output: How do I change the limit on my credit card 2539 3519 2345 1555?\n    input: [[TEXT STARTS]]&lt;PERSON&gt; was the chief science officer at &lt;ORGANIZATION&gt;.[[TEXT ENDS]]\n    output: Katherine Buckjov was the chief science officer at NASA.\n    input: [[TEXT STARTS]]Cameroon lives in &lt;LOCATION&gt;.[[TEXT ENDS]]\n    output: Vladimir lives in Moscow.\n    \n    input: [[TEXT STARTS]]{anonymized_text}[[TEXT ENDS]]\n    output:\"\"\"\n    return prompt\n</pre> def create_prompt(anonymized_text: str) -&gt; str:     \"\"\"     Create the prompt with instructions to GPT-3.          :param anonymized_text: Text with placeholders instead of PII values, e.g. My name is .     \"\"\"      prompt = f\"\"\"     Your role is to create synthetic text based on de-identified text with placeholders instead of Personally Identifiable Information (PII).     Replace the placeholders (e.g. ,, {{DATE}}, {{ip_address}}) with fake values.     Instructions:     a. Use completely random numbers, so every digit is drawn between 0 and 9.     b. Use realistic names that come from diverse genders, ethnicities and countries.     c. If there are no placeholders, return the text as is.     d. Keep the formatting as close to the original as possible.     e. If PII exists in the input, replace it with fake values in the output.     f. Remove whitespace before and after the generated text          input: [[TEXT STARTS]] How do I change the limit on my credit card {{credit_card_number}}?[[TEXT ENDS]]     output: How do I change the limit on my credit card 2539 3519 2345 1555?     input: [[TEXT STARTS]] was the chief science officer at .[[TEXT ENDS]]     output: Katherine Buckjov was the chief science officer at NASA.     input: [[TEXT STARTS]]Cameroon lives in .[[TEXT ENDS]]     output: Vladimir lives in Moscow.          input: [[TEXT STARTS]]{anonymized_text}[[TEXT ENDS]]     output:\"\"\"     return prompt In\u00a0[7]: Copied! <pre>print(\"This is the prompt with de-identified values:\")\nprint(create_prompt(anonymized_text))\n</pre> print(\"This is the prompt with de-identified values:\") print(create_prompt(anonymized_text)) <pre>This is the prompt with de-identified values:\n\n    Your role is to create synthetic text based on de-identified text with placeholders instead of Personally Identifiable Information (PII).\n    Replace the placeholders (e.g. ,&lt;PERSON&gt;, {DATE}, {ip_address}) with fake values.\n    Instructions:\n    a. Use completely random numbers, so every digit is drawn between 0 and 9.\n    b. Use realistic names that come from diverse genders, ethnicities and countries.\n    c. If there are no placeholders, return the text as is.\n    d. Keep the formatting as close to the original as possible.\n    e. If PII exists in the input, replace it with fake values in the output.\n    f. Remove whitespace before and after the generated text\n    \n    input: [[TEXT STARTS]] How do I change the limit on my credit card {credit_card_number}?[[TEXT ENDS]]\n    output: How do I change the limit on my credit card 2539 3519 2345 1555?\n    input: [[TEXT STARTS]]&lt;PERSON&gt; was the chief science officer at &lt;ORGANIZATION&gt;.[[TEXT ENDS]]\n    output: Katherine Buckjov was the chief science officer at NASA.\n    input: [[TEXT STARTS]]Cameroon lives in &lt;LOCATION&gt;.[[TEXT ENDS]]\n    output: Vladimir lives in Moscow.\n    \n    input: [[TEXT STARTS]]\nHello, my name is &lt;PERSON&gt; and I live in &lt;LOCATION&gt;.\nMy credit card number is &lt;CREDIT_CARD&gt; and my crypto wallet id is &lt;CRYPTO&gt;.\n\nOn &lt;DATE_TIME&gt; I visited &lt;URL&gt; and sent an email to &lt;EMAIL_ADDRESS&gt;,  from the IP &lt;IP_ADDRESS&gt;.\n\nMy passport: &lt;US_PASSPORT&gt; and my phone number: &lt;PHONE_NUMBER&gt;.\n\nThis is a valid International Bank Account Number: &lt;IBAN_CODE&gt; . Can you please check the status on bank account &lt;US_BANK_NUMBER&gt;?\n\n&lt;PERSON&gt;'s social security number is &lt;US_SSN&gt;.  Her driver license? it is &lt;US_DRIVER_LICENSE&gt;.\n[[TEXT ENDS]]\n    output:\n</pre> In\u00a0[8]: Copied! <pre>gpt_res = call_completion_model(create_prompt(anonymized_text))\n</pre> gpt_res = call_completion_model(create_prompt(anonymized_text)) In\u00a0[9]: Copied! <pre>print(gpt_res)\n</pre> print(gpt_res) <pre>Hello, my name is Aaliyah and I live in Tokyo.\nMy credit card number is 4928 7562 1034 8907 and my crypto wallet id is 0x3B 7a 5f 1C.\n\nOn 02/07/2023 15:45 I visited www.example.com and sent an email to example@email.com,  from the IP 127.0.0.1.\n\nMy passport: L921483B and my phone number: +1 (555) 123-4567.\n\nThis is a valid International Bank Account Number: FR76 1234 5789 1256 3321 7564 901. Can you please check the status on bank account 987654321?\n\nEliana's social security number is 123-45-6789.  Her driver license? it is DL12345678.\n</pre> In\u00a0[10]: Copied! <pre>import urllib\n\ntemplates = []\n\nurl = \"https://raw.githubusercontent.com/microsoft/presidio-research/master/presidio_evaluator/data_generator/raw_data/templates.txt\"\nfor line in urllib.request.urlopen(url):\n    templates.append(line.decode('utf-8')) \n</pre> import urllib  templates = []  url = \"https://raw.githubusercontent.com/microsoft/presidio-research/master/presidio_evaluator/data_generator/raw_data/templates.txt\" for line in urllib.request.urlopen(url):     templates.append(line.decode('utf-8'))  In\u00a0[11]: Copied! <pre>print(\"Example templates:\")\ntemplates[:5]\n</pre> print(\"Example templates:\") templates[:5] <pre>Example templates:\n</pre> Out[11]: <pre>['I want to increase limit on my card # {{credit_card_number}} for certain duration of time. is it possible?\\n',\n 'My credit card {{credit_card_number}} has been lost, Can I request you to block it.\\n',\n 'Need to change billing date of my card {{credit_card_number}}\\n',\n 'I want to update my primary and secondary address to the same: {{address}}\\n',\n \"In case of my child's account, we need to add {{person}} as guardian\\n\"]</pre> In\u00a0[12]: Copied! <pre>templates_to_use = templates[:5]\n\n\nimport time\npp = pprint.PrettyPrinter(indent=2, width=110)\nsentences = []\nfor template in templates_to_use:\n    synth_sentence = call_completion_model(create_prompt(template))\n    sentence_dict = {\"original\": template, \"synthetic\":synth_sentence.strip()}\n    sentences.append(sentence_dict)\n    pp.pprint(sentence_dict)\n    time.sleep(3) # wait to not get blocked by service (only applicable for the free tier)\n    print(\"--------------\")\n</pre> templates_to_use = templates[:5]   import time pp = pprint.PrettyPrinter(indent=2, width=110) sentences = [] for template in templates_to_use:     synth_sentence = call_completion_model(create_prompt(template))     sentence_dict = {\"original\": template, \"synthetic\":synth_sentence.strip()}     sentences.append(sentence_dict)     pp.pprint(sentence_dict)     time.sleep(3) # wait to not get blocked by service (only applicable for the free tier)     print(\"--------------\")  <pre>{ 'original': 'I want to increase limit on my card # {{credit_card_number}} for certain duration of time. is '\n              'it possible?\\n',\n  'synthetic': 'I want to increase limit on my card # 4701 2895 7462 8306 for certain duration of time. is '\n               'it possible?'}\n--------------\n{ 'original': 'My credit card {{credit_card_number}} has been lost, Can I request you to block it.\\n',\n  'synthetic': 'My credit card 4892 7634 1023 8756 has been lost, Can I request you to block it.'}\n--------------\n{ 'original': 'Need to change billing date of my card {{credit_card_number}}\\n',\n  'synthetic': 'Need to change billing date of my card 4876 2035 6981 7423'}\n--------------\n{ 'original': 'I want to update my primary and secondary address to the same: {{address}}\\n',\n  'synthetic': 'I want to update my primary and secondary address to the same: 123 Main Street, Apt 4.'}\n--------------\n{ 'original': \"In case of my child's account, we need to add {{person}} as guardian\\n\",\n  'synthetic': \"In case of my child's account, we need to add Abdul as guardian\"}\n--------------\n</pre> <pre>--------------\n{ 'original': '{{name}} lives at {{building_number}} {{street_name}}, {{city}}\\n',\n  'synthetic': 'John Smith lives at 635 Poplar Street, Houston'}\n--------------\n{ 'original': '{{first_name_male}} had given {{first_name}} his address: {{building_number}} '\n              '{{street_name}}\\n',\n  'synthetic': 'Adam had given Sarah his address: 44 Apple Street'}\n--------------\n{ 'original': '{{first_name_male}} had given {{first_name}} his address: {{building_number}} '\n              '{{street_name}}, {{city}}\\n',\n  'synthetic': 'David had given Emma his address: 515 Elm Street, Camden.'}\n--------------\n{ 'original': 'What is your address? it is {{address}}\\n',\n  'synthetic': 'What is your address? it is 3498 Allensby Street, Los Angeles, CA 90011.'}\n--------------\n{'original': 'We moved here from {{city}}\\n', 'synthetic': 'We moved here from Paris.'}\n--------------\n{'original': 'We moved here from {{country}}\\n', 'synthetic': 'We moved here from Venezuela.'}\n--------------\n{ 'original': '{{person}}\\\\n\\\\n{{building_number}} {{street_name}}\\\\n {{secondary_address}}\\\\n {{city}}\\\\n '\n              '{{country}} {{postcode}}\\\\n{{phone_number}}-Office\\\\,{{phone_number}}-Fax\\n',\n  'synthetic': 'Jessica Thompson\\n'\n               '    8745 West Drive\\n'\n               '    Suite 1402\\n'\n               '    Brooklyn, NY USA 12009\\n'\n               '    789-534-9921-Office, 567-945-0023-Fax'}\n--------------\n{ 'original': '{{person}}\\\\n{{job}}\\\\n{{organization}}\\\\n{{address}}\\n',\n  'synthetic': 'John Smith\\\\nAccountant\\\\nGlobalTech Solutions\\\\n25 Speedwell Street, Richmond, VA 23223'}\n--------------\n{ 'original': 'Our offices are located at {{address}}\\n',\n  'synthetic': 'Our offices are located at 1234 Main St, Los Angeles, CA 91234.'}\n--------------\n{ 'original': 'Please return to {{address}} in case of an issue.\\n',\n  'synthetic': 'Please return to 123 Cherry Street, El Monte, CA 91731 in case of an issue.'}\n--------------\n{ 'original': '{{organization}}\\\\n\\\\n{{address}}\\n',\n  'synthetic': 'ABC Inc.\\n     1234 Main Street, Anytown, ST 12345'}\n--------------\n{ 'original': 'The {{organization}} office is at {{address}}\\n',\n  'synthetic': 'The ABC Corporation office is at 123 Redwood Street, Anytown, USA.'}\n--------------\n{ 'original': '{{name}}\\\\n{{organization}}\\\\n{{address}}\\\\n{{phone_number}} office\\\\n{{phone_number}} '\n              'fax\\\\n{{phone_number}} mobile\\\\n\\n',\n  'synthetic': 'Larry Fernandez\\\\nStar Enterprises\\\\n421 5th Avenue, Los Angeles, CA 90012\\\\n123-456-7890 '\n               'office\\\\n456-789-0123 fax\\\\n256-454-2397 mobile\\\\n'}\n--------------\n{ 'original': '{{name}}\\\\n{{organization}}\\\\n{{address}}\\\\nMobile: {{phone_number}}\\\\nDesk: '\n              '{{phone_number}}\\\\nFax: {{phone_number}}\\\\n\\n',\n  'synthetic': 'John Brown\\n'\n               '    ABC Consulting\\n'\n               '    5th St., Suite 116, LA CA 90004\\n'\n               '    Mobile: 213-294-4497\\n'\n               '    Desk: 424-348-1275\\n'\n               '    Fax: 323-456-2545'}\n--------------\n{ 'original': 'Billing address: {{name}}\\\\n    {{building_number}} {{street_name}} '\n              '{{secondary_address}}\\\\n   {{city}}\\\\n    {{state_abbr}}\\\\n    {{zipcode}}\\\\n\\n',\n  'synthetic': 'Billing address: Mariam Rajput\\n'\n               '    576 Broadway Street Apartment A8\\n'\n               '   Sacramento\\n'\n               '    CA\\n'\n               '    95349'}\n--------------\n{ 'original': \"As promised, here's {{first_name}}'s address:\\\\n\\\\n{{address}}\\n\",\n  'synthetic': \"As promised, here's Aamir's address:\\\\n\\\\n2166 Sesame Street, Fortaleza, Cear\u00e1, Brazil.\"}\n--------------\n{ 'original': '&gt;{{name}}\\\\n&gt;{{organization}}\\\\n&gt;{{person}}\\\\n&gt;{{building_number}} '\n              '{{street_name}}\\\\n&gt;{{secondary_address}}\\\\n&gt;{{city}}\\\\n&gt;{{country}} {{postcode}}\\n',\n  'synthetic': '&gt;Freda Chen\\n'\n               '&gt;Example Inc.\\n'\n               '&gt;John Doe\\n'\n               '&gt;50 Main Street\\n'\n               '&gt;Apt. 2\\n'\n               '&gt;New York\\n'\n               '&gt;United States 10005'}\n--------------\n{ 'original': '??? {{name}}\\\\n??? {{organization}}\\\\n??? {{building_number}} {{street_name}}\\\\n??? '\n              '{{secondary_address}}\\\\n??? {{city}}\\\\n??? {{country}} {{postcode}}\\n',\n  'synthetic': 'John Smith\\n'\n               '     ABC Corporation\\n'\n               '     192 Main Street\\n'\n               '     Suite 100\\n'\n               '     Austin\\n'\n               '     United States 78701'}\n--------------\n{ 'original': '&gt; \\\\n&gt; {{name}}\\\\n&gt; {{organization}}\\\\n&gt; {{person}}\\\\n&gt; {{building_number}} '\n              '{{street_name}}\\\\n&gt; {{secondary_address}}\\\\n&gt; {{city}}\\\\n&gt; {{country}} {{postcode}}\\n',\n  'synthetic': '&gt; John Doe\\n'\n               '    &gt; ABC Corp\\n'\n               '    &gt; Jane Smith\\n'\n               '    &gt; 123 Main Street\\n'\n               '    &gt; APT 8\\n'\n               '    &gt; San Francisco\\n'\n               '    &gt; United States 12345'}\n--------------\n{ 'original': 'Pedestrians must enter on {{street_name}} St. the first three months\\n',\n  'synthetic': 'Pedestrians must enter on Jericho Avenue St. the first three months'}\n--------------\n{ 'original': 'When: {{date_time}}\\\\nWhere: {{city}} Country Club.\\n',\n  'synthetic': 'When: 05/01/2020 10:00am\\\\nWhere: Richmond Country Club.'}\n--------------\n{ 'original': \"We'll meet {{day_of_week}} at {{organization}}, {{building_number}} {{street_name}}, \"\n              '{{city}}\\n',\n  'synthetic': \"We'll meet Monday at Smartdel Solutions, 145 King Street, San Diego.\"}\n--------------\n{ 'original': 'They had 6: {{first_name}}, {{first_name}}, {{first_name}}, {{first_name}}, {{first_name}} '\n              'and {{first_name}}.\\n',\n  'synthetic': 'They had 6: Sarah, Micheal, Kanak, Hana, Mei and Dan.'}\n--------------\n{'original': 'She moved here from {{country}}\\n', 'synthetic': 'She moved here from Mexico.'}\n--------------\n{'original': 'My zip code is {{zipcode}}\\n', 'synthetic': 'My zip code is 47713.'}\n--------------\n{'original': 'ZIP: {{zipcode}}\\n', 'synthetic': 'ZIP: 08547'}\n--------------\n{'original': 'The bus station is on {{street_name}}\\n', 'synthetic': 'The bus station is on Wilson Avenue.'}\n--------------\n{ 'original': \"They're not answering at {{phone_number}}\\n\",\n  'synthetic': \"They're not answering at 654-339-1013.\"}\n--------------\n{ 'original': 'God gave rock and roll to you, gave rock and roll to you, put it in the soul of everyone.\\n',\n  'synthetic': 'God gave rock and roll to you, gave rock and roll to you, put it in the soul of everyone.'}\n--------------\n{'original': '3... 2... 1... liftoff!\\n', 'synthetic': '3... 2... 1... liftoff!'}\n--------------\n{ 'original': 'My great great grandfather was called {{name_male}}, and my great great grandmother was '\n              'called {{name_female}}\\n',\n  'synthetic': 'My great great grandfather was called Michael, and my great great grandmother was called '\n               'Emma.'}\n--------------\n{'original': 'She named him {{first_name_male}}\\n', 'synthetic': 'She named him Juan.'}\n--------------\n{ 'original': 'Name:    {{name}}\\\\nAddress:     {{address}}\\n',\n  'synthetic': 'Name:    Amari Walters\\\\nAddress:     32 Webster Street, Salem, MA 01819'}\n--------------\n{ 'original': 'Follow up with {{name}} in a couple of months.\\n',\n  'synthetic': 'Follow up with Beatriz Lawrence in a couple of months.'}\n--------------\n{ 'original': '{{prefix_male}} {{last_name_male}} is a {{age}} year old man who grew up in {{city}}.\\n',\n  'synthetic': 'Mr.Williams is a 28 year old man who grew up in Dallas.'}\n--------------\n{ 'original': 'Date: {{date_time}}\\\\nName: {{name}}\\\\nPhone: {{phone_number}}\\n',\n  'synthetic': 'Date: 01/03/2021 13:45 \\\\nName: Pratima Joshi \\\\nPhone: 467-562-8954'}\n--------------\n{ 'original': '{{first_name}}: \"Who are you?\"\\\\n{{first_name_female}}:\"I\\'m {{first_name}}\\'s daughter\".\\n',\n  'synthetic': 'Bob: \"Who are you?\"\\\\nMaria:\"I\\'m Bob\\'s daughter\".'}\n--------------\n{ 'original': 'At my suggestion, one morning over breakfast, she agreed, and on the last Sunday before Labor '\n              'Day we returned to {{city}} by helicopter.\\n',\n  'synthetic': 'At my suggestion, one morning over breakfast, she agreed, and on the last Sunday before '\n               'Labor Day we returned to Paris by helicopter.'}\n</pre> <pre>--------------\n{ 'original': \"It was a done thing between him and {{first_name}}'s kid; and everybody thought so.\\n\",\n  'synthetic': \"It was a done thing between him and Jeffery's kid; and everybody thought so.\"}\n--------------\n{ 'original': 'Capitalized words like Wisdom and Discipline are often mistaken with names.\\n',\n  'synthetic': 'Capitalized words like Wisdom and Discipline are often mistaken with names.'}\n--------------\n{ 'original': 'The letter arrived at {{address}} last night.\\n',\n  'synthetic': 'The letter arrived at 1143 Orange Street last night.'}\n--------------\n{ 'original': 'The Princess Royal arrived at {{city}} this morning from {{country}}.\\n',\n  'synthetic': 'The Princess Royal arrived at London this morning from France.'}\n--------------\n{'original': \"I'm in {{city}}, at the conference\\n\", 'synthetic': \"I'm in Toronto, at the conference.\"}\n--------------\n{ 'original': '{{name}}, the {{job}}, said: \"I\\'m glad to hear that this has been withdrawn \u2013 quite why they '\n              'thought this would go down well is beyond me.\"\\n',\n  'synthetic': 'Gloria Green, the Nurse Practitioner, said: \"I\\'m glad to hear that this has been withdrawn '\n               '\u2013 quite why they thought this would go down well is beyond me.\"'}\n--------------\n{ 'original': '\"I\\'m glad to hear that {{country}} is moving in that direction,\" says {{last_name}}.\\n',\n  'synthetic': '\"I\\'m glad to hear that Canada is moving in that direction,\" says Smith.'}\n--------------\n{ 'original': 'I am {{nation_woman}} but I live in {{country}}.\\n',\n  'synthetic': 'I am Marianna Montenegro but I live in Ukraine.'}\n--------------\n{'original': 'We are proud {{nation_plural}}\\n', 'synthetic': 'We are proud Americans.'}\n--------------\n{ 'original': \"{{person}}'s killers sentenced to life in prison\\n\",\n  'synthetic': \"John Smith's killers sentenced to life in prison\"}\n--------------\n{ 'original': \"{{country}} leader gives 'kill without warning' order\\n\",\n  'synthetic': \"Brazilian leader gives 'kill without warning' order\"}\n--------------\n{ 'original': 'The {{nationality}} Border Force have detained top-flight tennis player {{name_female}} over '\n              'visa disputes.\\n',\n  'synthetic': 'The British Border Force have detained top-flight tennis player Maria Rodriguez over visa '\n               'disputes.'}\n--------------\n{ 'original': 'You will be responsible for the husbandry and care of a large variety of species including '\n              'lemurs, antelope, camels, and more\\n',\n  'synthetic': 'You will be responsible for the husbandry and care of a large variety of species including '\n               'lemurs, antelope, camels, and more.'}\n--------------\n{ 'original': '{{name}}\\\\n\\\\n{{job}}\\\\n\\\\nPersonal '\n              'Info:\\\\nPhone:\\\\n{{phone_number}}\\\\n\\\\nE-mail:\\\\n{{email}}\\\\n\\\\nWebsite:\\\\n{{url}}\\\\n\\\\nAddress:\\\\n{{address}}.\\n',\n  'synthetic': 'Robert James\\\\n\\\\nSoftware Engineer\\\\n\\\\nPersonal '\n               'Info:\\\\nPhone:\\\\n555-847-8915\\\\n\\\\nE-mail:\\\\nrobertjames@example.com\\\\n\\\\nWebsite:\\\\nwww.example.com\\\\n\\\\nAddress:\\\\n277 '\n               'Park Ave North, Denver, CO 80100.'}\n--------------\n{ 'original': '{{name}}\\\\n\\\\n{{city}}\\\\n{{country}}\\n',\n  'synthetic': 'John Smith\\n     Los Angeles\\n     United States'}\n--------------\n{ 'original': 'Title VII of the Civil Rights Act of {{year}} protects individuals against employment '\n              'discrimination on the basis of race and color as well as national origin, sex, or religion.\\n',\n  'synthetic': 'Title VII of the Civil Rights Act of 1964 protects individuals against employment '\n               'discrimination on the basis of race and color as well as national origin, sex, or religion.'}\n--------------\n{ 'original': 'Energetic and driven salesperson with 8+ years of professional experience in inbound and '\n              'outbound sales. Awarded Salesperson of the Month three times. Helped increase inbound sales '\n              'by 16% within the first year of employment. Looking to support {{organization}} in {{city}} '\n              '{{zipcode}} in its mission to become a market-leading solution.\\n',\n  'synthetic': 'Energetic and driven salesperson with 8+ years of professional experience in inbound and '\n               'outbound sales. Awarded Salesperson of the Month three times. Helped increase inbound sales '\n               'by 16% within the first year of employment. Looking to support Acme Corporation in Los '\n               'Angeles 90018 in its mission to become a market-leading solution.'}\n--------------\n{ 'original': 'The bus drops you off at {{building_number}} {{street_name}} St.\\n',\n  'synthetic': 'The bus drops you off at 2774 Chestnut St.'}\n--------------\n{ 'original': 'Ask the driver to stop at the corner of {{street_name}} St. and {{street_name}} St.\\n',\n  'synthetic': 'Ask the driver to stop at the corner of Maple St. and Sycamore St.'}\n--------------\n{ 'original': 'He lives on the north side of {{street_name}}.\\n',\n  'synthetic': 'He lives on the north side of Abbey Road.'}\n--------------\n{ 'original': \"I used to work for {{organization}} as {{job}}, but quit a few months ago. Now I'm \"\n              'unemployed.\\n',\n  'synthetic': \"I used to work for ABC Corporation as Software Engineer, but quit a few months ago. Now I'm \"\n               'unemployed.'}\n--------------\n{'original': '{{city}} bridge is falling down.\\n', 'synthetic': 'Berlin bridge is falling down.'}\n--------------\n{ 'original': '{{name}} of {{organization}} is the CEO of the year. ABC Business considered several other '\n              \"influential CEOs for this year's honor, including {{name}} of {{organization}}, \"\n              \"{{organization}}'s {{name}}, {{name}} of {{organization}}'s, {{name}} of {{organization}}, \"\n              \"and {{organization}}'s {{name}}.\\n\",\n  'synthetic': 'Franklin Smith of Technology Solutions International is the CEO of the year. ABC Business '\n               \"considered several other influential CEOs for this year's honor, including Madison Chang of \"\n               \"Radiance Digital, Radiance Digital's Ashleigh Jones, Cash Huang of Clark &amp; Partner's, Sierra \"\n               \"Urbina of Intelicity, and Clark &amp; Partners's Asher Kenney.\"}\n--------------\n{ 'original': '{{organization}} is a design agency based in {{city}}.\\n',\n  'synthetic': 'Maestro Design Inc. is a design agency based in Amsterdam.'}\n--------------\n{ 'original': 'Action &amp; Adventure, Animation, Comedy, Kids &amp; Family, Mystery &amp; Suspense\\\\nDirected By:    '\n              '{{name}}\\n',\n  'synthetic': 'Action &amp; Adventure, Animation, Comedy, Kids &amp; Family, Mystery &amp; Suspense\\n'\n               'Directed By: Esther Jones'}\n--------------\n{ 'original': '{{first_name}}: What a wife.\\\\n{{first_name}}: Remember me, {{first_name}}? When I killed '\n              'your brother, I talked just like this!\\\\n{{first_name}}: You saved my life! How can I ever '\n              'repay you?\\n',\n  'synthetic': 'Emma: What a wife.\\n'\n               '    Emma: Remember me, Emma? When I killed your brother, I talked just like this!\\n'\n               '    Emma: You saved my life! How can I ever repay you?'}\n--------------\n{'original': 'He just turned {{age}} years old\\n', 'synthetic': 'He just turned 7 years old'}\n--------------\n{ 'original': \"I'm {{name}}, originally from {{city}}, and i'm {{age}} y/o.\\n\",\n  'synthetic': \"I'm Emily Evanston, originally from London, and I'm 24 y/o.\"}\n--------------\n{ 'original': 'Patient is a {{age}}-year-old male with a history of headaches\\n',\n  'synthetic': 'Patient is a 35-year-old male with a history of headaches'}\n--------------\n{'original': 'I just turned {{age}}\\n', 'synthetic': 'I just turned 24.'}\n--------------\n{'original': 'My father retired at the age of {{age}}\\n', 'synthetic': 'My father retired at the age of 60.'}\n--------------\n{ 'original': 'This {{age}} year old female complaining of stomach pain.\\n',\n  'synthetic': 'This 28 year old female complaining of stomach pain.'}\n--------------\n{ 'original': \"My birthday is on the weekend. I'll turn {{age}}.\\n\",\n  'synthetic': \"My birthday is on the weekend. I'll turn 20.\"}\n--------------\n{'original': 'My brother just turned {{age}}\\n', 'synthetic': 'My brother just turned 18.'}\n</pre> <pre>--------------\n{ 'original': '{{prefix}} {{last_name}} flew to {{city}} on {{day_of_week}} morning.',\n  'synthetic': 'Dr. Nguyen flew to Los Angeles on Tuesday morning.'}\n--------------\n</pre> <p>This notebook demonstrates how to leverage OpenAI models for fake/surrogate data generation. It uses Presidio to first de-identify data (as de-identification might be required prior to passing the model to OpenAI), and then uses OpenAI completion models to create synthetic/fake/surrogate data based on real data. OpenAI models would also potentially remove additional PII entities, if those are not detected by Presidio.</p> <p>Some impressions:</p> <ol> <li>LLMs sometimes gives additonal output, especially if the text is a question or concerning a human/bot interaction. Engineering the prompt can mitigate some of these issues. Potential post-processing might be required.</li> <li>LLMs sometimes creates fake values even in the absence of placeholders.</li> <li>LLMs re-uses context from other sentences, which could cause phone numbers are sometimes generated using a credit card pattern or other similar mistakes.</li> <li>Co-references are sometimes missed (i.e. two name placeholders that should be filled with the same name, or referencing he/she to a male/female name)</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"samples/python/synth_data_with_openai/#path-to-notebook-httpswwwgithubcommicrosoftpresidioblobmaindocssamplespythonsynth_data_with_openaiipynb","title":"Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/synth_data_with_openai.ipynb\u00b6","text":""},{"location":"samples/python/synth_data_with_openai/#use-presidio-openai-to-turn-real-text-into-fake-text","title":"Use Presidio + OpenAI to turn real text into fake text\u00b6","text":"<p>This notebook uses Presidio to turn text with PII into text where PII entities are replaced with placeholders, e.g. \"<code>My name is David</code>\" turns into \"<code>My name is {{PERSON}}</code>\". Then, it calls the OpenAI API to create a fake record which is based on the original one.</p> <p>Flow:</p> <ol> <li><code>My friend David lives in Paris. He likes it.</code></li> <li><code>My friend {{PERSON}} lives in {{CITY}}. He likes it.</code></li> <li><code>My friend Lucy lives in Beirut. She likes it.</code></li> </ol> <p>Note that OpenAI completion models could possibly detect PII values and replace them in one call, but it is suggested to validate that all PII entities are indeed detected.</p>"},{"location":"samples/python/synth_data_with_openai/#imports-and-set-up-openai-key","title":"Imports and set up OpenAI Key\u00b6","text":""},{"location":"samples/python/synth_data_with_openai/#define-request-for-the-openai-service","title":"Define request for the OpenAI service\u00b6","text":""},{"location":"samples/python/synth_data_with_openai/#de-identify-data-using-presidio-analyzer-and-anonymizer","title":"De-identify data using Presidio Analyzer and Anonymizer\u00b6","text":""},{"location":"samples/python/synth_data_with_openai/#create-prompt-instructions-text-to-manipulate","title":"Create prompt (instructions + text to manipulate)\u00b6","text":""},{"location":"samples/python/synth_data_with_openai/#call-the-llm","title":"Call the LLM\u00b6","text":""},{"location":"samples/python/synth_data_with_openai/#alternatively-run-on-a-list-of-template-sentences","title":"Alternatively, run on a list of template sentences:\u00b6","text":""},{"location":"samples/python/ahds/","title":"Azure Health Data Services de-identification Integration","text":""},{"location":"samples/python/ahds/#introduction","title":"Introduction","text":"<p>The Azure Health Data Services (AHDS) de-identification Service is a cloud-based  service that provides advanced natural language processing over  raw text. One of its main functions includes Named Entity Recognition  (NER), which has the ability to identify different entities in text and categorize them into pre-defined classes or types. This document will demonstrate Presidio integration with the AHDS De-Identification Service.</p>"},{"location":"samples/python/ahds/#supported-entity-categories-in-the-azure-health-data-services-de-identification-api","title":"Supported entity categories in the Azure Health Data Services de-identification API","text":"<p>Azure Health Data Services de-identification supports multiple PII entity categories. The Azure Health Data Services de-identification service runs a predictive model to identify and categorize named entities from an input document. The service's latest version includes the ability to detect personal (PII) and health (PHI) information. A list of all supported entities can be found in the official documentation.</p>"},{"location":"samples/python/ahds/#prerequisites","title":"Prerequisites","text":"<p>To use AHDS De-Identification with Preisido, an Azure De-Identification Service resource should first be created under an Azure subscription. Follow the official documentation for instructions. The endpoint, generated once the resource is created,  will be used when integrating with AHDS De-Identification, using a Presidio remote recognizer.</p>"},{"location":"samples/python/ahds/#azure-health-data-services-de-identification-recognizer","title":"Azure Health Data Services de-identification Recognizer","text":"<p>The implementation of a <code>AzureHealthDeid</code> recognizer can be found here.</p>"},{"location":"samples/python/ahds/#how-to-integrate-azure-health-data-services-de-identification-into-presidio","title":"How to integrate Azure Health Data Services de-identification into Presidio","text":"<ol> <li> <p>Install the package with the ahds extra:   <pre><code>pip install \"presidio-analyzer[ahds]\"\n</code></pre></p> </li> <li> <p>Define environment variables <code>AHDS_ENDPOINT</code></p> </li> <li> <p>Add the <code>AzureHealthDeidRecognizer</code> to the recognizer registry:</p> </li> </ol> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.predefined_recognizers import AzureHealthDeidRecognizer\n\nahds = AzureHealthDeidRecognizer()\n\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(ahds)\n\nanalyzer.analyze(text=\"My email is email@email.com\", language=\"en\")\n</code></pre> <p>See also: For a full surrogate integration example, see example_ahds_surrogate.py</p>"},{"location":"samples/python/ahds/__init__/","title":"init","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Presidio + AHDS example.\"\"\"\n</pre> \"\"\"Presidio + AHDS example.\"\"\""},{"location":"samples/python/ahds/example_ahds_surrogate/","title":"Example ahds surrogate","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Example usage of AHDS Surrogate operator using surrogate_ahds operation.\"\"\"\n</pre> \"\"\"Example usage of AHDS Surrogate operator using surrogate_ahds operation.\"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\n</pre> import os In\u00a0[\u00a0]: Copied! <pre>from presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig, RecognizerResult\n</pre> from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import OperatorConfig, RecognizerResult In\u00a0[\u00a0]: Copied! <pre>def main():\n    \"\"\"Use AHDS Surrogate operator with surrogate_ahds operation.\"\"\"\n\n    # Check if AHDS endpoint is available\n    if not os.getenv(\"AHDS_ENDPOINT\"):\n        print(\"AHDS_ENDPOINT environment variable is not set.\")\n        print(\"Please set it to your Azure Health Data Services endpoint.\")\n        return\n\n    try:\n        # Initialize the anonymizer engine\n        engine = AnonymizerEngine()\n\n        # Example text with PII\n        text = \"Patient John Doe was seen by Dr. Smith on 2024-01-15\"\n\n        # Example analyzer results (normally these would come from the analyzer)\n        analyzer_results = [\n            RecognizerResult(entity_type=\"PATIENT\", start=8, end=16, score=0.9),\n            RecognizerResult(entity_type=\"DOCTOR\", start=29, end=38, score=0.9),\n            RecognizerResult(entity_type=\"DATE\", start=42, end=52, score=0.9),\n        ]\n\n        # Use AHDS Surrogate operator\n        result = engine.anonymize(\n            text=text,\n            analyzer_results=analyzer_results,\n            operators={\n                \"DEFAULT\": OperatorConfig(\n                    \"surrogate_ahds\",\n                    {\n                        \"entities\": analyzer_results,\n                        \"input_locale\": \"en-US\",\n                        \"surrogate_locale\": \"en-US\"\n                    }\n                )\n            },\n        )\n\n        print(f\"Original text: {text}\")\n        print(f\"Anonymized text: {result.text}\")\n        print(\n            \"Note: Uses Azure Health Data Services \"\n            \"De-identification service surrogate_ahds operation \"\n            \"for realistic surrogate generation\"\n        )\n\n    except ImportError:\n        print(\"AHDS dependencies are not available.\")\n        print(\"Please install with: pip install presidio-anonymizer[ahds]\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n</pre> def main():     \"\"\"Use AHDS Surrogate operator with surrogate_ahds operation.\"\"\"      # Check if AHDS endpoint is available     if not os.getenv(\"AHDS_ENDPOINT\"):         print(\"AHDS_ENDPOINT environment variable is not set.\")         print(\"Please set it to your Azure Health Data Services endpoint.\")         return      try:         # Initialize the anonymizer engine         engine = AnonymizerEngine()          # Example text with PII         text = \"Patient John Doe was seen by Dr. Smith on 2024-01-15\"          # Example analyzer results (normally these would come from the analyzer)         analyzer_results = [             RecognizerResult(entity_type=\"PATIENT\", start=8, end=16, score=0.9),             RecognizerResult(entity_type=\"DOCTOR\", start=29, end=38, score=0.9),             RecognizerResult(entity_type=\"DATE\", start=42, end=52, score=0.9),         ]          # Use AHDS Surrogate operator         result = engine.anonymize(             text=text,             analyzer_results=analyzer_results,             operators={                 \"DEFAULT\": OperatorConfig(                     \"surrogate_ahds\",                     {                         \"entities\": analyzer_results,                         \"input_locale\": \"en-US\",                         \"surrogate_locale\": \"en-US\"                     }                 )             },         )          print(f\"Original text: {text}\")         print(f\"Anonymized text: {result.text}\")         print(             \"Note: Uses Azure Health Data Services \"             \"De-identification service surrogate_ahds operation \"             \"for realistic surrogate generation\"         )      except ImportError:         print(\"AHDS dependencies are not available.\")         print(\"Please install with: pip install presidio-anonymizer[ahds]\")     except Exception as e:         print(f\"Error: {e}\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"samples/python/streamlit/","title":"Simple demo website for Presidio","text":"<p>Here's a simple app, written in pure Python, to create a demo website for Presidio. The app is based on the streamlit package.</p> <p>A live version can be found here: https://huggingface.co/spaces/presidio/presidio_demo</p>"},{"location":"samples/python/streamlit/#requirements","title":"Requirements","text":"<ol> <li>Clone the repo and move to the <code>docs/samples/python/streamlit</code> folder</li> <li>Install dependencies (preferably in a virtual environment)</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Note: This would install additional packages such as <code>transformers</code> and <code>flair</code> which are not mandatory for using Presidio.</p> <ol> <li>Optional: Update the <code>analyzer_engine</code> and <code>anonymizer_engine</code> functions for your specific implementation (in <code>presidio_helpers.py</code>).</li> <li>Start the app:</li> </ol> <pre><code>streamlit run presidio_streamlit.py\n</code></pre> <ol> <li>Consider adding an <code>.env</code> file with the following environment variables, for further customizability: <pre><code>TA_KEY=YOUR_TEXT_ANALYTICS_KEY\nTA_ENDPOINT=YOUR_TEXT_ANALYTICS_ENDPOINT\nOPENAI_TYPE=\"Azure\" #or \"openai\"\nOPENAI_KEY=YOUR_OPENAI_KEY\nOPENAI_API_VERSION = \"2023-05-15\"\nAZURE_OPENAI_ENDPOINT=YOUR_AZURE_OPENAI_AZURE_OPENAI_ENDPOINT\nAZURE_OPENAI_DEPLOYMENT=text-davinci-003\nALLOW_OTHER_MODELS=true #true if the user could download new models\n</code></pre></li> </ol>"},{"location":"samples/python/streamlit/#output","title":"Output","text":"<p>Output should be similar to this screenshot: </p>"},{"location":"samples/python/streamlit/azure_ai_language_wrapper/","title":"Azure ai language wrapper","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nfrom typing import List, Optional\nimport logging\nimport dotenv\nfrom azure.ai.textanalytics import TextAnalyticsClient\nfrom azure.core.credentials import AzureKeyCredential\n</pre> import os from typing import List, Optional import logging import dotenv from azure.ai.textanalytics import TextAnalyticsClient from azure.core.credentials import AzureKeyCredential In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import EntityRecognizer, RecognizerResult, AnalysisExplanation\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n</pre> from presidio_analyzer import EntityRecognizer, RecognizerResult, AnalysisExplanation from presidio_analyzer.nlp_engine import NlpArtifacts In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(\"presidio-streamlit\")\n</pre> logger = logging.getLogger(\"presidio-streamlit\") In\u00a0[\u00a0]: Copied! <pre>class AzureAIServiceWrapper(EntityRecognizer):\n    from azure.ai.textanalytics._models import PiiEntityCategory\n\n    TA_SUPPORTED_ENTITIES = [r.value for r in PiiEntityCategory]\n\n    def __init__(\n        self,\n        supported_entities: Optional[List[str]] = None,\n        supported_language: str = \"en\",\n        ta_client: Optional[TextAnalyticsClient] = None,\n        ta_key: Optional[str] = None,\n        ta_endpoint: Optional[str] = None,\n    ):\n        \"\"\"\n        Wrapper for the Azure Text Analytics client\n        :param ta_client: object of type TextAnalyticsClient\n        :param ta_key: Azure cognitive Services for Language key\n        :param ta_endpoint: Azure cognitive Services for Language endpoint\n        \"\"\"\n\n        if not supported_entities:\n            supported_entities = self.TA_SUPPORTED_ENTITIES\n\n        super().__init__(\n            supported_entities=supported_entities,\n            supported_language=supported_language,\n            name=\"Azure AI Language PII\",\n        )\n\n        self.ta_key = ta_key\n        self.ta_endpoint = ta_endpoint\n\n        if not ta_client:\n            ta_client = self.__authenticate_client(ta_key, ta_endpoint)\n        self.ta_client = ta_client\n\n    @staticmethod\n    def __authenticate_client(key: str, endpoint: str):\n        ta_credential = AzureKeyCredential(key)\n        text_analytics_client = TextAnalyticsClient(\n            endpoint=endpoint, credential=ta_credential\n        )\n        return text_analytics_client\n\n    def analyze(\n        self, text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None\n    ) -&gt; List[RecognizerResult]:\n        if not entities:\n            entities = []\n        response = self.ta_client.recognize_pii_entities(\n            [text], language=self.supported_language\n        )\n        results = [doc for doc in response if not doc.is_error]\n        recognizer_results = []\n        for res in results:\n            for entity in res.entities:\n                if entity.category not in self.supported_entities:\n                    continue\n                analysis_explanation = AzureAIServiceWrapper._build_explanation(\n                    original_score=entity.confidence_score,\n                    entity_type=entity.category,\n                )\n                recognizer_results.append(\n                    RecognizerResult(\n                        entity_type=entity.category,\n                        start=entity.offset,\n                        end=entity.offset + len(entity.text),\n                        score=entity.confidence_score,\n                        analysis_explanation=analysis_explanation,\n                    )\n                )\n\n        return recognizer_results\n\n    @staticmethod\n    def _build_explanation(\n        original_score: float, entity_type: str\n    ) -&gt; AnalysisExplanation:\n        explanation = AnalysisExplanation(\n            recognizer=AzureAIServiceWrapper.__class__.__name__,\n            original_score=original_score,\n            textual_explanation=f\"Identified as {entity_type} by Text Analytics\",\n        )\n        return explanation\n\n    def load(self) -&gt; None:\n        pass\n</pre> class AzureAIServiceWrapper(EntityRecognizer):     from azure.ai.textanalytics._models import PiiEntityCategory      TA_SUPPORTED_ENTITIES = [r.value for r in PiiEntityCategory]      def __init__(         self,         supported_entities: Optional[List[str]] = None,         supported_language: str = \"en\",         ta_client: Optional[TextAnalyticsClient] = None,         ta_key: Optional[str] = None,         ta_endpoint: Optional[str] = None,     ):         \"\"\"         Wrapper for the Azure Text Analytics client         :param ta_client: object of type TextAnalyticsClient         :param ta_key: Azure cognitive Services for Language key         :param ta_endpoint: Azure cognitive Services for Language endpoint         \"\"\"          if not supported_entities:             supported_entities = self.TA_SUPPORTED_ENTITIES          super().__init__(             supported_entities=supported_entities,             supported_language=supported_language,             name=\"Azure AI Language PII\",         )          self.ta_key = ta_key         self.ta_endpoint = ta_endpoint          if not ta_client:             ta_client = self.__authenticate_client(ta_key, ta_endpoint)         self.ta_client = ta_client      @staticmethod     def __authenticate_client(key: str, endpoint: str):         ta_credential = AzureKeyCredential(key)         text_analytics_client = TextAnalyticsClient(             endpoint=endpoint, credential=ta_credential         )         return text_analytics_client      def analyze(         self, text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None     ) -&gt; List[RecognizerResult]:         if not entities:             entities = []         response = self.ta_client.recognize_pii_entities(             [text], language=self.supported_language         )         results = [doc for doc in response if not doc.is_error]         recognizer_results = []         for res in results:             for entity in res.entities:                 if entity.category not in self.supported_entities:                     continue                 analysis_explanation = AzureAIServiceWrapper._build_explanation(                     original_score=entity.confidence_score,                     entity_type=entity.category,                 )                 recognizer_results.append(                     RecognizerResult(                         entity_type=entity.category,                         start=entity.offset,                         end=entity.offset + len(entity.text),                         score=entity.confidence_score,                         analysis_explanation=analysis_explanation,                     )                 )          return recognizer_results      @staticmethod     def _build_explanation(         original_score: float, entity_type: str     ) -&gt; AnalysisExplanation:         explanation = AnalysisExplanation(             recognizer=AzureAIServiceWrapper.__class__.__name__,             original_score=original_score,             textual_explanation=f\"Identified as {entity_type} by Text Analytics\",         )         return explanation      def load(self) -&gt; None:         pass In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    import presidio_helpers\n\n    dotenv.load_dotenv()\n    text = \"\"\"\n    Here are a few example sentences we currently support:\n\n    Hello, my name is David Johnson and I live in Maine.\n    My credit card number is 4095-2609-9393-4932 and my crypto wallet id is 16Yeky6GMjeNkAiNcBY7ZhrLoMSgg1BoyZ.\n    \n    On September 18 I visited microsoft.com and sent an email to test@presidio.site,  from the IP 192.168.0.1.\n    \n    My passport: 191280342 and my phone number: (212) 555-1234.\n    \n    This is a valid International Bank Account Number: IL150120690000003111111 . Can you please check the status on bank account 954567876544?\n    \n    Kate's social security number is 078-05-1126.  Her driver license? it is 1234567A.\n    \"\"\"\n    analyzer = presidio_helpers.analyzer_engine(\n        model_path=\"Azure Text Analytics PII\",\n        ta_key=os.environ[\"TA_KEY\"],\n        ta_endpoint=os.environ[\"TA_ENDPOINT\"],\n    )\n    analyzer.analyze(text=text, language=\"en\")\n</pre> if __name__ == \"__main__\":     import presidio_helpers      dotenv.load_dotenv()     text = \"\"\"     Here are a few example sentences we currently support:      Hello, my name is David Johnson and I live in Maine.     My credit card number is 4095-2609-9393-4932 and my crypto wallet id is 16Yeky6GMjeNkAiNcBY7ZhrLoMSgg1BoyZ.          On September 18 I visited microsoft.com and sent an email to test@presidio.site,  from the IP 192.168.0.1.          My passport: 191280342 and my phone number: (212) 555-1234.          This is a valid International Bank Account Number: IL150120690000003111111 . Can you please check the status on bank account 954567876544?          Kate's social security number is 078-05-1126.  Her driver license? it is 1234567A.     \"\"\"     analyzer = presidio_helpers.analyzer_engine(         model_path=\"Azure Text Analytics PII\",         ta_key=os.environ[\"TA_KEY\"],         ta_endpoint=os.environ[\"TA_ENDPOINT\"],     )     analyzer.analyze(text=text, language=\"en\")"},{"location":"samples/python/streamlit/flair_recognizer/","title":"Flair recognizer","text":"In\u00a0[\u00a0]: Copied! <pre>import logging\nfrom typing import Optional, List, Tuple, Set\n</pre> import logging from typing import Optional, List, Tuple, Set In\u00a0[\u00a0]: Copied! <pre>from presidio_analyzer import (\n    RecognizerResult,\n    EntityRecognizer,\n    AnalysisExplanation,\n)\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n</pre> from presidio_analyzer import (     RecognizerResult,     EntityRecognizer,     AnalysisExplanation, ) from presidio_analyzer.nlp_engine import NlpArtifacts In\u00a0[\u00a0]: Copied! <pre>from flair.data import Sentence\nfrom flair.models import SequenceTagger\n</pre> from flair.data import Sentence from flair.models import SequenceTagger In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(\"presidio-analyzer\")\n</pre> logger = logging.getLogger(\"presidio-analyzer\") In\u00a0[\u00a0]: Copied! <pre>class FlairRecognizer(EntityRecognizer):\n    \"\"\"\n    Wrapper for a flair model, if needed to be used within Presidio Analyzer.\n\n    :example:\n    &gt;from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\n    &gt;flair_recognizer = FlairRecognizer()\n\n    &gt;registry = RecognizerRegistry()\n    &gt;registry.add_recognizer(flair_recognizer)\n\n    &gt;analyzer = AnalyzerEngine(registry=registry)\n\n    &gt;results = analyzer.analyze(\n    &gt;    \"My name is Christopher and I live in Irbid.\",\n    &gt;    language=\"en\",\n    &gt;    return_decision_process=True,\n    &gt;)\n    &gt;for result in results:\n    &gt;    print(result)\n    &gt;    print(result.analysis_explanation)\n\n\n    \"\"\"\n\n    ENTITIES = [\n        \"LOCATION\",\n        \"PERSON\",\n        \"ORGANIZATION\",\n        # \"MISCELLANEOUS\"   # - There are no direct correlation with Presidio entities.\n    ]\n\n    DEFAULT_EXPLANATION = \"Identified as {} by Flair's Named Entity Recognition\"\n\n    CHECK_LABEL_GROUPS = [\n        ({\"LOCATION\"}, {\"LOC\", \"LOCATION\"}),\n        ({\"PERSON\"}, {\"PER\", \"PERSON\"}),\n        ({\"ORGANIZATION\"}, {\"ORG\"}),\n        # ({\"MISCELLANEOUS\"}, {\"MISC\"}), # Probably not PII\n    ]\n\n    MODEL_LANGUAGES = {\"en\": \"flair/ner-english-large\"}\n\n    PRESIDIO_EQUIVALENCES = {\n        \"PER\": \"PERSON\",\n        \"LOC\": \"LOCATION\",\n        \"ORG\": \"ORGANIZATION\",\n        # 'MISC': 'MISCELLANEOUS'   # - Probably not PII\n    }\n\n    def __init__(\n        self,\n        supported_language: str = \"en\",\n        supported_entities: Optional[List[str]] = None,\n        check_label_groups: Optional[Tuple[Set, Set]] = None,\n        model: SequenceTagger = None,\n        model_path: Optional[str] = None,\n    ):\n        self.check_label_groups = (\n            check_label_groups if check_label_groups else self.CHECK_LABEL_GROUPS\n        )\n\n        supported_entities = supported_entities if supported_entities else self.ENTITIES\n\n        if model and model_path:\n            raise ValueError(\"Only one of model or model_path should be provided.\")\n        elif model and not model_path:\n            self.model = model\n        elif not model and model_path:\n            print(f\"Loading model from {model_path}\")\n            self.model = SequenceTagger.load(model_path)\n        else:\n            print(f\"Loading model for language {supported_language}\")\n            self.model = SequenceTagger.load(\n                self.MODEL_LANGUAGES.get(supported_language)\n            )\n\n        super().__init__(\n            supported_entities=supported_entities,\n            supported_language=supported_language,\n            name=\"Flair Analytics\",\n        )\n\n    def load(self) -&gt; None:\n        \"\"\"Load the model, not used. Model is loaded during initialization.\"\"\"\n        pass\n\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"\n        Return supported entities by this model.\n\n        :return: List of the supported entities.\n        \"\"\"\n        return self.supported_entities\n\n    # Class to use Flair with Presidio as an external recognizer.\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyze text using Text Analytics.\n\n        :param text: The text for analysis.\n        :param entities: Not working properly for this recognizer.\n        :param nlp_artifacts: Not used by this recognizer.\n        :param language: Text language. Supported languages in MODEL_LANGUAGES\n        :return: The list of Presidio RecognizerResult constructed from the recognized\n            Flair detections.\n        \"\"\"\n\n        results = []\n\n        sentences = Sentence(text)\n        self.model.predict(sentences)\n\n        # If there are no specific list of entities, we will look for all of it.\n        if not entities:\n            entities = self.supported_entities\n\n        for entity in entities:\n            if entity not in self.supported_entities:\n                continue\n\n            for ent in sentences.get_spans(\"ner\"):\n                if not self.__check_label(\n                    entity, ent.labels[0].value, self.check_label_groups\n                ):\n                    continue\n                textual_explanation = self.DEFAULT_EXPLANATION.format(\n                    ent.labels[0].value\n                )\n                explanation = self.build_flair_explanation(\n                    round(ent.score, 2), textual_explanation\n                )\n                flair_result = self._convert_to_recognizer_result(ent, explanation)\n\n                results.append(flair_result)\n\n        return results\n\n    def _convert_to_recognizer_result(self, entity, explanation) -&gt; RecognizerResult:\n        entity_type = self.PRESIDIO_EQUIVALENCES.get(entity.tag, entity.tag)\n        flair_score = round(entity.score, 2)\n\n        flair_results = RecognizerResult(\n            entity_type=entity_type,\n            start=entity.start_position,\n            end=entity.end_position,\n            score=flair_score,\n            analysis_explanation=explanation,\n        )\n\n        return flair_results\n\n    def build_flair_explanation(\n        self, original_score: float, explanation: str\n    ) -&gt; AnalysisExplanation:\n        \"\"\"\n        Create explanation for why this result was detected.\n\n        :param original_score: Score given by this recognizer\n        :param explanation: Explanation string\n        :return:\n        \"\"\"\n        explanation = AnalysisExplanation(\n            recognizer=self.__class__.__name__,\n            original_score=original_score,\n            textual_explanation=explanation,\n        )\n        return explanation\n\n    @staticmethod\n    def __check_label(\n        entity: str, label: str, check_label_groups: Tuple[Set, Set]\n    ) -&gt; bool:\n        return any(\n            [entity in egrp and label in lgrp for egrp, lgrp in check_label_groups]\n        )\n</pre> class FlairRecognizer(EntityRecognizer):     \"\"\"     Wrapper for a flair model, if needed to be used within Presidio Analyzer.      :example:     &gt;from presidio_analyzer import AnalyzerEngine, RecognizerRegistry      &gt;flair_recognizer = FlairRecognizer()      &gt;registry = RecognizerRegistry()     &gt;registry.add_recognizer(flair_recognizer)      &gt;analyzer = AnalyzerEngine(registry=registry)      &gt;results = analyzer.analyze(     &gt;    \"My name is Christopher and I live in Irbid.\",     &gt;    language=\"en\",     &gt;    return_decision_process=True,     &gt;)     &gt;for result in results:     &gt;    print(result)     &gt;    print(result.analysis_explanation)       \"\"\"      ENTITIES = [         \"LOCATION\",         \"PERSON\",         \"ORGANIZATION\",         # \"MISCELLANEOUS\"   # - There are no direct correlation with Presidio entities.     ]      DEFAULT_EXPLANATION = \"Identified as {} by Flair's Named Entity Recognition\"      CHECK_LABEL_GROUPS = [         ({\"LOCATION\"}, {\"LOC\", \"LOCATION\"}),         ({\"PERSON\"}, {\"PER\", \"PERSON\"}),         ({\"ORGANIZATION\"}, {\"ORG\"}),         # ({\"MISCELLANEOUS\"}, {\"MISC\"}), # Probably not PII     ]      MODEL_LANGUAGES = {\"en\": \"flair/ner-english-large\"}      PRESIDIO_EQUIVALENCES = {         \"PER\": \"PERSON\",         \"LOC\": \"LOCATION\",         \"ORG\": \"ORGANIZATION\",         # 'MISC': 'MISCELLANEOUS'   # - Probably not PII     }      def __init__(         self,         supported_language: str = \"en\",         supported_entities: Optional[List[str]] = None,         check_label_groups: Optional[Tuple[Set, Set]] = None,         model: SequenceTagger = None,         model_path: Optional[str] = None,     ):         self.check_label_groups = (             check_label_groups if check_label_groups else self.CHECK_LABEL_GROUPS         )          supported_entities = supported_entities if supported_entities else self.ENTITIES          if model and model_path:             raise ValueError(\"Only one of model or model_path should be provided.\")         elif model and not model_path:             self.model = model         elif not model and model_path:             print(f\"Loading model from {model_path}\")             self.model = SequenceTagger.load(model_path)         else:             print(f\"Loading model for language {supported_language}\")             self.model = SequenceTagger.load(                 self.MODEL_LANGUAGES.get(supported_language)             )          super().__init__(             supported_entities=supported_entities,             supported_language=supported_language,             name=\"Flair Analytics\",         )      def load(self) -&gt; None:         \"\"\"Load the model, not used. Model is loaded during initialization.\"\"\"         pass      def get_supported_entities(self) -&gt; List[str]:         \"\"\"         Return supported entities by this model.          :return: List of the supported entities.         \"\"\"         return self.supported_entities      # Class to use Flair with Presidio as an external recognizer.     def analyze(         self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None     ) -&gt; List[RecognizerResult]:         \"\"\"         Analyze text using Text Analytics.          :param text: The text for analysis.         :param entities: Not working properly for this recognizer.         :param nlp_artifacts: Not used by this recognizer.         :param language: Text language. Supported languages in MODEL_LANGUAGES         :return: The list of Presidio RecognizerResult constructed from the recognized             Flair detections.         \"\"\"          results = []          sentences = Sentence(text)         self.model.predict(sentences)          # If there are no specific list of entities, we will look for all of it.         if not entities:             entities = self.supported_entities          for entity in entities:             if entity not in self.supported_entities:                 continue              for ent in sentences.get_spans(\"ner\"):                 if not self.__check_label(                     entity, ent.labels[0].value, self.check_label_groups                 ):                     continue                 textual_explanation = self.DEFAULT_EXPLANATION.format(                     ent.labels[0].value                 )                 explanation = self.build_flair_explanation(                     round(ent.score, 2), textual_explanation                 )                 flair_result = self._convert_to_recognizer_result(ent, explanation)                  results.append(flair_result)          return results      def _convert_to_recognizer_result(self, entity, explanation) -&gt; RecognizerResult:         entity_type = self.PRESIDIO_EQUIVALENCES.get(entity.tag, entity.tag)         flair_score = round(entity.score, 2)          flair_results = RecognizerResult(             entity_type=entity_type,             start=entity.start_position,             end=entity.end_position,             score=flair_score,             analysis_explanation=explanation,         )          return flair_results      def build_flair_explanation(         self, original_score: float, explanation: str     ) -&gt; AnalysisExplanation:         \"\"\"         Create explanation for why this result was detected.          :param original_score: Score given by this recognizer         :param explanation: Explanation string         :return:         \"\"\"         explanation = AnalysisExplanation(             recognizer=self.__class__.__name__,             original_score=original_score,             textual_explanation=explanation,         )         return explanation      @staticmethod     def __check_label(         entity: str, label: str, check_label_groups: Tuple[Set, Set]     ) -&gt; bool:         return any(             [entity in egrp and label in lgrp for egrp, lgrp in check_label_groups]         )"},{"location":"samples/python/streamlit/flair_recognizer/#taken-from-httpsgithubcommicrosoftpresidioblobmaindocssamplespythonflair_recognizerpy","title":"Taken from https://github.com/microsoft/presidio/blob/main/docs/samples/python/flair_recognizer.py\u00b6","text":""},{"location":"samples/python/streamlit/openai_fake_data_generator/","title":"Openai fake data generator","text":"In\u00a0[\u00a0]: Copied! <pre>from collections import namedtuple\nfrom typing import Optional\n</pre> from collections import namedtuple from typing import Optional In\u00a0[\u00a0]: Copied! <pre>import openai\nfrom openai import OpenAI, AzureOpenAI\nimport logging\n</pre> import openai from openai import OpenAI, AzureOpenAI import logging In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(\"presidio-streamlit\")\n</pre> logger = logging.getLogger(\"presidio-streamlit\") In\u00a0[\u00a0]: Copied! <pre>OpenAIParams = namedtuple(\n    \"open_ai_params\",\n    [\"openai_key\", \"model\", \"api_base\", \"deployment_id\", \"api_version\", \"api_type\"],\n)\n</pre> OpenAIParams = namedtuple(     \"open_ai_params\",     [\"openai_key\", \"model\", \"api_base\", \"deployment_id\", \"api_version\", \"api_type\"], ) In\u00a0[\u00a0]: Copied! <pre>def call_completion_model(\n    prompt: str,\n    openai_params: OpenAIParams,\n    max_tokens: Optional[int] = 256,\n) -&gt; str:\n    \"\"\"Creates a request for the OpenAI Completion service and returns the response.\n\n    :param prompt: The prompt for the completion model\n    :param openai_params: OpenAI parameters for the completion model\n    :param max_tokens: The maximum number of tokens to generate.\n    \"\"\"\n    if openai_params.api_type.lower() == \"azure\":\n        client = AzureOpenAI(\n            api_version=openai_params.api_version,\n            api_key=openai_params.openai_key,\n            azure_endpoint=openai_params.api_base,\n            azure_deployment=openai_params.deployment_id,\n        )\n    else:\n        client = OpenAI(api_key=openai_params.openai_key)\n\n    response = client.completions.create(\n        model=openai_params.model,\n        prompt=prompt,\n        max_tokens=max_tokens,\n    )\n\n    return response.choices[0].text.strip()\n</pre> def call_completion_model(     prompt: str,     openai_params: OpenAIParams,     max_tokens: Optional[int] = 256, ) -&gt; str:     \"\"\"Creates a request for the OpenAI Completion service and returns the response.      :param prompt: The prompt for the completion model     :param openai_params: OpenAI parameters for the completion model     :param max_tokens: The maximum number of tokens to generate.     \"\"\"     if openai_params.api_type.lower() == \"azure\":         client = AzureOpenAI(             api_version=openai_params.api_version,             api_key=openai_params.openai_key,             azure_endpoint=openai_params.api_base,             azure_deployment=openai_params.deployment_id,         )     else:         client = OpenAI(api_key=openai_params.openai_key)      response = client.completions.create(         model=openai_params.model,         prompt=prompt,         max_tokens=max_tokens,     )      return response.choices[0].text.strip() In\u00a0[\u00a0]: Copied! <pre>def create_prompt(anonymized_text: str) -&gt; str:\n    \"\"\"\n    Create the prompt with instructions to GPT-3.\n\n    :param anonymized_text: Text with placeholders instead of PII values, e.g. My name is &lt;PERSON&gt;.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Your role is to create synthetic text based on de-identified text with placeholders instead of Personally Identifiable Information (PII).\n    Replace the placeholders (e.g. ,&lt;PERSON&gt;, {{DATE}}, {{ip_address}}) with fake values.\n\n    Instructions:\n\n    a. Use completely random numbers, so every digit is drawn between 0 and 9.\n    b. Use realistic names that come from diverse genders, ethnicities and countries.\n    c. If there are no placeholders, return the text as is.\n    d. Keep the formatting as close to the original as possible.\n    e. If PII exists in the input, replace it with fake values in the output.\n    f. Remove whitespace before and after the generated text\n    \n    input: [[TEXT STARTS]] How do I change the limit on my credit card {{credit_card_number}}?[[TEXT ENDS]]\n    output: How do I change the limit on my credit card 2539 3519 2345 1555?\n    input: [[TEXT STARTS]]&lt;PERSON&gt; was the chief science officer at &lt;ORGANIZATION&gt;.[[TEXT ENDS]]\n    output: Katherine Buckjov was the chief science officer at NASA.\n    input: [[TEXT STARTS]]Cameroon lives in &lt;LOCATION&gt;.[[TEXT ENDS]]\n    output: Vladimir lives in Moscow.\n    \n    input: [[TEXT STARTS]]{anonymized_text}[[TEXT ENDS]]\n    output:\"\"\"\n    return prompt\n</pre> def create_prompt(anonymized_text: str) -&gt; str:     \"\"\"     Create the prompt with instructions to GPT-3.      :param anonymized_text: Text with placeholders instead of PII values, e.g. My name is .     \"\"\"      prompt = f\"\"\"     Your role is to create synthetic text based on de-identified text with placeholders instead of Personally Identifiable Information (PII).     Replace the placeholders (e.g. ,, {{DATE}}, {{ip_address}}) with fake values.      Instructions:      a. Use completely random numbers, so every digit is drawn between 0 and 9.     b. Use realistic names that come from diverse genders, ethnicities and countries.     c. If there are no placeholders, return the text as is.     d. Keep the formatting as close to the original as possible.     e. If PII exists in the input, replace it with fake values in the output.     f. Remove whitespace before and after the generated text          input: [[TEXT STARTS]] How do I change the limit on my credit card {{credit_card_number}}?[[TEXT ENDS]]     output: How do I change the limit on my credit card 2539 3519 2345 1555?     input: [[TEXT STARTS]] was the chief science officer at .[[TEXT ENDS]]     output: Katherine Buckjov was the chief science officer at NASA.     input: [[TEXT STARTS]]Cameroon lives in .[[TEXT ENDS]]     output: Vladimir lives in Moscow.          input: [[TEXT STARTS]]{anonymized_text}[[TEXT ENDS]]     output:\"\"\"     return prompt"},{"location":"samples/python/streamlit/presidio_helpers/","title":"Presidio helpers","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nHelper methods for the Presidio Streamlit app\n\"\"\"\nfrom typing import List, Optional, Tuple\nimport logging\nimport streamlit as st\nfrom presidio_analyzer import (\n    AnalyzerEngine,\n    RecognizerResult,\n    RecognizerRegistry,\n    PatternRecognizer,\n    Pattern,\n)\nfrom presidio_analyzer.nlp_engine import NlpEngine\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\n</pre> \"\"\" Helper methods for the Presidio Streamlit app \"\"\" from typing import List, Optional, Tuple import logging import streamlit as st from presidio_analyzer import (     AnalyzerEngine,     RecognizerResult,     RecognizerRegistry,     PatternRecognizer,     Pattern, ) from presidio_analyzer.nlp_engine import NlpEngine from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import OperatorConfig In\u00a0[\u00a0]: Copied! <pre>from openai_fake_data_generator import (\n    call_completion_model,\n    OpenAIParams,\n    create_prompt,\n)\nfrom presidio_nlp_engine_config import (\n    create_nlp_engine_with_spacy,\n    create_nlp_engine_with_flair,\n    create_nlp_engine_with_transformers,\n    create_nlp_engine_with_azure_ai_language,\n    create_nlp_engine_with_stanza,\n)\n</pre> from openai_fake_data_generator import (     call_completion_model,     OpenAIParams,     create_prompt, ) from presidio_nlp_engine_config import (     create_nlp_engine_with_spacy,     create_nlp_engine_with_flair,     create_nlp_engine_with_transformers,     create_nlp_engine_with_azure_ai_language,     create_nlp_engine_with_stanza, ) In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(\"presidio-streamlit\")\n</pre> logger = logging.getLogger(\"presidio-streamlit\") In\u00a0[\u00a0]: Copied! <pre>@st.cache_resource\ndef nlp_engine_and_registry(\n    model_family: str,\n    model_path: str,\n    ta_key: Optional[str] = None,\n    ta_endpoint: Optional[str] = None,\n) -&gt; Tuple[NlpEngine, RecognizerRegistry]:\n    \"\"\"Create the NLP Engine instance based on the requested model.\n    :param model_family: Which model package to use for NER.\n    :param model_path: Which model to use for NER. E.g.,\n        \"StanfordAIMI/stanford-deidentifier-base\",\n        \"obi/deid_roberta_i2b2\",\n        \"en_core_web_lg\"\n    :param ta_key: Key to the Text Analytics endpoint (only if model_path = \"Azure Text Analytics\")\n    :param ta_endpoint: Endpoint of the Text Analytics instance (only if model_path = \"Azure Text Analytics\")\n    \"\"\"\n\n    # Set up NLP Engine according to the model of choice\n    if \"spacy\" in model_family.lower():\n        return create_nlp_engine_with_spacy(model_path)\n    if \"stanza\" in model_family.lower():\n        return create_nlp_engine_with_stanza(model_path)\n    elif \"flair\" in model_family.lower():\n        return create_nlp_engine_with_flair(model_path)\n    elif \"huggingface\" in model_family.lower():\n        return create_nlp_engine_with_transformers(model_path)\n    elif \"azure ai language\" in model_family.lower():\n        return create_nlp_engine_with_azure_ai_language(ta_key, ta_endpoint)\n    else:\n        raise ValueError(f\"Model family {model_family} not supported\")\n</pre> @st.cache_resource def nlp_engine_and_registry(     model_family: str,     model_path: str,     ta_key: Optional[str] = None,     ta_endpoint: Optional[str] = None, ) -&gt; Tuple[NlpEngine, RecognizerRegistry]:     \"\"\"Create the NLP Engine instance based on the requested model.     :param model_family: Which model package to use for NER.     :param model_path: Which model to use for NER. E.g.,         \"StanfordAIMI/stanford-deidentifier-base\",         \"obi/deid_roberta_i2b2\",         \"en_core_web_lg\"     :param ta_key: Key to the Text Analytics endpoint (only if model_path = \"Azure Text Analytics\")     :param ta_endpoint: Endpoint of the Text Analytics instance (only if model_path = \"Azure Text Analytics\")     \"\"\"      # Set up NLP Engine according to the model of choice     if \"spacy\" in model_family.lower():         return create_nlp_engine_with_spacy(model_path)     if \"stanza\" in model_family.lower():         return create_nlp_engine_with_stanza(model_path)     elif \"flair\" in model_family.lower():         return create_nlp_engine_with_flair(model_path)     elif \"huggingface\" in model_family.lower():         return create_nlp_engine_with_transformers(model_path)     elif \"azure ai language\" in model_family.lower():         return create_nlp_engine_with_azure_ai_language(ta_key, ta_endpoint)     else:         raise ValueError(f\"Model family {model_family} not supported\") In\u00a0[\u00a0]: Copied! <pre>@st.cache_resource\ndef analyzer_engine(\n    model_family: str,\n    model_path: str,\n    ta_key: Optional[str] = None,\n    ta_endpoint: Optional[str] = None,\n) -&gt; AnalyzerEngine:\n    \"\"\"Create the NLP Engine instance based on the requested model.\n    :param model_family: Which model package to use for NER.\n    :param model_path: Which model to use for NER:\n        \"StanfordAIMI/stanford-deidentifier-base\",\n        \"obi/deid_roberta_i2b2\",\n        \"en_core_web_lg\"\n    :param ta_key: Key to the Text Analytics endpoint (only if model_path = \"Azure Text Analytics\")\n    :param ta_endpoint: Endpoint of the Text Analytics instance (only if model_path = \"Azure Text Analytics\")\n    \"\"\"\n    nlp_engine, registry = nlp_engine_and_registry(\n        model_family, model_path, ta_key, ta_endpoint\n    )\n    analyzer = AnalyzerEngine(nlp_engine=nlp_engine, registry=registry)\n    return analyzer\n</pre> @st.cache_resource def analyzer_engine(     model_family: str,     model_path: str,     ta_key: Optional[str] = None,     ta_endpoint: Optional[str] = None, ) -&gt; AnalyzerEngine:     \"\"\"Create the NLP Engine instance based on the requested model.     :param model_family: Which model package to use for NER.     :param model_path: Which model to use for NER:         \"StanfordAIMI/stanford-deidentifier-base\",         \"obi/deid_roberta_i2b2\",         \"en_core_web_lg\"     :param ta_key: Key to the Text Analytics endpoint (only if model_path = \"Azure Text Analytics\")     :param ta_endpoint: Endpoint of the Text Analytics instance (only if model_path = \"Azure Text Analytics\")     \"\"\"     nlp_engine, registry = nlp_engine_and_registry(         model_family, model_path, ta_key, ta_endpoint     )     analyzer = AnalyzerEngine(nlp_engine=nlp_engine, registry=registry)     return analyzer In\u00a0[\u00a0]: Copied! <pre>@st.cache_resource\ndef anonymizer_engine():\n    \"\"\"Return AnonymizerEngine.\"\"\"\n    return AnonymizerEngine()\n</pre> @st.cache_resource def anonymizer_engine():     \"\"\"Return AnonymizerEngine.\"\"\"     return AnonymizerEngine() In\u00a0[\u00a0]: Copied! <pre>@st.cache_data\ndef get_supported_entities(\n    model_family: str, model_path: str, ta_key: str, ta_endpoint: str\n):\n    \"\"\"Return supported entities from the Analyzer Engine.\"\"\"\n    return analyzer_engine(\n        model_family, model_path, ta_key, ta_endpoint\n    ).get_supported_entities() + [\"GENERIC_PII\"]\n</pre> @st.cache_data def get_supported_entities(     model_family: str, model_path: str, ta_key: str, ta_endpoint: str ):     \"\"\"Return supported entities from the Analyzer Engine.\"\"\"     return analyzer_engine(         model_family, model_path, ta_key, ta_endpoint     ).get_supported_entities() + [\"GENERIC_PII\"] In\u00a0[\u00a0]: Copied! <pre>@st.cache_data\ndef analyze(\n    model_family: str, model_path: str, ta_key: str, ta_endpoint: str, **kwargs\n):\n    \"\"\"Analyze input using Analyzer engine and input arguments (kwargs).\"\"\"\n    if \"entities\" not in kwargs or \"All\" in kwargs[\"entities\"]:\n        kwargs[\"entities\"] = None\n\n    if \"deny_list\" in kwargs and kwargs[\"deny_list\"] is not None:\n        ad_hoc_recognizer = create_ad_hoc_deny_list_recognizer(kwargs[\"deny_list\"])\n        kwargs[\"ad_hoc_recognizers\"] = [ad_hoc_recognizer] if ad_hoc_recognizer else []\n        del kwargs[\"deny_list\"]\n\n    if \"regex_params\" in kwargs and len(kwargs[\"regex_params\"]) &gt; 0:\n        ad_hoc_recognizer = create_ad_hoc_regex_recognizer(*kwargs[\"regex_params\"])\n        kwargs[\"ad_hoc_recognizers\"] = [ad_hoc_recognizer] if ad_hoc_recognizer else []\n        del kwargs[\"regex_params\"]\n\n    return analyzer_engine(model_family, model_path, ta_key, ta_endpoint).analyze(\n        **kwargs\n    )\n</pre> @st.cache_data def analyze(     model_family: str, model_path: str, ta_key: str, ta_endpoint: str, **kwargs ):     \"\"\"Analyze input using Analyzer engine and input arguments (kwargs).\"\"\"     if \"entities\" not in kwargs or \"All\" in kwargs[\"entities\"]:         kwargs[\"entities\"] = None      if \"deny_list\" in kwargs and kwargs[\"deny_list\"] is not None:         ad_hoc_recognizer = create_ad_hoc_deny_list_recognizer(kwargs[\"deny_list\"])         kwargs[\"ad_hoc_recognizers\"] = [ad_hoc_recognizer] if ad_hoc_recognizer else []         del kwargs[\"deny_list\"]      if \"regex_params\" in kwargs and len(kwargs[\"regex_params\"]) &gt; 0:         ad_hoc_recognizer = create_ad_hoc_regex_recognizer(*kwargs[\"regex_params\"])         kwargs[\"ad_hoc_recognizers\"] = [ad_hoc_recognizer] if ad_hoc_recognizer else []         del kwargs[\"regex_params\"]      return analyzer_engine(model_family, model_path, ta_key, ta_endpoint).analyze(         **kwargs     ) In\u00a0[\u00a0]: Copied! <pre>def anonymize(\n    text: str,\n    operator: str,\n    analyze_results: List[RecognizerResult],\n    mask_char: Optional[str] = None,\n    number_of_chars: Optional[str] = None,\n    encrypt_key: Optional[str] = None,\n):\n    \"\"\"Anonymize identified input using Presidio Anonymizer.\n\n    :param text: Full text\n    :param operator: Operator name\n    :param mask_char: Mask char (for mask operator)\n    :param number_of_chars: Number of characters to mask (for mask operator)\n    :param encrypt_key: Encryption key (for encrypt operator)\n    :param analyze_results: list of results from presidio analyzer engine\n    \"\"\"\n\n    if operator == \"mask\":\n        operator_config = {\n            \"type\": \"mask\",\n            \"masking_char\": mask_char,\n            \"chars_to_mask\": number_of_chars,\n            \"from_end\": False,\n        }\n\n    # Define operator config\n    elif operator == \"encrypt\":\n        operator_config = {\"key\": encrypt_key}\n    elif operator == \"highlight\":\n        operator_config = {\"lambda\": lambda x: x}\n    else:\n        operator_config = None\n\n    # Change operator if needed as intermediate step\n    if operator == \"highlight\":\n        operator = \"custom\"\n    elif operator == \"synthesize\":\n        operator = \"replace\"\n    else:\n        operator = operator\n\n    res = anonymizer_engine().anonymize(\n        text,\n        analyze_results,\n        operators={\"DEFAULT\": OperatorConfig(operator, operator_config)},\n    )\n    return res\n</pre> def anonymize(     text: str,     operator: str,     analyze_results: List[RecognizerResult],     mask_char: Optional[str] = None,     number_of_chars: Optional[str] = None,     encrypt_key: Optional[str] = None, ):     \"\"\"Anonymize identified input using Presidio Anonymizer.      :param text: Full text     :param operator: Operator name     :param mask_char: Mask char (for mask operator)     :param number_of_chars: Number of characters to mask (for mask operator)     :param encrypt_key: Encryption key (for encrypt operator)     :param analyze_results: list of results from presidio analyzer engine     \"\"\"      if operator == \"mask\":         operator_config = {             \"type\": \"mask\",             \"masking_char\": mask_char,             \"chars_to_mask\": number_of_chars,             \"from_end\": False,         }      # Define operator config     elif operator == \"encrypt\":         operator_config = {\"key\": encrypt_key}     elif operator == \"highlight\":         operator_config = {\"lambda\": lambda x: x}     else:         operator_config = None      # Change operator if needed as intermediate step     if operator == \"highlight\":         operator = \"custom\"     elif operator == \"synthesize\":         operator = \"replace\"     else:         operator = operator      res = anonymizer_engine().anonymize(         text,         analyze_results,         operators={\"DEFAULT\": OperatorConfig(operator, operator_config)},     )     return res In\u00a0[\u00a0]: Copied! <pre>def annotate(text: str, analyze_results: List[RecognizerResult]):\n    \"\"\"Highlight the identified PII entities on the original text\n\n    :param text: Full text\n    :param analyze_results: list of results from presidio analyzer engine\n    \"\"\"\n    tokens = []\n\n    # Use the anonymizer to resolve overlaps\n    results = anonymize(\n        text=text,\n        operator=\"highlight\",\n        analyze_results=analyze_results,\n    )\n\n    # sort by start index\n    results = sorted(results.items, key=lambda x: x.start)\n    for i, res in enumerate(results):\n        if i == 0:\n            tokens.append(text[: res.start])\n\n        # append entity text and entity type\n        tokens.append((text[res.start : res.end], res.entity_type))\n\n        # if another entity coming i.e. we're not at the last results element, add text up to next entity\n        if i != len(results) - 1:\n            tokens.append(text[res.end : results[i + 1].start])\n        # if no more entities coming, add all remaining text\n        else:\n            tokens.append(text[res.end :])\n    return tokens\n</pre> def annotate(text: str, analyze_results: List[RecognizerResult]):     \"\"\"Highlight the identified PII entities on the original text      :param text: Full text     :param analyze_results: list of results from presidio analyzer engine     \"\"\"     tokens = []      # Use the anonymizer to resolve overlaps     results = anonymize(         text=text,         operator=\"highlight\",         analyze_results=analyze_results,     )      # sort by start index     results = sorted(results.items, key=lambda x: x.start)     for i, res in enumerate(results):         if i == 0:             tokens.append(text[: res.start])          # append entity text and entity type         tokens.append((text[res.start : res.end], res.entity_type))          # if another entity coming i.e. we're not at the last results element, add text up to next entity         if i != len(results) - 1:             tokens.append(text[res.end : results[i + 1].start])         # if no more entities coming, add all remaining text         else:             tokens.append(text[res.end :])     return tokens In\u00a0[\u00a0]: Copied! <pre>def create_fake_data(\n    text: str,\n    analyze_results: List[RecognizerResult],\n    openai_params: OpenAIParams,\n):\n    \"\"\"Creates a synthetic version of the text using OpenAI APIs\"\"\"\n    if not openai_params.openai_key:\n        return \"Please provide your OpenAI key\"\n    results = anonymize(text=text, operator=\"replace\", analyze_results=analyze_results)\n    prompt = create_prompt(results.text)\n    print(f\"Prompt: {prompt}\")\n    fake = call_completion_model(prompt=prompt, openai_params=openai_params)\n    return fake\n</pre> def create_fake_data(     text: str,     analyze_results: List[RecognizerResult],     openai_params: OpenAIParams, ):     \"\"\"Creates a synthetic version of the text using OpenAI APIs\"\"\"     if not openai_params.openai_key:         return \"Please provide your OpenAI key\"     results = anonymize(text=text, operator=\"replace\", analyze_results=analyze_results)     prompt = create_prompt(results.text)     print(f\"Prompt: {prompt}\")     fake = call_completion_model(prompt=prompt, openai_params=openai_params)     return fake In\u00a0[\u00a0]: Copied! <pre>@st.cache_data\ndef call_openai_api(\n    prompt: str, openai_model_name: str, openai_deployment_name: Optional[str] = None\n) -&gt; str:\n    fake_data = call_completion_model(\n        prompt, model=openai_model_name, deployment_id=openai_deployment_name\n    )\n    return fake_data\n</pre> @st.cache_data def call_openai_api(     prompt: str, openai_model_name: str, openai_deployment_name: Optional[str] = None ) -&gt; str:     fake_data = call_completion_model(         prompt, model=openai_model_name, deployment_id=openai_deployment_name     )     return fake_data In\u00a0[\u00a0]: Copied! <pre>def create_ad_hoc_deny_list_recognizer(\n    deny_list=Optional[List[str]],\n) -&gt; Optional[PatternRecognizer]:\n    if not deny_list:\n        return None\n\n    deny_list_recognizer = PatternRecognizer(\n        supported_entity=\"GENERIC_PII\", deny_list=deny_list\n    )\n    return deny_list_recognizer\n</pre> def create_ad_hoc_deny_list_recognizer(     deny_list=Optional[List[str]], ) -&gt; Optional[PatternRecognizer]:     if not deny_list:         return None      deny_list_recognizer = PatternRecognizer(         supported_entity=\"GENERIC_PII\", deny_list=deny_list     )     return deny_list_recognizer In\u00a0[\u00a0]: Copied! <pre>def create_ad_hoc_regex_recognizer(\n    regex: str, entity_type: str, score: float, context: Optional[List[str]] = None\n) -&gt; Optional[PatternRecognizer]:\n    if not regex:\n        return None\n    pattern = Pattern(name=\"Regex pattern\", regex=regex, score=score)\n    regex_recognizer = PatternRecognizer(\n        supported_entity=entity_type, patterns=[pattern], context=context\n    )\n    return regex_recognizer\n</pre> def create_ad_hoc_regex_recognizer(     regex: str, entity_type: str, score: float, context: Optional[List[str]] = None ) -&gt; Optional[PatternRecognizer]:     if not regex:         return None     pattern = Pattern(name=\"Regex pattern\", regex=regex, score=score)     regex_recognizer = PatternRecognizer(         supported_entity=entity_type, patterns=[pattern], context=context     )     return regex_recognizer"},{"location":"samples/python/streamlit/presidio_nlp_engine_config/","title":"Presidio nlp engine config","text":"In\u00a0[\u00a0]: Copied! <pre>import logging\nfrom typing import Tuple\n</pre> import logging from typing import Tuple In\u00a0[\u00a0]: Copied! <pre>import spacy\nfrom presidio_analyzer import RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import (\n    NlpEngine,\n    NlpEngineProvider,\n)\n</pre> import spacy from presidio_analyzer import RecognizerRegistry from presidio_analyzer.nlp_engine import (     NlpEngine,     NlpEngineProvider, ) In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(\"presidio-streamlit\")\n</pre> logger = logging.getLogger(\"presidio-streamlit\") In\u00a0[\u00a0]: Copied! <pre>def create_nlp_engine_with_spacy(\n    model_path: str,\n) -&gt; Tuple[NlpEngine, RecognizerRegistry]:\n    \"\"\"\n    Instantiate an NlpEngine with a spaCy model\n    :param model_path: path to model / model name.\n    \"\"\"\n    nlp_configuration = {\n        \"nlp_engine_name\": \"spacy\",\n        \"models\": [{\"lang_code\": \"en\", \"model_name\": model_path}],\n        \"ner_model_configuration\": {\n            \"model_to_presidio_entity_mapping\": {\n                \"PER\": \"PERSON\",\n                \"PERSON\": \"PERSON\",\n                \"NORP\": \"NRP\",\n                \"FAC\": \"FACILITY\",\n                \"LOC\": \"LOCATION\",\n                \"GPE\": \"LOCATION\",\n                \"LOCATION\": \"LOCATION\",\n                \"ORG\": \"ORGANIZATION\",\n                \"ORGANIZATION\": \"ORGANIZATION\",\n                \"DATE\": \"DATE_TIME\",\n                \"TIME\": \"DATE_TIME\",\n            },\n            \"low_confidence_score_multiplier\": 0.4,\n            \"low_score_entity_names\": [\"ORG\", \"ORGANIZATION\"],\n        },\n    }\n\n    nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n\n    registry = RecognizerRegistry()\n    registry.load_predefined_recognizers(nlp_engine=nlp_engine)\n\n    return nlp_engine, registry\n</pre> def create_nlp_engine_with_spacy(     model_path: str, ) -&gt; Tuple[NlpEngine, RecognizerRegistry]:     \"\"\"     Instantiate an NlpEngine with a spaCy model     :param model_path: path to model / model name.     \"\"\"     nlp_configuration = {         \"nlp_engine_name\": \"spacy\",         \"models\": [{\"lang_code\": \"en\", \"model_name\": model_path}],         \"ner_model_configuration\": {             \"model_to_presidio_entity_mapping\": {                 \"PER\": \"PERSON\",                 \"PERSON\": \"PERSON\",                 \"NORP\": \"NRP\",                 \"FAC\": \"FACILITY\",                 \"LOC\": \"LOCATION\",                 \"GPE\": \"LOCATION\",                 \"LOCATION\": \"LOCATION\",                 \"ORG\": \"ORGANIZATION\",                 \"ORGANIZATION\": \"ORGANIZATION\",                 \"DATE\": \"DATE_TIME\",                 \"TIME\": \"DATE_TIME\",             },             \"low_confidence_score_multiplier\": 0.4,             \"low_score_entity_names\": [\"ORG\", \"ORGANIZATION\"],         },     }      nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()      registry = RecognizerRegistry()     registry.load_predefined_recognizers(nlp_engine=nlp_engine)      return nlp_engine, registry In\u00a0[\u00a0]: Copied! <pre>def create_nlp_engine_with_stanza(\n    model_path: str,\n) -&gt; Tuple[NlpEngine, RecognizerRegistry]:\n    \"\"\"\n    Instantiate an NlpEngine with a stanza model\n    :param model_path: path to model / model name.\n    \"\"\"\n    nlp_configuration = {\n        \"nlp_engine_name\": \"stanza\",\n        \"models\": [{\"lang_code\": \"en\", \"model_name\": model_path}],\n        \"ner_model_configuration\": {\n            \"model_to_presidio_entity_mapping\": {\n                \"PER\": \"PERSON\",\n                \"PERSON\": \"PERSON\",\n                \"NORP\": \"NRP\",\n                \"FAC\": \"FACILITY\",\n                \"LOC\": \"LOCATION\",\n                \"GPE\": \"LOCATION\",\n                \"LOCATION\": \"LOCATION\",\n                \"ORG\": \"ORGANIZATION\",\n                \"ORGANIZATION\": \"ORGANIZATION\",\n                \"DATE\": \"DATE_TIME\",\n                \"TIME\": \"DATE_TIME\",\n            }\n        },\n    }\n\n    nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n\n    registry = RecognizerRegistry()\n    registry.load_predefined_recognizers(nlp_engine=nlp_engine)\n\n    return nlp_engine, registry\n</pre> def create_nlp_engine_with_stanza(     model_path: str, ) -&gt; Tuple[NlpEngine, RecognizerRegistry]:     \"\"\"     Instantiate an NlpEngine with a stanza model     :param model_path: path to model / model name.     \"\"\"     nlp_configuration = {         \"nlp_engine_name\": \"stanza\",         \"models\": [{\"lang_code\": \"en\", \"model_name\": model_path}],         \"ner_model_configuration\": {             \"model_to_presidio_entity_mapping\": {                 \"PER\": \"PERSON\",                 \"PERSON\": \"PERSON\",                 \"NORP\": \"NRP\",                 \"FAC\": \"FACILITY\",                 \"LOC\": \"LOCATION\",                 \"GPE\": \"LOCATION\",                 \"LOCATION\": \"LOCATION\",                 \"ORG\": \"ORGANIZATION\",                 \"ORGANIZATION\": \"ORGANIZATION\",                 \"DATE\": \"DATE_TIME\",                 \"TIME\": \"DATE_TIME\",             }         },     }      nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()      registry = RecognizerRegistry()     registry.load_predefined_recognizers(nlp_engine=nlp_engine)      return nlp_engine, registry In\u00a0[\u00a0]: Copied! <pre>def create_nlp_engine_with_transformers(\n    model_path: str,\n) -&gt; Tuple[NlpEngine, RecognizerRegistry]:\n    \"\"\"\n    Instantiate an NlpEngine with a TransformersRecognizer and a small spaCy model.\n    The TransformersRecognizer would return results from Transformers models, the spaCy model\n    would return NlpArtifacts such as POS and lemmas.\n    :param model_path: HuggingFace model path.\n    \"\"\"\n    print(f\"Loading Transformers model: {model_path} of type {type(model_path)}\")\n\n    nlp_configuration = {\n        \"nlp_engine_name\": \"transformers\",\n        \"models\": [\n            {\n                \"lang_code\": \"en\",\n                \"model_name\": {\"spacy\": \"en_core_web_sm\", \"transformers\": model_path},\n            }\n        ],\n        \"ner_model_configuration\": {\n            \"model_to_presidio_entity_mapping\": {\n                \"PER\": \"PERSON\",\n                \"PERSON\": \"PERSON\",\n                \"LOC\": \"LOCATION\",\n                \"LOCATION\": \"LOCATION\",\n                \"GPE\": \"LOCATION\",\n                \"ORG\": \"ORGANIZATION\",\n                \"ORGANIZATION\": \"ORGANIZATION\",\n                \"NORP\": \"NRP\",\n                \"AGE\": \"AGE\",\n                \"ID\": \"ID\",\n                \"EMAIL\": \"EMAIL\",\n                \"PATIENT\": \"PERSON\",\n                \"STAFF\": \"PERSON\",\n                \"HOSP\": \"ORGANIZATION\",\n                \"PATORG\": \"ORGANIZATION\",\n                \"DATE\": \"DATE_TIME\",\n                \"TIME\": \"DATE_TIME\",\n                \"PHONE\": \"PHONE_NUMBER\",\n                \"HCW\": \"PERSON\",\n                \"HOSPITAL\": \"ORGANIZATION\",\n                \"FACILITY\": \"LOCATION\",\n            },\n            \"low_confidence_score_multiplier\": 0.4,\n            \"low_score_entity_names\": [\"ID\"],\n            \"labels_to_ignore\": [\n                \"CARDINAL\",\n                \"EVENT\",\n                \"LANGUAGE\",\n                \"LAW\",\n                \"MONEY\",\n                \"ORDINAL\",\n                \"PERCENT\",\n                \"PRODUCT\",\n                \"QUANTITY\",\n                \"WORK_OF_ART\",\n            ],\n        },\n    }\n\n    nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n\n    registry = RecognizerRegistry()\n    registry.load_predefined_recognizers(nlp_engine=nlp_engine)\n\n    return nlp_engine, registry\n</pre> def create_nlp_engine_with_transformers(     model_path: str, ) -&gt; Tuple[NlpEngine, RecognizerRegistry]:     \"\"\"     Instantiate an NlpEngine with a TransformersRecognizer and a small spaCy model.     The TransformersRecognizer would return results from Transformers models, the spaCy model     would return NlpArtifacts such as POS and lemmas.     :param model_path: HuggingFace model path.     \"\"\"     print(f\"Loading Transformers model: {model_path} of type {type(model_path)}\")      nlp_configuration = {         \"nlp_engine_name\": \"transformers\",         \"models\": [             {                 \"lang_code\": \"en\",                 \"model_name\": {\"spacy\": \"en_core_web_sm\", \"transformers\": model_path},             }         ],         \"ner_model_configuration\": {             \"model_to_presidio_entity_mapping\": {                 \"PER\": \"PERSON\",                 \"PERSON\": \"PERSON\",                 \"LOC\": \"LOCATION\",                 \"LOCATION\": \"LOCATION\",                 \"GPE\": \"LOCATION\",                 \"ORG\": \"ORGANIZATION\",                 \"ORGANIZATION\": \"ORGANIZATION\",                 \"NORP\": \"NRP\",                 \"AGE\": \"AGE\",                 \"ID\": \"ID\",                 \"EMAIL\": \"EMAIL\",                 \"PATIENT\": \"PERSON\",                 \"STAFF\": \"PERSON\",                 \"HOSP\": \"ORGANIZATION\",                 \"PATORG\": \"ORGANIZATION\",                 \"DATE\": \"DATE_TIME\",                 \"TIME\": \"DATE_TIME\",                 \"PHONE\": \"PHONE_NUMBER\",                 \"HCW\": \"PERSON\",                 \"HOSPITAL\": \"ORGANIZATION\",                 \"FACILITY\": \"LOCATION\",             },             \"low_confidence_score_multiplier\": 0.4,             \"low_score_entity_names\": [\"ID\"],             \"labels_to_ignore\": [                 \"CARDINAL\",                 \"EVENT\",                 \"LANGUAGE\",                 \"LAW\",                 \"MONEY\",                 \"ORDINAL\",                 \"PERCENT\",                 \"PRODUCT\",                 \"QUANTITY\",                 \"WORK_OF_ART\",             ],         },     }      nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()      registry = RecognizerRegistry()     registry.load_predefined_recognizers(nlp_engine=nlp_engine)      return nlp_engine, registry In\u00a0[\u00a0]: Copied! <pre>def create_nlp_engine_with_flair(\n    model_path: str,\n) -&gt; Tuple[NlpEngine, RecognizerRegistry]:\n    \"\"\"\n    Instantiate an NlpEngine with a FlairRecognizer and a small spaCy model.\n    The FlairRecognizer would return results from Flair models, the spaCy model\n    would return NlpArtifacts such as POS and lemmas.\n    :param model_path: Flair model path.\n    \"\"\"\n    from flair_recognizer import FlairRecognizer\n\n    registry = RecognizerRegistry()\n    registry.load_predefined_recognizers()\n\n    # there is no official Flair NlpEngine, hence we load it as an additional recognizer\n\n    if not spacy.util.is_package(\"en_core_web_sm\"):\n        spacy.cli.download(\"en_core_web_sm\")\n    # Using a small spaCy model + a Flair NER model\n    flair_recognizer = FlairRecognizer(model_path=model_path)\n    nlp_configuration = {\n        \"nlp_engine_name\": \"spacy\",\n        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n    }\n    registry.add_recognizer(flair_recognizer)\n    registry.remove_recognizer(\"SpacyRecognizer\")\n\n    nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n\n    return nlp_engine, registry\n</pre> def create_nlp_engine_with_flair(     model_path: str, ) -&gt; Tuple[NlpEngine, RecognizerRegistry]:     \"\"\"     Instantiate an NlpEngine with a FlairRecognizer and a small spaCy model.     The FlairRecognizer would return results from Flair models, the spaCy model     would return NlpArtifacts such as POS and lemmas.     :param model_path: Flair model path.     \"\"\"     from flair_recognizer import FlairRecognizer      registry = RecognizerRegistry()     registry.load_predefined_recognizers()      # there is no official Flair NlpEngine, hence we load it as an additional recognizer      if not spacy.util.is_package(\"en_core_web_sm\"):         spacy.cli.download(\"en_core_web_sm\")     # Using a small spaCy model + a Flair NER model     flair_recognizer = FlairRecognizer(model_path=model_path)     nlp_configuration = {         \"nlp_engine_name\": \"spacy\",         \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],     }     registry.add_recognizer(flair_recognizer)     registry.remove_recognizer(\"SpacyRecognizer\")      nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()      return nlp_engine, registry In\u00a0[\u00a0]: Copied! <pre>def create_nlp_engine_with_azure_ai_language(ta_key: str, ta_endpoint: str):\n    \"\"\"\n    Instantiate an NlpEngine with a TextAnalyticsWrapper and a small spaCy model.\n    The TextAnalyticsWrapper would return results from calling Azure Text Analytics PII, the spaCy model\n    would return NlpArtifacts such as POS and lemmas.\n    :param ta_key: Azure Text Analytics key.\n    :param ta_endpoint: Azure Text Analytics endpoint.\n    \"\"\"\n    from azure_ai_language_wrapper import AzureAIServiceWrapper\n\n    if not ta_key or not ta_endpoint:\n        raise RuntimeError(\"Please fill in the Text Analytics endpoint details\")\n\n    registry = RecognizerRegistry()\n    registry.load_predefined_recognizers()\n\n    azure_ai_language_recognizer = AzureAIServiceWrapper(\n        ta_endpoint=ta_endpoint, ta_key=ta_key\n    )\n    nlp_configuration = {\n        \"nlp_engine_name\": \"spacy\",\n        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n    }\n\n    nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n\n    registry.add_recognizer(azure_ai_language_recognizer)\n    registry.remove_recognizer(\"SpacyRecognizer\")\n\n    return nlp_engine, registry\n</pre> def create_nlp_engine_with_azure_ai_language(ta_key: str, ta_endpoint: str):     \"\"\"     Instantiate an NlpEngine with a TextAnalyticsWrapper and a small spaCy model.     The TextAnalyticsWrapper would return results from calling Azure Text Analytics PII, the spaCy model     would return NlpArtifacts such as POS and lemmas.     :param ta_key: Azure Text Analytics key.     :param ta_endpoint: Azure Text Analytics endpoint.     \"\"\"     from azure_ai_language_wrapper import AzureAIServiceWrapper      if not ta_key or not ta_endpoint:         raise RuntimeError(\"Please fill in the Text Analytics endpoint details\")      registry = RecognizerRegistry()     registry.load_predefined_recognizers()      azure_ai_language_recognizer = AzureAIServiceWrapper(         ta_endpoint=ta_endpoint, ta_key=ta_key     )     nlp_configuration = {         \"nlp_engine_name\": \"spacy\",         \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],     }      nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()      registry.add_recognizer(azure_ai_language_recognizer)     registry.remove_recognizer(\"SpacyRecognizer\")      return nlp_engine, registry"},{"location":"samples/python/streamlit/presidio_streamlit/","title":"Presidio streamlit","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Streamlit app for Presidio.\"\"\"\nimport logging\nimport os\nimport traceback\n</pre> \"\"\"Streamlit app for Presidio.\"\"\" import logging import os import traceback In\u00a0[\u00a0]: Copied! <pre>import dotenv\nimport pandas as pd\nimport streamlit as st\nimport streamlit.components.v1 as components\nfrom annotated_text import annotated_text\nfrom streamlit_tags import st_tags\n</pre> import dotenv import pandas as pd import streamlit as st import streamlit.components.v1 as components from annotated_text import annotated_text from streamlit_tags import st_tags In\u00a0[\u00a0]: Copied! <pre>from openai_fake_data_generator import OpenAIParams\nfrom presidio_helpers import (\n    get_supported_entities,\n    analyze,\n    anonymize,\n    annotate,\n    create_fake_data,\n    analyzer_engine,\n)\n</pre> from openai_fake_data_generator import OpenAIParams from presidio_helpers import (     get_supported_entities,     analyze,     anonymize,     annotate,     create_fake_data,     analyzer_engine, ) In\u00a0[\u00a0]: Copied! <pre>st.set_page_config(\n    page_title=\"Presidio demo\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\",\n    menu_items={\n        \"About\": \"https://microsoft.github.io/presidio/\",\n    },\n)\n</pre> st.set_page_config(     page_title=\"Presidio demo\",     layout=\"wide\",     initial_sidebar_state=\"expanded\",     menu_items={         \"About\": \"https://microsoft.github.io/presidio/\",     }, ) In\u00a0[\u00a0]: Copied! <pre>dotenv.load_dotenv()\nlogger = logging.getLogger(\"presidio-streamlit\")\n</pre> dotenv.load_dotenv() logger = logging.getLogger(\"presidio-streamlit\") In\u00a0[\u00a0]: Copied! <pre>allow_other_models = os.getenv(\"ALLOW_OTHER_MODELS\", False)\n</pre> allow_other_models = os.getenv(\"ALLOW_OTHER_MODELS\", False) In\u00a0[\u00a0]: Copied! <pre># Sidebar\nst.sidebar.header(\n    \"\"\"\nPII De-Identification with [Microsoft Presidio](https://microsoft.github.io/presidio/)\n\"\"\"\n)\n</pre> # Sidebar st.sidebar.header(     \"\"\" PII De-Identification with [Microsoft Presidio](https://microsoft.github.io/presidio/) \"\"\" ) In\u00a0[\u00a0]: Copied! <pre>model_help_text = \"\"\"\n    Select which Named Entity Recognition (NER) model to use for PII detection, in parallel to rule-based recognizers.\n    Presidio supports multiple NER packages off-the-shelf, such as spaCy, Huggingface, Stanza and Flair,\n    as well as services such as Azure AI Language PII.\n    \"\"\"\nst_ta_key = st_ta_endpoint = \"\"\n</pre> model_help_text = \"\"\"     Select which Named Entity Recognition (NER) model to use for PII detection, in parallel to rule-based recognizers.     Presidio supports multiple NER packages off-the-shelf, such as spaCy, Huggingface, Stanza and Flair,     as well as services such as Azure AI Language PII.     \"\"\" st_ta_key = st_ta_endpoint = \"\" In\u00a0[\u00a0]: Copied! <pre>model_list = [\n    \"spaCy/en_core_web_lg\",\n    \"flair/ner-english-large\",\n    \"HuggingFace/obi/deid_roberta_i2b2\",\n    \"HuggingFace/StanfordAIMI/stanford-deidentifier-base\",\n    \"stanza/en\",\n    \"Azure AI Language\",\n    \"Other\",\n]\nif not allow_other_models:\n    model_list.pop()\n# Select model\nst_model = st.sidebar.selectbox(\n    \"NER model package\",\n    model_list,\n    index=2,\n    help=model_help_text,\n)\n</pre> model_list = [     \"spaCy/en_core_web_lg\",     \"flair/ner-english-large\",     \"HuggingFace/obi/deid_roberta_i2b2\",     \"HuggingFace/StanfordAIMI/stanford-deidentifier-base\",     \"stanza/en\",     \"Azure AI Language\",     \"Other\", ] if not allow_other_models:     model_list.pop() # Select model st_model = st.sidebar.selectbox(     \"NER model package\",     model_list,     index=2,     help=model_help_text, ) In\u00a0[\u00a0]: Copied! <pre># Extract model package.\nst_model_package = st_model.split(\"/\")[0]\n</pre> # Extract model package. st_model_package = st_model.split(\"/\")[0] In\u00a0[\u00a0]: Copied! <pre># Remove package prefix (if needed)\nst_model = (\n    st_model\n    if st_model_package.lower() not in (\"spacy\", \"stanza\", \"huggingface\")\n    else \"/\".join(st_model.split(\"/\")[1:])\n)\n</pre> # Remove package prefix (if needed) st_model = (     st_model     if st_model_package.lower() not in (\"spacy\", \"stanza\", \"huggingface\")     else \"/\".join(st_model.split(\"/\")[1:]) ) In\u00a0[\u00a0]: Copied! <pre>if st_model == \"Other\":\n    st_model_package = st.sidebar.selectbox(\n        \"NER model OSS package\", options=[\"spaCy\", \"stanza\", \"Flair\", \"HuggingFace\"]\n    )\n    st_model = st.sidebar.text_input(f\"NER model name\", value=\"\")\n</pre> if st_model == \"Other\":     st_model_package = st.sidebar.selectbox(         \"NER model OSS package\", options=[\"spaCy\", \"stanza\", \"Flair\", \"HuggingFace\"]     )     st_model = st.sidebar.text_input(f\"NER model name\", value=\"\") In\u00a0[\u00a0]: Copied! <pre>if st_model == \"Azure AI Language\":\n    st_ta_key = st.sidebar.text_input(\n        f\"Azure AI Language key\", value=os.getenv(\"TA_KEY\", \"\"), type=\"password\"\n    )\n    st_ta_endpoint = st.sidebar.text_input(\n        f\"Azure AI Language endpoint\",\n        value=os.getenv(\"TA_ENDPOINT\", default=\"\"),\n        help=\"For more info: https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/personally-identifiable-information/overview\",  # noqa: E501\n    )\n</pre> if st_model == \"Azure AI Language\":     st_ta_key = st.sidebar.text_input(         f\"Azure AI Language key\", value=os.getenv(\"TA_KEY\", \"\"), type=\"password\"     )     st_ta_endpoint = st.sidebar.text_input(         f\"Azure AI Language endpoint\",         value=os.getenv(\"TA_ENDPOINT\", default=\"\"),         help=\"For more info: https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/personally-identifiable-information/overview\",  # noqa: E501     ) In\u00a0[\u00a0]: Copied! <pre>st.sidebar.warning(\"Note: Models might take some time to download. \")\n</pre> st.sidebar.warning(\"Note: Models might take some time to download. \") In\u00a0[\u00a0]: Copied! <pre>analyzer_params = (st_model_package, st_model, st_ta_key, st_ta_endpoint)\nlogger.debug(f\"analyzer_params: {analyzer_params}\")\n</pre> analyzer_params = (st_model_package, st_model, st_ta_key, st_ta_endpoint) logger.debug(f\"analyzer_params: {analyzer_params}\") In\u00a0[\u00a0]: Copied! <pre>st_operator = st.sidebar.selectbox(\n    \"De-identification approach\",\n    [\"redact\", \"replace\", \"synthesize\", \"highlight\", \"mask\", \"hash\", \"encrypt\"],\n    index=1,\n    help=\"\"\"\n    Select which manipulation to the text is requested after PII has been identified.\\n\n    - Redact: Completely remove the PII text\\n\n    - Replace: Replace the PII text with a constant, e.g. &lt;PERSON&gt;\\n\n    - Synthesize: Replace with fake values (requires an OpenAI key)\\n\n    - Highlight: Shows the original text with PII highlighted in colors\\n\n    - Mask: Replaces a requested number of characters with an asterisk (or other mask character)\\n\n    - Hash: Replaces with the hash of the PII string\\n\n    - Encrypt: Replaces with an AES encryption of the PII string, allowing the process to be reversed\n         \"\"\",\n)\nst_mask_char = \"*\"\nst_number_of_chars = 15\nst_encrypt_key = \"WmZq4t7w!z%C&amp;F)J\"\n</pre> st_operator = st.sidebar.selectbox(     \"De-identification approach\",     [\"redact\", \"replace\", \"synthesize\", \"highlight\", \"mask\", \"hash\", \"encrypt\"],     index=1,     help=\"\"\"     Select which manipulation to the text is requested after PII has been identified.\\n     - Redact: Completely remove the PII text\\n     - Replace: Replace the PII text with a constant, e.g. \\n     - Synthesize: Replace with fake values (requires an OpenAI key)\\n     - Highlight: Shows the original text with PII highlighted in colors\\n     - Mask: Replaces a requested number of characters with an asterisk (or other mask character)\\n     - Hash: Replaces with the hash of the PII string\\n     - Encrypt: Replaces with an AES encryption of the PII string, allowing the process to be reversed          \"\"\", ) st_mask_char = \"*\" st_number_of_chars = 15 st_encrypt_key = \"WmZq4t7w!z%C&amp;F)J\" In\u00a0[\u00a0]: Copied! <pre>open_ai_params = None\n</pre> open_ai_params = None In\u00a0[\u00a0]: Copied! <pre>logger.debug(f\"st_operator: {st_operator}\")\n</pre> logger.debug(f\"st_operator: {st_operator}\") In\u00a0[\u00a0]: Copied! <pre>def set_up_openai_synthesis():\n    \"\"\"Set up the OpenAI API key and model for text synthesis.\"\"\"\n\n    if os.getenv(\"OPENAI_TYPE\", default=\"openai\") == \"Azure\":\n        openai_api_type = \"azure\"\n        st_openai_api_base = st.sidebar.text_input(\n            \"Azure OpenAI base URL\",\n            value=os.getenv(\"AZURE_OPENAI_ENDPOINT\", default=\"\"),\n        )\n        openai_key = os.getenv(\"AZURE_OPENAI_KEY\", default=\"\")\n        st_deployment_id = st.sidebar.text_input(\n            \"Deployment name\", value=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", default=\"\")\n        )\n        st_openai_version = st.sidebar.text_input(\n            \"OpenAI version\",\n            value=os.getenv(\"OPENAI_API_VERSION\", default=\"2023-05-15\"),\n        )\n    else:\n        openai_api_type = \"openai\"\n        st_openai_version = st_openai_api_base = None\n        st_deployment_id = \"\"\n        openai_key = os.getenv(\"OPENAI_KEY\", default=\"\")\n    st_openai_key = st.sidebar.text_input(\n        \"OPENAI_KEY\",\n        value=openai_key,\n        help=\"See https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key for more info.\",\n        type=\"password\",\n    )\n    st_openai_model = st.sidebar.text_input(\n        \"OpenAI model for text synthesis\",\n        value=os.getenv(\"OPENAI_MODEL\", default=\"text-davinci-003\"),\n        help=\"See more here: https://platform.openai.com/docs/models/\",\n    )\n    return (\n        openai_api_type,\n        st_openai_api_base,\n        st_deployment_id,\n        st_openai_version,\n        st_openai_key,\n        st_openai_model,\n    )\n</pre> def set_up_openai_synthesis():     \"\"\"Set up the OpenAI API key and model for text synthesis.\"\"\"      if os.getenv(\"OPENAI_TYPE\", default=\"openai\") == \"Azure\":         openai_api_type = \"azure\"         st_openai_api_base = st.sidebar.text_input(             \"Azure OpenAI base URL\",             value=os.getenv(\"AZURE_OPENAI_ENDPOINT\", default=\"\"),         )         openai_key = os.getenv(\"AZURE_OPENAI_KEY\", default=\"\")         st_deployment_id = st.sidebar.text_input(             \"Deployment name\", value=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", default=\"\")         )         st_openai_version = st.sidebar.text_input(             \"OpenAI version\",             value=os.getenv(\"OPENAI_API_VERSION\", default=\"2023-05-15\"),         )     else:         openai_api_type = \"openai\"         st_openai_version = st_openai_api_base = None         st_deployment_id = \"\"         openai_key = os.getenv(\"OPENAI_KEY\", default=\"\")     st_openai_key = st.sidebar.text_input(         \"OPENAI_KEY\",         value=openai_key,         help=\"See https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key for more info.\",         type=\"password\",     )     st_openai_model = st.sidebar.text_input(         \"OpenAI model for text synthesis\",         value=os.getenv(\"OPENAI_MODEL\", default=\"text-davinci-003\"),         help=\"See more here: https://platform.openai.com/docs/models/\",     )     return (         openai_api_type,         st_openai_api_base,         st_deployment_id,         st_openai_version,         st_openai_key,         st_openai_model,     ) In\u00a0[\u00a0]: Copied! <pre>if st_operator == \"mask\":\n    st_number_of_chars = st.sidebar.number_input(\n        \"number of chars\", value=st_number_of_chars, min_value=0, max_value=100\n    )\n    st_mask_char = st.sidebar.text_input(\n        \"Mask character\", value=st_mask_char, max_chars=1\n    )\nelif st_operator == \"encrypt\":\n    st_encrypt_key = st.sidebar.text_input(\"AES key\", value=st_encrypt_key)\nelif st_operator == \"synthesize\":\n    (\n        openai_api_type,\n        st_openai_api_base,\n        st_deployment_id,\n        st_openai_version,\n        st_openai_key,\n        st_openai_model,\n    ) = set_up_openai_synthesis()\n\n    open_ai_params = OpenAIParams(\n        openai_key=st_openai_key,\n        model=st_openai_model,\n        api_base=st_openai_api_base,\n        deployment_id=st_deployment_id,\n        api_version=st_openai_version,\n        api_type=openai_api_type,\n    )\n</pre> if st_operator == \"mask\":     st_number_of_chars = st.sidebar.number_input(         \"number of chars\", value=st_number_of_chars, min_value=0, max_value=100     )     st_mask_char = st.sidebar.text_input(         \"Mask character\", value=st_mask_char, max_chars=1     ) elif st_operator == \"encrypt\":     st_encrypt_key = st.sidebar.text_input(\"AES key\", value=st_encrypt_key) elif st_operator == \"synthesize\":     (         openai_api_type,         st_openai_api_base,         st_deployment_id,         st_openai_version,         st_openai_key,         st_openai_model,     ) = set_up_openai_synthesis()      open_ai_params = OpenAIParams(         openai_key=st_openai_key,         model=st_openai_model,         api_base=st_openai_api_base,         deployment_id=st_deployment_id,         api_version=st_openai_version,         api_type=openai_api_type,     ) In\u00a0[\u00a0]: Copied! <pre>st_threshold = st.sidebar.slider(\n    label=\"Acceptance threshold\",\n    min_value=0.0,\n    max_value=1.0,\n    value=0.35,\n    help=\"Define the threshold for accepting a detection as PII. See more here: \",\n)\n</pre> st_threshold = st.sidebar.slider(     label=\"Acceptance threshold\",     min_value=0.0,     max_value=1.0,     value=0.35,     help=\"Define the threshold for accepting a detection as PII. See more here: \", ) In\u00a0[\u00a0]: Copied! <pre>st_return_decision_process = st.sidebar.checkbox(\n    \"Add analysis explanations to findings\",\n    value=False,\n    help=\"Add the decision process to the output table. \"\n    \"More information can be found here: https://microsoft.github.io/presidio/analyzer/decision_process/\",\n)\n</pre> st_return_decision_process = st.sidebar.checkbox(     \"Add analysis explanations to findings\",     value=False,     help=\"Add the decision process to the output table. \"     \"More information can be found here: https://microsoft.github.io/presidio/analyzer/decision_process/\", ) In\u00a0[\u00a0]: Copied! <pre># Allow and deny lists\nst_deny_allow_expander = st.sidebar.expander(\n    \"Allowlists and denylists\",\n    expanded=False,\n)\n</pre> # Allow and deny lists st_deny_allow_expander = st.sidebar.expander(     \"Allowlists and denylists\",     expanded=False, ) In\u00a0[\u00a0]: Copied! <pre>with st_deny_allow_expander:\n    st_allow_list = st_tags(\n        label=\"Add words to the allowlist\", text=\"Enter word and press enter.\"\n    )\n    st.caption(\n        \"Allowlists contain words that are not considered PII, but are detected as such.\"\n    )\n\n    st_deny_list = st_tags(\n        label=\"Add words to the denylist\", text=\"Enter word and press enter.\"\n    )\n    st.caption(\n        \"Denylists contain words that are considered PII, but are not detected as such.\"\n    )\n# Main panel\n</pre> with st_deny_allow_expander:     st_allow_list = st_tags(         label=\"Add words to the allowlist\", text=\"Enter word and press enter.\"     )     st.caption(         \"Allowlists contain words that are not considered PII, but are detected as such.\"     )      st_deny_list = st_tags(         label=\"Add words to the denylist\", text=\"Enter word and press enter.\"     )     st.caption(         \"Denylists contain words that are considered PII, but are not detected as such.\"     ) # Main panel In\u00a0[\u00a0]: Copied! <pre>with st.expander(\"About this demo\", expanded=False):\n    st.info(\n        \"\"\"Presidio is an open source customizable framework for PII detection and de-identification.\n        \\n\\n[Code](https://aka.ms/presidio) | \n        [Tutorial](https://microsoft.github.io/presidio/tutorial/) | \n        [Installation](https://microsoft.github.io/presidio/installation/) | \n        [FAQ](https://microsoft.github.io/presidio/faq/) |\n        [Feedback](https://forms.office.com/r/9ufyYjfDaY) |\"\"\"\n    )\n\n    st.info(\n        \"\"\"\n    Use this demo to:\n    - Experiment with different off-the-shelf models and NLP packages.\n    - Explore the different de-identification options, including redaction, masking, encryption and more.\n    - Generate synthetic text with Microsoft Presidio and OpenAI.\n    - Configure allow and deny lists.\n    \n    This demo website shows some of Presidio's capabilities.\n    [Visit our website](https://microsoft.github.io/presidio) for more info,\n    samples and deployment options.    \n    \"\"\"\n    )\n\n    st.markdown(\n        \"[![Pypi Downloads](https://img.shields.io/pypi/dm/presidio-analyzer.svg)](https://img.shields.io/pypi/dm/presidio-analyzer.svg)\"  # noqa\n        \"[![MIT license](https://img.shields.io/badge/license-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)\"\n        \"![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/presidio?style=social)\"\n    )\n</pre> with st.expander(\"About this demo\", expanded=False):     st.info(         \"\"\"Presidio is an open source customizable framework for PII detection and de-identification.         \\n\\n[Code](https://aka.ms/presidio) |          [Tutorial](https://microsoft.github.io/presidio/tutorial/) |          [Installation](https://microsoft.github.io/presidio/installation/) |          [FAQ](https://microsoft.github.io/presidio/faq/) |         [Feedback](https://forms.office.com/r/9ufyYjfDaY) |\"\"\"     )      st.info(         \"\"\"     Use this demo to:     - Experiment with different off-the-shelf models and NLP packages.     - Explore the different de-identification options, including redaction, masking, encryption and more.     - Generate synthetic text with Microsoft Presidio and OpenAI.     - Configure allow and deny lists.          This demo website shows some of Presidio's capabilities.     [Visit our website](https://microsoft.github.io/presidio) for more info,     samples and deployment options.         \"\"\"     )      st.markdown(         \"[![Pypi Downloads](https://img.shields.io/pypi/dm/presidio-analyzer.svg)](https://img.shields.io/pypi/dm/presidio-analyzer.svg)\"  # noqa         \"[![MIT license](https://img.shields.io/badge/license-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)\"         \"![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/presidio?style=social)\"     ) In\u00a0[\u00a0]: Copied! <pre>analyzer_load_state = st.info(\"Starting Presidio analyzer...\")\n</pre> analyzer_load_state = st.info(\"Starting Presidio analyzer...\") In\u00a0[\u00a0]: Copied! <pre>analyzer_load_state.empty()\n</pre> analyzer_load_state.empty() In\u00a0[\u00a0]: Copied! <pre># Read default text\nwith open(\"demo_text.txt\") as f:\n    demo_text = f.readlines()\n</pre> # Read default text with open(\"demo_text.txt\") as f:     demo_text = f.readlines() In\u00a0[\u00a0]: Copied! <pre># Create two columns for before and after\ncol1, col2 = st.columns(2)\n</pre> # Create two columns for before and after col1, col2 = st.columns(2) In\u00a0[\u00a0]: Copied! <pre># Before:\ncol1.subheader(\"Input\")\nst_text = col1.text_area(\n    label=\"Enter text\", value=\"\".join(demo_text), height=400, key=\"text_input\"\n)\n</pre> # Before: col1.subheader(\"Input\") st_text = col1.text_area(     label=\"Enter text\", value=\"\".join(demo_text), height=400, key=\"text_input\" ) In\u00a0[\u00a0]: Copied! <pre>try:\n    # Choose entities\n    st_entities_expander = st.sidebar.expander(\"Choose entities to look for\")\n    st_entities = st_entities_expander.multiselect(\n        label=\"Which entities to look for?\",\n        options=get_supported_entities(*analyzer_params),\n        default=list(get_supported_entities(*analyzer_params)),\n        help=\"Limit the list of PII entities detected. \"\n        \"This list is dynamic and based on the NER model and registered recognizers. \"\n        \"More information can be found here: https://microsoft.github.io/presidio/analyzer/adding_recognizers/\",\n    )\n\n    # Before\n    analyzer_load_state = st.info(\"Starting Presidio analyzer...\")\n    analyzer = analyzer_engine(*analyzer_params)\n    analyzer_load_state.empty()\n\n    st_analyze_results = analyze(\n        *analyzer_params,\n        text=st_text,\n        entities=st_entities,\n        language=\"en\",\n        score_threshold=st_threshold,\n        return_decision_process=st_return_decision_process,\n        allow_list=st_allow_list,\n        deny_list=st_deny_list,\n    )\n\n    # After\n    if st_operator not in (\"highlight\", \"synthesize\"):\n        with col2:\n            st.subheader(f\"Output\")\n            st_anonymize_results = anonymize(\n                text=st_text,\n                operator=st_operator,\n                mask_char=st_mask_char,\n                number_of_chars=st_number_of_chars,\n                encrypt_key=st_encrypt_key,\n                analyze_results=st_analyze_results,\n            )\n            st.text_area(\n                label=\"De-identified\", value=st_anonymize_results.text, height=400\n            )\n    elif st_operator == \"synthesize\":\n        with col2:\n            st.subheader(f\"OpenAI Generated output\")\n            fake_data = create_fake_data(\n                st_text,\n                st_analyze_results,\n                open_ai_params,\n            )\n            st.text_area(label=\"Synthetic data\", value=fake_data, height=400)\n    else:\n        st.subheader(\"Highlighted\")\n        annotated_tokens = annotate(text=st_text, analyze_results=st_analyze_results)\n        # annotated_tokens\n        annotated_text(*annotated_tokens)\n\n    # table result\n    st.subheader(\n        \"Findings\"\n        if not st_return_decision_process\n        else \"Findings with decision factors\"\n    )\n    if st_analyze_results:\n        df = pd.DataFrame.from_records([r.to_dict() for r in st_analyze_results])\n        df[\"text\"] = [st_text[res.start : res.end] for res in st_analyze_results]\n\n        df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(\n            {\n                \"entity_type\": \"Entity type\",\n                \"text\": \"Text\",\n                \"start\": \"Start\",\n                \"end\": \"End\",\n                \"score\": \"Confidence\",\n            },\n            axis=1,\n        )\n        df_subset[\"Text\"] = [st_text[res.start : res.end] for res in st_analyze_results]\n        if st_return_decision_process:\n            analysis_explanation_df = pd.DataFrame.from_records(\n                [r.analysis_explanation.to_dict() for r in st_analyze_results]\n            )\n            df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)\n        st.dataframe(df_subset.reset_index(drop=True), use_container_width=True)\n    else:\n        st.text(\"No findings\")\n</pre> try:     # Choose entities     st_entities_expander = st.sidebar.expander(\"Choose entities to look for\")     st_entities = st_entities_expander.multiselect(         label=\"Which entities to look for?\",         options=get_supported_entities(*analyzer_params),         default=list(get_supported_entities(*analyzer_params)),         help=\"Limit the list of PII entities detected. \"         \"This list is dynamic and based on the NER model and registered recognizers. \"         \"More information can be found here: https://microsoft.github.io/presidio/analyzer/adding_recognizers/\",     )      # Before     analyzer_load_state = st.info(\"Starting Presidio analyzer...\")     analyzer = analyzer_engine(*analyzer_params)     analyzer_load_state.empty()      st_analyze_results = analyze(         *analyzer_params,         text=st_text,         entities=st_entities,         language=\"en\",         score_threshold=st_threshold,         return_decision_process=st_return_decision_process,         allow_list=st_allow_list,         deny_list=st_deny_list,     )      # After     if st_operator not in (\"highlight\", \"synthesize\"):         with col2:             st.subheader(f\"Output\")             st_anonymize_results = anonymize(                 text=st_text,                 operator=st_operator,                 mask_char=st_mask_char,                 number_of_chars=st_number_of_chars,                 encrypt_key=st_encrypt_key,                 analyze_results=st_analyze_results,             )             st.text_area(                 label=\"De-identified\", value=st_anonymize_results.text, height=400             )     elif st_operator == \"synthesize\":         with col2:             st.subheader(f\"OpenAI Generated output\")             fake_data = create_fake_data(                 st_text,                 st_analyze_results,                 open_ai_params,             )             st.text_area(label=\"Synthetic data\", value=fake_data, height=400)     else:         st.subheader(\"Highlighted\")         annotated_tokens = annotate(text=st_text, analyze_results=st_analyze_results)         # annotated_tokens         annotated_text(*annotated_tokens)      # table result     st.subheader(         \"Findings\"         if not st_return_decision_process         else \"Findings with decision factors\"     )     if st_analyze_results:         df = pd.DataFrame.from_records([r.to_dict() for r in st_analyze_results])         df[\"text\"] = [st_text[res.start : res.end] for res in st_analyze_results]          df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(             {                 \"entity_type\": \"Entity type\",                 \"text\": \"Text\",                 \"start\": \"Start\",                 \"end\": \"End\",                 \"score\": \"Confidence\",             },             axis=1,         )         df_subset[\"Text\"] = [st_text[res.start : res.end] for res in st_analyze_results]         if st_return_decision_process:             analysis_explanation_df = pd.DataFrame.from_records(                 [r.analysis_explanation.to_dict() for r in st_analyze_results]             )             df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)         st.dataframe(df_subset.reset_index(drop=True), use_container_width=True)     else:         st.text(\"No findings\") In\u00a0[\u00a0]: Copied! <pre>except Exception as e:\n    print(e)\n    traceback.print_exc()\n    st.error(e)\n</pre> except Exception as e:     print(e)     traceback.print_exc()     st.error(e) In\u00a0[\u00a0]: Copied! <pre>components.html(\n    \"\"\"\n    &lt;script type=\"text/javascript\"&gt;\n    (function(c,l,a,r,i,t,y){\n        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};\n        t=l.createElement(r);t.async=1;t.src=\"https://www.clarity.ms/tag/\"+i;\n        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);\n    })(window, document, \"clarity\", \"script\", \"h7f8bp42n8\");\n    &lt;/script&gt;\n    \"\"\"\n)\n</pre> components.html(     \"\"\"          \"\"\" )"},{"location":"samples/python/streamlit/test_streamlit/","title":"Test streamlit","text":"In\u00a0[\u00a0]: Copied! <pre>from presidio_helpers import analyzer_engine, analyze, anonymize\n</pre> from presidio_helpers import analyzer_engine, analyze, anonymize In\u00a0[\u00a0]: Copied! <pre>def test_streamlit_logic():\n    st_model = \"en\"  # st_model = \"StanfordAIMI/stanford-deidentifier-base\"\n    st_model_package = \"stanza\"  ##st_model_package = \"HuggingFace\"\n    st_ta_key = None\n    st_ta_endpoint = None\n\n    analyzer_params = (st_model_package, st_model, st_ta_key, st_ta_endpoint)\n\n    # Read default text\n    with open(\"demo_text.txt\") as f:\n        demo_text = f.readlines()\n\n    st_text = \"\".join(demo_text)\n\n    # instantiate and cache AnalyzerEngine\n    analyzer_engine(*analyzer_params)\n\n    # Analyze\n    st_analyze_results = analyze(\n        *analyzer_params,\n        text=st_text,\n        entities=\"All\",\n        language=\"en\",\n        score_threshold=0.35,\n        return_decision_process=True,\n        allow_list=[],\n        deny_list=[],\n    )\n\n    # Anonymize\n    st_anonymize_results = anonymize(\n        text=st_text,\n        operator=\"replace\",\n        mask_char=None,\n        number_of_chars=None,\n        encrypt_key=None,\n        analyze_results=st_analyze_results,\n    )\n\n    assert st_anonymize_results.text != \"\"\n</pre> def test_streamlit_logic():     st_model = \"en\"  # st_model = \"StanfordAIMI/stanford-deidentifier-base\"     st_model_package = \"stanza\"  ##st_model_package = \"HuggingFace\"     st_ta_key = None     st_ta_endpoint = None      analyzer_params = (st_model_package, st_model, st_ta_key, st_ta_endpoint)      # Read default text     with open(\"demo_text.txt\") as f:         demo_text = f.readlines()      st_text = \"\".join(demo_text)      # instantiate and cache AnalyzerEngine     analyzer_engine(*analyzer_params)      # Analyze     st_analyze_results = analyze(         *analyzer_params,         text=st_text,         entities=\"All\",         language=\"en\",         score_threshold=0.35,         return_decision_process=True,         allow_list=[],         deny_list=[],     )      # Anonymize     st_anonymize_results = anonymize(         text=st_text,         operator=\"replace\",         mask_char=None,         number_of_chars=None,         encrypt_key=None,         analyze_results=st_analyze_results,     )      assert st_anonymize_results.text != \"\""},{"location":"samples/python/text_analytics/","title":"Azure AI Language Integration","text":""},{"location":"samples/python/text_analytics/#introduction","title":"Introduction","text":"<p>Azure Text Analytics is a cloud-based service that provides advanced natural language processing over raw text. One of its main functions includes  Named Entity Recognition (NER), which has the ability to identify different entities in text and categorize them into pre-defined classes or types. This document will demonstrate Presidio integration with Azure Text Analytics.</p>"},{"location":"samples/python/text_analytics/#supported-entity-categories-in-the-text-analytics-api","title":"Supported entity categories in the Text Analytics API","text":"<p>Azure AI Language supports multiple PII entity categories. The Azure AI Laguage service runs a predictive model to identify and categorize named entities from an input document. The service's latest version includes the ability to detect personal (PII) and health (PHI) information. A list of all supported entities can be found in the official documentation.</p>"},{"location":"samples/python/text_analytics/#prerequisites","title":"Prerequisites","text":"<p>To use Azure AI Language with Preisido, an Azure AI Language resource should first be created under an Azure subscription. Follow the official documentation for instructions. The key and endpoint, generated once the resource is created,  will be used when integrating with Text Analytics, using a Presidio Text Analytics recognizer.</p>"},{"location":"samples/python/text_analytics/#azure-ai-language-recognizer","title":"Azure AI Language Recognizer","text":"<p>The implementation of a <code>AzureAILanguage</code> recognizer can be found here.</p>"},{"location":"samples/python/text_analytics/#how-to-integrate-azure-ai-language-into-presidio","title":"How to integrate Azure AI Language into Presidio","text":"<ol> <li> <p>Install the package with the azure-ai-language extra:   <pre><code>pip install \"presidio-analyzer[azure-ai-language]\"\n</code></pre></p> </li> <li> <p>Define environment varibles <code>AZURE_AI_KEY</code> and <code>AZURE_AI_ENDPOINT</code></p> </li> <li> <p>Add the <code>AzureAILanguageRecognizer</code> to the recognizer registry:</p> </li> </ol> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.predefined_recognizers import AzureAILanguageRecognizer\n\nazure_ai_language = AzureAILanguageRecognizer()\n\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(azure_ai_language)\n\nanalyzer.analyze(text=\"My email is email@email.com\", language=\"en\")\n</code></pre>"},{"location":"samples/python/text_analytics/__init__/","title":"init","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Presidio + Text Analytics example.\"\"\"\n</pre> \"\"\"Presidio + Text Analytics example.\"\"\""},{"location":"samples/python/transformers_recognizer/","title":"Add a Transformers model based EntityRecognizer","text":"<p>Note</p> <p>This example demonstrates how to create a Presidio Recognizer. To integrate a transformers model as a Presidio NLP Engine, see this documentation.</p> <p>We allow these two options, as a user might want to have multiple NER models running in parallel. In this case, one can create multiple <code>EntityRecognizer</code> instances, each serving a different model. If you only plan to use one NER model, consider creating a <code>TransformersNlpEngine</code> instead of the <code>TransformersRecognizer</code> described in this document.</p> <p>When initializing the <code>TransformersRecognizer</code>, choose from the following options:</p> <ol> <li> <p>A string referencing an uploaded model to HuggingFace. See the different available options for models here.</p> </li> <li> <p>Initialize your own <code>TokenClassificationPipeline</code> instance using your custom transformers model and use it for inference.</p> </li> <li> <p>Provide the path to your own local custom trained model.</p> </li> </ol> <p>Note</p> <p>For each combination of model &amp; dataset, it is recommended to create a configuration object which includes setting necessary parameters for getting the correct results. Please reference this configuration.py file for examples.</p>"},{"location":"samples/python/transformers_recognizer/#example-code","title":"Example Code","text":"<p>This example code uses a <code>TransformersRecognizer</code> for NER, and removes the default <code>SpacyRecognizer</code>. In order to be able to use spaCy features such as lemmas, we introduce the small (and faster) <code>en_core_web_sm</code> model.</p> <p>link to full TransformersRecognizer code</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nimport spacy\n\nmodel_path = \"obi/deid_roberta_i2b2\"\nsupported_entities = BERT_DEID_CONFIGURATION.get(\n    \"PRESIDIO_SUPPORTED_ENTITIES\")\ntransformers_recognizer = TransformersRecognizer(model_path=model_path,\n                                                 supported_entities=supported_entities)\n\n# This would download a large (~500Mb) model on the first run\ntransformers_recognizer.load_transformer(**BERT_DEID_CONFIGURATION)\n\n# Add transformers model to the registry\nregistry = RecognizerRegistry()\nregistry.add_recognizer(transformers_recognizer)\nregistry.remove_recognizer(\"SpacyRecognizer\")\n\n# Use small spacy model, for faster inference.\nif not spacy.util.is_package(\"en_core_web_sm\"):\n    spacy.cli.download(\"en_core_web_sm\")\n\nnlp_configuration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n}\n\nnlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n\nanalyzer = AnalyzerEngine(registry=registry, nlp_engine=nlp_engine)\n\nsample = \"My name is John and I live in NY\"\nresults = analyzer.analyze(sample, language=\"en\",\n                           return_decision_process=True,\n                           )\nprint(\"Found the following entities:\")\nfor result in results:\n    print(result, '----', sample[result.start:result.end])\n</code></pre>"},{"location":"samples/python/transformers_recognizer/__init__/","title":"init","text":""},{"location":"samples/python/transformers_recognizer/configuration/","title":"Configuration","text":"In\u00a0[\u00a0]: Copied! <pre>STANFORD_COFIGURATION = {\n    \"DEFAULT_MODEL_PATH\": \"StanfordAIMI/stanford-deidentifier-base\",\n    \"PRESIDIO_SUPPORTED_ENTITIES\": [\n        \"LOCATION\",\n        \"PERSON\",\n        \"ORGANIZATION\",\n        \"AGE\",\n        \"PHONE_NUMBER\",\n        \"EMAIL\",\n        \"DATE_TIME\",\n        \"DEVICE\",\n        \"ZIP\",\n        \"PROFESSION\",\n        \"USERNAME\",\n        \"ID\"\n\n    ],\n    \"LABELS_TO_IGNORE\": [\"O\"],\n    \"DEFAULT_EXPLANATION\": \"Identified as {} by the StanfordAIMI/stanford-deidentifier-base NER model\",\n    \"SUB_WORD_AGGREGATION\": \"simple\",\n    \"DATASET_TO_PRESIDIO_MAPPING\": {\n        \"DATE\": \"DATE_TIME\",\n        \"DOCTOR\": \"PERSON\",\n        \"PATIENT\": \"PERSON\",\n        \"HOSPITAL\": \"LOCATION\",\n        \"MEDICALRECORD\": \"ID\",\n        \"IDNUM\": \"ID\",\n        \"ORGANIZATION\": \"ORGANIZATION\",\n        \"ZIP\": \"ZIP\",\n        \"PHONE\": \"PHONE_NUMBER\",\n        \"USERNAME\": \"USERNAME\",\n        \"STREET\": \"LOCATION\",\n        \"PROFESSION\": \"PROFESSION\",\n        \"COUNTRY\": \"LOCATION\",\n        \"LOCATION-OTHER\": \"LOCATION\",\n        \"FAX\": \"PHONE_NUMBER\",\n        \"EMAIL\": \"EMAIL\",\n        \"STATE\": \"LOCATION\",\n        \"DEVICE\": \"DEVICE\",\n        \"ORG\": \"ORGANIZATION\",\n        \"AGE\": \"AGE\",\n    },\n    \"MODEL_TO_PRESIDIO_MAPPING\": {\n        \"PER\": \"PERSON\",\n        \"PERSON\": \"PERSON\",\n        \"LOC\": \"LOCATION\",\n        \"ORG\": \"ORGANIZATION\",\n        \"AGE\": \"AGE\",\n        \"PATIENT\": \"PERSON\",\n        \"HCW\": \"PERSON\",\n        \"HOSPITAL\": \"LOCATION\",\n        \"PATORG\": \"ORGANIZATION\",\n        \"DATE\": \"DATE_TIME\",\n        \"PHONE\": \"PHONE_NUMBER\",\n        \"VENDOR\": \"ORGANIZATION\",\n    },\n    \"CHUNK_OVERLAP_SIZE\": 40,\n    \"CHUNK_SIZE\": 600,\n    \"ID_SCORE_MULTIPLIER\": 0.4,\n    \"ID_ENTITY_NAME\": \"ID\"\n}\n</pre> STANFORD_COFIGURATION = {     \"DEFAULT_MODEL_PATH\": \"StanfordAIMI/stanford-deidentifier-base\",     \"PRESIDIO_SUPPORTED_ENTITIES\": [         \"LOCATION\",         \"PERSON\",         \"ORGANIZATION\",         \"AGE\",         \"PHONE_NUMBER\",         \"EMAIL\",         \"DATE_TIME\",         \"DEVICE\",         \"ZIP\",         \"PROFESSION\",         \"USERNAME\",         \"ID\"      ],     \"LABELS_TO_IGNORE\": [\"O\"],     \"DEFAULT_EXPLANATION\": \"Identified as {} by the StanfordAIMI/stanford-deidentifier-base NER model\",     \"SUB_WORD_AGGREGATION\": \"simple\",     \"DATASET_TO_PRESIDIO_MAPPING\": {         \"DATE\": \"DATE_TIME\",         \"DOCTOR\": \"PERSON\",         \"PATIENT\": \"PERSON\",         \"HOSPITAL\": \"LOCATION\",         \"MEDICALRECORD\": \"ID\",         \"IDNUM\": \"ID\",         \"ORGANIZATION\": \"ORGANIZATION\",         \"ZIP\": \"ZIP\",         \"PHONE\": \"PHONE_NUMBER\",         \"USERNAME\": \"USERNAME\",         \"STREET\": \"LOCATION\",         \"PROFESSION\": \"PROFESSION\",         \"COUNTRY\": \"LOCATION\",         \"LOCATION-OTHER\": \"LOCATION\",         \"FAX\": \"PHONE_NUMBER\",         \"EMAIL\": \"EMAIL\",         \"STATE\": \"LOCATION\",         \"DEVICE\": \"DEVICE\",         \"ORG\": \"ORGANIZATION\",         \"AGE\": \"AGE\",     },     \"MODEL_TO_PRESIDIO_MAPPING\": {         \"PER\": \"PERSON\",         \"PERSON\": \"PERSON\",         \"LOC\": \"LOCATION\",         \"ORG\": \"ORGANIZATION\",         \"AGE\": \"AGE\",         \"PATIENT\": \"PERSON\",         \"HCW\": \"PERSON\",         \"HOSPITAL\": \"LOCATION\",         \"PATORG\": \"ORGANIZATION\",         \"DATE\": \"DATE_TIME\",         \"PHONE\": \"PHONE_NUMBER\",         \"VENDOR\": \"ORGANIZATION\",     },     \"CHUNK_OVERLAP_SIZE\": 40,     \"CHUNK_SIZE\": 600,     \"ID_SCORE_MULTIPLIER\": 0.4,     \"ID_ENTITY_NAME\": \"ID\" } In\u00a0[\u00a0]: Copied! <pre>BERT_DEID_CONFIGURATION = {\n    \"PRESIDIO_SUPPORTED_ENTITIES\": [\n        \"LOCATION\",\n        \"PERSON\",\n        \"ORGANIZATION\",\n        \"AGE\",\n        \"PHONE_NUMBER\",\n        \"EMAIL\",\n        \"DATE_TIME\",\n        \"ZIP\",\n        \"PROFESSION\",\n        \"USERNAME\",\n        \"ID\"\n    ],\n    \"DEFAULT_MODEL_PATH\": \"obi/deid_roberta_i2b2\",\n    \"LABELS_TO_IGNORE\": [\"O\"],\n    \"DEFAULT_EXPLANATION\": \"Identified as {} by the obi/deid_roberta_i2b2 NER model\",\n    \"SUB_WORD_AGGREGATION\": \"simple\",\n    \"DATASET_TO_PRESIDIO_MAPPING\": {\n        \"DATE\": \"DATE_TIME\",\n        \"DOCTOR\": \"PERSON\",\n        \"PATIENT\": \"PERSON\",\n        \"HOSPITAL\": \"ORGANIZATION\",\n        \"MEDICALRECORD\": \"O\",\n        \"IDNUM\": \"O\",\n        \"ORGANIZATION\": \"ORGANIZATION\",\n        \"ZIP\": \"O\",\n        \"PHONE\": \"PHONE_NUMBER\",\n        \"USERNAME\": \"\",\n        \"STREET\": \"LOCATION\",\n        \"PROFESSION\": \"PROFESSION\",\n        \"COUNTRY\": \"LOCATION\",\n        \"LOCATION-OTHER\": \"LOCATION\",\n        \"FAX\": \"PHONE_NUMBER\",\n        \"EMAIL\": \"EMAIL\",\n        \"STATE\": \"LOCATION\",\n        \"DEVICE\": \"O\",\n        \"ORG\": \"ORGANIZATION\",\n        \"AGE\": \"AGE\",\n    },\n    \"MODEL_TO_PRESIDIO_MAPPING\": {\n        \"PER\": \"PERSON\",\n        \"LOC\": \"LOCATION\",\n        \"ORG\": \"ORGANIZATION\",\n        \"AGE\": \"AGE\",\n        \"ID\": \"ID\",\n        \"EMAIL\": \"EMAIL\",\n        \"PATIENT\": \"PERSON\",\n        \"STAFF\": \"PERSON\",\n        \"HOSP\": \"ORGANIZATION\",\n        \"PATORG\": \"ORGANIZATION\",\n        \"DATE\": \"DATE_TIME\",\n        \"PHONE\": \"PHONE_NUMBER\",\n    },\n    \"CHUNK_OVERLAP_SIZE\": 40,\n    \"CHUNK_SIZE\": 600,\n    \"ID_SCORE_MULTIPLIER\": 0.4,\n    \"ID_ENTITY_NAME\": \"ID\"\n}\n</pre> BERT_DEID_CONFIGURATION = {     \"PRESIDIO_SUPPORTED_ENTITIES\": [         \"LOCATION\",         \"PERSON\",         \"ORGANIZATION\",         \"AGE\",         \"PHONE_NUMBER\",         \"EMAIL\",         \"DATE_TIME\",         \"ZIP\",         \"PROFESSION\",         \"USERNAME\",         \"ID\"     ],     \"DEFAULT_MODEL_PATH\": \"obi/deid_roberta_i2b2\",     \"LABELS_TO_IGNORE\": [\"O\"],     \"DEFAULT_EXPLANATION\": \"Identified as {} by the obi/deid_roberta_i2b2 NER model\",     \"SUB_WORD_AGGREGATION\": \"simple\",     \"DATASET_TO_PRESIDIO_MAPPING\": {         \"DATE\": \"DATE_TIME\",         \"DOCTOR\": \"PERSON\",         \"PATIENT\": \"PERSON\",         \"HOSPITAL\": \"ORGANIZATION\",         \"MEDICALRECORD\": \"O\",         \"IDNUM\": \"O\",         \"ORGANIZATION\": \"ORGANIZATION\",         \"ZIP\": \"O\",         \"PHONE\": \"PHONE_NUMBER\",         \"USERNAME\": \"\",         \"STREET\": \"LOCATION\",         \"PROFESSION\": \"PROFESSION\",         \"COUNTRY\": \"LOCATION\",         \"LOCATION-OTHER\": \"LOCATION\",         \"FAX\": \"PHONE_NUMBER\",         \"EMAIL\": \"EMAIL\",         \"STATE\": \"LOCATION\",         \"DEVICE\": \"O\",         \"ORG\": \"ORGANIZATION\",         \"AGE\": \"AGE\",     },     \"MODEL_TO_PRESIDIO_MAPPING\": {         \"PER\": \"PERSON\",         \"LOC\": \"LOCATION\",         \"ORG\": \"ORGANIZATION\",         \"AGE\": \"AGE\",         \"ID\": \"ID\",         \"EMAIL\": \"EMAIL\",         \"PATIENT\": \"PERSON\",         \"STAFF\": \"PERSON\",         \"HOSP\": \"ORGANIZATION\",         \"PATORG\": \"ORGANIZATION\",         \"DATE\": \"DATE_TIME\",         \"PHONE\": \"PHONE_NUMBER\",     },     \"CHUNK_OVERLAP_SIZE\": 40,     \"CHUNK_SIZE\": 600,     \"ID_SCORE_MULTIPLIER\": 0.4,     \"ID_ENTITY_NAME\": \"ID\" }"},{"location":"samples/python/transformers_recognizer/transformer_recognizer/","title":"Transformer recognizer","text":"In\u00a0[\u00a0]: Copied! <pre>import copy\nimport logging\nfrom typing import Optional, List\n</pre> import copy import logging from typing import Optional, List In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom presidio_analyzer import (\n    RecognizerResult,\n    EntityRecognizer,\n    AnalysisExplanation,\n)\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n</pre> import torch from presidio_analyzer import (     RecognizerResult,     EntityRecognizer,     AnalysisExplanation, ) from presidio_analyzer.nlp_engine import NlpArtifacts In\u00a0[\u00a0]: Copied! <pre>from .configuration import BERT_DEID_CONFIGURATION\n</pre> from .configuration import BERT_DEID_CONFIGURATION In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(\"presidio-analyzer\")\n</pre> logger = logging.getLogger(\"presidio-analyzer\") In\u00a0[\u00a0]: Copied! <pre>try:\n    from transformers import (\n        AutoTokenizer,\n        AutoModelForTokenClassification,\n        pipeline,\n        TokenClassificationPipeline,\n    )\n</pre> try:     from transformers import (         AutoTokenizer,         AutoModelForTokenClassification,         pipeline,         TokenClassificationPipeline,     ) In\u00a0[\u00a0]: Copied! <pre>except ImportError:\n    logger.error(\"transformers is not installed\")\n</pre> except ImportError:     logger.error(\"transformers is not installed\") In\u00a0[\u00a0]: Copied! <pre>class TransformersRecognizer(EntityRecognizer):\n    \"\"\"\n    Wrapper for a transformers model, if needed to be used within Presidio Analyzer.\n    The class loads models hosted on HuggingFace - https://huggingface.co/\n    and loads the model and tokenizer into a TokenClassification pipeline.\n    Samples are split into short text chunks, ideally shorter than max_length input_ids of the individual model,\n    to avoid truncation by the Tokenizer and loss of information\n\n    A configuration object should be maintained for each dataset-model combination and translate\n    entities names into a standardized view. A sample of a configuration file is attached in\n    the example.\n    :param supported_entities: List of entities to run inference on\n    :type supported_entities: Optional[List[str]]\n    :param pipeline: Instance of a TokenClassificationPipeline including a Tokenizer and a Model, defaults to None\n    :type pipeline: Optional[TokenClassificationPipeline], optional\n    :param model_path: string referencing a HuggingFace uploaded model to be used for Inference, defaults to None\n    :type model_path: Optional[str], optional\n\n    :example\n    &gt;from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n    &gt;model_path = \"obi/deid_roberta_i2b2\"\n    &gt;transformers_recognizer = TransformersRecognizer(model_path=model_path,\n    &gt;supported_entities = model_configuration.get(\"PRESIDIO_SUPPORTED_ENTITIES\"))\n    &gt;transformers_recognizer.load_transformer(**model_configuration)\n    &gt;registry = RecognizerRegistry()\n    &gt;registry.add_recognizer(transformers_recognizer)\n    &gt;analyzer = AnalyzerEngine(registry=registry)\n    &gt;sample = \"My name is Christopher and I live in Irbid.\"\n    &gt;results = analyzer.analyze(sample, language=\"en\",return_decision_process=True)\n\n    &gt;for result in results:\n    &gt;    print(result,'----', sample[result.start:result.end])\n    \"\"\"\n\n    def load(self) -&gt; None:\n        pass\n\n    def __init__(\n        self,\n        model_path: Optional[str] = None,\n        pipeline: Optional[TokenClassificationPipeline] = None,\n        supported_entities: Optional[List[str]] = None,\n    ):\n        if not supported_entities:\n            supported_entities = BERT_DEID_CONFIGURATION[\n                \"PRESIDIO_SUPPORTED_ENTITIES\"\n            ]\n        super().__init__(\n            supported_entities=supported_entities,\n            name=f\"Transformers model {model_path}\",\n        )\n\n        self.model_path = model_path\n        self.pipeline = pipeline\n        self.is_loaded = False\n\n        self.aggregation_mechanism = None\n        self.ignore_labels = None\n        self.model_to_presidio_mapping = None\n        self.entity_mapping = None\n        self.default_explanation = None\n        self.text_overlap_length = None\n        self.chunk_length = None\n        self.id_entity_name = None\n        self.id_score_reduction = None\n\n    def load_transformer(self, **kwargs) -&gt; None:\n        \"\"\"Load external configuration parameters and set default values.\n\n        :param kwargs: define default values for class attributes and modify pipeline behavior\n        **DATASET_TO_PRESIDIO_MAPPING (dict) - defines mapping entity strings from dataset format to Presidio format\n        **MODEL_TO_PRESIDIO_MAPPING (dict) -  defines mapping entity strings from chosen model format to Presidio format\n        **SUB_WORD_AGGREGATION(str) - define how to aggregate sub-word tokens into full words and spans as defined\n        in HuggingFace https://huggingface.co/transformers/v4.8.0/main_classes/pipelines.html#transformers.TokenClassificationPipeline # noqa\n        **CHUNK_OVERLAP_SIZE (int) - number of overlapping characters in each text chunk\n        when splitting a single text into multiple inferences\n        **CHUNK_SIZE (int) - number of characters in each chunk of text\n        **LABELS_TO_IGNORE (List(str)) - List of entities to skip evaluation. Defaults to [\"O\"]\n        **DEFAULT_EXPLANATION (str) - string format to use for prediction explanations\n        **ID_ENTITY_NAME (str) - name of the ID entity\n        **ID_SCORE_REDUCTION (float) - score multiplier for ID entities\n        \"\"\"\n\n        self.entity_mapping = kwargs.get(\"DATASET_TO_PRESIDIO_MAPPING\", {})\n        self.model_to_presidio_mapping = kwargs.get(\"MODEL_TO_PRESIDIO_MAPPING\", {})\n        self.ignore_labels = kwargs.get(\"LABELS_TO_IGNORE\", [\"O\"])\n        self.aggregation_mechanism = kwargs.get(\"SUB_WORD_AGGREGATION\", \"simple\")\n        self.default_explanation = kwargs.get(\"DEFAULT_EXPLANATION\", None)\n        self.text_overlap_length = kwargs.get(\"CHUNK_OVERLAP_SIZE\", 40)\n        self.chunk_length = kwargs.get(\"CHUNK_SIZE\", 600)\n        self.id_entity_name = kwargs.get(\"ID_ENTITY_NAME\", \"ID\")\n        self.id_score_reduction = kwargs.get(\"ID_SCORE_REDUCTION\", 0.5)\n\n        if not self.pipeline:\n            if not self.model_path:\n                self.model_path = \"obi/deid_roberta_i2b2\"\n                logger.warning(\n                    f\"Both 'model' and 'model_path' arguments are None. Using default model_path={self.model_path}\"\n                )\n\n        self._load_pipeline()\n\n    def _load_pipeline(self) -&gt; None:\n        \"\"\"Initialize NER transformers pipeline using the model_path provided\"\"\"\n\n        logging.debug(f\"Initializing NER pipeline using {self.model_path} path\")\n        device = 0 if torch.cuda.is_available() else -1\n        self.pipeline = pipeline(\n            \"ner\",\n            model=AutoModelForTokenClassification.from_pretrained(self.model_path),\n            tokenizer=AutoTokenizer.from_pretrained(self.model_path),\n            # Will attempt to group sub-entities to word level\n            aggregation_strategy=self.aggregation_mechanism,\n            device=device,\n            framework=\"pt\",\n            ignore_labels=self.ignore_labels,\n        )\n\n        self.is_loaded = True\n\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"\n        Return supported entities by this model.\n        :return: List of the supported entities.\n        \"\"\"\n        return self.supported_entities\n\n    # Class to use transformers with Presidio as an external recognizer.\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyze text using transformers model to produce NER tagging.\n        :param text : The text for analysis.\n        :param entities: The list of entities this recognizer is able to detect\n        :param nlp_artifacts: Not used by this recognizer.\n        :return: The list of Presidio RecognizerResult constructed from the recognized\n            transformers detections.\n        \"\"\"\n\n        results = list()\n        # Run transformer model on the provided text\n        ner_results = self._get_ner_results_for_text(text)\n\n        for res in ner_results:\n            res[\"entity_group\"] = self.__check_label_transformer(res[\"entity_group\"])\n            if not res[\"entity_group\"] or res[\"entity_group\"] not in entities:\n                continue\n\n            if res[\"entity_group\"] == self.id_entity_name:\n                print(f\"ID entity found, multiplying score by {self.id_score_reduction}\")\n                res[\"score\"] = res[\"score\"] * self.id_score_reduction\n\n            textual_explanation = self.default_explanation.format(res[\"entity_group\"])\n            explanation = self.build_transformers_explanation(\n                float(round(res[\"score\"], 2)), textual_explanation, res[\"word\"]\n            )\n            transformers_result = self._convert_to_recognizer_result(res, explanation)\n\n            results.append(transformers_result)\n\n        return results\n\n    @staticmethod\n    def split_text_to_word_chunks(\n        input_length: int, chunk_length: int, overlap_length: int\n    ) -&gt; List[List]:\n        \"\"\"The function calculates chunks of text with size chunk_length. Each chunk has overlap_length number of\n        words to create context and continuity for the model\n\n        :param input_length: Length of input_ids for a given text\n        :type input_length: int\n        :param chunk_length: Length of each chunk of input_ids.\n        Should match the max input length of the transformer model\n        :type chunk_length: int\n        :param overlap_length: Number of overlapping words in each chunk\n        :type overlap_length: int\n        :return: List of start and end positions for individual text chunks\n        :rtype: List[List]\n        \"\"\"\n        if input_length &lt; chunk_length:\n            return [[0, input_length]]\n        if chunk_length &lt;= overlap_length:\n            logger.warning(\n                \"overlap_length should be shorter than chunk_length, setting overlap_length to by half of chunk_length\"\n            )\n            overlap_length = chunk_length // 2\n        return [\n            [i, min([i + chunk_length, input_length])]\n            for i in range(\n                0, input_length - overlap_length, chunk_length - overlap_length\n            )\n        ]\n\n    def _get_ner_results_for_text(self, text: str) -&gt; List[dict]:\n        \"\"\"The function runs model inference on the provided text.\n        The text is split into chunks with n overlapping characters.\n        The results are then aggregated and duplications are removed.\n\n        :param text: The text to run inference on\n        :type text: str\n        :return: List of entity predictions on the word level\n        :rtype: List[dict]\n        \"\"\"\n        model_max_length = self.pipeline.tokenizer.model_max_length\n        # calculate inputs based on the text\n        text_length = len(text)\n        # split text into chunks\n        if text_length &lt;= model_max_length:\n            predictions = self.pipeline(text)\n        else:\n            logger.info(\n                f\"splitting the text into chunks, length {text_length} &gt; {model_max_length}\"\n            )\n            predictions = list()\n            chunk_indexes = TransformersRecognizer.split_text_to_word_chunks(\n                text_length, self.chunk_length, self.text_overlap_length\n                )\n\n            # iterate over text chunks and run inference\n            for chunk_start, chunk_end in chunk_indexes:\n                chunk_text = text[chunk_start:chunk_end]\n                chunk_preds = self.pipeline(chunk_text)\n\n                # align indexes to match the original text - add to each position the value of chunk_start\n                aligned_predictions = list()\n                for prediction in chunk_preds:\n                    prediction_tmp = copy.deepcopy(prediction)\n                    prediction_tmp[\"start\"] += chunk_start\n                    prediction_tmp[\"end\"] += chunk_start\n                    aligned_predictions.append(prediction_tmp)\n\n                predictions.extend(aligned_predictions)\n\n        # remove duplicates\n        predictions = [dict(t) for t in {tuple(d.items()) for d in predictions}]\n        return predictions\n\n    @staticmethod\n    def _convert_to_recognizer_result(\n        prediction_result: dict, explanation: AnalysisExplanation\n    ) -&gt; RecognizerResult:\n        \"\"\"The method parses NER model predictions into a RecognizerResult format to enable down the stream analysis\n\n        :param prediction_result: A single example of entity prediction\n        :type prediction_result: dict\n        :param explanation: Textual representation of model prediction\n        :type explanation: str\n        :return: An instance of RecognizerResult which is used to model evaluation calculations\n        :rtype: RecognizerResult\n        \"\"\"\n\n        transformers_results = RecognizerResult(\n            entity_type=prediction_result[\"entity_group\"],\n            start=prediction_result[\"start\"],\n            end=prediction_result[\"end\"],\n            score=float(round(prediction_result[\"score\"], 2)),\n            analysis_explanation=explanation,\n        )\n\n        return transformers_results\n\n    def build_transformers_explanation(\n        self,\n        original_score: float,\n        explanation: str,\n        pattern: str,\n    ) -&gt; AnalysisExplanation:\n        \"\"\"\n        Create explanation for why this result was detected.\n        :param original_score: Score given by this recognizer\n        :param explanation: Explanation string\n        :param pattern: Regex pattern used\n        :return Structured explanation and scores of a NER model prediction\n        :rtype: AnalysisExplanation\n        \"\"\"\n        explanation = AnalysisExplanation(\n            recognizer=self.__class__.__name__,\n            original_score=float(original_score),\n            textual_explanation=explanation,\n            pattern=pattern,\n        )\n        return explanation\n\n    def __check_label_transformer(self, label: str) -&gt; Optional[str]:\n        \"\"\"The function validates the predicted label is identified by Presidio\n        and maps the string into a Presidio representation\n        :param label: Predicted label by the model\n        :return: Returns the adjusted entity name\n        \"\"\"\n\n        # convert model label to presidio label\n        entity = self.model_to_presidio_mapping.get(label, None)\n\n        if entity in self.ignore_labels:\n            return None\n\n        if entity is None:\n            logger.warning(f\"Found unrecognized label {label}, returning entity as is\")\n            return label\n\n        if entity not in self.supported_entities:\n            logger.warning(f\"Found entity {entity} which is not supported by Presidio\")\n            return entity\n        return entity\n</pre> class TransformersRecognizer(EntityRecognizer):     \"\"\"     Wrapper for a transformers model, if needed to be used within Presidio Analyzer.     The class loads models hosted on HuggingFace - https://huggingface.co/     and loads the model and tokenizer into a TokenClassification pipeline.     Samples are split into short text chunks, ideally shorter than max_length input_ids of the individual model,     to avoid truncation by the Tokenizer and loss of information      A configuration object should be maintained for each dataset-model combination and translate     entities names into a standardized view. A sample of a configuration file is attached in     the example.     :param supported_entities: List of entities to run inference on     :type supported_entities: Optional[List[str]]     :param pipeline: Instance of a TokenClassificationPipeline including a Tokenizer and a Model, defaults to None     :type pipeline: Optional[TokenClassificationPipeline], optional     :param model_path: string referencing a HuggingFace uploaded model to be used for Inference, defaults to None     :type model_path: Optional[str], optional      :example     &gt;from presidio_analyzer import AnalyzerEngine, RecognizerRegistry     &gt;model_path = \"obi/deid_roberta_i2b2\"     &gt;transformers_recognizer = TransformersRecognizer(model_path=model_path,     &gt;supported_entities = model_configuration.get(\"PRESIDIO_SUPPORTED_ENTITIES\"))     &gt;transformers_recognizer.load_transformer(**model_configuration)     &gt;registry = RecognizerRegistry()     &gt;registry.add_recognizer(transformers_recognizer)     &gt;analyzer = AnalyzerEngine(registry=registry)     &gt;sample = \"My name is Christopher and I live in Irbid.\"     &gt;results = analyzer.analyze(sample, language=\"en\",return_decision_process=True)      &gt;for result in results:     &gt;    print(result,'----', sample[result.start:result.end])     \"\"\"      def load(self) -&gt; None:         pass      def __init__(         self,         model_path: Optional[str] = None,         pipeline: Optional[TokenClassificationPipeline] = None,         supported_entities: Optional[List[str]] = None,     ):         if not supported_entities:             supported_entities = BERT_DEID_CONFIGURATION[                 \"PRESIDIO_SUPPORTED_ENTITIES\"             ]         super().__init__(             supported_entities=supported_entities,             name=f\"Transformers model {model_path}\",         )          self.model_path = model_path         self.pipeline = pipeline         self.is_loaded = False          self.aggregation_mechanism = None         self.ignore_labels = None         self.model_to_presidio_mapping = None         self.entity_mapping = None         self.default_explanation = None         self.text_overlap_length = None         self.chunk_length = None         self.id_entity_name = None         self.id_score_reduction = None      def load_transformer(self, **kwargs) -&gt; None:         \"\"\"Load external configuration parameters and set default values.          :param kwargs: define default values for class attributes and modify pipeline behavior         **DATASET_TO_PRESIDIO_MAPPING (dict) - defines mapping entity strings from dataset format to Presidio format         **MODEL_TO_PRESIDIO_MAPPING (dict) -  defines mapping entity strings from chosen model format to Presidio format         **SUB_WORD_AGGREGATION(str) - define how to aggregate sub-word tokens into full words and spans as defined         in HuggingFace https://huggingface.co/transformers/v4.8.0/main_classes/pipelines.html#transformers.TokenClassificationPipeline # noqa         **CHUNK_OVERLAP_SIZE (int) - number of overlapping characters in each text chunk         when splitting a single text into multiple inferences         **CHUNK_SIZE (int) - number of characters in each chunk of text         **LABELS_TO_IGNORE (List(str)) - List of entities to skip evaluation. Defaults to [\"O\"]         **DEFAULT_EXPLANATION (str) - string format to use for prediction explanations         **ID_ENTITY_NAME (str) - name of the ID entity         **ID_SCORE_REDUCTION (float) - score multiplier for ID entities         \"\"\"          self.entity_mapping = kwargs.get(\"DATASET_TO_PRESIDIO_MAPPING\", {})         self.model_to_presidio_mapping = kwargs.get(\"MODEL_TO_PRESIDIO_MAPPING\", {})         self.ignore_labels = kwargs.get(\"LABELS_TO_IGNORE\", [\"O\"])         self.aggregation_mechanism = kwargs.get(\"SUB_WORD_AGGREGATION\", \"simple\")         self.default_explanation = kwargs.get(\"DEFAULT_EXPLANATION\", None)         self.text_overlap_length = kwargs.get(\"CHUNK_OVERLAP_SIZE\", 40)         self.chunk_length = kwargs.get(\"CHUNK_SIZE\", 600)         self.id_entity_name = kwargs.get(\"ID_ENTITY_NAME\", \"ID\")         self.id_score_reduction = kwargs.get(\"ID_SCORE_REDUCTION\", 0.5)          if not self.pipeline:             if not self.model_path:                 self.model_path = \"obi/deid_roberta_i2b2\"                 logger.warning(                     f\"Both 'model' and 'model_path' arguments are None. Using default model_path={self.model_path}\"                 )          self._load_pipeline()      def _load_pipeline(self) -&gt; None:         \"\"\"Initialize NER transformers pipeline using the model_path provided\"\"\"          logging.debug(f\"Initializing NER pipeline using {self.model_path} path\")         device = 0 if torch.cuda.is_available() else -1         self.pipeline = pipeline(             \"ner\",             model=AutoModelForTokenClassification.from_pretrained(self.model_path),             tokenizer=AutoTokenizer.from_pretrained(self.model_path),             # Will attempt to group sub-entities to word level             aggregation_strategy=self.aggregation_mechanism,             device=device,             framework=\"pt\",             ignore_labels=self.ignore_labels,         )          self.is_loaded = True      def get_supported_entities(self) -&gt; List[str]:         \"\"\"         Return supported entities by this model.         :return: List of the supported entities.         \"\"\"         return self.supported_entities      # Class to use transformers with Presidio as an external recognizer.     def analyze(         self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None     ) -&gt; List[RecognizerResult]:         \"\"\"         Analyze text using transformers model to produce NER tagging.         :param text : The text for analysis.         :param entities: The list of entities this recognizer is able to detect         :param nlp_artifacts: Not used by this recognizer.         :return: The list of Presidio RecognizerResult constructed from the recognized             transformers detections.         \"\"\"          results = list()         # Run transformer model on the provided text         ner_results = self._get_ner_results_for_text(text)          for res in ner_results:             res[\"entity_group\"] = self.__check_label_transformer(res[\"entity_group\"])             if not res[\"entity_group\"] or res[\"entity_group\"] not in entities:                 continue              if res[\"entity_group\"] == self.id_entity_name:                 print(f\"ID entity found, multiplying score by {self.id_score_reduction}\")                 res[\"score\"] = res[\"score\"] * self.id_score_reduction              textual_explanation = self.default_explanation.format(res[\"entity_group\"])             explanation = self.build_transformers_explanation(                 float(round(res[\"score\"], 2)), textual_explanation, res[\"word\"]             )             transformers_result = self._convert_to_recognizer_result(res, explanation)              results.append(transformers_result)          return results      @staticmethod     def split_text_to_word_chunks(         input_length: int, chunk_length: int, overlap_length: int     ) -&gt; List[List]:         \"\"\"The function calculates chunks of text with size chunk_length. Each chunk has overlap_length number of         words to create context and continuity for the model          :param input_length: Length of input_ids for a given text         :type input_length: int         :param chunk_length: Length of each chunk of input_ids.         Should match the max input length of the transformer model         :type chunk_length: int         :param overlap_length: Number of overlapping words in each chunk         :type overlap_length: int         :return: List of start and end positions for individual text chunks         :rtype: List[List]         \"\"\"         if input_length &lt; chunk_length:             return [[0, input_length]]         if chunk_length &lt;= overlap_length:             logger.warning(                 \"overlap_length should be shorter than chunk_length, setting overlap_length to by half of chunk_length\"             )             overlap_length = chunk_length // 2         return [             [i, min([i + chunk_length, input_length])]             for i in range(                 0, input_length - overlap_length, chunk_length - overlap_length             )         ]      def _get_ner_results_for_text(self, text: str) -&gt; List[dict]:         \"\"\"The function runs model inference on the provided text.         The text is split into chunks with n overlapping characters.         The results are then aggregated and duplications are removed.          :param text: The text to run inference on         :type text: str         :return: List of entity predictions on the word level         :rtype: List[dict]         \"\"\"         model_max_length = self.pipeline.tokenizer.model_max_length         # calculate inputs based on the text         text_length = len(text)         # split text into chunks         if text_length &lt;= model_max_length:             predictions = self.pipeline(text)         else:             logger.info(                 f\"splitting the text into chunks, length {text_length} &gt; {model_max_length}\"             )             predictions = list()             chunk_indexes = TransformersRecognizer.split_text_to_word_chunks(                 text_length, self.chunk_length, self.text_overlap_length                 )              # iterate over text chunks and run inference             for chunk_start, chunk_end in chunk_indexes:                 chunk_text = text[chunk_start:chunk_end]                 chunk_preds = self.pipeline(chunk_text)                  # align indexes to match the original text - add to each position the value of chunk_start                 aligned_predictions = list()                 for prediction in chunk_preds:                     prediction_tmp = copy.deepcopy(prediction)                     prediction_tmp[\"start\"] += chunk_start                     prediction_tmp[\"end\"] += chunk_start                     aligned_predictions.append(prediction_tmp)                  predictions.extend(aligned_predictions)          # remove duplicates         predictions = [dict(t) for t in {tuple(d.items()) for d in predictions}]         return predictions      @staticmethod     def _convert_to_recognizer_result(         prediction_result: dict, explanation: AnalysisExplanation     ) -&gt; RecognizerResult:         \"\"\"The method parses NER model predictions into a RecognizerResult format to enable down the stream analysis          :param prediction_result: A single example of entity prediction         :type prediction_result: dict         :param explanation: Textual representation of model prediction         :type explanation: str         :return: An instance of RecognizerResult which is used to model evaluation calculations         :rtype: RecognizerResult         \"\"\"          transformers_results = RecognizerResult(             entity_type=prediction_result[\"entity_group\"],             start=prediction_result[\"start\"],             end=prediction_result[\"end\"],             score=float(round(prediction_result[\"score\"], 2)),             analysis_explanation=explanation,         )          return transformers_results      def build_transformers_explanation(         self,         original_score: float,         explanation: str,         pattern: str,     ) -&gt; AnalysisExplanation:         \"\"\"         Create explanation for why this result was detected.         :param original_score: Score given by this recognizer         :param explanation: Explanation string         :param pattern: Regex pattern used         :return Structured explanation and scores of a NER model prediction         :rtype: AnalysisExplanation         \"\"\"         explanation = AnalysisExplanation(             recognizer=self.__class__.__name__,             original_score=float(original_score),             textual_explanation=explanation,             pattern=pattern,         )         return explanation      def __check_label_transformer(self, label: str) -&gt; Optional[str]:         \"\"\"The function validates the predicted label is identified by Presidio         and maps the string into a Presidio representation         :param label: Predicted label by the model         :return: Returns the adjusted entity name         \"\"\"          # convert model label to presidio label         entity = self.model_to_presidio_mapping.get(label, None)          if entity in self.ignore_labels:             return None          if entity is None:             logger.warning(f\"Found unrecognized label {label}, returning entity as is\")             return label          if entity not in self.supported_entities:             logger.warning(f\"Found entity {entity} which is not supported by Presidio\")             return entity         return entity In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n\n    from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n    from presidio_analyzer.nlp_engine import NlpEngineProvider\n    import spacy\n\n    model_path = \"obi/deid_roberta_i2b2\"\n    supported_entities = BERT_DEID_CONFIGURATION.get(\n        \"PRESIDIO_SUPPORTED_ENTITIES\")\n    transformers_recognizer = TransformersRecognizer(model_path=model_path,\n                                                     supported_entities=supported_entities)\n\n    # This would download a large (~500Mb) model on the first run\n    transformers_recognizer.load_transformer(**BERT_DEID_CONFIGURATION)\n\n    # Add transformers model to the registry\n    registry = RecognizerRegistry()\n    registry.add_recognizer(transformers_recognizer)\n    registry.remove_recognizer(\"SpacyRecognizer\")\n\n    # Use small spacy model, for faster inference.\n    if not spacy.util.is_package(\"en_core_web_sm\"):\n        spacy.cli.download(\"en_core_web_sm\")\n\n    nlp_configuration = {\n        \"nlp_engine_name\": \"spacy\",\n        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n    }\n\n    nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n\n    analyzer = AnalyzerEngine(registry=registry, nlp_engine=nlp_engine)\n\n    sample = \"My name is John and I live in NY\"\n    results = analyzer.analyze(sample, language=\"en\",\n                               return_decision_process=True,\n                               )\n    print(\"Found the following entities:\")\n    for result in results:\n        print(result, '----', sample[result.start:result.end])\n\n    # Found the following entities:\n    # type: PERSON, start: 11, end: 15, score: 1.0 ---- John\n    # type: LOCATION, start: 30, end: 32, score: 1.0 ---- NY\n</pre> if __name__ == \"__main__\":      from presidio_analyzer import AnalyzerEngine, RecognizerRegistry     from presidio_analyzer.nlp_engine import NlpEngineProvider     import spacy      model_path = \"obi/deid_roberta_i2b2\"     supported_entities = BERT_DEID_CONFIGURATION.get(         \"PRESIDIO_SUPPORTED_ENTITIES\")     transformers_recognizer = TransformersRecognizer(model_path=model_path,                                                      supported_entities=supported_entities)      # This would download a large (~500Mb) model on the first run     transformers_recognizer.load_transformer(**BERT_DEID_CONFIGURATION)      # Add transformers model to the registry     registry = RecognizerRegistry()     registry.add_recognizer(transformers_recognizer)     registry.remove_recognizer(\"SpacyRecognizer\")      # Use small spacy model, for faster inference.     if not spacy.util.is_package(\"en_core_web_sm\"):         spacy.cli.download(\"en_core_web_sm\")      nlp_configuration = {         \"nlp_engine_name\": \"spacy\",         \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],     }      nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()      analyzer = AnalyzerEngine(registry=registry, nlp_engine=nlp_engine)      sample = \"My name is John and I live in NY\"     results = analyzer.analyze(sample, language=\"en\",                                return_decision_process=True,                                )     print(\"Found the following entities:\")     for result in results:         print(result, '----', sample[result.start:result.end])      # Found the following entities:     # type: PERSON, start: 11, end: 15, score: 1.0 ---- John     # type: LOCATION, start: 30, end: 32, score: 1.0 ---- NY"},{"location":"structured/","title":"Presidio structured","text":""},{"location":"structured/#description","title":"Description","text":"<p>The Presidio structured package is a flexible and customizable framework designed to identify and protect structured sensitive data.</p> <p>This tool extends the capabilities of Presidio, focusing on structured data formats such as tabular formats and semi-structured formats (JSON). It leverages the detection capabilities of Presidio-Analyzer to identify columns or keys containing personally identifiable information (PII), and establishes a mapping between these column/keys names and the detected PII entities.</p> <p>Following the detection, Presidio-Anonymizer is used to apply de-identification techniques to each value in columns identified as containing PII, ensuring the sensitive data is appropriately protected.</p> <p>Note that sensitive data might not be automatically detected in some cases. Consequently, additional systems and protections should be employed.</p>"},{"location":"structured/#installation","title":"Installation","text":""},{"location":"structured/#as-a-python-package","title":"As a python package","text":"<p>To install the <code>presidio-structured</code> package, run the following command:</p> <pre><code>pip install presidio-structured\n</code></pre>"},{"location":"structured/#getting-started","title":"Getting started","text":""},{"location":"structured/#example-1-anonymizing-dataframes","title":"Example 1: Anonymizing DataFrames","text":"<pre><code>import pandas as pd\nfrom presidio_structured import StructuredEngine, PandasAnalysisBuilder\nfrom presidio_anonymizer.entities import OperatorConfig\nfrom faker import Faker # optionally using faker as an example\n\n# Initialize the engine with a Pandas data processor (default)\npandas_engine = StructuredEngine()\n\n# Create a sample DataFrame\nsample_df = pd.DataFrame({'name': ['John Doe', 'Jane Smith'], 'email': ['john.doe@example.com', 'jane.smith@example.com']})\n\n# Generate a tabular analysis which describes PII entities in the DataFrame.\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df)\n\n# Define anonymization operators\nfake = Faker()\noperators = {\n    \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"REDACTED\"}),\n    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.safe_email()})\n}\n\n# Anonymize DataFrame\nanonymized_df = pandas_engine.anonymize(sample_df, tabular_analysis, operators=operators)\nprint(anonymized_df)\n</code></pre>"},{"location":"structured/#example-2-anonymizing-json-data","title":"Example 2: Anonymizing JSON Data","text":"<pre><code>from presidio_structured import StructuredEngine, JsonAnalysisBuilder, StructuredAnalysis, JsonDataProcessor\nfrom presidio_anonymizer.entities import OperatorConfig\nfrom faker import Faker # optionally using faker as an example\n\n# Initialize the engine with a JSON data processor\njson_engine = StructuredEngine(data_processor=JsonDataProcessor())\n\n\n# Sample JSON data\nsample_json = {\n    \"user\": {\n        \"name\": \"John Doe\",\n        \"email\": \"john.doe@example.com\"\n    }\n}\n\n# Generate analysis for simple JSON data\njson_analysis = JsonAnalysisBuilder().generate_analysis(sample_json)\n\n# Define anonymization operators\nfake = Faker() # using faker for email generation.\noperators = {\n    \"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"REDACTED\"}),\n    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": lambda x: fake.safe_email()})\n}\n\n# Anonymize JSON data\nanonymized_json = json_engine.anonymize(sample_json, json_analysis, operators=operators)\nprint(anonymized_json)\n\n# Handling Json Data with nested objects in lists\nsample_complex_json = {\n    \"users\": [\n        {\"name\": \"John Doe\", \"email\": \"john.doe@example.com\"},\n        {\"name\": \"Jane Smith\", \"email\": \"jane.smith@example.com\"}\n    ]\n}\n\n# Nesting objects in lists is not supported in JsonAnalysisBuilder for now,\n# Manually defining the analysis for complex JSON data\njson_complex_analysis = StructuredAnalysis(entity_mapping={\n    \"users.name\": \"PERSON\",\n    \"users.email\": \"EMAIL_ADDRESS\"\n})\n\n# Anonymize complex JSON data\nanonymized_complex_json = json_engine.anonymize(sample_complex_json, json_complex_analysis, operators=operators)\nprint(anonymized_complex_json)\n</code></pre> <p>A more detailed sample can be found here:</p> <ul> <li>https://github.com/microsoft/presidio/blob/main/docs/samples/python/example_structured.ipynb</li> </ul>"},{"location":"structured/#selection-strategy-for-entity-detection-in-tabular-data","title":"Selection Strategy for Entity Detection in Tabular Data","text":"<ul> <li>Most Common (default):  Identifies the most frequently occurring PII entity in a data column or field.</li> <li>Highest Confidence:  Selects PII entities based on the highest confidence scores, irrespective of their occurrence frequency.</li> <li>Mixed:  Combines the strengths of both the above strategies. It selects the entity with the highest confidence score if that score exceeds a specified threshold (controlled by <code>mixed_strategy_threshold</code>); otherwise, it defaults to the most common entity.</li> </ul>"},{"location":"structured/#usage","title":"Usage","text":"<p>Specify the <code>selection_strategy</code> and optionally the <code>mixed_strategy_threshold</code> in the <code>generate_analysis()</code> method:</p> <pre><code># Generate a tabular analysis using the most common strategy\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df)\n\n# Generate a tabular analysis using the highest confidence strategy\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df, selection_strategy=\"highest_confidence\")\n\n# Generate a tabular analysis using the mixed strategy\ntabular_analysis = PandasAnalysisBuilder().generate_analysis(sample_df, selection_strategy=\"mixed\", mixed_strategy_threshold=0.75)\n</code></pre>"},{"location":"structured/#future-work","title":"Future work","text":"<ul> <li>Improve support for datasets with mixed free-text and structure data (e.g. some columns contain free text)</li> <li>Add support for the detection of sensitive column names</li> <li>PySpark implementation</li> <li>Integration of additional anonymization techniques such as K-Anonymity and Differential Privacy.</li> </ul> <p>Contributions are welcome! Please refer to the Contributing Guide.</p>"},{"location":"structured/#more-information","title":"More information","text":"<ul> <li>API documentation</li> <li>Sample code</li> <li>Join the discussion</li> <li>Relevant issues on Github</li> </ul>"},{"location":"tutorial/","title":"Tutorial: Customization in Microsoft Presidio","text":"<p>This tutorials covers different customization use cases to:</p> <ol> <li>Adapt Presidio to detect new types of PII entities.</li> <li>Adapt Presidio to detect PII entities in a new language.</li> <li>Embed new types of detection modules into Presidio, to improve the coverage of the service.</li> <li>Operate on identified entities: simple de-identification, custom operators and encryption.</li> </ol>"},{"location":"tutorial/#table-of-contents","title":"Table of contents","text":"<ul> <li>Getting started</li> <li>Deny-list based recognizers</li> <li>Regex based PII recognition</li> <li>Rule based logic recognizer</li> <li>Supporting new models and languages</li> <li>Calling an external service for PII detection</li> <li>Using context words</li> <li>Tracing the decision process</li> <li>Loading recognizers from file</li> <li>Ad-Hoc recognizers</li> <li>Simple anonymization</li> <li>Custom anonymization</li> <li>Encryption/Decryption</li> </ul>"},{"location":"tutorial/00_getting_started/","title":"Getting started","text":""},{"location":"tutorial/00_getting_started/#installation","title":"Installation","text":"<p>First, let's install presidio using <code>pip</code>. For detailed documentation, see the installation docs. Install from PyPI:</p> <pre><code>pip install presidio_analyzer\npip install presidio_anonymizer\npython -m spacy download en_core_web_lg\n</code></pre>"},{"location":"tutorial/00_getting_started/#simple-flow","title":"Simple flow","text":"<p>A simple call to Presidio Analyzer:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\ntext = \"His name is Mr. Jones and his phone number is 212-555-5555\"\n\nanalyzer = AnalyzerEngine()\nanalyzer_results = analyzer.analyze(text=text, language=\"en\")\n\nprint(analyzer_results)\n</code></pre> <p>Next, we'll go over ways to customize Presidio to specific needs by adding PII recognizers, using context words, NER models and more.</p>"},{"location":"tutorial/01_deny_list/","title":"Example 1: Deny-list based PII recognition","text":"<p>In this example, we will pass a short list of tokens which should be marked as PII if detected. First, let's define the tokens we want to treat as PII. In this case it would be a list of titles:</p> <pre><code>titles_list = [\n    \"Sir\",\n    \"Ma'am\",\n    \"Madam\",\n    \"Mr.\",\n    \"Mrs.\",\n    \"Ms.\",\n    \"Miss\",\n    \"Dr.\",\n    \"Professor\",\n]\n</code></pre> <p>Second, let's create a <code>PatternRecognizer</code> which would scan for those titles, by passing a <code>deny_list</code>:</p> <pre><code>from presidio_analyzer import PatternRecognizer\n\ntitles_recognizer = PatternRecognizer(supported_entity=\"TITLE\", deny_list=titles_list)\n</code></pre> <p>At this point we can call our recognizer directly:</p> <pre><code>from presidio_analyzer import PatternRecognizer\n\ntext1 = \"I suspect Professor Plum, in the Dining Room, with the candlestick\"\nresult = titles_recognizer.analyze(text1, entities=[\"TITLE\"])\nprint(f\"Result:\\n {result}\")\n</code></pre> <p>Finally, let's add this new recognizer to the list of recognizers used by the Presidio <code>AnalyzerEngine</code>:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(titles_recognizer)\n</code></pre> <p>When initializing the <code>AnalyzerEngine</code>, Presidio loads all available recognizers, including the <code>NlpEngine</code> used to detect entities, and extract tokens, lemmas and other linguistic features.</p> <p>Let's run the analyzer with the new recognizer in place:</p> <pre><code>results = analyzer.analyze(text=text1, language=\"en\")\n</code></pre> <pre><code>print(\"Results:\")\nprint(results)\n</code></pre> <p>As expected, both the name \"Plum\" and the title were identified as PII:</p> <pre><code>print(\"Identified these PII entities:\")\nfor result in results:\n    print(f\"- {text1[result.start:result.end]} as {result.entity_type}\")\n</code></pre>"},{"location":"tutorial/02_regex/","title":"Example 2: Regular-expressions based PII recognition","text":"<p>Another simple recognizer we can add is based on regular expressions. Let's assume we want to be extremely conservative and treat any token which contains a number as PII.</p> <pre><code>from presidio_analyzer import Pattern, PatternRecognizer\n\n# Define the regex pattern in a Presidio `Pattern` object:\nnumbers_pattern = Pattern(name=\"numbers_pattern\", regex=\"\\d+\", score=0.5)\n\n# Define the recognizer with one or more patterns\nnumber_recognizer = PatternRecognizer(\n    supported_entity=\"NUMBER\", patterns=[numbers_pattern]\n)\n</code></pre> <p>Testing the recognizer itself:</p> <pre><code>text2 = \"I live in 510 Broad st.\"\n\nnumbers_result = number_recognizer.analyze(text=text2, entities=[\"NUMBER\"])\n\nprint(\"Result:\")\nprint(numbers_result)\n</code></pre> <p>It's important to mention that recognizers are likely to have errors, both false-positive and false-negative, which would impact the entire performance of Presidio. Consider testing each recognizer on a representative dataset prior to integrating it into Presidio. For more info, see the best practices for developing recognizers documentation.</p>"},{"location":"tutorial/03_rule_based/","title":"Example 3: Rule based logic recognizer","text":"<p>Taking the numbers recognizer one step further, let's say we also would like to detect numbers within words, e.g. \"Number One\". We can leverage the underlying <code>spaCy</code> token attributes, or write our own logic to detect such entities.</p> <p>Notes:</p> <ul> <li> <p>In this example we would create a new class, which implements <code>EntityRecognizer</code>, the basic recognizer in Presidio. This abstract class requires us to implement the <code>load</code> method and <code>analyze</code> method.</p> </li> <li> <p>Each recognizer accepts an object of type <code>NlpArtifacts</code>, which holds pre-computed attributes on the input text.</p> </li> </ul> <p>A new recognizer should have this structure:</p> <pre><code>from typing import List\nfrom presidio_analyzer import EntityRecognizer, RecognizerResult\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n\n\nclass MyRecognizer(EntityRecognizer):\n    def load(self) -&gt; None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Logic for detecting a specific PII\n        \"\"\"\n        pass\n</code></pre> <p>For example, detecting numbers in either numerical or alphabetic (e.g. Forty five) form:</p> <pre><code>from typing import List\nfrom presidio_analyzer import EntityRecognizer, RecognizerResult\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n\n\nclass NumbersRecognizer(EntityRecognizer):\n\n    expected_confidence_level = 0.7  # expected confidence level for this recognizer\n\n    def load(self) -&gt; None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyzes test to find tokens which represent numbers (either 123 or One Two Three).\n        \"\"\"\n        results = []\n\n        # iterate over the spaCy tokens, and call `token.like_num`\n        for token in nlp_artifacts.tokens:\n            if token.like_num:\n                result = RecognizerResult(\n                    entity_type=\"NUMBER\",\n                    start=token.idx,\n                    end=token.idx + len(token),\n                    score=self.expected_confidence_level,\n                )\n                results.append(result)\n        return results\n\n\n# Instantiate the new NumbersRecognizer:\nnew_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n</code></pre> <p>Since this recognizer requires the <code>NlpArtifacts</code>, we would have to call it as part of the <code>AnalyzerEngine</code> flow:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\ntext3 = \"Roberto lives in Five 10 Broad st.\"\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(new_numbers_recognizer)\n\nnumbers_results2 = analyzer.analyze(text=text3, language=\"en\")\nprint(\"Results:\")\nprint(\"\\n\".join([str(res) for res in numbers_results2]))\n</code></pre> <p>The analyzer was able to pick up both numeric and alphabetical numbers, including other types of PII entities from other recognizers (PERSON in this case).</p>"},{"location":"tutorial/04_external_services/","title":"Example 4: Calling an external service/framework for PII detection","text":"<p>In a similar way to example 3, we can write logic to call external services for PII detection. There are two types of external services we support:</p> <ol> <li>Remote services such as a PII detection model hosted somewhere. In this case, the recognizer would do the actual REST request and translate the results to a list of <code>RecognizerResult</code>.</li> <li>Calling PII models from other frameworks, such as transformers or Flair.</li> </ol>"},{"location":"tutorial/04_external_services/#calling-a-remote-service","title":"Calling a remote service","text":"<ol> <li> <p>Documentation on remote recognizers.</p> </li> <li> <p>A sample implementation of a remote recognizer.</p> </li> </ol>"},{"location":"tutorial/04_external_services/#calling-a-model-in-a-different-framework","title":"Calling a model in a different framework","text":"<ul> <li>This example shows a Presidio wrapper for a Flair model.</li> <li>Using a similar approach, we could create wrappers for HuggingFace models, Conditional Random Fields or any other framework.</li> </ul>"},{"location":"tutorial/05_languages/","title":"Example 5: Supporting new models and languages","text":"<p>Two main parts in Presidio handle the text, and should be adapted if a new language is required:</p> <ol> <li>The <code>NlpEngine</code> containing the NLP model which performs tokenization, lemmatization, Named Entity Recognition and other NLP tasks.</li> <li>The different PII recognizers (<code>EntityRecognizer</code> objects) should be adapted or created.</li> </ol>"},{"location":"tutorial/05_languages/#adapting-the-nlp-engine","title":"Adapting the NLP engine","text":"<p>As its internal NLP engine, Presidio supports both spaCy and Stanza. Make sure you download the required models from spacy/stanza prior to using them. More details here. For example, to download the Spanish medium spaCy model: <code>python -m spacy download es_core_news_md</code></p> <p>In this example we will configure Presidio to use spaCy as its underlying NLP framework, with NLP models in English and Spanish:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\n# import spacy\n# spacy.cli.download(\"es_core_news_md\")\n\n# Create configuration containing engine name and models\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [\n        {\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n    ],\n}\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es Morris\", language=\"es\")\nprint(\"Results from Spanish request:\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(\"Results from English request:\")\nprint(results_english)\n</code></pre> <p>See this documentation for more details on setting up additional NLP models and languages.</p>"},{"location":"tutorial/05_languages/#using-external-modelsframeworks","title":"Using external models/frameworks","text":"<p>Some languages are not supported by spaCy/Stanza/huggingface, or have very limited support in those. In this case, other frameworks could be leveraged. (see example 4 for more information).</p> <p>Since Presidio requires a spaCy model to be passed, we propose to use a simple spaCy pipeline such as <code>en_core_web_sm</code> as the NLP engine's model, and a recognizer calling an external framework/service as the Named Entity Recognition (NER) model.</p>"},{"location":"tutorial/06_context/","title":"Example 6: Leveraging context words","text":"<p>Presidio has an internal mechanism for leveraging context words. This mechanism would increase the detection confidence of a PII entity in case a specific word appears before or after it.</p> <p>Furthermore, it is possible to create your own context enhancer, if you require a different logic for identifying context terms. The default context-aware enhancer in Presidio is the <code>LemmaContextAwareEnhancer</code> which compares each recognizer's context terms with the lemma of each token in the sentence.</p> <p>In this example we would first implement a zip code recognizer without context, and then add context to see how the confidence changes. Zip regex patterns (essentially 5 digits) are very weak, so we would want the initial confidence to be low, and increased with the existence of context words.</p>"},{"location":"tutorial/06_context/#example-adding-context-words-support-to-recognizers","title":"Example: Adding context words support to recognizers","text":"<p>First, let's create a simple <code>US_ZIP_CODE</code> recognizer:</p> <pre><code>from presidio_analyzer import (\n    Pattern,\n    PatternRecognizer,\n    RecognizerRegistry,\n    AnalyzerEngine,\n)\n\n# Define the regex pattern\nregex = r\"(\\b\\d{5}(?:\\-\\d{4})?\\b)\"  # very weak regex pattern\nzipcode_pattern = Pattern(name=\"zip code (weak)\", regex=regex, score=0.01)\n\n# Define the recognizer with the defined pattern\nzipcode_recognizer = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\", patterns=[zipcode_pattern]\n)\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\n\nprint(f\"Result:\\n {results}\")\n</code></pre> <p>So this is working, but would catch any 5 digit string. This is why we set the score to 0.01. Let's use context words to increase score:</p> <pre><code>from presidio_analyzer import PatternRecognizer\n\n# Define the recognizer with the defined pattern and context words\nzipcode_recognizer_w_context = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\",\n    patterns=[zipcode_pattern],\n    context=[\"zip\", \"zipcode\"],\n)\n</code></pre> <p>When creating an <code>AnalyzerEngine</code> we can provide our own context enhancement logic by passing it to <code>context_aware_enhancer</code> parameter. <code>AnalyzerEngine</code> will create <code>LemmaContextAwareEnhancer</code> by default if not passed, which will enhance score of each matched result if its recognizer holds context words and the lemma of those words are found in the surroundings of the matched entity.</p> <p>Creating the <code>AnalyzerEngine</code> and adding the new recognizer:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer_w_context)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\nprint(\"Result:\")\nprint(results)\n</code></pre> <p>The confidence score is now 0.4, instead of 0.01, since the <code>LemmaContextAwareEnhancer</code> default context similarity factor is 0.35 and default minimum score with context similarity is 0.4. We can change that by passing other values to the <code>context_similarity_factor</code> and <code>min_score_with_context_similarity</code> parameters of the <code>LemmaContextAwareEnhancer</code> object. For example:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n\ncontext_aware_enhancer = LemmaContextAwareEnhancer(\n    context_similarity_factor=0.45, min_score_with_context_similarity=0.4\n)\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer_w_context)\nanalyzer = AnalyzerEngine(\n    registry=registry, context_aware_enhancer=context_aware_enhancer\n)\n\n# Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\nprint(\"Result:\")\nprint(results)\n</code></pre> <p>The confidence score is now 0.46 because it got enhanced from 0.01 with 0.45 and is more than the minimum of 0.4.</p> <p>In addition to surrounding words, additional context words could be passed on the request level. This is useful when there is context coming from metadata such as column names or a specific user input. In the following example, notice how the \"zip\" context word doesn't appear in the text but still enhances the confidence score from 0.01 to 0.4:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, PatternRecognizer\n\n# Define the recognizer with the defined pattern and context words\nzipcode_recognizer = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\",\n    patterns=[zipcode_pattern],\n    context=[\"zip\", \"zipcode\"],\n)\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test with an example record having a column name which could be injected as context\nrecord = {\"column_name\": \"zip\", \"text\": \"My code is 90210\"}\n\nresult = analyzer.analyze(\n    text=record[\"text\"], language=\"en\", context=[record[\"column_name\"]]\n)\n\nprint(\"Result:\")\nprint(result)\n</code></pre>"},{"location":"tutorial/07_decision_process/","title":"Example 7: Tracing the decision process","text":"<p>Presidio-analyzer's decision process exposes information on why a specific PII was detected. Such information could contain:</p> <ul> <li>Which recognizer detected the entity</li> <li>Which regex pattern was used</li> <li>Interpretability mechanisms in ML models</li> <li>Which context words improved the score</li> <li>Confidence scores before and after each step And more.</li> </ul> <p>For more information, refer to the decision process documentation.</p> <p>Let's use the decision process output to understand how the zip code value was detected:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nimport pprint\n\nanalyzer = AnalyzerEngine()\n\nresults = analyzer.analyze(\n    text=\"My zip code is 90210\", language=\"en\", return_decision_process=True\n)\n\ndecision_process = results[0].analysis_explanation\n\npp = pprint.PrettyPrinter()\nprint(\"Decision process output:\\n\")\npp.pprint(decision_process.__dict__)\n</code></pre> <p>When developing new recognizers, one can add information to this explanation and extend it with additional findings.</p>"},{"location":"tutorial/08_no_code/","title":"No code configuration","text":"<p>No-code configuration can be helpful in three scenarios:</p> <ol> <li>There's an existing set of regular expressions / deny-lists that should be leveraged within Presidio.</li> <li>As a simple way to configure which recognizers to enable and disable, and how to configure the NLP engine.</li> <li>For team members interested in changing the configuration without writing code.</li> </ol> <p>In this example, we'll show how to create a no-code configuration in Presidio. We start by creating YAML configuration files that are based on the default ones. The default configuration files for Presidio can be found here:</p> <ul> <li>Analyzer configuration</li> <li>Recognizer registry configuration</li> <li>NLP engine configuration</li> </ul> <p>Alternatively, one can create one configuration file for all three components. In this example, we'll tweak the configuration to reduce the number of predefinedrecognizers to only a few, and add a new custom one. We'll also adjust the context words to support the detection of a different language (Spanish).</p> <pre><code>import yaml\nimport json\nimport tempfile\nfrom pprint import pprint\nfrom presidio_analyzer import AnalyzerEngineProvider\n</code></pre> <p>In this example we're going to create the yaml as a string for illustration purposes, but the more common scenario is to create these YAML files and load them into the <code>PresidioAnalyzerProvider</code>.</p>"},{"location":"tutorial/08_no_code/#defining-the-configuration-in-yaml-format","title":"Defining the configuration in YAML format","text":""},{"location":"tutorial/08_no_code/#general-analyzer-parameters","title":"General Analyzer parameters","text":"<p>(default file)</p> <pre><code>analyzer_config_yaml = \"\"\"\nsupported_languages: \n  - en\n  - es\ndefault_score_threshold: 0.4\n\"\"\"\n</code></pre>"},{"location":"tutorial/08_no_code/#recognizer-registry-parameters","title":"Recognizer Registry parameters","text":"<p>(default file)</p> <pre><code>recognizer_registry_config_yaml = \"\"\"\nrecognizer_registry:\n  supported_languages: \n  - en\n  - es\n  global_regex_flags: 26\n\n  recognizers:\n  - name: CreditCardRecognizer\n    supported_languages:\n    - language: en\n      context: [credit, card, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    - language: es\n      context: [tarjeta, credito, visa, mastercard, cc, amex, discover, jcb, diners, maestro, instapayment]\n    type: predefined\n\n  - name: DateRecognizer\n    supported_languages:\n    - language: en\n      context: [date, time, birthday, birthdate, dob]\n    - language: es\n      context: [fecha, tiempo, hora, nacimiento, dob]\n    type: predefined\n\n  - name: EmailRecognizer\n    supported_languages:\n    - language: en\n      context: [email, mail, address]\n    - language: es\n      context: [correo, electr\u00f3nico, email]\n    type: predefined\n\n  - name: PhoneRecognizer\n    type: predefined\n    supported_languages:\n    - language: en\n      context: [phone, number, telephone, fax]\n    - language: es\n      context: [tel\u00e9fono, n\u00famero, fax]\n\n  - name: \"Titles recognizer (en)\"\n    supported_language: \"en\"\n    supported_entity: \"TITLE\"\n    deny_list:\n      - Mr.\n      - Mrs.\n      - Ms.\n      - Miss\n      - Dr.\n      - Prof.\n      - Doctor\n      - Professor\n  - name: \"Titles recognizer (es)\"\n    supported_language: \"es\"\n    supported_entity: \"TITLE\"\n    deny_list:\n      - Sr.\n      - Se\u00f1or\n      - Sra.\n      - Se\u00f1ora\n      - Srta.\n      - Se\u00f1orita\n      - Dr.\n      - Doctor\n      - Doctora\n      - Prof.\n      - Profesor\n      - Profesora\n\"\"\"\n</code></pre>"},{"location":"tutorial/08_no_code/#nlp-engine-parameters","title":"NLP Engine parameters","text":"<p>(default file)</p> <pre><code>nlp_engine_yaml = \"\"\"\nnlp_configuration:\n    nlp_engine_name: transformers\n    models:\n      -\n        lang_code: en\n        model_name:\n          spacy: en_core_web_sm\n          transformers: StanfordAIMI/stanford-deidentifier-base\n      -\n        lang_code: es\n        model_name:\n          spacy: es_core_news_sm\n          transformers: MMG/xlm-roberta-large-ner-spanish  \n    ner_model_configuration:\n      labels_to_ignore:\n      - O\n      aggregation_strategy: first # \"simple\", \"first\", \"average\", \"max\"\n      stride: 16\n      alignment_mode: expand # \"strict\", \"contract\", \"expand\"\n      model_to_presidio_entity_mapping:\n        PER: PERSON\n        PERSON: PERSON\n        LOC: LOCATION\n        LOCATION: LOCATION\n        GPE: LOCATION\n        ORG: ORGANIZATION\n        ORGANIZATION: ORGANIZATION\n        NORP: NRP\n        AGE: AGE\n        ID: ID\n        EMAIL: EMAIL\n        PATIENT: PERSON\n        STAFF: PERSON\n        HOSP: ORGANIZATION\n        PATORG: ORGANIZATION\n        DATE: DATE_TIME\n        TIME: DATE_TIME\n        PHONE: PHONE_NUMBER\n        HCW: PERSON\n        HOSPITAL: LOCATION\n        FACILITY: LOCATION\n        VENDOR: ORGANIZATION\n        MISC: ID\n\n      low_confidence_score_multiplier: 0.4\n      low_score_entity_names:\n      - ID\n\"\"\"\n</code></pre>"},{"location":"tutorial/08_no_code/#creating-the-analyzer-engine-and-running-it","title":"Creating the analyzer engine and running it","text":""},{"location":"tutorial/08_no_code/#create-a-unified-yaml-file-and-save-it-as-a-temp-file","title":"Create a unified YAML file and save it as a temp file","text":"<pre><code>full_config = f\"{analyzer_config_yaml}\\n{recognizer_registry_config_yaml}\\n{nlp_engine_yaml}\"\n\nwith tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.yaml') as temp_file:\n    # Write the YAML string to the temp file\n    temp_file.write(full_config)\n    temp_file_path = temp_file.name\n</code></pre>"},{"location":"tutorial/08_no_code/#pass-the-yaml-file-to-analyzerengineprovider-to-create-an-analyzerengine-instance","title":"Pass the YAML file to <code>AnalyzerEngineProvider</code> to create an <code>AnalyzerEngine</code> instance","text":"<pre><code>analyzer_engine = AnalyzerEngineProvider(analyzer_engine_conf_file=temp_file_path).create_engine()\n</code></pre>"},{"location":"tutorial/08_no_code/#print-the-loaded-configuration-for-both-languages","title":"Print the loaded configuration for both languages","text":"<pre><code>for lang in (\"en\", \"es\"):\n    pprint(f\"Supported entities for {lang}:\")\n    print(\"\\n\")\n    pprint(analyzer_engine.get_supported_entities(lang), compact=True)\n\n    print(f\"\\nLoaded recognizers for {lang}:\")\n    pprint([rec.name for rec in analyzer_engine.registry.get_recognizers(lang, all_fields=True)], compact=True)\n    print(\"\\n\")\n\nprint(f\"\\nLoaded NER models:\")\npprint(analyzer_engine.nlp_engine.models)\n</code></pre>"},{"location":"tutorial/08_no_code/#run-two-requests-one-in-english-and-one-in-spanish","title":"Run two requests, one in English and one in Spanish","text":"<pre><code>es_text = \"Hola, me llamo David Johnson y soy originalmente de Liverpool. Mi n\u00famero de tarjeta de cr\u00e9dito es 4095260993934932\"\nanalyzer_engine.analyze(es_text, language=\"es\")\n</code></pre> <pre><code>en_text = \"Hi, my name is David Johnson and I'm originally from Liverpool. My credit card number is 4095260993934932\"\nanalyzer_engine.analyze(en_text, language=\"en\")\n</code></pre>"},{"location":"tutorial/09_ad_hoc/","title":"Example 9: Ad-hoc recognizers","text":"<p>In addition to recognizers in code or in a YAML file, it is possible to create ad-hoc recognizers via the Presidio Analyzer API for regex and deny-list based logic. These recognizers, in JSON form, are added to the <code>/analyze</code> request and are only used in the context of this request.</p> <p>Note</p> <p>These ad-hoc recognizers could be useful if Presidio is already deployed, but requires additional detection logic to be added.</p> <ul> <li> <p>The json structure for a regex ad-hoc recognizer is the following:</p> <pre><code>{\n    \"text\": \"John Smith drivers license is AC432223. Zip code: 10023\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Zip code Recognizer\",\n        \"supported_language\": \"en\",\n        \"patterns\": [\n            {\n            \"name\": \"zip code (weak)\", \n            \"regex\": \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\", \n            \"score\": 0.01\n            }\n        ],\n        \"context\": [\"zip\", \"code\"],\n        \"supported_entity\":\"ZIP\"\n        }\n    ]\n}\n</code></pre> </li> <li> <p>The json structure for deny-list based recognizers is the following:</p> <pre><code>{\n    \"text\": \"Mr. John Smith's drivers license is AC432223\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Mr. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Mr\", \"Mr.\", \"Mister\"],\n        \"supported_entity\":\"MR_TITLE\"\n        },\n        {\n        \"name\": \"Ms. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Ms\", \"Ms.\", \"Miss\", \"Mrs\", \"Mrs.\"],\n        \"supported_entity\":\"MS_TITLE\"\n        }\n    ]\n}\n</code></pre> </li> </ul> <p>In both examples, the <code>/analyze</code> request is extended with a list of <code>ad_hoc_recognizers</code>, which could be either <code>patterns</code>, <code>deny_list</code> or both.</p> <p>Example call to the <code>/analyze</code> service:</p> <pre><code>{\n  \"text\": \"John Smith drivers license is AC432223 and the zip code is 12345\",\n  \"language\": \"en\",\n  \"return_decision_process\": false,\n  \"correlation_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"score_threshold\": 0.6,\n  \"entities\": [\n    \"US_DRIVER_LICENSE\",\n    \"ZIP\"\n  ],\n  \"trace\": false,\n  \"ad_hoc_recognizers\": [\n    {\n      \"name\": \"Zip code Recognizer\",\n      \"supported_language\": \"en\",\n      \"patterns\": [\n        {\n          \"name\": \"zip code (weak)\",\n          \"regex\": \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\",\n          \"score\": 0.01\n        }\n      ],\n      \"context\": [\n        \"zip\",\n        \"code\"\n      ],\n      \"supported_entity\": \"ZIP\"\n    }\n  ]\n}\n</code></pre> <p>For more examples of deny-list recognizers, see this sample.</p>"},{"location":"tutorial/10_simple_anonymization/","title":"Example 10: Simple anonymization","text":"<p>Once we have the identified PII entities, we can perform different de-identification operations on them. For more information on the supported operators, see the anonymizer documentation.</p> <p>The anonymizer requires a configuration specifying the requested operation on each entity type. There's also a default operator which replaces a PII entity with the entity type name.</p> <p>Each operator has a unique configuration with the parameters needed to perform the operation (redact, hash, mask, replace, encrypt etc.)</p> <p>Here's an simple example of using presidio-anonymizer:</p> <pre><code>from presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import RecognizerResult\n\n# Analyzer output\nanalyzer_results = [\n    RecognizerResult(entity_type=\"PERSON\", start=11, end=15, score=0.8),\n    RecognizerResult(entity_type=\"PERSON\", start=17, end=27, score=0.8),\n]\n\n# Initialize the engine:\nengine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text,\n# analyzer results (potentially coming from presidio-analyzer) and\n# Operators to get the anonymization output:\nresult = engine.anonymize(\n    text=\"My name is Bond, James Bond\", analyzer_results=analyzer_results\n)\n\nprint(\"De-identified text\")\nprint(result.text)\n</code></pre> <p>To introduce additional operators, we can pass an <code>OperatorConfig</code>. In this example we:</p> <ol> <li>Mask the last 12 chars of a <code>PHONE_NUMBER</code> entity and replace them with <code>*</code></li> <li>Redact a <code>TITLE</code> entity</li> <li>Replace all other entities with the string <code>&lt;ANONYMIZED&gt;</code>.</li> </ol> <p>Defining the operators:</p> <pre><code># Define anonymization operators\noperators = {\n    \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"&lt;ANONYMIZED&gt;\"}),\n    \"PHONE_NUMBER\": OperatorConfig(\n        \"mask\",\n        {\n            \"type\": \"mask\",\n            \"masking_char\": \"*\",\n            \"chars_to_mask\": 12,\n            \"from_end\": True,\n        },\n    ),\n    \"TITLE\": OperatorConfig(\"redact\", {}),\n}\n</code></pre> <p>Full example:</p> <pre><code>from pprint import pprint\nimport json\n\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig, RecognizerResult\n\n\n# Analyzer output\nanalyzer_results = [\n    RecognizerResult(entity_type=\"PERSON\", start=11, end=15, score=0.8),\n    RecognizerResult(entity_type=\"PERSON\", start=17, end=27, score=0.8),\n]\n\ntext_to_anonymize = \"My name is Bond, James Bond\"\n\nanonymizer = AnonymizerEngine()\n\n# Define anonymization operators\noperators = {\n    \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"&lt;ANONYMIZED&gt;\"}),\n    \"PHONE_NUMBER\": OperatorConfig(\n        \"mask\",\n        {\n            \"type\": \"mask\",\n            \"masking_char\": \"*\",\n            \"chars_to_mask\": 12,\n            \"from_end\": True,\n        },\n    ),\n    \"TITLE\": OperatorConfig(\"redact\", {}),\n}\n\nanonymized_results = anonymizer.anonymize(\n    text=text_to_anonymize, analyzer_results=analyzer_results, operators=operators\n)\n\nprint(f\"text: {anonymized_results.text}\")\nprint(\"detailed result:\")\n\npprint(json.loads(anonymized_results.to_json()))\n</code></pre>"},{"location":"tutorial/11_custom_anonymization/","title":"Example 11: Custom anonymization","text":"<p>Presidio-anonymizer can accept arbitrary operations to perform on identified entities. These operations can be passed in the form of a lambda function.</p> <p>In the following example, we use fake values to perform pseudonymization. First, let's look at the operator:</p> <pre><code>from faker import Faker\nfrom presidio_anonymizer.entities import OperatorConfig\n\nfake = Faker()\n\n# Create faker function (note that it has to receive a value)\ndef fake_name(x):\n    return fake.name()\n\n\n# Create custom operator for the PERSON entity\noperators = {\"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name})}\n</code></pre> <p>Full example:</p> <pre><code>from presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig, EngineResult, RecognizerResult\nfrom faker import Faker\n\n\nfake = Faker()\n\n# Create faker function (note that it has to receive a value)\ndef fake_name(x):\n    return fake.name()\n\n\n# Create custom operator for the PERSON entity\noperators = {\"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name})}\n\n# Analyzer output\nanalyzer_results = [RecognizerResult(entity_type=\"PERSON\", start=11, end=18, score=0.8)]\n\ntext_to_anonymize = \"My name is Raphael and I like to fish.\"\n\nanonymizer = AnonymizerEngine()\n\nanonymized_results = anonymizer.anonymize(\n    text=text_to_anonymize, analyzer_results=analyzer_results, operators=operators\n)\n\nprint(anonymized_results.text)\n</code></pre> <p>This is a simple example, but here are some examples for more advanced anonymization options:</p> <ul> <li>Identify the gender and create a random value from the same gender (e.g., Laura -&gt; Pam)</li> <li>Identifying the date pattern and perform date shift (01-01-2020 -&gt; 05-01-2020)</li> <li>Identify the age and bucket by decade (89 -&gt; 80-90)</li> </ul>"},{"location":"tutorial/12_encryption/","title":"Example 12: Encryption and decryption","text":"<p>This sample shows how to use Presidio Anonymizer built-in functionality, to encrypt and decrypt identified entities. The encryption is using AES cypher in CBC mode and requires a cryptographic key as an input for both the encryption and the decryption.</p>"},{"location":"tutorial/12_encryption/#set-up-imports","title":"Set up imports","text":"<pre><code>from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine\nfrom presidio_anonymizer.entities import (\n    RecognizerResult,\n    OperatorResult,\n    OperatorConfig,\n)\n</code></pre>"},{"location":"tutorial/12_encryption/#define-a-cryptographic-key-for-both-encryption-and-decryption","title":"Define a cryptographic key (for both encryption and decryption)","text":"<pre><code>crypto_key = \"WmZq4t7w!z%C&amp;F)J\"\n</code></pre>"},{"location":"tutorial/12_encryption/#presidio-anonymizer-encrypt","title":"Presidio Anonymizer: Encrypt","text":"<pre><code>engine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text,\n# analyzer results (potentially coming from presidio-analyzer)\n# and an 'encrypt' operator to get an encrypted anonymization output:\nanonymize_result = engine.anonymize(\n    text=\"My name is James Bond\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),\n    ],\n    operators={\"PERSON\": OperatorConfig(\"encrypt\", {\"key\": crypto_key})},\n)\n\nanonymize_result\n</code></pre> <p>The output contains both the anonymized text, as well as the location of the encrypted entities. This is useful as we would need to decrypt only the entities and not the full text:</p> <pre><code># Fetch the anonymized text from the result.\nanonymized_text = anonymize_result.text\n\n# Fetch the anonynized entities from the result.\nanonymized_entities = anonymize_result.items\n</code></pre>"},{"location":"tutorial/12_encryption/#presidio-anonymizer-decrypt","title":"Presidio Anonymizer: Decrypt","text":"<pre><code># Initialize the engine:\nengine = DeanonymizeEngine()\n\n# Invoke the deanonymize function with the text, anonymizer results\n# and a 'decrypt' operator to get the original text as output.\ndeanonymized_result = engine.deanonymize(\n    text=anonymized_text,\n    entities=anonymized_entities,\n    operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": crypto_key})},\n)\n\ndeanonymized_result\n</code></pre>"},{"location":"tutorial/12_encryption/#alternatively-call-the-decrypt-operator-directly","title":"Alternatively, call the Decrypt operator directly","text":"<pre><code>from presidio_anonymizer.operators import Decrypt\n\n# Fetch the encrypted entity value from the previous stage\nencrypted_entity_value = anonymize_result.items[0].text\n\n# Restore the original entity value\nDecrypt().operate(text=encrypted_entity_value, params={\"key\": crypto_key})\n</code></pre>"},{"location":"tutorial/13_allow_list/","title":"Example 13: Allow-list to exclude words from being identified as PII","text":"<p>In this example, we will define a list of tokens that should not be marked as PII even if we want to identify others of that kind.</p> <p>In this example, we will pass a short list of tokens which should not be marked as PII even if detected by one of the recognizers.</p> <pre><code>websites_list = [\n    \"bing.com\",\n    \"microsoft.com\"\n]\n</code></pre> <p>We will use the built in recognizers that include the <code>URLRecognizer</code> and the NLP model <code>EntityRecognizer</code> and see the default functionality if we don't specify any list of words for the detector to allow to keep in the text.</p> <pre><code>from presidio_analyzer import AnalyzerEngine\ntext1 = \"My favorite website is bing.com, his is microsoft.com\"\nanalyzer = AnalyzerEngine()\nresult = analyzer.analyze(text = text1, language = 'en')\nprint(f\"Result: \\n {result}\")\n</code></pre> <p>To specify an allow list we just pass a list of values we want to keep as a parameter to call to <code>analyze</code>. Now we can see that in the results, <code>bing.com</code> is no longer being recognized as a PII item, only <code>microsoft.com</code> is still recognized since we did include it in the allow list.</p> <pre><code>result = analyzer.analyze(text = text1, language = 'en', allow_list = [\"bing.com\"] )\nprint(f\"Result:\\n {result}\")\n</code></pre>"}]}