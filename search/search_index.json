{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Presidio: Data Protection and De-identification SDK","text":"<p>Presidio (Origin from Latin praesidium \u2018protection, garrison\u2019) helps to ensure sensitive data is properly managed and governed. It provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.</p>"},{"location":"#goals","title":"Goals","text":"<ul> <li>Allow organizations to preserve privacy in a simpler way by democratizing de-identification technologies and introducing transparency in decisions.</li> <li>Embrace extensibility and customizability to a specific business need.</li> <li>Facilitate both fully automated and semi-automated PII de-identification flows on multiple platforms.</li> </ul>"},{"location":"#how-it-works","title":"How it works","text":""},{"location":"#main-features","title":"Main features","text":"<ol> <li>Predefined or custom PII recognizers leveraging Named Entity Recognition, regular expressions, rule based logic and checksum with relevant context in multiple languages.</li> <li>Options for connecting to external PII detection models.</li> <li>Multiple usage options, from Python or PySpark workloads through Docker to Kubernetes.</li> <li>Customizability in PII identification and anonymization.</li> <li>Module for redacting PII text in images.</li> </ol> <p>Warning</p> <p>Presidio can help identify sensitive/PII data in un/structured text. However, because it is using automated detection mechanisms, there is no guarantee that Presidio will find all sensitive information. Consequently, additional systems and protections should be employed.</p>"},{"location":"#demo-frequently-asked-questions","title":"Demo | Frequently Asked Questions","text":""},{"location":"#mailbox_with_mail-are-you-using-presidio-wed-love-to-know-how-mailbox_with_mail","title":":mailbox_with_mail: Are you using Presidio? We'd love to know how :mailbox_with_mail:","text":"<p>Please help us improve by taking this short anonymous survey.</p>"},{"location":"#presidios-modules","title":"Presidio's modules","text":"<ol> <li>Presidio analyzer: PII identification in text</li> <li>Presidio anonymizer: De-identify detected PII entities using different operators</li> <li>Presidio image redactor: Redact PII entities from images using OCR and PII identification</li> </ol>"},{"location":"#installing-presidio","title":"Installing Presidio","text":"<ol> <li>Supported Python Versions</li> <li>Using pip</li> <li>Using Docker</li> <li>From source</li> <li>Migrating from V1 to V2</li> </ol>"},{"location":"#running-presidio","title":"Running Presidio","text":"<ol> <li>Samples for running Presidio via code</li> <li>Running Presidio as an HTTP service</li> <li>Setting up a development environment</li> <li>Perform PII identification using presidio-analyzer</li> <li>Perform PII de-identification using presidio-anonymizer</li> <li>Perform PII identification and redaction in images using presidio-image-redactor</li> <li>Example deployments</li> </ol>"},{"location":"#support","title":"Support","text":"<ul> <li>Before you submit an issue, please go over the documentation. For general discussions, please use the Github repo's discussion board.</li> <li>If you have a usage question, found a bug or have a suggestion for improvement, please file a Github issue.</li> <li>For other matters, please email presidio@microsoft.com.</li> </ul>"},{"location":"api/","title":"Presidio API","text":"<p>Api reference for Presidio's main python modules</p> <ul> <li>Presidio analyzer Python API reference</li> <li>Presidio anonymizer Python API reference</li> <li>Presidio image redactor Python API reference</li> </ul>"},{"location":"build_release/","title":"Build and release process","text":"<p>Presidio leverages Azure DevOps YAML pipelines to validate, build, release and deliver presidio. The pipelines make use of templates for code reuse using YAML Schema.</p>"},{"location":"build_release/#description","title":"Description","text":"<p>The following pipelines are provided and maintained as part of presidio development process:</p> <ul> <li>PR Validation - used to validate pull requests.<ul> <li>Linting</li> <li>Security and compliance analysis</li> <li>Unit tests</li> <li>E2E tests</li> </ul> </li> <li>CI - triggered on merge to main branch.<ul> <li>Linting</li> <li>Security and compliance analysis</li> <li>Unit tests</li> <li>E2E tests</li> <li>deploys the artifacts to an internal dev environment.</li> </ul> </li> <li>Release - manually triggered.<ul> <li>releases presidio official artifacts<ul> <li>pypi</li> <li>Microsoft container registry (and docker hub)</li> <li>GitHub</li> </ul> </li> <li>updates the official demo environment.</li> </ul> </li> </ul>"},{"location":"build_release/#variables-used-by-the-pipelines","title":"Variables used by the pipelines","text":""},{"location":"build_release/#ci-pipeline","title":"CI Pipeline","text":"<ul> <li>ACR_AZURE_SUBSCRIPTION - Service connection to Azure subscription where Azure Container Registry is.</li> <li>ACR_REGISTRY_NAME - Name of Azure Container Registry.</li> <li>ANALYZER_DEV_APP_NAME - Name of existing App Service for Analyzer (development environment).</li> <li>ANONYMIZER_DEV_APP_NAME - Name of existing App Service for Anonymizer (development environment).</li> <li>IMAGE_REDACTOR_DEV_APP_NAME - Name of existing App Service for Image Redactor (development environment).</li> <li>DEV_AZURE_SUBSCRIPTION - Service connection to Azure subscription where App Services are (development environment).</li> <li>DEV_RESOURCE_GROUP_NAME - Name of resource group where App Services are (development environment).</li> </ul>"},{"location":"build_release/#release-pipeline","title":"Release Pipeline","text":"<ul> <li>ACR_AZURE_SUBSCRIPTION - Service connection to Azure subscription where Azure Container Registry is.</li> <li>ACR_REGISTRY_NAME - Name of Azure Container Registry.</li> <li>ANALYZER_PROD_APP_NAME - Name of existing App Service for Analyzer (production environment).</li> <li>ANONYMIZER_PROD_APP_NAME - Name of existing App Service for Anonymizer (production environment).</li> <li>PROD_AZURE_SUBSCRIPTION - Service connection to Azure subscription where App Services are (production environment).</li> <li>PROD_RESOURCE_GROUP_NAME - Name of resource group where App Services are (production environment).</li> </ul>"},{"location":"build_release/#import-a-pipeline-to-azure-devops","title":"Import a pipeline to Azure Devops","text":"<ul> <li>Sign in to your Azure DevOps organization and navigate to your project.</li> <li>In your project, navigate to the Pipelines page. Then choose the action to create a new pipeline.</li> <li>Walk through the steps of the wizard by first selecting 'Use the classic editor, and select GitHub as the location of your source code.</li> <li>You might be redirected to GitHub to sign in. If so, enter your GitHub credentials.</li> <li>When the list of repositories appears, select presidio repository.</li> <li>Point Azure Pipelines to the relevant yaml definition you'd like to import.     Set the pipeline's name, the required triggers and variables and Select Save and run.</li> <li>A new run is started. Wait for the run to finish.</li> </ul>"},{"location":"community/","title":"Presidio eco-system","text":"<p>This section collects different resources developed with Presidio. </p>"},{"location":"community/#resources","title":"Resources","text":"Resource Description Obsei Obsei is an open-source, low-code, AI powered automation tool data-describe data-describe is a Python toolkit for Exploratory Data Analysis (EDA). It aims to accelerate data exploration and analysis by providing automated and polished analysis widgets. Azure Search Power Skills Power Skills are a collection of useful functions to be deployed as custom skills for Azure Cognitive Search. The skills can be used as templates or starting points for your own custom skills, or they can be deployed and used as they are if they happen to meet your requirements. DataOps for the Modern Data Warehouse Contains numerous code samples and artifacts on how to apply DevOps principles to data pipelines built according to the Modern Data Warehouse (MDW) architectural pattern on Microsoft Azure. Extending Power BI with Python and R Code repository for Extending Power BI with Python and R, published by Packt. HebSafeHarbor Clinical notes anonymization in Hebrew. Presidio Github Action Github Action that analyzes text for PII entities with Microsoft's Presidio framework. Presidio CLI CLI tool that analyzes text for PII Entities with Microsoft Presidio framework. <ul> <li>Please create a PR if you're interested in adding your tool to this list.</li> </ul>"},{"location":"design/","title":"Presidio design","text":""},{"location":"design/#analyzer","title":"Analyzer","text":""},{"location":"design/#anonymizer","title":"Anonymizer","text":""},{"location":"design/#image-redactor","title":"Image Redactor","text":""},{"location":"design/#standard-image-types","title":"Standard Image Types","text":""},{"location":"design/#dicom-images","title":"DICOM Images","text":""},{"location":"development/","title":"Setting Up a Development Environment","text":""},{"location":"development/#getting-started","title":"Getting started","text":""},{"location":"development/#cloning-the-repo","title":"Cloning the repo","text":"<p>To create a local copy of Presidio repository, follow Github instructions on how to clone a project using git. The project is structured so that:</p> <ul> <li>Each Presidio service has a designated directory:</li> <li>The service logic.</li> <li>Tests, both unit and integration.</li> <li>Serving it as an HTTP service (found in app.py).</li> <li>Python Packaging setup script (setup.py).</li> <li>In the project root directory, you will find common code for using, serving and testing Presidio     as a cluster of services, as well as CI/CD pipelines codebase and documentation.</li> </ul>"},{"location":"development/#setting-up-pipenv","title":"Setting up Pipenv","text":"<p>Pipenv is a Python workflow manager, handling dependencies and environment for Python packages. It is used by each Presidio service as the dependencies manager, to be aligned with the specific requirements versions. Follow these steps when starting to work on a Presidio service with Pipenv:</p> <ol> <li> <p>Install Pipenv</p> <ul> <li> <p>Using Pip</p> <pre><code>pip install --user pipenv\n</code></pre> </li> <li> <p>Using Homebrew (in MacOS)</p> <pre><code>brew install pipenv\n</code></pre> </li> </ul> <p>Additional installation instructions for Pipenv: https://pipenv.readthedocs.io/en/latest/install/#installing-pipenv</p> </li> <li> <p>Have Pipenv create a virtualenv for the project and install all requirements in the Pipfile,     including dev requirements.</p> <p>For example, in the <code>presidio-analyzer</code> folder, run:</p> <pre><code>pipenv install --dev --skip-lock\n</code></pre> </li> <li> <p>Run all tests:</p> <pre><code>pipenv run pytest\n</code></pre> </li> <li> <p>To run arbitrary scripts within the virtual env, start the command with     <code>pipenv run</code>. For example:</p> <ol> <li><code>pipenv run flake8</code></li> <li><code>pipenv run pip freeze</code></li> <li><code>pipenv run python -m spacy download en_core_web_lg</code></li> </ol> <p>Command 3 downloads the default spacy model needed for Presidio Analyzer.`</p> </li> </ol>"},{"location":"development/#alternatively-activate-the-virtual-environment-and-use-the-commands-by-starting-a-pipenv-shell","title":"Alternatively, activate the virtual environment and use the commands by starting a pipenv shell","text":"<ol> <li> <p>Start shell:</p> <pre><code>pipenv shell\n</code></pre> </li> <li> <p>Run commands in the shell</p> <pre><code>pytest\npip freeze\n</code></pre> </li> </ol>"},{"location":"development/#development-guidelines","title":"Development guidelines","text":"<ul> <li>A Github issue suggesting the change should be opened prior to a PR.</li> <li>All contributions should be documented, tested and linted. Please verify that all tests and lint checks pass successfully before proposing a change.</li> <li>To make the linting process easier, you can use pre-commit hooks to verify and automatically format code upon a git commit</li> <li>In order for a pull request to be accepted, the CI (containing unit tests, e2e tests and linting) needs to succeed, in addition to approvals from two maintainers.</li> <li>PRs should be small and solve/improve one issue at a time. If you have multiple suggestions for improvement, please open multiple PRs.</li> </ul>"},{"location":"development/#local-build-process","title":"Local build process","text":"<p>After modifying presidio codebase, you might want to build presidio cluster locally, and run tests to spot regressions. The recommended way of doing so is using docker-compose (bundled with 'Docker Desktop' for Windows and Mac systems, more information can be found here). Once installed, to start presidio cluster with all of its services in HTTP mode, run from the project root:</p> <pre><code>docker-compose up --build -d\n</code></pre> <p>Note</p> <p>Building for the first time might take some time, mainly on downloading the default spacy models.</p> <p>To validate that the services were built and started successfully, and to see the designated port for each, use docker-compose ps:</p> <pre><code>&gt;docker-compose ps\nCONTAINER ID   IMAGE                       COMMAND                  CREATED         STATUS         PORTS                    NAMES\n6d5a258d19c2   presidio-anonymizer         \"/bin/sh -c 'pipenv \u2026\"   6 minutes ago   Up 6 minutes   0.0.0.0:5001-&gt;5001/tcp   presidio_presidio-anonymizer_1\n9aad2b68f93c   presidio-analyzer           \"/bin/sh -c 'pipenv \u2026\"   2 days ago      Up 6 minutes   0.0.0.0:5002-&gt;5001/tcp   presidio_presidio-analyzer_1\n1448dfb3ec2b   presidio-image-redactor     \"/bin/sh -c 'pipenv \u2026\"   2 seconds ago   Up 2 seconds   0.0.0.0:5003-&gt;5001/tcp   presidio_presidio-image-redactor_1\n</code></pre> <p>Edit docker-compose.yml configuration file to change the default ports.</p> <p>Starting part of the cluster, or one service only, can be done by stating its image name as argument for docker-compose. For example for analyzer service:</p> <pre><code>docker-compose up --build -d presidio-analyzer\n</code></pre>"},{"location":"development/#testing","title":"Testing","text":"<p>We strive to have a full test coverage in Presidio, and expect every pull request to include tests.</p> <p>In each service directory, a 'test' directory can be found. In it, both unit tests, for testing single files or classes, and integration tests, for testing integration between the service components, or integration with external packages.</p>"},{"location":"development/#basic-conventions","title":"Basic conventions","text":"<p>For tests to be consistent and predictable, we use the following basic conventions:</p> <ol> <li>Treat tests as production code. Keep the tests concise and readable, with descriptive namings.</li> <li>Assert on one behavior at a time in each test.</li> <li>Test names should follow a pattern of <code>test_when_[condition_to_test]_then_[expected_behavior]</code>.    For example: <code>test_given_an_unknown_entity_then_anonymize_uses_defaults</code>.</li> <li>Use test doubles and mocks    when writing unit tests. Make less use of them when writing integration tests.</li> </ol>"},{"location":"development/#running-tests","title":"Running tests","text":"<p>Presidio uses the pytest framework for testing. See the pytest documentation for more information.</p> <p>Running the tests locally can be done in two ways:</p> <ol> <li> <p>Using cli, from each service directory, run:</p> <pre><code>pipenv run pytest\n</code></pre> </li> <li> <p>Using your IDE.    See configuration examples for    JetBrains PyCharm / IntelliJ IDEA    and Visual Studio Code</p> </li> </ol>"},{"location":"development/#end-to-end-tests","title":"End-to-end tests","text":"<p>Since Presidio services can function as HTTP servers, Presidio uses an additional end-to-end (e2e) testing layer to test their REST APIs. This e2e test framework is located under 'e2e-tests' directory. In it, you can also find test scenarios testing the integration between Presidio services through REST API. These tests should be annotated with 'integration' pytest marker <code>@pytest.mark.integration</code>, while tests calling a single service API layer should be annotated with 'api' pytest marker <code>@pytest.mark.api</code>.</p> <p>Running the e2e-tests locally can be done in two ways:</p> <ol> <li> <p>Using cli, from e2e-tests directory, run:</p> <p>On Mac / Linux / WSL:</p> <pre><code># Create a virtualenv named presidio-e2e (needs to be done only on the first run)\npython -m venv presidio-e2e\n# Activate the virtualenv\nsource presidio-e2e/bin/activate\n# Install e2e-tests requirements using pip\npip install -r requirements.txt\n# Run pytest\npytest\n# Deactivate the virtualenv\ndeactivate\n</code></pre> <p>On Windows CMD / Powershell:</p> <pre><code># Create a virtualenv named presidio-e2e (needs to be done only on the first run)\npy -m venv presidio-e2e\n# Activate the virtualenv\npresidio-e2e\\Scripts\\activate\n# Install e2e-tests requirements using pip\npip install -r requirements.txt\n# Run pytest\npytest\n# Deactivate the virtualenv\ndeactivate\n</code></pre> </li> <li> <p>Using your IDE</p> <p>See references in the section above.</p> </li> </ol> <p>Note</p> <p>The e2e tests require a Presidio cluster to be up, for example using the containerized cluster with docker-compose.</p>"},{"location":"development/#build-and-run-end-to-end-tests-locally","title":"Build and run end-to-end tests locally","text":"<p>Building and testing presidio locally, as explained above, can give good assurance on new changes and on regressions that might have introduced during development. As an easier method to build and automatically run end-to-end tests, is to use the <code>run.bat</code> script found in the project root:</p> <p>On Mac / Linux / WSL:</p> <pre><code>chmod +x run.bat\n./run.bat\n</code></pre> <p>On Windows CMD / Powershell:</p> <pre><code>run.bat\n</code></pre>"},{"location":"development/#linting","title":"Linting","text":"<p>Presidio services are PEP8 compliant and continuously enforced on style guide issues during the build process using <code>flake8</code>.</p> <p>Running flake8 locally, using <code>pipenv run flake8</code>, you can check for those issues prior to committing a change.</p> <p>In addition to the basic <code>flake8</code> functionality, Presidio uses the following extensions:</p> <ul> <li>pep8-naming: To check that variable names are PEP8 compliant.</li> <li>flake8-docstrings: To check that docstrings are compliant.</li> </ul>"},{"location":"development/#automatically-format-code-and-check-for-code-styling","title":"Automatically format code and check for code styling","text":"<p>To make the linting process easier, you can use pre-commit hooks to verify and automatically format code upon a git commit, using <code>black</code>:</p> <ol> <li> <p>Install pre-commit package manager locally.</p> </li> <li> <p>From the project's root, enable pre-commit, installing git hooks in the <code>.git/</code> directory by running: <code>pre-commit install</code>.</p> </li> <li> <p>Commit non PEP8 compliant code will cause commit failure and automatically     format your code using <code>black</code>, as well as checking code formatting using <code>flake8</code></p> <pre><code>```sh\n&gt;git commit -m 'autoformat' presidio-analyzer/presidio_analyzer/predefined_recognizers/us_ssn_recognizer.py\n\nblack....................................................................Failed\n- hook id: black\n- files were modified by this hook\n\nreformatted presidio-analyzer/presidio_analyzer/predefined_recognizers/us_ssn_recognizer.py\nAll done!\n1 file reformatted.\n\nflake8...................................................................Passed\n\n```\n</code></pre> </li> <li> <p>Committing again will finish successfully, with a well-formatted code.</p> </li> </ol>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<ul> <li>General</li> <li>What is Presidio?</li> <li>Why did Microsoft create Presidio?</li> <li>Is Microsoft Presidio an official Microsoft product?</li> <li>What is the difference between Presidio and different PII detection services like Azure Text Analytics and Amazon Comprehend?</li> <li>Using Presidio</li> <li>How can I start using Presidio?</li> <li>What are the main building blocks in Presidio?</li> <li>Customizing Presidio</li> <li>How can Presidio be customized to my needs?</li> <li>What NLP frameworks does Presidio support?</li> <li>Can Presidio be used for Pseudonymization?</li> <li>Does Presidio work on structured/tabular data?</li> <li>Improving detection accuracy</li> <li>What can I do if Presidio does not detect some of the PII entities in my data (False Negatives)?</li> <li>What can I do if Presidio falsely detects text as PII entities (False Positives)?</li> <li>How can I evaluate the performance of my Presidio instance?</li> <li>Deployment</li> <li>How can I deploy Presidio into my environment?</li> <li>Contributing</li> <li>How can I contribute to Presidio?</li> <li>How can I report security vulnerabilities?</li> </ul>"},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#what-is-presidio","title":"What is Presidio?","text":"<p>Presidio (Origin from Latin praesidium \u2018protection, garrison\u2019) helps to ensure sensitive data is properly managed and governed. It provides fast identification and anonymization modules for private entities in text and images. It is fully customizable and pluggable, can be adapted to your needs and be deployed into various environments.</p> <p>Note</p> <p>Presidio is a library or SDK rather than a service. It is meant to be customized to the user's or organization's specific needs.</p> <p>Warning</p> <p>Presidio can help identify sensitive/PII data in un/structured text. However, because Presidio is using trained ML models, there is no guarantee that Presidio will find all sensitive information. Consequently, additional systems and protections should be employed.</p>"},{"location":"faq/#why-did-microsoft-create-presidio","title":"Why did Microsoft create Presidio?","text":"<p>By developing Presidio, our goals are:</p> <ol> <li>Allow organizations to preserve privacy in a simpler way by democratizing de-identification technologies and introducing transparency in decisions.</li> <li>Embrace extensibility and customizability to a specific business need.</li> <li>Facilitate both fully automated and semi-automated PII de-identification flows on multiple platforms.</li> </ol>"},{"location":"faq/#is-microsoft-presidio-an-official-microsoft-product","title":"Is Microsoft Presidio an official Microsoft product?","text":"<p>The authors and maintainers of Presidio come from the Industry Solutions Engineering team. We work with customers on various engineering problems, and have found the proper handling of private and sensitive data a recurring challenge across many customers and industries.</p> <p>Note</p> <p>Microsoft Presidio is not an official Microsoft product. Usage terms are defined in the repository's license.</p>"},{"location":"faq/#what-is-the-difference-between-presidio-and-different-pii-detection-services-like-azure-text-analytics-and-amazon-comprehend","title":"What is the difference between Presidio and different PII detection services like Azure Text Analytics and Amazon Comprehend?","text":"<p>In a nutshell, Presidio is a library which is meant to be customized, whereas different SaaS tools for PII detection have less customization capabilities. Most of these SaaS offerings use dedicated ML models and other logic for PII detection and often have better entity coverage or accuracy than Presidio.</p> <p>Based on our internal research, leveraging Presidio in parallel to 3rd party PII detection services like Azure Text Analytics can bring optimal results mainly when the data in hand has entity types or values not supported by the 3rd party service. (see example here).</p>"},{"location":"faq/#using-presidio","title":"Using Presidio","text":""},{"location":"faq/#how-can-i-start-using-presidio","title":"How can I start using Presidio?","text":"<ol> <li>Check out the installation docs.</li> <li>Take a look at the different samples.</li> <li>Try the demo website.</li> </ol>"},{"location":"faq/#what-are-the-main-building-blocks-in-presidio","title":"What are the main building blocks in Presidio?","text":"<p>Presidio is a suite built of several packages and building blocks:</p> <ol> <li>Presidio Analyzer: a package for detecting PII entities in natural language.</li> <li>Presidio Anonymizer: a package for manipulating PII entities in text (e.g. remove, redact, hash, encrypt).</li> <li>Presidio Image Redactor: A package for detecting PII entities in image using OCR.</li> <li>A set of sample deployments as Python packages or Docker containers for Kubernetes, Azure Data Factory, Spark and more.</li> </ol>"},{"location":"faq/#customizing-presidio","title":"Customizing Presidio","text":""},{"location":"faq/#how-can-presidio-be-customized-to-my-needs","title":"How can Presidio be customized to my needs?","text":"<p>Users can customize Presidio in multiple ways:</p> <ol> <li>Create new or updated PII recognizers (docs).</li> <li>Adapt Presidio to new languages (docs).</li> <li>Leverage state of the art Named Entity Recognition models (docs).</li> <li>Add new types of anonymizers (docs).</li> <li>Create PII analysis and anonymization pipelines on different environments using Docker or Python (samples).</li> </ol> <p>And more.</p>"},{"location":"faq/#what-nlp-frameworks-does-presidio-support","title":"What NLP frameworks does Presidio support?","text":"<p>Presidio supports spaCy version 3+ for Named Entity Recognition, tokenization, lemmatization and more. We also support Stanza using the spacy-stanza package, and it is further possible to create PII recognizers leveraging other frameworks like transformers or Flair.</p> <p>For more information, see the docs.</p>"},{"location":"faq/#can-presidio-be-used-for-pseudonymization","title":"Can Presidio be used for Pseudonymization?","text":"<p>Pseudonymization is a de-identification technique in which the real data is replaced with fake data in a reversible way. Since there are various ways and approaches for this, we provide a simple sample which can be extended for more sophisticated usage. If you have a question or a request on this topic, please open an issue on the repo.</p>"},{"location":"faq/#does-presidio-work-on-structuredtabular-data","title":"Does Presidio work on structured/tabular data?","text":"<p>This is an area we are actively looking into. We have an example implementation of using Presidio on structured/semi-structured data. Also see the different discussions on this topic on the Discussions section. If you have a question, suggestion, or a contribution in this area, please reach out by opening an issue, starting a discussion or reaching us directly at presidio@microsoft.com</p>"},{"location":"faq/#improving-detection-accuracy","title":"Improving detection accuracy","text":""},{"location":"faq/#what-can-i-do-if-presidio-does-not-detect-some-of-the-pii-entities-in-my-data-false-negatives","title":"What can I do if Presidio does not detect some of the PII entities in my data (False Negatives)?","text":"<p>Presidio comes loaded with several PII recognizers (see list here), however its main strength lies in its customization capabilities to new entities, specific datasets, languages or use cases. For a recommended process for improving detection accuracy, see these guidelines.</p>"},{"location":"faq/#what-can-i-do-if-presidio-falsely-detects-text-as-pii-entities-false-positives","title":"What can I do if Presidio falsely detects text as PII entities (False Positives)?","text":"<p>Some PII recognizers are less specific than others. A driver's license number, for example, could be any 9-digit number. While Presidio leverages context words and other logic to improve the detection quality, it could still falsely detect non-entity values as PII entities.</p> <p>In order to avoid false positives, one could try to:</p> <ol> <li>Change the acceptance threshold, which defines what is the minimum confidence value for a detected entity to be returned.</li> <li>Remove unnecessary PII recognizers, if the dataset does not contain these entities.</li> <li>Update/replace the logic of specific recognizers to better suit a specific dataset or use case.</li> <li>Replace PII recognizers with those coming from 3rd party services.</li> </ol> <p>Every PII identification logic would have its errors, and there is a trade-off between false positives (falsely detected text) and false negatives (PII entities which are not detected).</p>"},{"location":"faq/#how-can-i-evaluate-the-performance-of-my-presidio-instance","title":"How can I evaluate the performance of my Presidio instance?","text":"<p>In addition to Presidio, we maintain a repo focused on evaluation of models and PII recognizers here. It also features a simple PII data generator.</p>"},{"location":"faq/#deployment","title":"Deployment","text":""},{"location":"faq/#how-can-i-deploy-presidio-into-my-environment","title":"How can I deploy Presidio into my environment?","text":"<p>The main Presidio modules (analyzer, anonymizer, image-redactor) can be used both as a Python package and as a dockerized REST API. See the different deployment samples for example deployments.</p>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#how-can-i-contribute-to-presidio","title":"How can I contribute to Presidio?","text":"<p>First, review the contribution guidelines, and feel free to reach out by opening an issue, posting a discussion or emailing us at presidio@microsoft.com</p>"},{"location":"faq/#how-can-i-report-security-vulnerabilities","title":"How can I report security vulnerabilities?","text":"<p>Please see the security information.</p>"},{"location":"getting_started/","title":"Getting started with Presidio","text":""},{"location":"getting_started/#simple-flow","title":"Simple flow","text":"<p>Using Presidio's modules as Python packages to get started:</p> Anonymize PII in text (Default spaCy model)Anonymize PII in text (transformers) <ol> <li> <p>Install Presidio</p> <pre><code>pip install presidio-analyzer\npip install presidio-anonymizer\npython -m spacy download en_core_web_lg\n</code></pre> </li> <li> <p>Analyze + Anonymize</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\ntext=\"My phone number is 212-555-5555\"\n\n# Set up the engine, loads the NLP module (spaCy model by default) \n# and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text=text,\n                           entities=[\"PHONE_NUMBER\"],\n                           language='en')\nprint(results)\n\n# Analyzer results are passed to the AnonymizerEngine for anonymization\n\nanonymizer = AnonymizerEngine()\n\nanonymized_text = anonymizer.anonymize(text=text,analyzer_results=results)\n\nprint(anonymized_text)\n</code></pre> </li> </ol> <ol> <li> <p>Install Presidio</p> <pre><code>pip install \"presidio-analyzer[transformers]\"\npip install presidio-anonymizer\npython -m spacy download en_core_web_sm\n</code></pre> </li> <li> <p>Analyze + Anonymize</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import TransformersNlpEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\ntext = \"My name is Don and my phone number is 212-555-5555\"\n\n# Define which transformers model to use\nmodel_config = [{\"lang_code\": \"en\", \"model_name\": {\n    \"spacy\": \"en_core_web_sm\",  # use a small spaCy model for lemmas, tokens etc.\n    \"transformers\": \"dslim/bert-base-NER\"\n    }\n}]\n\nnlp_engine = TransformersNlpEngine(models=model_config)\n\n# Set up the engine, loads the NLP module (spaCy model by default) \n# and other PII recognizers\nanalyzer = AnalyzerEngine(nlp_engine=nlp_engine)\n\n# Call analyzer to get results\nresults = analyzer.analyze(text=text, language='en')\nprint(results)\n\n# Analyzer results are passed to the AnonymizerEngine for anonymization\n\nanonymizer = AnonymizerEngine()\n\nanonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n\nprint(anonymized_text)\n</code></pre> <p>Tip: Downloading models</p> <p>If not available, the transformers model and the spacy model would be downloaded on the first call to the <code>AnalyzerEngine</code>. To pre-download, see this doc.</p> </li> </ol>"},{"location":"getting_started/#simple-flow-images","title":"Simple flow: Images","text":"Anonymize PII in imagesRedact text PII in DICOM images <ol> <li> <p>Install presidio-image-redactor</p> <pre><code>pip install presidio-image-redactor\n</code></pre> </li> <li> <p>Redact PII from image</p> <pre><code>from presidio_image_redactor import ImageRedactorEngine\nfrom PIL import Image\n\nimage = Image.open(path_to_image_file)\n\nredactor = ImageRedactorEngine()\nredactor.redact(image=image)\n</code></pre> </li> </ol> <ol> <li> <p>Install presidio-image-redactor</p> <pre><code>pip install presidio-image-redactor\n</code></pre> </li> <li> <p>Redact text PII from DICOM image</p> <pre><code>import pydicom\nfrom presidio_image_redactor import DicomImageRedactorEngine\n\n# Set input and output paths\ninput_path = \"path/to/your/dicom/file.dcm\"\noutput_dir = \"./output\"\n\n# Initialize the engine\nengine = DicomImageRedactorEngine()\n\n# Option 1: Redact from a loaded DICOM image\ndicom_image = pydicom.dcmread(input_path)\nredacted_dicom_image = engine.redact(dicom_image, fill=\"contrast\")\n\n# Option 2: Redact from DICOM file\nengine.redact_from_file(input_path, output_dir, padding_width=25, fill=\"contrast\")\n\n# Option 3: Redact from directory\nengine.redact_from_directory(\"path/to/your/dicom\", output_dir, padding_width=25, fill=\"contrast\")\n</code></pre> </li> </ol>"},{"location":"getting_started/#read-more","title":"Read more","text":"<ul> <li>Installing Presidio</li> <li>PII detection in text</li> <li>PII anonymization in text</li> <li>PII redaction in images</li> <li>Discussion board</li> </ul>"},{"location":"installation/","title":"Installing Presidio","text":""},{"location":"installation/#description","title":"Description","text":"<p>This document describes the installation of the entire Presidio suite using <code>pip</code> (as Python packages) or using <code>Docker</code> (As containerized services).</p>"},{"location":"installation/#using-pip","title":"Using pip","text":"<p>Note</p> <p>Consider installing the Presidio python packages in a virtual environment like venv or conda.</p>"},{"location":"installation/#supported-python-versions","title":"Supported Python Versions","text":"<p>Presidio is supported for the following python versions:</p> <ul> <li>3.7</li> <li>3.8</li> <li>3.9</li> <li>3.10</li> <li>3.11</li> </ul>"},{"location":"installation/#pii-anonymization-on-text","title":"PII anonymization on text","text":"<p>For PII anonymization on text, install the <code>presidio-analyzer</code> and <code>presidio-anonymizer</code> packages with at least one NLP engine (<code>spaCy</code>, <code>transformers</code> or <code>stanza</code>):</p> spaCy (default)TransformersStanza <pre><code>pip install presidio_analyzer\npip install presidio_anonymizer\npython -m spacy download en_core_web_lg\n</code></pre> <pre><code>pip install \"presidio_analyzer[transformers]\"\npip install presidio_anonymizer\npython -m spacy download en_core_web_sm\n</code></pre> <p>Note</p> <p>When using a transformers NLP engine, Presidio would still use spaCy for other capabilities, therefore a small spaCy model (such as en_core_web_sm) is required.  Transformers models would be loaded lazily. To pre-load them, see: Downloading a pre-trained model</p> <pre><code>pip install \"presidio_analyzer[stanza]\"\npip install presidio_anonymizer\n</code></pre> <p>Note</p> <p>Stanza models would be loaded lazily. To pre-load them, see: Downloading a pre-trained model.</p>"},{"location":"installation/#pii-redaction-in-images","title":"PII redaction in images","text":"<p>For PII redaction in images, install the <code>presidio-image-redactor</code> package:</p> <pre><code>pip install presidio_image_redactor\n\n# Presidio image redactor uses the presidio-analyzer\n# which requires a spaCy language model:\npython -m spacy download en_core_web_lg\n</code></pre>"},{"location":"installation/#using-docker","title":"Using Docker","text":"<p>Presidio can expose REST endpoints for each service using Flask and Docker. To download the Presidio Docker containers, run the following command:</p> <p>Note</p> <p>This requires Docker to be installed. Download Docker.</p>"},{"location":"installation/#for-pii-anonymization-in-text","title":"For PII anonymization in text","text":"<p>For PII detection and anonymization in text, the <code>presidio-analyzer</code> and <code>presidio-anonymizer</code> modules are required.</p> <pre><code># Download Docker images\ndocker pull mcr.microsoft.com/presidio-analyzer\ndocker pull mcr.microsoft.com/presidio-anonymizer\n\n# Run containers with default ports\ndocker run -d -p 5001:3000 mcr.microsoft.com/presidio-analyzer:latest\n\ndocker run -d -p 5002:3000 mcr.microsoft.com/presidio-anonymizer:latest\n</code></pre>"},{"location":"installation/#for-pii-redaction-in-images","title":"For PII redaction in images","text":"<p>For PII detection in images, the <code>presidio-image-redactor</code> is required.</p> <pre><code># Download Docker image\ndocker pull mcr.microsoft.com/presidio-image-redactor\n\n# Run container with the default port\ndocker run -d -p 5003:3000 mcr.microsoft.com/presidio-image-redactor:latest\n</code></pre> <p>Once the services are running, their APIs are available. API reference and example calls can be found here.</p>"},{"location":"installation/#install-from-source","title":"Install from source","text":"<p>To install Presidio from source, first clone the repo:</p> <ul> <li>using HTTPS</li> </ul> <pre><code>git clone https://github.com/microsoft/presidio.git\n</code></pre> <ul> <li>Using SSH</li> </ul> <pre><code>git clone git@github.com:microsoft/presidio.git\n</code></pre> <p>Then, build the containers locally.</p> <p>Note</p> <p>Presidio uses docker-compose to manage the different Presidio containers.</p> <p>From the root folder of the repo:</p> <pre><code>docker-compose --build\n</code></pre> <p>To run all Presidio services:</p> <pre><code>docker-compose up -d\n</code></pre> <p>Alternatively, you can build and run individual services. For example, for the <code>presidio-anonymizer</code> service:</p> <pre><code>docker build ./presidio-anonymizer -t presidio/presidio-anonymizer\n</code></pre> <p>And run:</p> <pre><code>docker run -d -p 5002:5001 presidio/presidio-anonymizer\n</code></pre> <p>For more information on developing locally, refer to the setting up a development environment section.</p>"},{"location":"presidio_V2/","title":"Presidio Revamp (aka V2)","text":"<p>As of March 2021, Presidio had undergo a revamp to a new version refereed to as V2.</p> <p>The main changes introduced in V2 are:</p> <ol> <li>gRPC replaced with HTTP to allow more customizable APIs and easier debugging</li> <li> <p>Focus on the Analyzer and Anonymizer services.</p> <ol> <li>Presidio Anonymizer is now Python based and pip installable.</li> <li>Presidio Analyzer does not use templates and external recognizer store.</li> <li>Image Redactor (formerly presidio-image-anonymizer) is in early beta and is Python based and pip installable.</li> <li>Other services are deprecated and potentially be migrated over time to V2 with the help of the community.</li> </ol> </li> <li> <p>Improved documentation, samples and build flows.</p> </li> <li> <p>Format Preserving Encryption replaced with Advanced Encryption Standard (AES) </p> </li> </ol>"},{"location":"presidio_V2/#v1-availability","title":"V1 Availability","text":"<p>Version V1 (legacy) is still available for download. To continue using the previous version: -   For docker containers, use tag=v1  -   For python packages, download version &lt; 2 (e.g. pip install presidio-analyzer==0.95)</p> <p>Note</p> <p>The legacy V1 code base will continue to be available under branch V1 but will no longer be officially supported.</p>"},{"location":"presidio_V2/#api-changes","title":"API Changes","text":"<p>The move from gRPC to HTTP based APIs included changes to the API requests.</p> <ol> <li> <p>Change in payload - moving from structures to jsons.</p> </li> <li> <p>Removing templates from the API - includes flattening the json.</p> </li> <li>Using snake_case instead of camelCase .</li> </ol> <p>Below is a detailed outline of all the changes done to the Analyzer and Anonymizer.</p>"},{"location":"presidio_V2/#analyzer-api-changes","title":"Analyzer API Changes","text":""},{"location":"presidio_V2/#legacy-json-request-grpc","title":"Legacy json request (gRPC)","text":"<pre><code>{\n    \"text\": \"My phone number is 212-555-5555\",\n    \"AnalyzeTemplateId\": \"1234\",\n    \"AnalyzeTemplate\": {\n        \"Fields\": [\n            {\n                \"Name\": \"PHONE_NUMBER\",\n                \"MinScore\": \"0.5\"\n            }\n        ],\n        \"AllFields\": true,\n        \"Description\": \"template description\",\n        \"CreateTime\": \"template creation time\",\n        \"ModifiedTime\": \"template modification time\",\n        \"Language\": \"fr\",\n        \"ResultsScoreThreshold\": 0.5\n    }\n}\n</code></pre>"},{"location":"presidio_V2/#v2-json-request-http","title":"V2 json request (HTTP)","text":"<pre><code>{\n    \"text\": \"My phone number is 212-555-5555\",\n    \"entities\": [\"PHONE_NUMBER\"],\n    \"language\": \"en\",\n    \"correlation_id\": \"213\",\n    \"score_threshold\": 0.5,\n    \"trace\": true,\n    \"return_decision_process\": true\n}\n</code></pre>"},{"location":"presidio_V2/#anonymizer-api-changes","title":"Anonymizer API Changes","text":""},{"location":"presidio_V2/#legacy-json-request-grpc_1","title":"Legacy json request (gRPC)","text":"<pre><code>{\n  \"text\": \"hello world, my name is Jane Doe. My number is: 034453334\",\n  \"template\": {\n    \"description\": \"DEPRECATED\",\n    \"create_time\": \"DEPRECATED\",\n    \"modified_time\": \"DEPRECATED\",\n    \"default_transformation\": {\n      \"replace_value\": {...},\n      \"redact_value\": {...},\n      \"hash_value\": {...},\n      \"mask_value\": {...},\n      \"fpe_value\": {...}\n    },\n    \"field_type_transformations\": [\n      {\n        \"fields\": [\n          {\n            \"name\": \"FIRST_NAME\",\n            \"min_score\": \"0.2\"\n          }\n        ],\n        \"transfomarion\": {\n          \"replace_value\": {...},\n          \"redact_value\": {...},\n          \"hash_value\": {...},\n          \"mask_value\": {...},\n          \"fpe_value\": {...}\n        }\n      }\n    ],\n    \"analyze_results\": [\n      {\n        \"text\": \"Jane\",\n        \"field\": {\n          \"name\": \"FIRST_NAME\",\n          \"min_score\": \"0.5\"\n        },\n        \"location\": {\n          \"start\": 24,\n          \"end\": 32,\n          \"length\": 6\n        },\n        \"score\": 0.8\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"presidio_V2/#v2-json-request-http_1","title":"V2 json request (HTTP)","text":"<pre><code>{\n    \"text\": \"hello world, my name is Jane Doe. My number is: 034453334\",\n    \"anonymizers\": {\n        \"DEFAULT\": {\n            \"type\": \"replace\",\n            \"new_value\": \"val\"\n        },\n        \"PHONE_NUMBER\": {\n            \"type\": \"mask\",\n            \"masking_char\": \"*\",\n            \"chars_to_mask\": 4,\n            \"from_end\": true\n        }\n    },\n    \"analyzer_results\": [\n        {\n            \"start\": 24,\n            \"end\": 32,\n            \"score\": 0.8,\n            \"entity_type\": \"NAME\"\n        },\n        {\n            \"start\": 24,\n            \"end\": 28,\n            \"score\": 0.8,\n            \"entity_type\": \"FIRST_NAME\"\n        },\n        {\n            \"start\": 29,\n            \"end\": 32,\n            \"score\": 0.6,\n            \"entity_type\": \"LAST_NAME\"\n        },\n        {\n            \"start\": 48,\n            \"end\": 57,\n            \"score\": 0.95,\n            \"entity_type\": \"PHONE_NUMBER\"\n        }\n    ]\n}\n</code></pre> <p>Specific for each anonymization type:</p> Anonymization name Legacy format (V1) New json format (V2) Replace <pre>string newValue = 1;</pre> <pre>{ \"new_value\": \"VALUE\" }</pre> Redact NONE NONE Mask <pre>string maskingCharacter = 1;int32 charsToMask = 2; bool fromEnd = 3;</pre> <pre>{ \"chars_to_mask\": 10, \"from_end\": true, \"masking_char\": \"*\" }</pre> Hash NONE <pre>{\"hash_type\": \"VALUE\"}</pre> FPE (now Encrypt) <pre>string key = 3t6w9z$C&amp;F)J@NcR;int32 tweak = D8E7920AFA330A73</pre> <pre>{\"key\": \"3t6w9z$C&amp;F)J@NcR\"}</pre> <p>Note</p> <p>The V2 API keeps changing please follow the change log for updates.</p>"},{"location":"supported_entities/","title":"PII entities supported by Presidio","text":"<p>Presidio contains predefined recognizers for PII entities. This page describes the different entities Presidio can detect and the method Presidio employs to detect those.</p> <p>In addition, Presidio allows you to add custom entity recognizers. For more information, refer to the adding new recognizers documentation.</p>"},{"location":"supported_entities/#list-of-supported-entities","title":"List of supported entities","text":""},{"location":"supported_entities/#global","title":"Global","text":"Entity Type Description Detection Method CREDIT_CARD A credit card number is between 12 to 19 digits. https://en.wikipedia.org/wiki/Payment_card_number Pattern match and checksum CRYPTO A Crypto wallet number. Currently only Bitcoin address is supported Pattern match, context and checksum DATE_TIME Absolute or relative dates or periods or times smaller than a day. Pattern match and context EMAIL_ADDRESS An email address identifies an email box to which email messages are delivered Pattern match, context and RFC-822 validation IBAN_CODE The International Bank Account Number (IBAN) is an internationally agreed system of identifying bank accounts across national borders to facilitate the communication and processing of cross border transactions with a reduced risk of transcription errors. Pattern match, context and checksum IP_ADDRESS An Internet Protocol (IP) address (either IPv4 or IPv6). Pattern match, context and checksum NRP A person\u2019s Nationality, religious or political group. Custom logic and context LOCATION Name of politically or geographically defined location (cities, provinces, countries, international regions, bodies of water, mountains Custom logic and context PERSON A full person name, which can include first names, middle names or initials, and last names. Custom logic and context PHONE_NUMBER A telephone number Custom logic, pattern match and context MEDICAL_LICENSE Common medical license numbers. Pattern match, context and checksum URL A URL (Uniform Resource Locator), unique identifier used to locate a resource on the Internet Pattern match, context and top level url validation"},{"location":"supported_entities/#usa","title":"USA","text":"FieldType Description Detection Method US_BANK_NUMBER A US bank account number is between 8 to 17 digits. Pattern match and context US_DRIVER_LICENSE A US driver license according to https://ntsi.com/drivers-license-format/ Pattern match and context US_ITIN US Individual Taxpayer Identification Number (ITIN). Nine digits that start with a \"9\" and contain a \"7\" or \"8\" as the 4 digit. Pattern match and context US_PASSPORT A US passport number with 9 digits. Pattern match and context US_SSN A US Social Security Number (SSN) with 9 digits. Pattern match and context"},{"location":"supported_entities/#uk","title":"UK","text":"FieldType Description Detection Method UK_NHS A UK NHS number is 10 digits. Pattern match, context and checksum"},{"location":"supported_entities/#spain","title":"Spain","text":"FieldType Description Detection Method ES_NIF A spanish NIF number (Personal tax ID) . Pattern match, context and checksum"},{"location":"supported_entities/#italy","title":"Italy","text":"FieldType Description Detection Method IT_FISCAL_CODE An Italian personal identification code. https://en.wikipedia.org/wiki/Italian_fiscal_code Pattern match, context and checksum IT_DRIVER_LICENSE An Italian driver license number. Pattern match and context IT_VAT_CODE An Italian VAT code number Pattern match, context and checksum IT_PASSPORT An Italian passport number. Pattern match and context IT_IDENTITY_CARD An Italian identity card number. https://en.wikipedia.org/wiki/Italian_electronic_identity_card Pattern match and context"},{"location":"supported_entities/#singapore","title":"Singapore","text":"FieldType Description Detection Method SG_NRIC_FIN A National Registration Identification Card Pattern match and context"},{"location":"supported_entities/#australia","title":"Australia","text":"FieldType Description Detection Method AU_ABN The Australian Business Number (ABN) is a unique 11 digit identifier issued to all entities registered in the Australian Business Register (ABR). Pattern match, context, and checksum AU_ACN An Australian Company Number is a unique nine-digit number issued by the Australian Securities and Investments Commission to every company registered under the Commonwealth Corporations Act 2001 as an identifier. Pattern match, context, and checksum AU_TFN The tax file number (TFN) is a unique identifier issued by the Australian Taxation Office to each taxpaying entity Pattern match, context, and checksum AU_MEDICARE Medicare number is a unique identifier issued by Australian Government that enables the cardholder to receive a rebates of medical expenses under Australia's Medicare system Pattern match, context, and checksum"},{"location":"supported_entities/#adding-a-custom-pii-entity","title":"Adding a custom PII entity","text":"<p>See this documentation for instructions on how to add a new Recognizer for a new type of PII entity.</p>"},{"location":"supported_entities/#connecting-to-3rd-party-pii-detectors","title":"Connecting to 3rd party PII detectors","text":"<p>See this documentation for instructions on how to implement an external PII detector for a new or existing type of PII entity.</p>"},{"location":"text_anonymization/","title":"Text anonymization","text":"<p>Presidio's features two main modules for anonymization PII in text:</p> <ul> <li>Presidio analyzer: Identification of PII in text</li> <li>Presidio anonymizer: De-identify detected PII entities using different operators</li> </ul> <p>In most cases, we would run the Presidio analyzer to detect where PII entities exist, and then the Presidio anonymizer to remove those using specific operators (such as redact, replace, hash or encrypt)</p> <p>This figure presents the overall flow in high level:</p> <p></p> <ul> <li>The Presidio Analyzer holds multiple recognizers, each one capable of detecting specific PII entities. These recognizers leverage regular expressions, deny lists, checksum, rule based logic, Named Entity Recognition ML models and context from surrounding words.</li> <li>The Presidio Anonymizer holds multiple operators, each one can be used to anonymize the PII entity in a different way. Additionally, it can be used to de-anonymize an already anonymized entity (For example, decrypt an encrypted entity)</li> </ul>"},{"location":"analyzer/","title":"Presidio Analyzer","text":"<p>The Presidio analyzer is a Python based service for detecting PII entities in text.</p> <p>During analysis, it runs a set of different PII Recognizers, each one in charge of detecting one or more PII entities using different mechanisms.</p> <p>Presidio analyzer comes with a set of predefined recognizers, but can easily be extended with other types of custom recognizers. Predefined and custom recognizers leverage regex, Named Entity Recognition and other types of logic to detect PII in unstructured text.</p> <p></p>"},{"location":"analyzer/#installation","title":"Installation","text":"<p>see Installing Presidio.</p>"},{"location":"analyzer/#getting-started","title":"Getting started","text":"PythonAs an HTTP server <p>Once the Presidio-analyzer package is installed, run this simple analysis script:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text=\"My phone number is 212-555-5555\",\n                           entities=[\"PHONE_NUMBER\"],\n                           language='en')\nprint(results)\n</code></pre> <p>You can run presidio analyzer as an http server using either python runtime or using a docker container.</p>"},{"location":"analyzer/#using-docker-container","title":"Using docker container","text":"<pre><code>cd presidio-analyzer\ndocker run -p 5002:3000 presidio-analyzer\n</code></pre>"},{"location":"analyzer/#using-python-runtime","title":"Using python runtime","text":"<p>Note</p> <p>This requires the Presidio Github repository to be cloned.</p> <pre><code>cd presidio-analyzer\npython app.py\ncurl -d '{\"text\":\"John Smith drivers license is AC432223\", \"language\":\"en\"}' -H \"Content-Type: application/json\" -X POST http://localhost:3000/analyze\n</code></pre>"},{"location":"analyzer/#creating-pii-recognizers","title":"Creating PII recognizers","text":"<p>Presidio analyzer can be easily extended to support additional PII entities. See this tutorial on adding new PII recognizers for more information.</p>"},{"location":"analyzer/#multi-language-support","title":"Multi-language support","text":"<p>Presidio can be used to detect PII entities in multiple languages. Refer to the multi-language support for more information.</p>"},{"location":"analyzer/#outputting-the-analyzer-decision-process","title":"Outputting the analyzer decision process","text":"<p>Presidio analyzer has a built in mechanism for tracing each decision made. This can be useful when attempting to understand a specific PII detection. For more info, see the decision process documentation.</p>"},{"location":"analyzer/#supported-entities","title":"Supported entities","text":"<p>For a list of the current supported entities: Supported entities.</p>"},{"location":"analyzer/#api-reference","title":"API reference","text":"<p>Follow the API Spec for the Analyzer REST API reference details and Analyzer Python API for Python API reference</p>"},{"location":"analyzer/#samples","title":"Samples","text":"<p>Samples illustrating the usage of the Presidio Analyzer can be found in the Python samples.</p>"},{"location":"analyzer/adding_recognizers/","title":"Supporting detection of new types of PII entities","text":"<p>Presidio can be extended to support detection of new types of PII entities, and to support additional languages. These PII recognizers could be added via code or ad-hoc as part of the request.</p>"},{"location":"analyzer/adding_recognizers/#introduction-to-recognizer-development","title":"Introduction to recognizer development","text":"<p>Entity recognizers are Python objects capable of detecting one or more entities in a specific language. In order to extend Presidio's detection capabilities to new types of PII entities, these <code>EntityRecognizer</code> objects should be added to the existing list of recognizers.</p>"},{"location":"analyzer/adding_recognizers/#types-of-recognizer-classes-in-presidio","title":"Types of recognizer classes in Presidio","text":"<p>The following class diagram shows the different types of recognizer families Presidio contains.</p> <p></p> <ul> <li>The <code>EntityRecognizer</code> is an abstract class for all recognizers.</li> <li>The <code>RemoteRecognizer</code> is an abstract class for calling external PII detectors. See more info here.</li> <li>The abstract class <code>LocalRecognizer</code> is implemented by all recognizers running within the Presidio-analyzer process.</li> <li>The <code>PatternRecognizer</code> is an class for supporting regex and deny-list based recognition logic, including validation (e.g., with checksum) and context support. See an example here.</li> </ul>"},{"location":"analyzer/adding_recognizers/#extending-the-analyzer-for-additional-pii-entities","title":"Extending the analyzer for additional PII entities","text":"<ol> <li>Create a new class based on <code>EntityRecognizer</code>.</li> <li>Add the new recognizer to the recognizer registry so that the <code>AnalyzerEngine</code> can use the new recognizer during analysis.</li> </ol>"},{"location":"analyzer/adding_recognizers/#simple-example","title":"Simple example","text":"<p>For simple recognizers based on regular expressions or deny-lists, we can leverage the provided <code>PatternRecognizer</code>:</p> <pre><code>from presidio_analyzer import PatternRecognizer\ntitles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n                                      deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n</code></pre> <p>Calling the recognizer itself:</p> <pre><code>titles_recognizer.analyze(text=\"Mr. Schmidt\", entities=\"TITLE\")\n</code></pre> <p>Adding it to the list of recognizers:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nregistry = RecognizerRegistry()\nregistry.load_predefined_recognizers()\n\n# Add the recognizer to the existing list of recognizers\nregistry.add_recognizer(titles_recognizer)\n\n# Set up analyzer with our updated recognizer registry\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Run with input text\ntext=\"His name is Mr. Jones\"\nresults = analyzer.analyze(text=text, language=\"en\")\nprint(results)\n</code></pre> <p>Alternatively, we can add the recognizer directly to the existing registry:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\nanalyzer = AnalyzerEngine()\n\nanalyzer.registry.add_recognizer(titles_recognizer)\n\nresults = analyzer.analyze(text=text,language=\"en\")\nprint(results)\n</code></pre> <p>For pattern based recognizers, it is possible to change the regex flags, either for one recognizer or for all. For one recognizer, use the <code>global_regex_flags</code> parameter  in the <code>PatternRecognizer</code> constructor. For all recognizers, use the <code>global_regex_flags</code> parameter in the <code>RecognizerRegistry</code> constructor:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nimport regex as re\n\nregistry = RecognizerRegistry(global_regex_flags=re.DOTALL | re.MULTILINE | re.IGNORECASE)\nengine = AnalyzerEngine(registry=registry)\nengine.analyze(...)\n</code></pre>"},{"location":"analyzer/adding_recognizers/#creating-a-new-entityrecognizer-in-code","title":"Creating a new <code>EntityRecognizer</code> in code","text":"<p>To create a new recognizer via code:</p> <ol> <li> <p>Create a new Python class which implements LocalRecognizer. (<code>LocalRecognizer</code> implements the base EntityRecognizer class)</p> <p>This class has the following functions:</p> <p>i. load: load a model / resource to be used during recognition</p> <p> <pre><code>def load(self)\n</code></pre></p> <p>ii. analyze: The main function to be called for getting entities out of the new recognizer:</p> <p> <pre><code>def analyze(self, text, entities, nlp_artifacts)\n</code></pre></p> <p>Notes: 1. Each recognizer has access to different NLP assets such as tokens, lemmas, and more. These are given through the <code>nlp_artifacts</code> parameter. Refer to the source code for more information.</p> <ol> <li>The <code>analyze</code> method should return a list of RecognizerResult.</li> </ol> </li> <li> <p>Add it to the recognizer registry using <code>registry.add_recognizer(my_recognizer)</code>.</p> </li> </ol> <p>For more examples, see the Customizing Presidio Analyzer jupyter notebook.</p>"},{"location":"analyzer/adding_recognizers/#creating-a-remote-recognizer","title":"Creating a remote recognizer","text":"<p>A remote recognizer is an <code>EntityRecognizer</code> object interacting with an external service. The external service could be a 3rd party PII detection service or a custom service deployed in parallel to Presidio.</p> <p>Sample implementation of a <code>RemoteRecognizer</code>. In this example, an external PII detection service exposes two APIs: <code>detect</code> and <code>supported_entities</code>. The class implemented here, <code>ExampleRemoteRecognizer</code>, uses the <code>requests</code> package to call the external service via HTTP.</p> <p>In this code snippet, we simulate the external PII detector by using the Presidio analyzer. In reality, we would adapt this code to fit the external PII detector we have in hand.</p> <p>For an example of integrating a <code>RemoteRecognizer</code> with Presidio-Analyzer, see this example.</p>"},{"location":"analyzer/adding_recognizers/#creating-pre-defined-recognizers","title":"Creating pre-defined recognizers","text":"<p>Once a recognizer is created, it can either be added to the <code>RecognizerRegistry</code> via the <code>add_recognizer</code> method, or it could be added into the list of predefined recognizers. To add a recognizer to the list of pre-defined recognizers:</p> <ol> <li>Clone the repo.</li> <li>Create a file containing the new recognizer Python class.</li> <li>Add the recognizer to the <code>recognizers_map</code> dict in the <code>RecognizerRegistry.load_predefined_recognizers</code> method. In this map, the key is the language the recognizer supports, and the value is the class itself. If your recognizer detects entities in multiple languages, add it to under the \"ALL\" key.</li> <li>Optional: Update documentation (e.g., the supported entities list).</li> </ol>"},{"location":"analyzer/adding_recognizers/#azure-text-analytics-recognizer","title":"Azure Text Analytics recognizer","text":"<p>On how to integrate Presidio with Azure Text Analytics, and a sample for a Text Analytics Remote Recognizer, refer to the Azure Text Analytics Integration document.</p>"},{"location":"analyzer/adding_recognizers/#creating-ad-hoc-recognizers","title":"Creating ad-hoc recognizers","text":"<p>In addition to recognizers in code, it is possible to create ad-hoc recognizers via the Presidio Analyzer API for regex and deny-list based logic. These recognizers, in JSON form, are added to the <code>/analyze</code> request and are only used in the context of this request.</p> <ul> <li> <p>The json structure for a regex ad-hoc recognizer is the following:</p> <pre><code>{\n    \"text\": \"John Smith drivers license is AC432223. Zip code: 10023\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Zip code Recognizer\",\n        \"supported_language\": \"en\",\n        \"patterns\": [\n            {\n            \"name\": \"zip code (weak)\", \n            \"regex\": \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\", \n            \"score\": 0.01\n            }\n        ],\n        \"context\": [\"zip\", \"code\"],\n        \"supported_entity\":\"ZIP\"\n        }\n    ]\n}\n</code></pre> </li> <li> <p>The json structure for a deny-list based recognizers is the following:</p> <pre><code>{\n    \"text\": \"Mr. John Smith's drivers license is AC432223\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Mr. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Mr\", \"Mr.\", \"Mister\"],\n        \"supported_entity\":\"MR_TITLE\"\n        },\n        {\n        \"name\": \"Ms. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Ms\", \"Ms.\", \"Miss\", \"Mrs\", \"Mrs.\"],\n        \"supported_entity\":\"MS_TITLE\"\n        }\n    ]\n}\n</code></pre> </li> </ul> <p>In both examples, the <code>/analyze</code> request is extended with a list of <code>ad_hoc_recognizers</code>, which could be either <code>patterns</code>, <code>deny_list</code> or both.</p> <p>Additional examples can be found in the OpenAPI spec.</p>"},{"location":"analyzer/adding_recognizers/#reading-pattern-recognizers-from-yaml","title":"Reading pattern recognizers from YAML","text":"<p>Recognizers can be loaded from a YAML file, which allows users to add recognition logic without writing code. An example YAML file can be found here.</p> <p>Once the YAML file is created, it can be loaded into the <code>RecognizerRegistry</code> instance.</p> <p>This example creates a <code>RecognizerRegistry</code> holding only the recognizers in the YAML file:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nyaml_file = \"recognizers.yaml\"\nregistry = RecognizerRegistry()\nregistry.add_recognizers_from_yaml(yaml_file)\n\nanalyzer = AnalyzerEngine(registry=registry)\nanalyzer.analyze(text=\"Mr. and Mrs. Smith\", language=\"en\")\n</code></pre> <p>This example adds the new recognizers to the predefined recognizers in Presidio:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nyaml_file = \"recognizers.yaml\"\nregistry = RecognizerRegistry()\nregistry.load_predefined_recognizers()\n\nregistry.add_recognizers_from_yaml(yaml_file)\n\nanalyzer = AnalyzerEngine()\nanalyzer.analyze(text=\"Mr. and Mrs. Smith\", language=\"en\")\n</code></pre> <p>Further reading:</p> <ol> <li>PII detection in different languages.</li> <li>Customizing the NLP model.</li> <li>Best practices for developing PII recognizers.</li> <li>Code samples for customizing Presidio Analyzer with new recognizers.</li> </ol>"},{"location":"analyzer/customizing_nlp_models/","title":"Customizing the NLP engine in Presidio Analyzer","text":"<p>Presidio uses NLP engines for two main tasks: NER based PII identification, and feature extraction for downstream rule based logic (such as leveraging context words for improved detection). While Presidio comes with an open-source model (the <code>en_core_web_lg</code> model from spaCy), additional NLP models and frameworks could be plugged in, either public or proprietary. These models can be trained or downloaded from existing NLP frameworks like spaCy, Stanza and transformers.</p> <p>In addition, other types of NLP frameworks can be integrated into Presidio.</p>"},{"location":"analyzer/customizing_nlp_models/#setting-up-a-custom-nlp-model","title":"Setting up a custom NLP model","text":"<ul> <li>spaCy or stanza</li> <li>transformers</li> </ul>"},{"location":"analyzer/customizing_nlp_models/#configure-presidio-to-use-the-new-model","title":"Configure Presidio to use the new model","text":"<p>Configuration can be done in two ways:</p> <ul> <li> <p>Via code: Create an <code>NlpEngine</code> using the <code>NlpEnginerProvider</code> class, and pass it to the <code>AnalyzerEngine</code> as input:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\n# Create configuration containing engine name and models\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n                {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n}\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, \n    supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es Morris\", language=\"es\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(results_english)\n</code></pre> </li> <li> <p>Via configuration: Set up the models which should be used in the default <code>conf</code> file.</p> <p>An example Conf file:</p> <pre><code>nlp_engine_name: spacy\nmodels:\n    -\n    lang_code: en\n    model_name: en_core_web_lg\n    -\n    lang_code: es\n    model_name: es_core_news_md \nner_model_configuration:\nlabels_to_ignore:\n- O\nmodel_to_presidio_entity_mapping:\n    PER: PERSON\n    LOC: LOCATION\n    ORG: ORGANIZATION\n    AGE: AGE\n    ID: ID\n    DATE: DATE_TIME\nlow_confidence_score_multiplier: 0.4\nlow_score_entity_names:\n- ID\n- ORG\n</code></pre> <p>The <code>ner_model_configuration</code> section contains the following parameters:</p> </li> <li> <p><code>labels_to_ignore</code>: A list of labels to ignore. For example, <code>O</code> (no entity) or entities you are not interested in returning.</p> </li> <li><code>model_to_presidio_entity_mapping</code>: A mapping between the transformers model labels and the Presidio entity types.</li> <li><code>low_confidence_score_multiplier</code>: A multiplier to apply to the score of entities with low confidence.</li> <li> <p><code>low_score_entity_names</code>: A list of entity types to apply the low confidence score multiplier to.</p> <p>The default conf file is read during the default initialization of the <code>AnalyzerEngine</code>. Alternatively, the path to a custom configuration file can be passed to the <code>NlpEngineProvider</code>:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nLANGUAGES_CONFIG_FILE = \"./docs/analyzer/languages-config.yml\"\n\n# Create NLP engine based on configuration file\nprovider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, \n    supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es David\", language=\"es\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is David\", language=\"en\")\nprint(results_english)\n</code></pre> <p>In this examples we:     a. create an <code>NlpEngine</code> holding two spaCy models (one in English: <code>en_core_web_lg</code> and one in Spanish: <code>es_core_news_md</code>).     b. define the <code>supported_languages</code> parameter accordingly.     c. pass requests in each of these languages.</p> <p>Note</p> <p>Presidio can currently use one NER model per language via the <code>NlpEngine</code>. If multiple are required, consider wrapping NER models as additional recognizers (see sample here).</p> </li> </ul>"},{"location":"analyzer/customizing_nlp_models/#leverage-frameworks-other-than-spacy-stanza-and-transformers-for-ml-based-pii-detection","title":"Leverage frameworks other than spaCy, Stanza and transformers for ML based PII detection","text":"<p>In addition to the built-in spaCy/Stanza/transformers capabitilies, it is possible to create new recognizers which serve as interfaces to other models. For more information:</p> <ul> <li>Remote recognizer documentation and samples.</li> <li>Flair recognizer example</li> </ul> <p>For considerations for creating such recognizers, see the best practices for adding ML recognizers documentation.</p>"},{"location":"analyzer/decision_process/","title":"The Presidio-analyzer decision process","text":""},{"location":"analyzer/decision_process/#background","title":"Background","text":"<p>Presidio-analyzer's decision process exposes information on why a specific PII was detected. Such information could contain:</p> <ul> <li>Which recognizer detected the entity</li> <li>Which regex pattern was used</li> <li>Interpretability mechanisms in ML models</li> <li>Which context words improved the score</li> <li>Confidence scores before and after each step</li> </ul> <p>And more.</p>"},{"location":"analyzer/decision_process/#usage","title":"Usage","text":"<p>The decision process can be leveraged in two ways:</p> <ol> <li>Presidio-analyzer can log its decision process into a designated logger, which allows you to investigate a specific api request, by exposing a <code>correlation-id</code> as part of the api response headers.</li> <li>The decision process can be returned as part of the <code>/analyze</code>  response.</li> </ol>"},{"location":"analyzer/decision_process/#getting-the-decision-process-as-part-of-the-response","title":"Getting the decision process as part of the response","text":"<p>The decision process result can be added to the response. To enable it, call the <code>analyze</code> method with <code>return_decision_process</code> set as True.</p> <p>For example:</p> HTTPPython <pre><code>curl -d '{\n    \"text\": \"John Smith drivers license is AC432223\", \n    \"language\": \"en\", \n    \"return_decision_process\": true}' -H \"Content-Type: application/json\" -X POST http://localhost:3000/analyze\n</code></pre> <pre><code>from presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default)\n# and other PII recognizers\nanalyzer = AnalyzerEngine()\n\n# Call analyzer to get results\nresults = analyzer.analyze(text='My phone number is 212-555-5555', \n                        entities=['PHONE_NUMBER'], \n                        language='en', \n                        return_decision_process=True)\n\n# Get the decision process results for the first result\nprint(results[0].analysis_explanation)\n</code></pre>"},{"location":"analyzer/decision_process/#logging-the-decision-process","title":"Logging the decision process","text":"<p>Logging of the decision process is turned off by default. To turn it on, create the <code>AnalyzerEngine</code> object with <code>log_decision_process=True</code>.</p> <p>For example:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\n# Set up the engine, loads the NLP module (spaCy model by default)\n# and other PII recognizers\nanalyzer = AnalyzerEngine(log_decision_process=True)\n\n# Call analyzer to get results\nresults = analyzer.analyze(text='My phone number is 212-555-5555', \n                           entities=['PHONE_NUMBER'], \n                           language='en', \n                           correlation_id=\"xyz\")\n</code></pre> <p>The decision process logs will be written to standard output. Note that it is possible to define a <code>correlation-id</code> which is the trace identification. It will help you to query the stdout logs. The id can be retrieved from each API response header: <code>x-correlation-id</code>.</p> <p>By having the traces written into the <code>stdout</code> it's very easy to configure a monitoring solution to ease the process of reading processing the tracing logs in a distributed system.</p>"},{"location":"analyzer/decision_process/#examples","title":"Examples","text":"<p>For the a request with the following text:</p> <pre><code>My name is Bart Simpson, my Credit card is: 4095-2609-9393-4932,  my phone is 425 8829090 \n</code></pre> <p>The following traces will be written to log, with this format:</p> <p><code>[Date Time][decision_process][Log Level][Unique Correlation ID][Trace Message]</code></p> <pre><code>[2019-07-14 14:22:32,409][decision_process][INFO][00000000-0000-0000-0000-000000000000][nlp artifacts:{'entities': (Bart Simpson, 4095, 425), 'tokens': ['My', 'name', 'is', 'Bart', 'Simpson', ',', 'my', 'Credit', 'card', 'is', ':', '4095', '-', '2609', '-', '9393', '-', '4932', ',', ' ', 'my', 'phone', 'is', '425', '8829090'], 'lemmas': ['My', 'name', 'be', 'Bart', 'Simpson', ',', 'my', 'Credit', 'card', 'be', ':', '4095', '-', '2609', '-', '9393', '-', '4932', ',', ' ', 'my', 'phone', 'be', '425', '8829090'], 'tokens_indices': [0, 3, 8, 11, 16, 23, 25, 28, 35, 40, 42, 44, 48, 49, 53, 54, 58, 59, 63, 65, 66, 69, 75, 78, 82], 'keywords': ['bart', 'simpson', 'credit', 'card', '4095', '2609', '9393', '4932', ' ', 'phone', '425', '8829090']}]\n\n[2019-07-14 14:22:32,417][decision_process][INFO][00000000-0000-0000-0000-000000000000][[\"{'entity_type': 'CREDIT_CARD', 'start': 44, 'end': 63, 'score': 1.0, 'analysis_explanation': {'recognizer': 'CreditCardRecognizer', 'pattern_name': 'All Credit Cards (weak)', 'pattern': '\\\\\\\\b((4\\\\\\\\d{3})|(5[0-5]\\\\\\\\d{2})|(6\\\\\\\\d{3})|(1\\\\\\\\d{3})|(3\\\\\\\\d{3}))[- ]?(\\\\\\\\d{3,4})[- ]?(\\\\\\\\d{3,4})[- ]?(\\\\\\\\d{3,5})\\\\\\\\b', 'original_score': 0.3, 'score': 1.0, 'textual_explanation': None, 'score_context_improvement': 0.7, 'supportive_context_word': 'credit', 'validation_result': True}}\", \"{'entity_type': 'PERSON', 'start': 11, 'end': 23, 'score': 0.85, 'analysis_explanation': {'recognizer': 'SpacyRecognizer', 'pattern_name': None, 'pattern': None, 'original_score': 0.85, 'score': 0.85, 'textual_explanation': \\\"Identified as PERSON by Spacy's Named Entity Recognition\\\", 'score_context_improvement': 0, 'supportive_context_word': '', 'validation_result': None}}\", \"{'entity_type': 'PHONE_NUMBER', 'start': 78, 'end': 89, 'score': 0.85, 'analysis_explanation': {'recognizer': 'UsPhoneRecognizer', 'pattern_name': 'Phone (medium)', 'pattern': '\\\\\\\\b(\\\\\\\\d{3}[-\\\\\\\\.\\\\\\\\s]\\\\\\\\d{3}[-\\\\\\\\.\\\\\\\\s]??\\\\\\\\d{4})\\\\\\\\b', 'original_score': 0.5, 'score': 0.85, 'textual_explanation': None, 'score_context_improvement': 0.35, 'supportive_context_word': 'phone', 'validation_result': None}}\"]]\n</code></pre>"},{"location":"analyzer/decision_process/#writing-custom-decision-process-for-a-recognizer","title":"Writing custom decision process for a recognizer","text":"<p>When creating new PII recognizers, it is possible to add information about the recognizer's decision process. This information will be traced or returned to the user, depending on the configuration.</p> <p>For example, the spacy_recognizer.py implements a custom trace as follows:</p> <pre><code>SPACY_DEFAULT_EXPLANATION = \"Identified as {} by Spacy's Named Entity Recognition\"\n\ndef build_spacy_explanation(recognizer_name, original_score, entity):\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        textual_explanation=SPACY_DEFAULT_EXPLANATION.format(entity))\n    return explanation\n</code></pre> <p>The <code>textual_explanation</code> field in <code>AnalysisExplanation</code> class allows you to add your own custom text into the final trace which will be written.</p> <p>Note</p> <p>These traces leverage the Python <code>logging</code> mechanisms. In the default configuration, A <code>StreamHandler</code> is used to write these logs to <code>sys.stdout</code>.</p> <p>Warning</p> <p>Decision-process traces explain why PIIs were detected, but not why they were not detected!</p>"},{"location":"analyzer/developing_recognizers/","title":"Recognizers Development - Best Practices and Considerations","text":"<p>Recognizers are the main building blocks in Presidio. Each recognizer is in charge of detecting one or more entities in one or more languages. Recognizers define the logic for detection, as well as the confidence a prediction receives and a list of words to be used when context is leveraged.</p>"},{"location":"analyzer/developing_recognizers/#implementation-considerations","title":"Implementation Considerations","text":""},{"location":"analyzer/developing_recognizers/#accuracy","title":"Accuracy","text":"<p>Each recognizer, regardless of its complexity, could have false positives and false negatives. When adding new recognizers, we try to balance the effect of each recognizer on the entire system.  A recognizer with many false positives would affect the system's usability, while a recognizer with many false negatives might require more work before it can be integrated. For reproducibility purposes, it is be best to note how the recognizer's accuracy was tested, and on which datasets. For tools and documentation on evaluating and analyzing recognizers, refer to the presidio-research Github repository.</p> <p>Note</p> <p>When contributing recognizers to the Presidio OSS, new predefined recognizers should be added to the supported entities list, and follow the contribution guidelines.</p>"},{"location":"analyzer/developing_recognizers/#performance","title":"Performance","text":"<p>Make sure your recognizer doesn't take too long to process text. Anything above 100ms per request with 100 tokens is probably not good enough.</p>"},{"location":"analyzer/developing_recognizers/#environment","title":"Environment","text":"<p>When adding new recognizers that have 3rd party dependencies, make sure that the new dependencies don't interfere with Presidio's dependencies.  In the case of a conflict, one can create an isolated model environment (outside the main presidio-analyzer process) and implement a <code>RemoteRecognizer</code> on the presidio-analyzer side to interact with the model's endpoint.</p>"},{"location":"analyzer/developing_recognizers/#recognizer-types","title":"Recognizer Types","text":"<p>Generally speaking, there are three types of recognizers:</p>"},{"location":"analyzer/developing_recognizers/#deny-lists","title":"Deny Lists","text":"<p>A deny list is a list of words that should be removed during text analysis. For example, it can include a list of titles (<code>[\"Mr.\", \"Mrs.\", \"Ms.\", \"Dr.\"]</code> to detect a \"Title\" entity.)</p> <p>See this documentation on adding a new recognizer. The <code>PatternRecognizer</code> class has built-in support for a deny-list input.</p>"},{"location":"analyzer/developing_recognizers/#pattern-based","title":"Pattern Based","text":"<p>Pattern based recognizers use regular expressions to identify entities in text. See this documentation on adding a new recognizer via code. The <code>PatternRecognizer</code> class should be extended. See some examples here:</p> <p>Examples</p> <p>Examples of pattern based recognizers are the <code>CreditCardRecognizer</code> and <code>EmailRecognizer</code>.</p>"},{"location":"analyzer/developing_recognizers/#machine-learning-ml-based-or-rule-based","title":"Machine Learning (ML) Based or Rule-Based","text":"<p>Many PII entities are undetectable using naive approaches like deny-lists or regular expressions. In these cases, we would wish to utilize a Machine Learning model capable of identifying entities in free text, or a rule-based recognizer.</p>"},{"location":"analyzer/developing_recognizers/#ml-utilize-spacy-stanza-or-transformers","title":"ML: Utilize SpaCy, Stanza or Transformers","text":"<p>Presidio currently uses spaCy as a framework for text analysis and Named Entity Recognition (NER), and stanza and huggingface transformers as an alternative. To avoid introducing new tools, it is recommended to first try to use <code>spaCy</code>, <code>stanza</code> or <code>transformers</code> over other tools if possible. <code>spaCy</code> provides descent results compared to state-of-the-art NER models, but with much better computational performance. <code>spaCy</code>, <code>stanza</code> and <code>transformers</code> models could be trained from scratch, used in combination with pre-trained embeddings, or be fine-tuned.</p> <p>In addition to those, it is also possible to use other ML models. In that case, a new <code>EntityRecognizer</code> should be created.  See an example using Flair here.</p>"},{"location":"analyzer/developing_recognizers/#apply-custom-logic","title":"Apply Custom Logic","text":"<p>In some cases, rule-based logic provides reasonable ways for detecting entities. The Presidio <code>EntityRecognizer</code> API allows you to use <code>spaCy</code> extracted features like lemmas, part of speech, dependencies and more to create your logic.  When integrating such logic into Presidio, a class inheriting from the <code>EntityRecognizer</code> should be created.</p> <p>Considerations for selecting one option over another</p> <ul> <li>Accuracy.</li> <li>Ease of integration.</li> <li>Runtime considerations (For example if the new model requires a GPU).</li> <li>3rd party dependencies of the new model vs. the existing <code>presidio-analyzer</code> package.</li> </ul>"},{"location":"analyzer/languages/","title":"PII detection in different languages","text":"<p>Presidio supports PII detection in multiple languages. In its default configuration, it contains recognizers and models for English.</p> <p>To extend Presidio to detect PII in an additional language, these modules require modification:</p> <ol> <li>The <code>NlpEngine</code> containing the NLP model which performs tokenization, lemmatization, Named Entity Recognition and other NLP tasks.</li> <li>PII recognizers (different <code>EntityRecognizer</code> objects) should be adapted or created.</li> </ol> <p>Note</p> <p>While different detection mechanisms such as regular expressions are language agnostic, the context words used to increase the PII detection confidence aren't. Consider updating the list of context words for each recognizer to leverage context words in additional languages.</p>"},{"location":"analyzer/languages/#table-of-contents","title":"Table of contents","text":"<ul> <li>Configuring the NLP Engine</li> <li>Set up language specific recognizers</li> <li>Automatically install NLP models into the Docker container</li> </ul>"},{"location":"analyzer/languages/#configuring-the-nlp-engine","title":"Configuring the NLP Engine","text":"<p>Presidio's NLP engine can be adapted to support multiple languages and frameworks (such as spaCy, Stanza and transformers). Configuring the NLP engine for a new language or NLP framework is done by downloading or using a model trained on a different language, and providing a configuration. See the NLP model customization documentation for details on how to configure models for new languages.</p>"},{"location":"analyzer/languages/#set-up-language-specific-recognizers","title":"Set up language specific recognizers","text":"<p>Recognizers are language dependent either by their logic or by the context words used while scanning the surrounding of a detected entity. As these context words are used to increase score, they should be in the expected input language.</p> <p>Consider updating the context words of existing recognizers or add new recognizers to support new languages. Each recognizer can support one language. For example:</p> <p><pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.predefined_recognizers import EmailRecognizer\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\nLANGUAGES_CONFIG_FILE = \"./docs/analyzer/languages-config.yml\"\n\n# Create NLP engine based on configuration file\nprovider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Setting up an English Email recognizer:\nemail_recognizer_en = EmailRecognizer(supported_language=\"en\", context=[\"email\", \"mail\"])\n\n# Setting up a Spanish Email recognizer\nemail_recognizer_es = EmailRecognizer(supported_language=\"es\", context=[\"correo\", \"electr\u00f3nico\"])\n\nregistry = RecognizerRegistry()\n\n# Add recognizers to registry\nregistry.add_recognizer(email_recognizer_en)\nregistry.add_recognizer(email_recognizer_es)\n\n# Set up analyzer with our updated recognizer registry\nanalyzer = AnalyzerEngine(\n    registry=registry,\n    supported_languages=[\"en\",\"es\"],\n    nlp_engine=nlp_engine_with_spanish)\n\nanalyzer.analyze(text=\"My name is David\", language=\"en\")\n</code></pre> Link to LANGUAGES_CONFIG_FILE=languages-config.yml</p>"},{"location":"analyzer/languages/#automatically-install-nlp-models-into-the-docker-container","title":"Automatically install NLP models into the Docker container","text":"<p>When packaging the code into a Docker container, NLP models are automatically installed. To define which models should be installed, update the conf/default.yaml file. This file is read during the <code>docker build</code> phase and the models defined in it are installed automatically.</p> <p>For <code>transformers</code> based models, the configuration can be found here.  A docker file supporting transformers models can be found here.</p>"},{"location":"analyzer/nlp_engines/spacy_stanza/","title":"spaCy/Stanza NLP engine","text":"<p>Presidio can be loaded with pre-trained or custom models coming from spaCy or Stanza.</p>"},{"location":"analyzer/nlp_engines/spacy_stanza/#using-a-public-pre-trained-spacystanza-model","title":"Using a public pre-trained spaCy/Stanza model","text":""},{"location":"analyzer/nlp_engines/spacy_stanza/#download-the-pre-trained-model","title":"Download the pre-trained model","text":"<p>To replace the default model with a different public model, first download the desired spaCy/Stanza NER models.</p> <ul> <li> <p>To download a new model with spaCy:</p> <pre><code>python -m spacy download es_core_news_md\n</code></pre> <p>In this example we download the medium size model for Spanish.</p> </li> <li> <p>To download a new model with Stanza:</p> <p> <pre><code>import stanza\nstanza.download(\"en\") # where en is the language code of the model.\n</code></pre></p> </li> </ul> <p>For the available models, follow these links: spaCy, stanza.</p> <p>Tip</p> <p>For Person, Location and Organization detection, it could be useful to try out the transformers based models (e.g. <code>en_core_web_trf</code>) which uses a more modern deep-learning architecture, but is generally slower than the default <code>en_core_web_lg</code> model.</p>"},{"location":"analyzer/nlp_engines/spacy_stanza/#configure-presidio-to-use-the-pre-trained-model","title":"Configure Presidio to use the pre-trained model","text":"<p>Once created, see the NLP configuration documentation for more information.</p>"},{"location":"analyzer/nlp_engines/spacy_stanza/#how-ner-results-flow-within-presidio","title":"How NER results flow within Presidio","text":"<p>This diagram describes the flow of NER results within Presidio, and the relationship between the <code>SpacyNlpEngine</code> component and the <code>SpacyRecognizer</code> component: <pre><code>sequenceDiagram\n    AnalyzerEngine-&gt;&gt;SpacyNlpEngine: Call engine.process_text(text) &lt;br&gt;to get model results\n    SpacyNlpEngine-&gt;&gt;spaCy: Call spaCy pipeline\n    spaCy-&gt;&gt;SpacyNlpEngine: return entities and other attributes\n    Note over SpacyNlpEngine: Map entity names to Presidio's, &lt;BR&gt;update scores, &lt;BR&gt;remove unwanted entities &lt;BR&gt; based on NerModelConfiguration\n    SpacyNlpEngine-&gt;&gt;AnalyzerEngine: Pass NlpArtifacts&lt;BR&gt;(Entities, lemmas, tokens, scores etc.)\n    Note over AnalyzerEngine: Call all recognizers\n    AnalyzerEngine-&gt;&gt;SpacyRecognizer: Pass NlpArtifacts\n    Note over SpacyRecognizer: Extract PII entities out of NlpArtifacts\n    SpacyRecognizer-&gt;&gt;AnalyzerEngine: Return List[RecognizerResult]\n</code></pre></p>"},{"location":"analyzer/nlp_engines/spacy_stanza/#training-your-own-model","title":"Training your own model","text":"<p>Note</p> <p>A labeled dataset containing text and labeled PII entities is required for training a new model.</p> <p>For more information on model training and evaluation for Presidio, see the Presidio-Research Github repository.</p> <p>To train your own model, see these links on spaCy and Stanza:</p> <ul> <li>Train your own spaCy model.</li> <li>Train your own Stanza model.</li> </ul> <p>Once models are trained, they should be installed locally in the same environment as Presidio Analyzer.</p>"},{"location":"analyzer/nlp_engines/spacy_stanza/#using-a-previously-loaded-spacy-pipeline","title":"Using a previously loaded spaCy pipeline","text":"<p>If the app is already loading an existing spaCy NLP pipeline, it can be re-used to prevent presidio from loading it again by extending the relevant engine.</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import SpacyNlpEngine\nimport spacy\n\n# Create a class inheriting from SpacyNlpEngine\nclass LoadedSpacyNlpEngine(SpacyNlpEngine):\n    def __init__(self, loaded_spacy_model):\n        self.nlp = {\"en\": loaded_spacy_model}\n\n# Load a model a-priori\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Pass the loaded model to the new LoadedSpacyNlpEngine\nloaded_nlp_engine = LoadedSpacyNlpEngine(loaded_spacy_model = nlp)\n\n# Pass the engine to the analyzer\nanalyzer = AnalyzerEngine(nlp_engine = loaded_nlp_engine)\n\n# Analyze text\nanalyzer.analyze(text=\"My name is Bob\", language=\"en\")\n</code></pre>"},{"location":"analyzer/nlp_engines/transformers/","title":"Transformers based Named Entity Recognition models","text":"<p>Presidio's <code>TransformersNlpEngine</code> consists of a spaCy pipeline which encapsulates a Huggingface Transformers model instead of the spaCy NER component:</p> <p></p> <p>Presidio leverages other types of information from spaCy such as tokens, lemmas and part-of-speech. Therefore the pipeline returns both the NER model results as well as results from other pipeline components.</p>"},{"location":"analyzer/nlp_engines/transformers/#how-ner-results-flow-within-presidio","title":"How NER results flow within Presidio","text":"<p>This diagram describes the flow of NER results within Presidio, and the relationship between the <code>TransformersNlpEngine</code> component and the <code>TransformersRecognizer</code> component: <pre><code>sequenceDiagram\n    AnalyzerEngine-&gt;&gt;TransformersNlpEngine: Call engine.process_text(text) &lt;br&gt;to get model results\n    TransformersNlpEngine-&gt;&gt;spaCy: Call spaCy pipeline\n    spaCy-&gt;&gt;transformers: call NER model\n    transformers-&gt;&gt;spaCy: get entities\n    spaCy-&gt;&gt;TransformersNlpEngine: return transformers entities &lt;BR&gt;+ spaCy attributes\n    Note over TransformersNlpEngine: Map entity names to Presidio's, &lt;BR&gt;update scores, &lt;BR&gt;remove unwanted entities &lt;BR&gt; based on NerModelConfiguration\n    TransformersNlpEngine-&gt;&gt;AnalyzerEngine: Pass NlpArtifacts&lt;BR&gt;(Entities, lemmas, tokens, scores etc.)\n    Note over AnalyzerEngine: Call all recognizers\n    AnalyzerEngine-&gt;&gt;TransformersRecognizer: Pass NlpArtifacts\n    Note over TransformersRecognizer: Extract PII entities out of NlpArtifacts\n    TransformersRecognizer-&gt;&gt;AnalyzerEngine: Return List[RecognizerResult]\n</code></pre></p>"},{"location":"analyzer/nlp_engines/transformers/#adding-a-new-model","title":"Adding a new model","text":"<p>As the underlying transformers model, you can choose from either a public pretrained model or a custom model.</p>"},{"location":"analyzer/nlp_engines/transformers/#using-a-public-pre-trained-transformers-model","title":"Using a public pre-trained transformers model","text":""},{"location":"analyzer/nlp_engines/transformers/#downloading-a-pre-trained-model","title":"Downloading a pre-trained model","text":"<p>To download the desired NER model from HuggingFace:</p> <pre><code>import transformers\nfrom huggingface_hub import snapshot_download\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntransformers_model = &lt;PATH_TO_MODEL&gt; # e.g. \"obi/deid_roberta_i2b2\"\n\nsnapshot_download(repo_id=transformers_model)\n\n# Instantiate to make sure it's downloaded during installation and not runtime\nAutoTokenizer.from_pretrained(transformers_model)\nAutoModelForTokenClassification.from_pretrained(transformers_model)\n</code></pre> <p>Then, also download a spaCy pipeline/model:</p> <pre><code>python -m spacy download en_core_web_sm\n</code></pre>"},{"location":"analyzer/nlp_engines/transformers/#creating-a-configuration-file","title":"Creating a configuration file","text":"<p>Once the models are downloaded, one option to configure them is to create a YAML configuration file. Note that the configuration needs to contain both a <code>spaCy</code> pipeline name and a transformers model name. In addition, different configurations for parsing the results of the transformers model can be added.</p> <p>Example configuration (in YAML):</p> <pre><code>nlp_engine_name: transformers\nmodels:\n  -\n    lang_code: en\n    model_name:\n      spacy: en_core_web_sm\n      transformers: StanfordAIMI/stanford-deidentifier-base\n\nner_model_configuration:\n  labels_to_ignore:\n  - O\n  aggregation_strategy: simple # \"simple\", \"first\", \"average\", \"max\"\n  stride: 16\n  alignment_mode: strict # \"strict\", \"contract\", \"expand\"\n  model_to_presidio_entity_mapping:\n    PER: PERSON\n    LOC: LOCATION\n    ORG: ORGANIZATION\n    AGE: AGE\n    ID: ID\n    EMAIL: EMAIL\n    PATIENT: PERSON\n    STAFF: PERSON\n    HOSP: ORGANIZATION\n    PATORG: ORGANIZATION\n    DATE: DATE_TIME\n    PHONE: PHONE_NUMBER\n    HCW: PERSON\n    HOSPITAL: ORGANIZATION\n\n  low_confidence_score_multiplier: 0.4\n  low_score_entity_names:\n  - ID\n</code></pre> <p>Where:</p> <ul> <li><code>model_name.spacy</code> is a name of a spaCy model/pipeline, which would wrap the transformers NER model. For example, <code>en_core_web_sm</code>.</li> <li>The <code>model_name.transformers</code> is the full path for a huggingface model. Models can be found on HuggingFace Models Hub. For example, <code>obi/deid_roberta_i2b2</code></li> </ul> <p>The <code>ner_model_configuration</code> section contains the following parameters:</p> <ul> <li><code>labels_to_ignore</code>: A list of labels to ignore. For example, <code>O</code> (no entity) or entities you are not interested in returning.</li> <li><code>aggregation_strategy</code>: The strategy to use when aggregating the results of the transformers model.</li> <li><code>stride</code>: The value is the length of the window overlap in transformer tokenizer tokens.</li> <li><code>alignment_mode</code>: The strategy to use when aligning the results of the transformers model to the original text.</li> <li><code>model_to_presidio_entity_mapping</code>: A mapping between the transformers model labels and the Presidio entity types.</li> <li><code>low_confidence_score_multiplier</code>: A multiplier to apply to the score of entities with low confidence.</li> <li><code>low_score_entity_names</code>: A list of entity types to apply the low confidence score multiplier to.</li> </ul> <p>See more information on parameters on the spacy-huggingface-pipelines Github repo.</p> <p>Once created, see the NLP configuration documentation for more information.</p>"},{"location":"analyzer/nlp_engines/transformers/#calling-the-new-model","title":"Calling the new model","text":"<p>Once the configuration file is created, it can be used to create a new <code>TransformersNlpEngine</code>:</p> <pre><code>    from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n    from presidio_analyzer.nlp_engine import NlpEngineProvider\n\n    # Create configuration containing engine name and models\n    conf_file = PATH_TO_CONF_FILE\n\n    # Create NLP engine based on configuration\n    provider = NlpEngineProvider(conf_file=conf_file)\n    nlp_engine = provider.create_engine()\n\n    # Pass the created NLP engine and supported_languages to the AnalyzerEngine\n    analyzer = AnalyzerEngine(\n        nlp_engine=nlp_engine, \n        supported_languages=[\"en\"]\n    )\n\n    results_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\n    print(results_english)\n</code></pre>"},{"location":"analyzer/nlp_engines/transformers/#training-your-own-model","title":"Training your own model","text":"<p>Note</p> <p>A labeled dataset containing text and labeled PII entities is required for training a new model.</p> <p>For more information on model training and evaluation for Presidio, see the Presidio-Research Github repository.</p> <p>To train your own model, see this tutorial: Train your own transformers model.</p>"},{"location":"analyzer/nlp_engines/transformers/#using-a-transformers-model-as-an-entityrecognizer","title":"Using a transformers model as an <code>EntityRecognizer</code>","text":"<p>In addition to the approach described in this document, one can decide to integrate a transformers model as a recognizer. We allow these two options, as a user might want to have multiple NER models running in parallel. In this case, one can create multiple <code>EntityRecognizer</code> instances, each serving a different model, instead of one model used in an <code>NlpEngine</code>. See this sample for more info on integrating a transformers model as a Presidio recognizer and not as a Presidio <code>NLPEngine</code>.</p>"},{"location":"anonymizer/","title":"Presidio Anonymizer","text":"<p>The Presidio anonymizer is a Python based module for anonymizing detected PII text entities with desired values. Presidio anonymizer supports both anonymization and deanonymization by applying different operators. Operators are built-in text manipulation classes which can be easily extended.</p> <p></p> <p>The Presidio-Anonymizer package contains both <code>Anonymizers</code> and <code>Deanonymizers</code>.</p> <ul> <li>Anonymizers are used to replace a PII entity text with some other value by applying a certain operator (e.g. replace, mask, redact, encrypt)</li> <li>Deanonymizers are used to revert the anonymization operation.   (e.g. to decrypt an encrypted text).</li> </ul>"},{"location":"anonymizer/#installation","title":"Installation","text":"<p>see Installing Presidio.</p>"},{"location":"anonymizer/#getting-started","title":"Getting started","text":"PythonAs an HTTP server <p>Simple example:</p> <pre><code>from presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n\n# Initialize the engine:\nengine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text, \n# analyzer results (potentially coming from presidio-analyzer) and\n# Operators to get the anonymization output:\nresult = engine.anonymize(\n    text=\"My name is Bond, James Bond\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=15, score=0.8),\n        RecognizerResult(entity_type=\"PERSON\", start=17, end=27, score=0.8),\n    ],\n    operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"BIP\"})},\n)\n\nprint(result)\n</code></pre> <p>This example takes the output of the <code>AnonymizerEngine</code>  containing an encrypted PII entity, and decrypts it back to the original text:</p> <pre><code>from presidio_anonymizer import DeanonymizeEngine\nfrom presidio_anonymizer.entities import OperatorResult, OperatorConfig\n\n# Initialize the engine:\nengine = DeanonymizeEngine()\n\n# Invoke the deanonymize function with the text, anonymizer results and\n# Operators to define the deanonymization type.\nresult = engine.deanonymize(\n    text=\"My name is S184CMt9Drj7QaKQ21JTrpYzghnboTF9pn/neN8JME0=\",\n    entities=[\n        OperatorResult(start=11, end=55, entity_type=\"PERSON\"),\n    ],\n    operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": \"WmZq4t7w!z%C&amp;F)J\"})},\n)\n\nprint(result)\n</code></pre> <p>You can run presidio anonymizer as an http server using either python runtime or using a docker container.</p>"},{"location":"anonymizer/#using-docker-container","title":"Using docker container","text":"<pre><code>cd presidio-anonymizer\ndocker run -p 5001:3000 presidio-anonymizer \n</code></pre>"},{"location":"anonymizer/#using-python-runtime","title":"Using python runtime","text":"<p>Note</p> <p>This requires the Presidio Github repository to be cloned.</p> <pre><code>cd presidio-anonymizer\npython app.py\n\nAnonymize:\n\ncurl -XPOST http://localhost:3000/anonymize -H \"Content-Type: application/json\" -d @payload\n\npayload example:\n{\n\"text\": \"hello world, my name is Jane Doe. My number is: 034453334\",\n\"anonymizers\": {\n    \"PHONE_NUMBER\": {\n        \"type\": \"mask\",\n        \"masking_char\": \"*\",\n        \"chars_to_mask\": 4,\n        \"from_end\": true\n    }\n},\n\"analyzer_results\": [\n    {\n        \"start\": 24,\n        \"end\": 32,\n        \"score\": 0.8,\n        \"entity_type\": \"NAME\"\n    },\n    {\n        \"start\": 24,\n        \"end\": 28,\n        \"score\": 0.8,\n        \"entity_type\": \"FIRST_NAME\"\n    },\n    {\n        \"start\": 29,\n        \"end\": 32,\n        \"score\": 0.6,\n        \"entity_type\": \"LAST_NAME\"\n    },\n    {\n        \"start\": 48,\n        \"end\": 57,\n        \"score\": 0.95,\n        \"entity_type\": \"PHONE_NUMBER\"\n    }\n]}\n\nDeanonymize:\n\ncurl -XPOST http://localhost:3000/deanonymize -H \"Content-Type: application/json\" -d @payload\n\npayload example:\n{\n\"text\": \"My name is S184CMt9Drj7QaKQ21JTrpYzghnboTF9pn/neN8JME0=\",\n\"deanonymizers\": {\n    \"PERSON\": {\n        \"type\": \"decrypt\",\n        \"key\": \"WmZq4t7w!z%C&amp;F)J\"\n    }\n},\n\"anonymizer_results\": [\n    {\n        \"start\": 11,\n        \"end\": 55,\n        \"entity_type\": \"PERSON\"\n    }\n]}\n</code></pre>"},{"location":"anonymizer/#built-in-operators","title":"Built-in operators","text":"Operator type Operator name Description Parameters Anonymize replace Replace the PII with desired value <code>new_value</code>: replaces existing text with the given value. If <code>new_value</code> is not supplied or empty, default behavior will be: &lt;entity_type&gt; e.g: &lt;PHONE_NUMBER&gt; Anonymize redact Remove the PII completely from text None Anonymize hash Hashes the PII text <code>hash_type</code>: sets the type of hashing. Can be either <code>sha256</code>, <code>sha512</code> or <code>md5</code>.  The default hash type is <code>sha256</code>. Anonymize mask Replace the PII with a given character <code>chars_to_mask</code>: the amount of characters out of the PII that should be replaced.  <code>masking_char</code>: the character to be replaced with.  <code>from_end</code>: Whether to mask the PII from it's end. Anonymize encrypt Encrypt the PII using a given key <code>key</code>: a cryptographic key used for the encryption. Anonymize custom Replace the PII with the result of the function executed on the PII <code>lambda</code>: lambda to execute on the PII data. The lambda return type must be a string. Anonymize keep Preserver the PII unmodified None Deanonymize decrypt Decrypt the encrypted PII in the text using the encryption key <code>key</code>: a cryptographic key used for the encryption is also used for the decryption. <p>Note</p> <p>When performing anonymization, if anonymizers map is empty or \"DEFAULT\" key is not stated, the default anonymization operator is \"replace\" for all entities. The replacing value will be the entity type e.g.: &lt;PHONE_NUMBER&gt;</p>"},{"location":"anonymizer/#handling-overlaps-between-entities","title":"Handling overlaps between entities","text":"<p>As the input text could potentially have overlapping PII entities, there are different anonymization scenarios:</p> <ul> <li>No overlap (single PII): When there is no overlap in spans of entities,     Presidio Anonymizer uses a given or default anonymization operator to anonymize     and replace the PII text entity.</li> <li>Full overlap of PII entitie spans: When entities have overlapping substrings,     the PII with the higher score will be taken.     Between PIIs with identical scores, the selection is arbitrary.</li> <li>One PII is contained in another: Presidio Anonymizer will use the PII with the larger text even if it's score is lower.</li> <li> <p>Partial intersection: Presidio Anonymizer will anonymize each individually and will return a concatenation of the anonymized text.     For example:     For the text</p> <pre><code>I'm George Washington Square Park.\n</code></pre> <p>Assuming one entity is <code>George Washington</code> and the other is <code>Washington State Park</code> and assuming we're using the default anonymizer, the result would be:</p> <pre><code>I'm &lt;PERSON&gt;&lt;LOCATION&gt;.\n</code></pre> </li> </ul>"},{"location":"anonymizer/#additional-examples-for-overlapping-pii-scenarios","title":"Additional examples for overlapping PII scenarios","text":"<p>Text:</p> <pre><code>My name is Inigo Montoya. You Killed my Father. Prepare to die. BTW my number is:\n03-232323.\n</code></pre> <p>Results:</p> <ul> <li> <p>No overlaps: Assuming only <code>Inigo</code> is recognized as NAME:</p> <pre><code>My name is &lt;NAME&gt; Montoya. You Killed my Father. Prepare to die. BTW my number is:\n03-232323.\n</code></pre> </li> <li> <p>Full overlap: Assuming the number is recognized as PHONE_NUMBER with score of 0.7 and as SSN     with score of 0.6, the higher score would count:</p> <pre><code>My name is Inigo Montoya. You Killed my Father. Prepare to die. BTW my number is: &lt;\nPHONE_NUMBER&gt;.\n</code></pre> </li> <li> <p>One PII is contained is another: Assuming Inigo is recognized as FIRST_NAME and Inigo Montoya     was recognized as NAME, the larger one will be used:</p> <pre><code>My name is &lt;NAME&gt;. You Killed my Father. Prepare to die. BTW my number is: 03-232323.\n</code></pre> </li> <li> <p>Partial intersection: Assuming the number 03-2323 is recognized as a PHONE_NUMBER but 232323     is recognized as SSN:</p> <pre><code>My name is Inigo Montoya. You Killed my Father. Prepare to die. BTW my number is: &lt;\nPHONE_NUMBER&gt;&lt;SSN&gt;.\n</code></pre> </li> </ul>"},{"location":"anonymizer/#creating-a-new-operator","title":"Creating a new <code>operator</code>","text":"<p>Presidio anonymizer can be easily extended to support additional operators. See this tutorial on adding new operators for more information.</p>"},{"location":"anonymizer/#api-reference","title":"API reference","text":"<p>Follow the API Spec for the Anonymizer REST API reference details and Anonymizer Python API for Python API reference.</p>"},{"location":"anonymizer/adding_operators/","title":"Supporting new types of PII operators","text":"<p>Operators are the presidio-anonymizer actions over the text. </p> <p>There are two types of operators:  - Anonymize (hash, replace, redact, encrypt, mask) - Deanonymize (decrypt)</p> <p>Presidio anonymizer can be easily extended to support additional anonnymization and deanonymization methods.</p>"},{"location":"anonymizer/adding_operators/#extending-presidio-anonymizer-for-additional-pii-operators","title":"Extending presidio-anonymizer for additional PII operators:","text":"<ol> <li>Under the path presidio_anonymizer/operators create new python class implementing the abstract Operator class </li> <li>Implement the methods: <ul> <li><code>operate</code> - gets the data and returns a new text expected to replace the old one.</li> <li><code>validate</code> - validate the parameters entered for the anonymizer exists and valid.</li> <li><code>operator_name</code> - this method helps to automatically load the existing anonymizers.</li> <li><code>operator_type</code> - either Anonymize or Deanonymize. Will be mapped to the proper engine.</li> </ul> </li> <li>Add the class to presidio_anonymizer/operators/init.py.    </li> <li>Restart the anonymizer.</li> </ol> <p>Note</p> <p>The list of operators is being loaded dynamically each time Presidio Anonymizer is started.</p>"},{"location":"api/analyzer_python/","title":"Presidio Analyzer API Reference","text":"<p>Presidio analyzer package.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalysisExplanation","title":"<code>AnalysisExplanation</code>","text":"<p>Hold tracing information to explain why PII entities were identified as such.</p> <p>Parameters:</p> Name Type Description Default <code>recognizer</code> <code>str</code> <p>name of recognizer that made the decision</p> required <code>original_score</code> <code>float</code> <p>recognizer's confidence in result</p> required <code>pattern_name</code> <code>str</code> <p>name of pattern (if decision was made by a PatternRecognizer)</p> <code>None</code> <code>pattern</code> <code>str</code> <p>regex pattern that was applied (if PatternRecognizer)</p> <code>None</code> <code>validation_result</code> <code>float</code> <p>result of a validation (e.g. checksum)</p> <code>None</code> <code>textual_explanation</code> <code>str</code> <p>Free text for describing a decision of a logic or model</p> <code>None</code> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>class AnalysisExplanation:\n    \"\"\"\n    Hold tracing information to explain why PII entities were identified as such.\n\n    :param recognizer: name of recognizer that made the decision\n    :param original_score: recognizer's confidence in result\n    :param pattern_name: name of pattern\n            (if decision was made by a PatternRecognizer)\n    :param pattern: regex pattern that was applied (if PatternRecognizer)\n    :param validation_result: result of a validation (e.g. checksum)\n    :param textual_explanation: Free text for describing\n            a decision of a logic or model\n    \"\"\"\n\n    def __init__(\n        self,\n        recognizer: str,\n        original_score: float,\n        pattern_name: str = None,\n        pattern: str = None,\n        validation_result: float = None,\n        textual_explanation: str = None,\n        regex_flags: int = None,\n    ):\n\n        self.recognizer = recognizer\n        self.pattern_name = pattern_name\n        self.pattern = pattern\n        self.original_score = original_score\n        self.score = original_score\n        self.textual_explanation = textual_explanation\n        self.score_context_improvement = 0\n        self.supportive_context_word = \"\"\n        self.validation_result = validation_result\n        self.regex_flags = regex_flags\n\n    def __repr__(self):\n        \"\"\"Create string representation of the object.\"\"\"\n        return str(self.__dict__)\n\n    def set_improved_score(self, score: float) -&gt; None:\n        \"\"\"Update the score and calculate the difference from the original score.\"\"\"\n        self.score = score\n        self.score_context_improvement = self.score - self.original_score\n\n    def set_supportive_context_word(self, word: str) -&gt; None:\n        \"\"\"Set the context word which helped increase the score.\"\"\"\n        self.supportive_context_word = word\n\n    def append_textual_explanation_line(self, text: str) -&gt; None:\n        \"\"\"Append a new line to textual_explanation field.\"\"\"\n        if self.textual_explanation is None:\n            self.textual_explanation = text\n        else:\n            self.textual_explanation = \"{}\\n{}\".format(self.textual_explanation, text)\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"\n        Serialize self to dictionary.\n\n        :return: a dictionary\n        \"\"\"\n        return self.__dict__\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalysisExplanation.__repr__","title":"<code>__repr__()</code>","text":"<p>Create string representation of the object.</p> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>def __repr__(self):\n    \"\"\"Create string representation of the object.\"\"\"\n    return str(self.__dict__)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalysisExplanation.append_textual_explanation_line","title":"<code>append_textual_explanation_line(text)</code>","text":"<p>Append a new line to textual_explanation field.</p> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>def append_textual_explanation_line(self, text: str) -&gt; None:\n    \"\"\"Append a new line to textual_explanation field.\"\"\"\n    if self.textual_explanation is None:\n        self.textual_explanation = text\n    else:\n        self.textual_explanation = \"{}\\n{}\".format(self.textual_explanation, text)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalysisExplanation.set_improved_score","title":"<code>set_improved_score(score)</code>","text":"<p>Update the score and calculate the difference from the original score.</p> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>def set_improved_score(self, score: float) -&gt; None:\n    \"\"\"Update the score and calculate the difference from the original score.\"\"\"\n    self.score = score\n    self.score_context_improvement = self.score - self.original_score\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalysisExplanation.set_supportive_context_word","title":"<code>set_supportive_context_word(word)</code>","text":"<p>Set the context word which helped increase the score.</p> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>def set_supportive_context_word(self, word: str) -&gt; None:\n    \"\"\"Set the context word which helped increase the score.\"\"\"\n    self.supportive_context_word = word\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalysisExplanation.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize self to dictionary.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/analysis_explanation.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return self.__dict__\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine","title":"<code>AnalyzerEngine</code>","text":"<p>Entry point for Presidio Analyzer.</p> <p>Orchestrating the detection of PII entities and all related logic.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>RecognizerRegistry</code> <p>instance of type RecognizerRegistry</p> <code>None</code> <code>nlp_engine</code> <code>NlpEngine</code> <p>instance of type NlpEngine (for example SpacyNlpEngine)</p> <code>None</code> <code>app_tracer</code> <code>AppTracer</code> <p>instance of type AppTracer, used to trace the logic used during each request for interpretability reasons.</p> <code>None</code> <code>log_decision_process</code> <code>bool</code> <p>bool, defines whether the decision process within the analyzer should be logged or not.</p> <code>False</code> <code>default_score_threshold</code> <code>float</code> <p>Minimum confidence value for detected entities to be returned</p> <code>0</code> <code>supported_languages</code> <code>List[str]</code> <p>List of possible languages this engine could be run on. Used for loading the right NLP models and recognizers for these languages.</p> <code>None</code> <code>context_aware_enhancer</code> <code>Optional[ContextAwareEnhancer]</code> <p>instance of type ContextAwareEnhancer for enhancing confidence score based on context words, (LemmaContextAwareEnhancer will be created by default if None passed)</p> <code>None</code> Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>class AnalyzerEngine:\n    \"\"\"\n    Entry point for Presidio Analyzer.\n\n    Orchestrating the detection of PII entities and all related logic.\n\n    :param registry: instance of type RecognizerRegistry\n    :param nlp_engine: instance of type NlpEngine\n    (for example SpacyNlpEngine)\n    :param app_tracer: instance of type AppTracer, used to trace the logic\n    used during each request for interpretability reasons.\n    :param log_decision_process: bool,\n    defines whether the decision process within the analyzer should be logged or not.\n    :param default_score_threshold: Minimum confidence value\n    for detected entities to be returned\n    :param supported_languages: List of possible languages this engine could be run on.\n    Used for loading the right NLP models and recognizers for these languages.\n    :param context_aware_enhancer: instance of type ContextAwareEnhancer for enhancing\n    confidence score based on context words, (LemmaContextAwareEnhancer will be created\n    by default if None passed)\n    \"\"\"\n\n    def __init__(\n        self,\n        registry: RecognizerRegistry = None,\n        nlp_engine: NlpEngine = None,\n        app_tracer: AppTracer = None,\n        log_decision_process: bool = False,\n        default_score_threshold: float = 0,\n        supported_languages: List[str] = None,\n        context_aware_enhancer: Optional[ContextAwareEnhancer] = None,\n    ):\n        if not supported_languages:\n            supported_languages = [\"en\"]\n\n        if not nlp_engine:\n            logger.info(\"nlp_engine not provided, creating default.\")\n            provider = NlpEngineProvider()\n            nlp_engine = provider.create_engine()\n\n        if not registry:\n            logger.info(\"registry not provided, creating default.\")\n            registry = RecognizerRegistry()\n        if not app_tracer:\n            app_tracer = AppTracer()\n        self.app_tracer = app_tracer\n\n        self.supported_languages = supported_languages\n\n        self.nlp_engine = nlp_engine\n        if not self.nlp_engine.is_loaded():\n            self.nlp_engine.load()\n\n        self.registry = registry\n\n        # load all recognizers\n        if not registry.recognizers:\n            registry.load_predefined_recognizers(\n                nlp_engine=self.nlp_engine, languages=self.supported_languages\n            )\n\n        self.log_decision_process = log_decision_process\n        self.default_score_threshold = default_score_threshold\n\n        if not context_aware_enhancer:\n            logger.debug(\n                \"context aware enhancer not provided, creating default\"\n                + \" lemma based enhancer.\"\n            )\n            context_aware_enhancer = LemmaContextAwareEnhancer()\n\n        self.context_aware_enhancer = context_aware_enhancer\n\n    def get_recognizers(self, language: Optional[str] = None) -&gt; List[EntityRecognizer]:\n        \"\"\"\n        Return a list of PII recognizers currently loaded.\n\n        :param language: Return the recognizers supporting a given language.\n        :return: List of [Recognizer] as a RecognizersAllResponse\n        \"\"\"\n        if not language:\n            languages = self.supported_languages\n        else:\n            languages = [language]\n\n        recognizers = []\n        for language in languages:\n            logger.info(f\"Fetching all recognizers for language {language}\")\n            recognizers.extend(\n                self.registry.get_recognizers(language=language, all_fields=True)\n            )\n\n        return list(set(recognizers))\n\n    def get_supported_entities(self, language: Optional[str] = None) -&gt; List[str]:\n        \"\"\"\n        Return a list of the entities that can be detected.\n\n        :param language: Return only entities supported in a specific language.\n        :return: List of entity names\n        \"\"\"\n        recognizers = self.get_recognizers(language=language)\n        supported_entities = []\n        for recognizer in recognizers:\n            supported_entities.extend(recognizer.get_supported_entities())\n\n        return list(set(supported_entities))\n\n    def analyze(\n        self,\n        text: str,\n        language: str,\n        entities: Optional[List[str]] = None,\n        correlation_id: Optional[str] = None,\n        score_threshold: Optional[float] = None,\n        return_decision_process: Optional[bool] = False,\n        ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n        context: Optional[List[str]] = None,\n        allow_list: Optional[List[str]] = None,\n        nlp_artifacts: Optional[NlpArtifacts] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Find PII entities in text using different PII recognizers for a given language.\n\n        :param text: the text to analyze\n        :param language: the language of the text\n        :param entities: List of PII entities that should be looked for in the text.\n        If entities=None then all entities are looked for.\n        :param correlation_id: cross call ID for this request\n        :param score_threshold: A minimum value for which\n        to return an identified entity\n        :param return_decision_process: Whether the analysis decision process steps\n        returned in the response.\n        :param ad_hoc_recognizers: List of recognizers which will be used only\n        for this specific request.\n        :param context: List of context words to enhance confidence score if matched\n        with the recognized entity's recognizer context\n        :param allow_list: List of words that the user defines as being allowed to keep\n        in the text\n        :param nlp_artifacts: precomputed NlpArtifacts\n        :return: an array of the found entities in the text\n\n        :example:\n\n        &gt;&gt;&gt; from presidio_analyzer import AnalyzerEngine\n\n        &gt;&gt;&gt; # Set up the engine, loads the NLP module (spaCy model by default)\n        &gt;&gt;&gt; # and other PII recognizers\n        &gt;&gt;&gt; analyzer = AnalyzerEngine()\n\n        &gt;&gt;&gt; # Call analyzer to get results\n        &gt;&gt;&gt; results = analyzer.analyze(text='My phone number is 212-555-5555', entities=['PHONE_NUMBER'], language='en') # noqa D501\n        &gt;&gt;&gt; print(results)\n        [type: PHONE_NUMBER, start: 19, end: 31, score: 0.85]\n        \"\"\"\n        all_fields = not entities\n\n        recognizers = self.registry.get_recognizers(\n            language=language,\n            entities=entities,\n            all_fields=all_fields,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n        )\n\n        if all_fields:\n            # Since all_fields=True, list all entities by iterating\n            # over all recognizers\n            entities = self.get_supported_entities(language=language)\n\n        # run the nlp pipeline over the given text, store the results in\n        # a NlpArtifacts instance\n        if not nlp_artifacts:\n            nlp_artifacts = self.nlp_engine.process_text(text, language)\n\n        if self.log_decision_process:\n            self.app_tracer.trace(\n                correlation_id, \"nlp artifacts:\" + nlp_artifacts.to_json()\n            )\n\n        results = []\n        for recognizer in recognizers:\n            # Lazy loading of the relevant recognizers\n            if not recognizer.is_loaded:\n                recognizer.load()\n                recognizer.is_loaded = True\n\n            # analyze using the current recognizer and append the results\n            current_results = recognizer.analyze(\n                text=text, entities=entities, nlp_artifacts=nlp_artifacts\n            )\n            if current_results:\n                # add recognizer name to recognition metadata inside results\n                # if not exists\n                self.__add_recognizer_id_if_not_exists(current_results, recognizer)\n                results.extend(current_results)\n\n        results = self._enhance_using_context(\n            text, results, nlp_artifacts, recognizers, context\n        )\n\n        if self.log_decision_process:\n            self.app_tracer.trace(\n                correlation_id,\n                json.dumps([str(result.to_dict()) for result in results]),\n            )\n\n        # Remove duplicates or low score results\n        results = EntityRecognizer.remove_duplicates(results)\n        results = self.__remove_low_scores(results, score_threshold)\n\n        if allow_list:\n            results = self._remove_allow_list(results, allow_list, text)\n\n        if not return_decision_process:\n            results = self.__remove_decision_process(results)\n\n        return results\n\n    def _enhance_using_context(\n        self,\n        text: str,\n        raw_results: List[RecognizerResult],\n        nlp_artifacts: NlpArtifacts,\n        recognizers: List[EntityRecognizer],\n        context: Optional[List[str]] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Enhance confidence score using context words.\n\n        :param text: The actual text that was analyzed\n        :param raw_results: Recognizer results which didn't take\n                            context into consideration\n        :param nlp_artifacts: The nlp artifacts contains elements\n                              such as lemmatized tokens for better\n                              accuracy of the context enhancement process\n        :param recognizers: the list of recognizers\n        :param context: list of context words\n        \"\"\"\n        results = []\n\n        for recognizer in recognizers:\n            recognizer_results = [\n                r\n                for r in raw_results\n                if r.recognition_metadata[RecognizerResult.RECOGNIZER_IDENTIFIER_KEY]\n                == recognizer.id\n            ]\n            other_recognizer_results = [\n                r\n                for r in raw_results\n                if r.recognition_metadata[RecognizerResult.RECOGNIZER_IDENTIFIER_KEY]\n                != recognizer.id\n            ]\n\n            # enhance score using context in recognizer level if implemented\n            recognizer_results = recognizer.enhance_using_context(\n                text=text,\n                # each recognizer will get access to all recognizer results\n                # to allow related entities contex enhancement\n                raw_recognizer_results=recognizer_results,\n                other_raw_recognizer_results=other_recognizer_results,\n                nlp_artifacts=nlp_artifacts,\n                context=context,\n            )\n\n            results.extend(recognizer_results)\n\n        # Update results in case surrounding words or external context are relevant to\n        # the context words.\n        results = self.context_aware_enhancer.enhance_using_context(\n            text=text,\n            raw_results=results,\n            nlp_artifacts=nlp_artifacts,\n            recognizers=recognizers,\n            context=context,\n        )\n\n        return results\n\n    def __remove_low_scores(\n        self, results: List[RecognizerResult], score_threshold: float = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Remove results for which the confidence is lower than the threshold.\n\n        :param results: List of RecognizerResult\n        :param score_threshold: float value for minimum possible confidence\n        :return: List[RecognizerResult]\n        \"\"\"\n        if score_threshold is None:\n            score_threshold = self.default_score_threshold\n\n        new_results = [result for result in results if result.score &gt;= score_threshold]\n        return new_results\n\n    @staticmethod\n    def _remove_allow_list(\n        results: List[RecognizerResult], allow_list: List[str], text: str\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Remove results which are part of the allow list.\n\n        :param results: List of RecognizerResult\n        :param allow_list: list of allowed terms\n        :param text: the text to analyze\n        :return: List[RecognizerResult]\n        \"\"\"\n        new_results = []\n        for result in results:\n            word = text[result.start : result.end]\n            # if the word is not specified to be allowed, keep in the PII entities\n            if word not in allow_list:\n                new_results.append(result)\n\n        return new_results\n\n    @staticmethod\n    def __add_recognizer_id_if_not_exists(\n        results: List[RecognizerResult], recognizer: EntityRecognizer\n    ) -&gt; None:\n        \"\"\"Ensure recognition metadata with recognizer id existence.\n\n        Ensure recognizer result list contains recognizer id inside recognition\n        metadata dictionary, and if not create it. recognizer_id is needed\n        for context aware enhancement.\n\n        :param results: List of RecognizerResult\n        :param recognizer: Entity recognizer\n        \"\"\"\n        for result in results:\n            if not result.recognition_metadata:\n                result.recognition_metadata = dict()\n            if (\n                RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n                not in result.recognition_metadata\n            ):\n                result.recognition_metadata[\n                    RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n                ] = recognizer.id\n            if RecognizerResult.RECOGNIZER_NAME_KEY not in result.recognition_metadata:\n                result.recognition_metadata[\n                    RecognizerResult.RECOGNIZER_NAME_KEY\n                ] = recognizer.name\n\n    @staticmethod\n    def __remove_decision_process(\n        results: List[RecognizerResult],\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Remove decision process / analysis explanation from response.\"\"\"\n\n        for result in results:\n            result.analysis_explanation = None\n\n        return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.__add_recognizer_id_if_not_exists","title":"<code>__add_recognizer_id_if_not_exists(results, recognizer)</code>  <code>staticmethod</code>","text":"<p>Ensure recognition metadata with recognizer id existence.</p> <p>Ensure recognizer result list contains recognizer id inside recognition metadata dictionary, and if not create it. recognizer_id is needed for context aware enhancement.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[RecognizerResult]</code> <p>List of RecognizerResult</p> required <code>recognizer</code> <code>EntityRecognizer</code> <p>Entity recognizer</p> required Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>@staticmethod\ndef __add_recognizer_id_if_not_exists(\n    results: List[RecognizerResult], recognizer: EntityRecognizer\n) -&gt; None:\n    \"\"\"Ensure recognition metadata with recognizer id existence.\n\n    Ensure recognizer result list contains recognizer id inside recognition\n    metadata dictionary, and if not create it. recognizer_id is needed\n    for context aware enhancement.\n\n    :param results: List of RecognizerResult\n    :param recognizer: Entity recognizer\n    \"\"\"\n    for result in results:\n        if not result.recognition_metadata:\n            result.recognition_metadata = dict()\n        if (\n            RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n            not in result.recognition_metadata\n        ):\n            result.recognition_metadata[\n                RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n            ] = recognizer.id\n        if RecognizerResult.RECOGNIZER_NAME_KEY not in result.recognition_metadata:\n            result.recognition_metadata[\n                RecognizerResult.RECOGNIZER_NAME_KEY\n            ] = recognizer.name\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.__remove_decision_process","title":"<code>__remove_decision_process(results)</code>  <code>staticmethod</code>","text":"<p>Remove decision process / analysis explanation from response.</p> Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>@staticmethod\ndef __remove_decision_process(\n    results: List[RecognizerResult],\n) -&gt; List[RecognizerResult]:\n    \"\"\"Remove decision process / analysis explanation from response.\"\"\"\n\n    for result in results:\n        result.analysis_explanation = None\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.__remove_low_scores","title":"<code>__remove_low_scores(results, score_threshold=None)</code>","text":"<p>Remove results for which the confidence is lower than the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[RecognizerResult]</code> <p>List of RecognizerResult</p> required <code>score_threshold</code> <code>float</code> <p>float value for minimum possible confidence</p> <code>None</code> <p>Returns:</p> Type Description <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>def __remove_low_scores(\n    self, results: List[RecognizerResult], score_threshold: float = None\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove results for which the confidence is lower than the threshold.\n\n    :param results: List of RecognizerResult\n    :param score_threshold: float value for minimum possible confidence\n    :return: List[RecognizerResult]\n    \"\"\"\n    if score_threshold is None:\n        score_threshold = self.default_score_threshold\n\n    new_results = [result for result in results if result.score &gt;= score_threshold]\n    return new_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.analyze","title":"<code>analyze(text, language, entities=None, correlation_id=None, score_threshold=None, return_decision_process=False, ad_hoc_recognizers=None, context=None, allow_list=None, nlp_artifacts=None)</code>","text":"<p>Find PII entities in text using different PII recognizers for a given language.</p> <p>:example:</p> <p>from presidio_analyzer import AnalyzerEngine</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>the text to analyze</p> required <code>language</code> <code>str</code> <p>the language of the text</p> required <code>entities</code> <code>Optional[List[str]]</code> <p>List of PII entities that should be looked for in the text. If entities=None then all entities are looked for.</p> <code>None</code> <code>correlation_id</code> <code>Optional[str]</code> <p>cross call ID for this request</p> <code>None</code> <code>score_threshold</code> <code>Optional[float]</code> <p>A minimum value for which to return an identified entity</p> <code>None</code> <code>return_decision_process</code> <code>Optional[bool]</code> <p>Whether the analysis decision process steps returned in the response.</p> <code>False</code> <code>ad_hoc_recognizers</code> <code>Optional[List[EntityRecognizer]]</code> <p>List of recognizers which will be used only for this specific request.</p> <code>None</code> <code>context</code> <code>Optional[List[str]]</code> <p>List of context words to enhance confidence score if matched with the recognized entity's recognizer context</p> <code>None</code> <code>allow_list</code> <code>Optional[List[str]]</code> <p>List of words that the user defines as being allowed to keep in the text</p> <code>None</code> <code>nlp_artifacts</code> <code>Optional[NlpArtifacts]</code> <p>precomputed NlpArtifacts</p> <code>None</code> <p>Returns:</p> Type Description <code>List[RecognizerResult]</code> <p>an array of the found entities in the text</p> Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    language: str,\n    entities: Optional[List[str]] = None,\n    correlation_id: Optional[str] = None,\n    score_threshold: Optional[float] = None,\n    return_decision_process: Optional[bool] = False,\n    ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n    context: Optional[List[str]] = None,\n    allow_list: Optional[List[str]] = None,\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Find PII entities in text using different PII recognizers for a given language.\n\n    :param text: the text to analyze\n    :param language: the language of the text\n    :param entities: List of PII entities that should be looked for in the text.\n    If entities=None then all entities are looked for.\n    :param correlation_id: cross call ID for this request\n    :param score_threshold: A minimum value for which\n    to return an identified entity\n    :param return_decision_process: Whether the analysis decision process steps\n    returned in the response.\n    :param ad_hoc_recognizers: List of recognizers which will be used only\n    for this specific request.\n    :param context: List of context words to enhance confidence score if matched\n    with the recognized entity's recognizer context\n    :param allow_list: List of words that the user defines as being allowed to keep\n    in the text\n    :param nlp_artifacts: precomputed NlpArtifacts\n    :return: an array of the found entities in the text\n\n    :example:\n\n    &gt;&gt;&gt; from presidio_analyzer import AnalyzerEngine\n\n    &gt;&gt;&gt; # Set up the engine, loads the NLP module (spaCy model by default)\n    &gt;&gt;&gt; # and other PII recognizers\n    &gt;&gt;&gt; analyzer = AnalyzerEngine()\n\n    &gt;&gt;&gt; # Call analyzer to get results\n    &gt;&gt;&gt; results = analyzer.analyze(text='My phone number is 212-555-5555', entities=['PHONE_NUMBER'], language='en') # noqa D501\n    &gt;&gt;&gt; print(results)\n    [type: PHONE_NUMBER, start: 19, end: 31, score: 0.85]\n    \"\"\"\n    all_fields = not entities\n\n    recognizers = self.registry.get_recognizers(\n        language=language,\n        entities=entities,\n        all_fields=all_fields,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n    )\n\n    if all_fields:\n        # Since all_fields=True, list all entities by iterating\n        # over all recognizers\n        entities = self.get_supported_entities(language=language)\n\n    # run the nlp pipeline over the given text, store the results in\n    # a NlpArtifacts instance\n    if not nlp_artifacts:\n        nlp_artifacts = self.nlp_engine.process_text(text, language)\n\n    if self.log_decision_process:\n        self.app_tracer.trace(\n            correlation_id, \"nlp artifacts:\" + nlp_artifacts.to_json()\n        )\n\n    results = []\n    for recognizer in recognizers:\n        # Lazy loading of the relevant recognizers\n        if not recognizer.is_loaded:\n            recognizer.load()\n            recognizer.is_loaded = True\n\n        # analyze using the current recognizer and append the results\n        current_results = recognizer.analyze(\n            text=text, entities=entities, nlp_artifacts=nlp_artifacts\n        )\n        if current_results:\n            # add recognizer name to recognition metadata inside results\n            # if not exists\n            self.__add_recognizer_id_if_not_exists(current_results, recognizer)\n            results.extend(current_results)\n\n    results = self._enhance_using_context(\n        text, results, nlp_artifacts, recognizers, context\n    )\n\n    if self.log_decision_process:\n        self.app_tracer.trace(\n            correlation_id,\n            json.dumps([str(result.to_dict()) for result in results]),\n        )\n\n    # Remove duplicates or low score results\n    results = EntityRecognizer.remove_duplicates(results)\n    results = self.__remove_low_scores(results, score_threshold)\n\n    if allow_list:\n        results = self._remove_allow_list(results, allow_list, text)\n\n    if not return_decision_process:\n        results = self.__remove_decision_process(results)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.analyze--set-up-the-engine-loads-the-nlp-module-spacy-model-by-default","title":"Set up the engine, loads the NLP module (spaCy model by default)","text":""},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.analyze--and-other-pii-recognizers","title":"and other PII recognizers","text":"<p>analyzer = AnalyzerEngine()</p>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.analyze--call-analyzer-to-get-results","title":"Call analyzer to get results","text":"<p>results = analyzer.analyze(text='My phone number is 212-555-5555', entities=['PHONE_NUMBER'], language='en') # noqa D501 print(results) [type: PHONE_NUMBER, start: 19, end: 31, score: 0.85]</p>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.get_recognizers","title":"<code>get_recognizers(language=None)</code>","text":"<p>Return a list of PII recognizers currently loaded.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>Optional[str]</code> <p>Return the recognizers supporting a given language.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[EntityRecognizer]</code> <p>List of [Recognizer] as a RecognizersAllResponse</p> Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>def get_recognizers(self, language: Optional[str] = None) -&gt; List[EntityRecognizer]:\n    \"\"\"\n    Return a list of PII recognizers currently loaded.\n\n    :param language: Return the recognizers supporting a given language.\n    :return: List of [Recognizer] as a RecognizersAllResponse\n    \"\"\"\n    if not language:\n        languages = self.supported_languages\n    else:\n        languages = [language]\n\n    recognizers = []\n    for language in languages:\n        logger.info(f\"Fetching all recognizers for language {language}\")\n        recognizers.extend(\n            self.registry.get_recognizers(language=language, all_fields=True)\n        )\n\n    return list(set(recognizers))\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerEngine.get_supported_entities","title":"<code>get_supported_entities(language=None)</code>","text":"<p>Return a list of the entities that can be detected.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>Optional[str]</code> <p>Return only entities supported in a specific language.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of entity names</p> Source code in <code>presidio_analyzer/analyzer_engine.py</code> <pre><code>def get_supported_entities(self, language: Optional[str] = None) -&gt; List[str]:\n    \"\"\"\n    Return a list of the entities that can be detected.\n\n    :param language: Return only entities supported in a specific language.\n    :return: List of entity names\n    \"\"\"\n    recognizers = self.get_recognizers(language=language)\n    supported_entities = []\n    for recognizer in recognizers:\n        supported_entities.extend(recognizer.get_supported_entities())\n\n    return list(set(supported_entities))\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.AnalyzerRequest","title":"<code>AnalyzerRequest</code>","text":"<p>Analyzer request data.</p> <p>Parameters:</p> Name Type Description Default <code>req_data</code> <code>Dict</code> <p>A request dictionary with the following fields: text: the text to analyze language: the language of the text entities: List of PII entities that should be looked for in the text. If entities=None then all entities are looked for. correlation_id: cross call ID for this request score_threshold: A minimum value for which to return an identified entity log_decision_process: Should the decision points within the analysis be logged return_decision_process: Should the decision points within the analysis returned as part of the response</p> required Source code in <code>presidio_analyzer/analyzer_request.py</code> <pre><code>class AnalyzerRequest:\n    \"\"\"\n    Analyzer request data.\n\n    :param req_data: A request dictionary with the following fields:\n        text: the text to analyze\n        language: the language of the text\n        entities: List of PII entities that should be looked for in the text.\n        If entities=None then all entities are looked for.\n        correlation_id: cross call ID for this request\n        score_threshold: A minimum value for which to return an identified entity\n        log_decision_process: Should the decision points within the analysis\n        be logged\n        return_decision_process: Should the decision points within the analysis\n        returned as part of the response\n    \"\"\"\n\n    def __init__(self, req_data: Dict):\n        self.text = req_data.get(\"text\")\n        self.language = req_data.get(\"language\")\n        self.entities = req_data.get(\"entities\")\n        self.correlation_id = req_data.get(\"correlation_id\")\n        self.score_threshold = req_data.get(\"score_threshold\")\n        self.return_decision_process = req_data.get(\"return_decision_process\")\n        ad_hoc_recognizers = req_data.get(\"ad_hoc_recognizers\")\n        self.ad_hoc_recognizers = []\n        if ad_hoc_recognizers:\n            self.ad_hoc_recognizers = [\n                PatternRecognizer.from_dict(rec) for rec in ad_hoc_recognizers\n            ]\n        self.context = req_data.get(\"context\")\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.BatchAnalyzerEngine","title":"<code>BatchAnalyzerEngine</code>","text":"<p>Batch analysis of documents (tables, lists, dicts).</p> <p>Wrapper class to run Presidio Analyzer Engine on multiple values, either lists/iterators of strings, or dictionaries.</p> Source code in <code>presidio_analyzer/batch_analyzer_engine.py</code> <pre><code>class BatchAnalyzerEngine:\n    \"\"\"\n    Batch analysis of documents (tables, lists, dicts).\n\n    Wrapper class to run Presidio Analyzer Engine on multiple values,\n    either lists/iterators of strings, or dictionaries.\n\n    :param: analyzer_engine: AnalyzerEngine instance to use\n    for handling the values in those collections.\n    \"\"\"\n\n    def __init__(self, analyzer_engine: Optional[AnalyzerEngine] = None):\n\n        self.analyzer_engine = analyzer_engine\n        if not analyzer_engine:\n            self.analyzer_engine = AnalyzerEngine()\n\n    def analyze_iterator(\n        self,\n        texts: Iterable[Union[str, bool, float, int]],\n        language: str,\n        **kwargs,\n    ) -&gt; List[List[RecognizerResult]]:\n        \"\"\"\n        Analyze an iterable of strings.\n\n        :param texts: An list containing strings to be analyzed.\n        :param language: Input language\n        :param kwargs: Additional parameters for the `AnalyzerEngine.analyze` method.\n        \"\"\"\n\n        # validate types\n        texts = self._validate_types(texts)\n\n        # Process the texts as batch for improved performance\n        nlp_artifacts_batch: Iterator[\n            Tuple[str, NlpArtifacts]\n        ] = self.analyzer_engine.nlp_engine.process_batch(\n            texts=texts, language=language\n        )\n\n        list_results = []\n        for text, nlp_artifacts in nlp_artifacts_batch:\n            results = self.analyzer_engine.analyze(\n                text=str(text), nlp_artifacts=nlp_artifacts, language=language, **kwargs\n            )\n\n            list_results.append(results)\n\n        return list_results\n\n    def analyze_dict(\n        self,\n        input_dict: Dict[str, Union[Any, Iterable[Any]]],\n        language: str,\n        keys_to_skip: Optional[List[str]] = None,\n        **kwargs,\n    ) -&gt; Iterator[DictAnalyzerResult]:\n        \"\"\"\n        Analyze a dictionary of keys (strings) and values/iterable of values.\n\n        Non-string values are returned as is.\n\n        :param input_dict: The input dictionary for analysis\n        :param language: Input language\n        :param keys_to_skip: Keys to ignore during analysis\n        :param kwargs: Additional keyword arguments\n        for the `AnalyzerEngine.analyze` method.\n        Use this to pass arguments to the analyze method,\n        such as `ad_hoc_recognizers`, `context`, `return_decision_process`.\n        See `AnalyzerEngine.analyze` for the full list.\n        \"\"\"\n\n        context = []\n        if \"context\" in kwargs:\n            context = kwargs[\"context\"]\n            del kwargs[\"context\"]\n\n        if not keys_to_skip:\n            keys_to_skip = []\n\n        for key, value in input_dict.items():\n            if not value or key in keys_to_skip:\n                yield DictAnalyzerResult(key=key, value=value, recognizer_results=[])\n                continue  # skip this key as requested\n\n            # Add the key as an additional context\n            specific_context = context[:]\n            specific_context.append(key)\n\n            if type(value) in (str, int, bool, float):\n                results: List[RecognizerResult] = self.analyzer_engine.analyze(\n                    text=str(value), language=language, context=[key], **kwargs\n                )\n            elif isinstance(value, dict):\n                new_keys_to_skip = self._get_nested_keys_to_skip(key, keys_to_skip)\n                results = self.analyze_dict(\n                    input_dict=value,\n                    language=language,\n                    context=specific_context,\n                    keys_to_skip=new_keys_to_skip,\n                    **kwargs,\n                )\n            elif isinstance(value, Iterable):\n                # Recursively iterate nested dicts\n\n                results: List[List[RecognizerResult]] = self.analyze_iterator(\n                    texts=value,\n                    language=language,\n                    context=specific_context,\n                    **kwargs,\n                )\n            else:\n                raise ValueError(f\"type {type(value)} is unsupported.\")\n\n            yield DictAnalyzerResult(key=key, value=value, recognizer_results=results)\n\n    @staticmethod\n    def _validate_types(value_iterator: Iterable[Any]) -&gt; Iterator[Any]:\n        for val in value_iterator:\n            if val and not type(val) in (int, float, bool, str):\n                err_msg = (\n                    \"Analyzer.analyze_iterator only works \"\n                    \"on primitive types (int, float, bool, str). \"\n                    \"Lists of objects are not yet supported.\"\n                )\n                logger.error(err_msg)\n                raise ValueError(err_msg)\n            yield val\n\n    @staticmethod\n    def _get_nested_keys_to_skip(key, keys_to_skip):\n        new_keys_to_skip = [\n            k.replace(f\"{key}.\", \"\") for k in keys_to_skip if k.startswith(key)\n        ]\n        return new_keys_to_skip\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.BatchAnalyzerEngine.analyze_dict","title":"<code>analyze_dict(input_dict, language, keys_to_skip=None, **kwargs)</code>","text":"<p>Analyze a dictionary of keys (strings) and values/iterable of values.</p> <p>Non-string values are returned as is.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>Dict[str, Union[Any, Iterable[Any]]]</code> <p>The input dictionary for analysis</p> required <code>language</code> <code>str</code> <p>Input language</p> required <code>keys_to_skip</code> <code>Optional[List[str]]</code> <p>Keys to ignore during analysis</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments for the <code>AnalyzerEngine.analyze</code> method. Use this to pass arguments to the analyze method, such as <code>ad_hoc_recognizers</code>, <code>context</code>, <code>return_decision_process</code>. See <code>AnalyzerEngine.analyze</code> for the full list.</p> <code>{}</code> Source code in <code>presidio_analyzer/batch_analyzer_engine.py</code> <pre><code>def analyze_dict(\n    self,\n    input_dict: Dict[str, Union[Any, Iterable[Any]]],\n    language: str,\n    keys_to_skip: Optional[List[str]] = None,\n    **kwargs,\n) -&gt; Iterator[DictAnalyzerResult]:\n    \"\"\"\n    Analyze a dictionary of keys (strings) and values/iterable of values.\n\n    Non-string values are returned as is.\n\n    :param input_dict: The input dictionary for analysis\n    :param language: Input language\n    :param keys_to_skip: Keys to ignore during analysis\n    :param kwargs: Additional keyword arguments\n    for the `AnalyzerEngine.analyze` method.\n    Use this to pass arguments to the analyze method,\n    such as `ad_hoc_recognizers`, `context`, `return_decision_process`.\n    See `AnalyzerEngine.analyze` for the full list.\n    \"\"\"\n\n    context = []\n    if \"context\" in kwargs:\n        context = kwargs[\"context\"]\n        del kwargs[\"context\"]\n\n    if not keys_to_skip:\n        keys_to_skip = []\n\n    for key, value in input_dict.items():\n        if not value or key in keys_to_skip:\n            yield DictAnalyzerResult(key=key, value=value, recognizer_results=[])\n            continue  # skip this key as requested\n\n        # Add the key as an additional context\n        specific_context = context[:]\n        specific_context.append(key)\n\n        if type(value) in (str, int, bool, float):\n            results: List[RecognizerResult] = self.analyzer_engine.analyze(\n                text=str(value), language=language, context=[key], **kwargs\n            )\n        elif isinstance(value, dict):\n            new_keys_to_skip = self._get_nested_keys_to_skip(key, keys_to_skip)\n            results = self.analyze_dict(\n                input_dict=value,\n                language=language,\n                context=specific_context,\n                keys_to_skip=new_keys_to_skip,\n                **kwargs,\n            )\n        elif isinstance(value, Iterable):\n            # Recursively iterate nested dicts\n\n            results: List[List[RecognizerResult]] = self.analyze_iterator(\n                texts=value,\n                language=language,\n                context=specific_context,\n                **kwargs,\n            )\n        else:\n            raise ValueError(f\"type {type(value)} is unsupported.\")\n\n        yield DictAnalyzerResult(key=key, value=value, recognizer_results=results)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.BatchAnalyzerEngine.analyze_iterator","title":"<code>analyze_iterator(texts, language, **kwargs)</code>","text":"<p>Analyze an iterable of strings.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[Union[str, bool, float, int]]</code> <p>An list containing strings to be analyzed.</p> required <code>language</code> <code>str</code> <p>Input language</p> required <code>kwargs</code> <p>Additional parameters for the <code>AnalyzerEngine.analyze</code> method.</p> <code>{}</code> Source code in <code>presidio_analyzer/batch_analyzer_engine.py</code> <pre><code>def analyze_iterator(\n    self,\n    texts: Iterable[Union[str, bool, float, int]],\n    language: str,\n    **kwargs,\n) -&gt; List[List[RecognizerResult]]:\n    \"\"\"\n    Analyze an iterable of strings.\n\n    :param texts: An list containing strings to be analyzed.\n    :param language: Input language\n    :param kwargs: Additional parameters for the `AnalyzerEngine.analyze` method.\n    \"\"\"\n\n    # validate types\n    texts = self._validate_types(texts)\n\n    # Process the texts as batch for improved performance\n    nlp_artifacts_batch: Iterator[\n        Tuple[str, NlpArtifacts]\n    ] = self.analyzer_engine.nlp_engine.process_batch(\n        texts=texts, language=language\n    )\n\n    list_results = []\n    for text, nlp_artifacts in nlp_artifacts_batch:\n        results = self.analyzer_engine.analyze(\n            text=str(text), nlp_artifacts=nlp_artifacts, language=language, **kwargs\n        )\n\n        list_results.append(results)\n\n    return list_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.ContextAwareEnhancer","title":"<code>ContextAwareEnhancer</code>","text":"<p>A class representing an abstract context aware enhancer.</p> <p>Context words might enhance confidence score of a recognized entity, ContextAwareEnhancer is an abstract class to be inherited by a context aware enhancer logic.</p> <p>Parameters:</p> Name Type Description Default <code>context_similarity_factor</code> <code>float</code> <p>How much to enhance confidence of match entity</p> required <code>min_score_with_context_similarity</code> <code>float</code> <p>Minimum confidence score</p> required <code>context_prefix_count</code> <code>int</code> <p>how many words before the entity to match context</p> required <code>context_suffix_count</code> <code>int</code> <p>how many words after the entity to match context</p> required Source code in <code>presidio_analyzer/context_aware_enhancers/context_aware_enhancer.py</code> <pre><code>class ContextAwareEnhancer:\n    \"\"\"\n    A class representing an abstract context aware enhancer.\n\n    Context words might enhance confidence score of a recognized entity,\n    ContextAwareEnhancer is an abstract class to be inherited by a context aware\n    enhancer logic.\n\n    :param context_similarity_factor: How much to enhance confidence of match entity\n    :param min_score_with_context_similarity: Minimum confidence score\n    :param context_prefix_count: how many words before the entity to match context\n    :param context_suffix_count: how many words after the entity to match context\n    \"\"\"\n\n    MIN_SCORE = 0\n    MAX_SCORE = 1.0\n\n    def __init__(\n        self,\n        context_similarity_factor: float,\n        min_score_with_context_similarity: float,\n        context_prefix_count: int,\n        context_suffix_count: int,\n    ):\n\n        self.context_similarity_factor = context_similarity_factor\n        self.min_score_with_context_similarity = min_score_with_context_similarity\n        self.context_prefix_count = context_prefix_count\n        self.context_suffix_count = context_suffix_count\n\n    @abstractmethod\n    def enhance_using_context(\n        self,\n        text: str,\n        raw_results: List[RecognizerResult],\n        nlp_artifacts: NlpArtifacts,\n        recognizers: List[EntityRecognizer],\n        context: Optional[List[str]] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Update results in case surrounding words are relevant to the context words.\n\n        Using the surrounding words of the actual word matches, look\n        for specific strings that if found contribute to the score\n        of the result, improving the confidence that the match is\n        indeed of that PII entity type\n\n        :param text: The actual text that was analyzed\n        :param raw_results: Recognizer results which didn't take\n                            context into consideration\n        :param nlp_artifacts: The nlp artifacts contains elements\n                              such as lemmatized tokens for better\n                              accuracy of the context enhancement process\n        :param recognizers: the list of recognizers\n        :param context: list of context words\n        \"\"\"\n        return raw_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.ContextAwareEnhancer.enhance_using_context","title":"<code>enhance_using_context(text, raw_results, nlp_artifacts, recognizers, context=None)</code>  <code>abstractmethod</code>","text":"<p>Update results in case surrounding words are relevant to the context words.</p> <p>Using the surrounding words of the actual word matches, look for specific strings that if found contribute to the score of the result, improving the confidence that the match is indeed of that PII entity type</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The actual text that was analyzed</p> required <code>raw_results</code> <code>List[RecognizerResult]</code> <p>Recognizer results which didn't take context into consideration</p> required <code>nlp_artifacts</code> <code>NlpArtifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> required <code>recognizers</code> <code>List[EntityRecognizer]</code> <p>the list of recognizers</p> required <code>context</code> <code>Optional[List[str]]</code> <p>list of context words</p> <code>None</code> Source code in <code>presidio_analyzer/context_aware_enhancers/context_aware_enhancer.py</code> <pre><code>@abstractmethod\ndef enhance_using_context(\n    self,\n    text: str,\n    raw_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    recognizers: List[EntityRecognizer],\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Update results in case surrounding words are relevant to the context words.\n\n    Using the surrounding words of the actual word matches, look\n    for specific strings that if found contribute to the score\n    of the result, improving the confidence that the match is\n    indeed of that PII entity type\n\n    :param text: The actual text that was analyzed\n    :param raw_results: Recognizer results which didn't take\n                        context into consideration\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param recognizers: the list of recognizers\n    :param context: list of context words\n    \"\"\"\n    return raw_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.DictAnalyzerResult","title":"<code>DictAnalyzerResult</code>  <code>dataclass</code>","text":"<p>Data class for holding the output of the Presidio Analyzer on dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key in dictionary</p> required <code>value</code> <code>Union[str, List[str], dict]</code> <p>value to run analysis on (either string or list of strings)</p> required <code>recognizer_results</code> <code>Union[List[RecognizerResult], List[List[RecognizerResult]], Iterator[DictAnalyzerResult]]</code> <p>Analyzer output for one value. Could be either: - A list of recognizer results if the input is one string - A list of lists of recognizer results, if the input is a list of strings. - An iterator of a DictAnalyzerResult, if the input is a dictionary. In this case the recognizer_results would be the iterator of the DictAnalyzerResults next level in the dictionary.</p> required Source code in <code>presidio_analyzer/dict_analyzer_result.py</code> <pre><code>@dataclass\nclass DictAnalyzerResult:\n    \"\"\"\n    Data class for holding the output of the Presidio Analyzer on dictionaries.\n\n    :param key: key in dictionary\n    :param value: value to run analysis on (either string or list of strings)\n    :param recognizer_results: Analyzer output for one value.\n    Could be either:\n     - A list of recognizer results if the input is one string\n     - A list of lists of recognizer results, if the input is a list of strings.\n     - An iterator of a DictAnalyzerResult, if the input is a dictionary.\n     In this case the recognizer_results would be the iterator\n     of the DictAnalyzerResults next level in the dictionary.\n    \"\"\"\n\n    key: str\n    value: Union[str, List[str], dict]\n    recognizer_results: Union[\n        List[RecognizerResult],\n        List[List[RecognizerResult]],\n        Iterator[\"DictAnalyzerResult\"],\n    ]\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer","title":"<code>EntityRecognizer</code>","text":"<p>A class representing an abstract PII entity recognizer.</p> <p>EntityRecognizer is an abstract class to be inherited by Recognizers which hold the logic for recognizing specific PII entities.</p> <p>EntityRecognizer exposes a method called enhance_using_context which can be overridden in case a custom context aware enhancement is needed in derived class of a recognizer.</p> <p>Parameters:</p> Name Type Description Default <code>supported_entities</code> <code>List[str]</code> <p>the entities supported by this recognizer (for example, phone number, address, etc.)</p> required <code>supported_language</code> <code>str</code> <p>the language supported by this recognizer. The supported langauge code is iso6391Name</p> <code>'en'</code> <code>name</code> <code>str</code> <p>the name of this recognizer (optional)</p> <code>None</code> <code>version</code> <code>str</code> <p>the recognizer current version</p> <code>'0.0.1'</code> <code>context</code> <code>Optional[List[str]]</code> <p>a list of words which can help boost confidence score when they appear in context of the matched entity</p> <code>None</code> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>class EntityRecognizer:\n    \"\"\"\n    A class representing an abstract PII entity recognizer.\n\n    EntityRecognizer is an abstract class to be inherited by\n    Recognizers which hold the logic for recognizing specific PII entities.\n\n    EntityRecognizer exposes a method called enhance_using_context which\n    can be overridden in case a custom context aware enhancement is needed\n    in derived class of a recognizer.\n\n    :param supported_entities: the entities supported by this recognizer\n    (for example, phone number, address, etc.)\n    :param supported_language: the language supported by this recognizer.\n    The supported langauge code is iso6391Name\n    :param name: the name of this recognizer (optional)\n    :param version: the recognizer current version\n    :param context: a list of words which can help boost confidence score\n    when they appear in context of the matched entity\n    \"\"\"\n\n    MIN_SCORE = 0\n    MAX_SCORE = 1.0\n\n    def __init__(\n        self,\n        supported_entities: List[str],\n        name: str = None,\n        supported_language: str = \"en\",\n        version: str = \"0.0.1\",\n        context: Optional[List[str]] = None,\n    ):\n\n        self.supported_entities = supported_entities\n\n        if name is None:\n            self.name = self.__class__.__name__  # assign class name as name\n        else:\n            self.name = name\n\n        self._id = f\"{self.name}_{id(self)}\"\n\n        self.supported_language = supported_language\n        self.version = version\n        self.is_loaded = False\n        self.context = context if context else []\n\n        self.load()\n        logger.info(\"Loaded recognizer: %s\", self.name)\n        self.is_loaded = True\n\n    @property\n    def id(self):\n        \"\"\"Return a unique identifier of this recognizer.\"\"\"\n\n        return self._id\n\n    @abstractmethod\n    def load(self) -&gt; None:\n        \"\"\"\n        Initialize the recognizer assets if needed.\n\n        (e.g. machine learning models)\n        \"\"\"\n\n    @abstractmethod\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyze text to identify entities.\n\n        :param text: The text to be analyzed\n        :param entities: The list of entities this recognizer is able to detect\n        :param nlp_artifacts: A group of attributes which are the result of\n        an NLP process over the input text.\n        :return: List of results detected by this recognizer.\n        \"\"\"\n        return None\n\n    def enhance_using_context(\n        self,\n        text: str,\n        raw_recognizer_results: List[RecognizerResult],\n        other_raw_recognizer_results: List[RecognizerResult],\n        nlp_artifacts: NlpArtifacts,\n        context: Optional[List[str]] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Enhance confidence score using context of the entity.\n\n        Override this method in derived class in case a custom logic\n        is needed, otherwise return value will be equal to\n        raw_results.\n\n        in case a result score is boosted, derived class need to update\n        result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n        :param text: The actual text that was analyzed\n        :param raw_recognizer_results: This recognizer's results, to be updated\n        based on recognizer specific context.\n        :param other_raw_recognizer_results: Other recognizer results matched in\n        the given text to allow related entity context enhancement\n        :param nlp_artifacts: The nlp artifacts contains elements\n                              such as lemmatized tokens for better\n                              accuracy of the context enhancement process\n        :param context: list of context words\n        \"\"\"\n        return raw_recognizer_results\n\n    def get_supported_entities(self) -&gt; List[str]:\n        \"\"\"\n        Return the list of entities this recognizer can identify.\n\n        :return: A list of the supported entities by this recognizer\n        \"\"\"\n        return self.supported_entities\n\n    def get_supported_language(self) -&gt; str:\n        \"\"\"\n        Return the language this recognizer can support.\n\n        :return: A list of the supported language by this recognizer\n        \"\"\"\n        return self.supported_language\n\n    def get_version(self) -&gt; str:\n        \"\"\"\n        Return the version of this recognizer.\n\n        :return: The current version of this recognizer\n        \"\"\"\n        return self.version\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"\n        Serialize self to dictionary.\n\n        :return: a dictionary\n        \"\"\"\n        return_dict = {\n            \"supported_entities\": self.supported_entities,\n            \"supported_language\": self.supported_language,\n            \"name\": self.name,\n            \"version\": self.version,\n        }\n        return return_dict\n\n    @classmethod\n    def from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n        \"\"\"\n        Create EntityRecognizer from a dict input.\n\n        :param entity_recognizer_dict: Dict containing keys and values for instantiation\n        \"\"\"\n        return cls(**entity_recognizer_dict)\n\n    @staticmethod\n    def remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Remove duplicate results.\n\n        Remove duplicates in case the two results\n        have identical start and ends and types.\n        :param results: List[RecognizerResult]\n        :return: List[RecognizerResult]\n        \"\"\"\n        results = list(set(results))\n        results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n        filtered_results = []\n\n        for result in results:\n            if result.score == 0:\n                continue\n\n            to_keep = result not in filtered_results  # equals based comparison\n            if to_keep:\n                for filtered in filtered_results:\n                    # If result is contained in one of the other results\n                    if (\n                        result.contained_in(filtered)\n                        and result.entity_type == filtered.entity_type\n                    ):\n                        to_keep = False\n                        break\n\n            if to_keep:\n                filtered_results.append(result)\n\n        return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer.id","title":"<code>id</code>  <code>property</code>","text":"<p>Return a unique identifier of this recognizer.</p>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer.analyze","title":"<code>analyze(text, entities, nlp_artifacts)</code>  <code>abstractmethod</code>","text":"<p>Analyze text to identify entities.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be analyzed</p> required <code>entities</code> <code>List[str]</code> <p>The list of entities this recognizer is able to detect</p> required <code>nlp_artifacts</code> <code>NlpArtifacts</code> <p>A group of attributes which are the result of an NLP process over the input text.</p> required <p>Returns:</p> Type Description <code>List[RecognizerResult]</code> <p>List of results detected by this recognizer.</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@abstractmethod\ndef analyze(\n    self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyze text to identify entities.\n\n    :param text: The text to be analyzed\n    :param entities: The list of entities this recognizer is able to detect\n    :param nlp_artifacts: A group of attributes which are the result of\n    an NLP process over the input text.\n    :return: List of results detected by this recognizer.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer.enhance_using_context","title":"<code>enhance_using_context(text, raw_recognizer_results, other_raw_recognizer_results, nlp_artifacts, context=None)</code>","text":"<p>Enhance confidence score using context of the entity.</p> <p>Override this method in derived class in case a custom logic is needed, otherwise return value will be equal to raw_results.</p> <p>in case a result score is boosted, derived class need to update result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The actual text that was analyzed</p> required <code>raw_recognizer_results</code> <code>List[RecognizerResult]</code> <p>This recognizer's results, to be updated based on recognizer specific context.</p> required <code>other_raw_recognizer_results</code> <code>List[RecognizerResult]</code> <p>Other recognizer results matched in the given text to allow related entity context enhancement</p> required <code>nlp_artifacts</code> <code>NlpArtifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> required <code>context</code> <code>Optional[List[str]]</code> <p>list of context words</p> <code>None</code> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_recognizer_results: List[RecognizerResult],\n    other_raw_recognizer_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"Enhance confidence score using context of the entity.\n\n    Override this method in derived class in case a custom logic\n    is needed, otherwise return value will be equal to\n    raw_results.\n\n    in case a result score is boosted, derived class need to update\n    result.recognition_metadata[RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY]\n\n    :param text: The actual text that was analyzed\n    :param raw_recognizer_results: This recognizer's results, to be updated\n    based on recognizer specific context.\n    :param other_raw_recognizer_results: Other recognizer results matched in\n    the given text to allow related entity context enhancement\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param context: list of context words\n    \"\"\"\n    return raw_recognizer_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer.from_dict","title":"<code>from_dict(entity_recognizer_dict)</code>  <code>classmethod</code>","text":"<p>Create EntityRecognizer from a dict input.</p> <p>Parameters:</p> Name Type Description Default <code>entity_recognizer_dict</code> <code>Dict</code> <p>Dict containing keys and values for instantiation</p> required Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"EntityRecognizer\":\n    \"\"\"\n    Create EntityRecognizer from a dict input.\n\n    :param entity_recognizer_dict: Dict containing keys and values for instantiation\n    \"\"\"\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer.get_supported_entities","title":"<code>get_supported_entities()</code>","text":"<p>Return the list of entities this recognizer can identify.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of the supported entities by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_entities(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of entities this recognizer can identify.\n\n    :return: A list of the supported entities by this recognizer\n    \"\"\"\n    return self.supported_entities\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer.get_supported_language","title":"<code>get_supported_language()</code>","text":"<p>Return the language this recognizer can support.</p> <p>Returns:</p> Type Description <code>str</code> <p>A list of the supported language by this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_supported_language(self) -&gt; str:\n    \"\"\"\n    Return the language this recognizer can support.\n\n    :return: A list of the supported language by this recognizer\n    \"\"\"\n    return self.supported_language\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer.get_version","title":"<code>get_version()</code>","text":"<p>Return the version of this recognizer.</p> <p>Returns:</p> Type Description <code>str</code> <p>The current version of this recognizer</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def get_version(self) -&gt; str:\n    \"\"\"\n    Return the version of this recognizer.\n\n    :return: The current version of this recognizer\n    \"\"\"\n    return self.version\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer.load","title":"<code>load()</code>  <code>abstractmethod</code>","text":"<p>Initialize the recognizer assets if needed.</p> <p>(e.g. machine learning models)</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@abstractmethod\ndef load(self) -&gt; None:\n    \"\"\"\n    Initialize the recognizer assets if needed.\n\n    (e.g. machine learning models)\n    \"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer.remove_duplicates","title":"<code>remove_duplicates(results)</code>  <code>staticmethod</code>","text":"<p>Remove duplicate results.</p> <p>Remove duplicates in case the two results have identical start and ends and types.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> required <p>Returns:</p> Type Description <code>List[RecognizerResult]</code> <p>List[RecognizerResult]</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>@staticmethod\ndef remove_duplicates(results: List[RecognizerResult]) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Remove duplicate results.\n\n    Remove duplicates in case the two results\n    have identical start and ends and types.\n    :param results: List[RecognizerResult]\n    :return: List[RecognizerResult]\n    \"\"\"\n    results = list(set(results))\n    results = sorted(results, key=lambda x: (-x.score, x.start, -(x.end - x.start)))\n    filtered_results = []\n\n    for result in results:\n        if result.score == 0:\n            continue\n\n        to_keep = result not in filtered_results  # equals based comparison\n        if to_keep:\n            for filtered in filtered_results:\n                # If result is contained in one of the other results\n                if (\n                    result.contained_in(filtered)\n                    and result.entity_type == filtered.entity_type\n                ):\n                    to_keep = False\n                    break\n\n        if to_keep:\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.EntityRecognizer.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize self to dictionary.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/entity_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\n        \"supported_entities\": self.supported_entities,\n        \"supported_language\": self.supported_language,\n        \"name\": self.name,\n        \"version\": self.version,\n    }\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.LemmaContextAwareEnhancer","title":"<code>LemmaContextAwareEnhancer</code>","text":"<p>             Bases: <code>ContextAwareEnhancer</code></p> <p>A class representing a lemma based context aware enhancer logic.</p> <p>Context words might enhance confidence score of a recognized entity, LemmaContextAwareEnhancer is an implementation of Lemma based context aware logic, it compares spacy lemmas of each word in context of the matched entity to given context and the recognizer context words, if matched it enhance the recognized entity confidence score by a given factor.</p> <p>Parameters:</p> Name Type Description Default <code>context_similarity_factor</code> <code>float</code> <p>How much to enhance confidence of match entity</p> <code>0.35</code> <code>min_score_with_context_similarity</code> <code>float</code> <p>Minimum confidence score</p> <code>0.4</code> <code>context_prefix_count</code> <code>int</code> <p>how many words before the entity to match context</p> <code>5</code> <code>context_suffix_count</code> <code>int</code> <p>how many words after the entity to match context</p> <code>0</code> Source code in <code>presidio_analyzer/context_aware_enhancers/lemma_context_aware_enhancer.py</code> <pre><code>class LemmaContextAwareEnhancer(ContextAwareEnhancer):\n    \"\"\"\n    A class representing a lemma based context aware enhancer logic.\n\n    Context words might enhance confidence score of a recognized entity,\n    LemmaContextAwareEnhancer is an implementation of Lemma based context aware logic,\n    it compares spacy lemmas of each word in context of the matched entity to given\n    context and the recognizer context words,\n    if matched it enhance the recognized entity confidence score by a given factor.\n\n    :param context_similarity_factor: How much to enhance confidence of match entity\n    :param min_score_with_context_similarity: Minimum confidence score\n    :param context_prefix_count: how many words before the entity to match context\n    :param context_suffix_count: how many words after the entity to match context\n    \"\"\"\n\n    def __init__(\n        self,\n        context_similarity_factor: float = 0.35,\n        min_score_with_context_similarity: float = 0.4,\n        context_prefix_count: int = 5,\n        context_suffix_count: int = 0,\n    ):\n        super().__init__(\n            context_similarity_factor=context_similarity_factor,\n            min_score_with_context_similarity=min_score_with_context_similarity,\n            context_prefix_count=context_prefix_count,\n            context_suffix_count=context_suffix_count,\n        )\n\n    def enhance_using_context(\n        self,\n        text: str,\n        raw_results: List[RecognizerResult],\n        nlp_artifacts: NlpArtifacts,\n        recognizers: List[EntityRecognizer],\n        context: Optional[List[str]] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Update results in case the lemmas of surrounding words or input context\n        words are identical to the context words.\n\n        Using the surrounding words of the actual word matches, look\n        for specific strings that if found contribute to the score\n        of the result, improving the confidence that the match is\n        indeed of that PII entity type\n\n        :param text: The actual text that was analyzed\n        :param raw_results: Recognizer results which didn't take\n                            context into consideration\n        :param nlp_artifacts: The nlp artifacts contains elements\n                              such as lemmatized tokens for better\n                              accuracy of the context enhancement process\n        :param recognizers: the list of recognizers\n        :param context: list of context words\n        \"\"\"  # noqa D205 D400\n\n        # create a deep copy of the results object, so we can manipulate it\n        results = copy.deepcopy(raw_results)\n\n        # create recognizer context dictionary\n        recognizers_dict = {recognizer.id: recognizer for recognizer in recognizers}\n\n        # Create empty list in None or lowercase all context words in the list\n        if not context:\n            context = []\n        else:\n            context = [word.lower() for word in context]\n\n        # Sanity\n        if nlp_artifacts is None:\n            logger.warning(\"NLP artifacts were not provided\")\n            return results\n\n        for result in results:\n            recognizer = None\n            # get recognizer matching the result, if found.\n            if (\n                result.recognition_metadata\n                and RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n                in result.recognition_metadata.keys()\n            ):\n                recognizer = recognizers_dict.get(\n                    result.recognition_metadata[\n                        RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n                    ]\n                )\n\n            if not recognizer:\n                logger.debug(\n                    \"Recognizer name not found as part of the \"\n                    \"recognition_metadata dict in the RecognizerResult. \"\n                )\n                continue\n\n            # skip recognizer result if the recognizer doesn't support\n            # context enhancement\n            if not recognizer.context:\n                logger.debug(\n                    \"recognizer '%s' does not support context enhancement\",\n                    recognizer.name,\n                )\n                continue\n\n            # skip context enhancement if already boosted by recognizer level\n            if result.recognition_metadata.get(\n                RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY\n            ):\n                logger.debug(\"result score already boosted, skipping\")\n                continue\n\n            # extract lemmatized context from the surrounding of the match\n            word = text[result.start : result.end]\n\n            surrounding_words = self._extract_surrounding_words(\n                nlp_artifacts=nlp_artifacts, word=word, start=result.start\n            )\n\n            # combine other sources of context with surrounding words\n            surrounding_words.extend(context)\n\n            supportive_context_word = self._find_supportive_word_in_context(\n                surrounding_words, recognizer.context\n            )\n            if supportive_context_word != \"\":\n                result.score += self.context_similarity_factor\n                result.score = max(result.score, self.min_score_with_context_similarity)\n                result.score = min(result.score, ContextAwareEnhancer.MAX_SCORE)\n\n                # Update the explainability object with context information\n                # helped to improve the score\n                result.analysis_explanation.set_supportive_context_word(\n                    supportive_context_word\n                )\n                result.analysis_explanation.set_improved_score(result.score)\n        return results\n\n    @staticmethod\n    def _find_supportive_word_in_context(\n        context_list: List[str], recognizer_context_list: List[str]\n    ) -&gt; str:\n        \"\"\"\n        Find words in the text which are relevant for context evaluation.\n\n        A word is considered a supportive context word if there's exact match\n        between a keyword in context_text and any keyword in context_list.\n\n        :param context_list words before and after the matched entity within\n               a specified window size\n        :param recognizer_context_list a list of words considered as\n                context keywords manually specified by the recognizer's author\n        \"\"\"\n        word = \"\"\n        # If the context list is empty, no need to continue\n        if context_list is None or recognizer_context_list is None:\n            return word\n\n        for predefined_context_word in recognizer_context_list:\n            # result == true only if any of the predefined context words\n            # is found exactly or as a substring in any of the collected\n            # context words\n            result = next(\n                (\n                    True\n                    for keyword in context_list\n                    if predefined_context_word in keyword\n                ),\n                False,\n            )\n            if result:\n                logger.debug(\"Found context keyword '%s'\", predefined_context_word)\n                word = predefined_context_word\n                break\n\n        return word\n\n    def _extract_surrounding_words(\n        self, nlp_artifacts: NlpArtifacts, word: str, start: int\n    ) -&gt; List[str]:\n        \"\"\"Extract words surrounding another given word.\n\n        The text from which the context is extracted is given in the nlp\n        doc.\n\n        :param nlp_artifacts: An abstraction layer which holds different\n                              items which are the result of a NLP pipeline\n                              execution on a given text\n        :param word: The word to look for context around\n        :param start: The start index of the word in the original text\n        \"\"\"\n        if not nlp_artifacts.tokens:\n            logger.info(\"Skipping context extraction due to lack of NLP artifacts\")\n            # if there are no nlp artifacts, this is ok, we can\n            # extract context and we return a valid, yet empty\n            # context\n            return [\"\"]\n\n        # Get the already prepared words in the given text, in their\n        # LEMMATIZED version\n        lemmatized_keywords = nlp_artifacts.keywords\n\n        # since the list of tokens is not necessarily aligned\n        # with the actual index of the match, we look for the\n        # token index which corresponds to the match\n        token_index = self._find_index_of_match_token(\n            word, start, nlp_artifacts.tokens, nlp_artifacts.tokens_indices\n        )\n\n        # index i belongs to the PII entity, take the preceding n words\n        # and the successing m words into a context list\n\n        backward_context = self._add_n_words_backward(\n            token_index,\n            self.context_prefix_count,\n            nlp_artifacts.lemmas,\n            lemmatized_keywords,\n        )\n        forward_context = self._add_n_words_forward(\n            token_index,\n            self.context_suffix_count,\n            nlp_artifacts.lemmas,\n            lemmatized_keywords,\n        )\n\n        context_list = []\n        context_list.extend(backward_context)\n        context_list.extend(forward_context)\n        context_list = list(set(context_list))\n        logger.debug(\"Context list is: %s\", \" \".join(context_list))\n        return context_list\n\n    @staticmethod\n    def _find_index_of_match_token(\n        word: str, start: int, tokens, tokens_indices: List[int]  # noqa ANN001\n    ) -&gt; int:\n        found = False\n        # we use the known start index of the original word to find the actual\n        # token at that index, we are not checking for equivilance since the\n        # token might be just a substring of that word (e.g. for phone number\n        # 555-124564 the first token might be just '555' or for a match like '\n        # rocket' the actual token will just be 'rocket' hence the misalignment\n        # of indices)\n        # Note: we are iterating over the original tokens (not the lemmatized)\n        i = -1\n        for i, token in enumerate(tokens, 0):\n            # Either we found a token with the exact location, or\n            # we take a token which its characters indices covers\n            # the index we are looking for.\n            if (tokens_indices[i] == start) or (start &lt; tokens_indices[i] + len(token)):\n                # found the interesting token, the one that around it\n                # we take n words, we save the matching lemma\n                found = True\n                break\n\n        if not found:\n            raise ValueError(\n                \"Did not find word '\" + word + \"' \"\n                \"in the list of tokens although it \"\n                \"is expected to be found\"\n            )\n        return i\n\n    @staticmethod\n    def _add_n_words(\n        index: int,\n        n_words: int,\n        lemmas: List[str],\n        lemmatized_filtered_keywords: List[str],\n        is_backward: bool,\n    ) -&gt; List[str]:\n        \"\"\"\n        Prepare a string of context words.\n\n        Return a list of words which surrounds a lemma at a given index.\n        The words will be collected only if exist in the filtered array\n\n        :param index: index of the lemma that its surrounding words we want\n        :param n_words: number of words to take\n        :param lemmas: array of lemmas\n        :param lemmatized_filtered_keywords: the array of filtered\n               lemmas from the original sentence,\n        :param is_backward: if true take the preceeding words, if false,\n                            take the successing words\n        \"\"\"\n        i = index\n        context_words = []\n        # The entity itself is no interest to us...however we want to\n        # consider it anyway for cases were it is attached with no spaces\n        # to an interesting context word, so we allow it and add 1 to\n        # the number of collected words\n\n        # collect at most n words (in lower case)\n        remaining = n_words + 1\n        while 0 &lt;= i &lt; len(lemmas) and remaining &gt; 0:\n            lower_lemma = lemmas[i].lower()\n            if lower_lemma in lemmatized_filtered_keywords:\n                context_words.append(lower_lemma)\n                remaining -= 1\n            i = i - 1 if is_backward else i + 1\n        return context_words\n\n    def _add_n_words_forward(\n        self,\n        index: int,\n        n_words: int,\n        lemmas: List[str],\n        lemmatized_filtered_keywords: List[str],\n    ) -&gt; List[str]:\n        return self._add_n_words(\n            index, n_words, lemmas, lemmatized_filtered_keywords, False\n        )\n\n    def _add_n_words_backward(\n        self,\n        index: int,\n        n_words: int,\n        lemmas: List[str],\n        lemmatized_filtered_keywords: List[str],\n    ) -&gt; List[str]:\n        return self._add_n_words(\n            index, n_words, lemmas, lemmatized_filtered_keywords, True\n        )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.LemmaContextAwareEnhancer.enhance_using_context","title":"<code>enhance_using_context(text, raw_results, nlp_artifacts, recognizers, context=None)</code>","text":"<p>Update results in case the lemmas of surrounding words or input context words are identical to the context words.</p> <p>Using the surrounding words of the actual word matches, look for specific strings that if found contribute to the score of the result, improving the confidence that the match is indeed of that PII entity type</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The actual text that was analyzed</p> required <code>raw_results</code> <code>List[RecognizerResult]</code> <p>Recognizer results which didn't take context into consideration</p> required <code>nlp_artifacts</code> <code>NlpArtifacts</code> <p>The nlp artifacts contains elements such as lemmatized tokens for better accuracy of the context enhancement process</p> required <code>recognizers</code> <code>List[EntityRecognizer]</code> <p>the list of recognizers</p> required <code>context</code> <code>Optional[List[str]]</code> <p>list of context words</p> <code>None</code> Source code in <code>presidio_analyzer/context_aware_enhancers/lemma_context_aware_enhancer.py</code> <pre><code>def enhance_using_context(\n    self,\n    text: str,\n    raw_results: List[RecognizerResult],\n    nlp_artifacts: NlpArtifacts,\n    recognizers: List[EntityRecognizer],\n    context: Optional[List[str]] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Update results in case the lemmas of surrounding words or input context\n    words are identical to the context words.\n\n    Using the surrounding words of the actual word matches, look\n    for specific strings that if found contribute to the score\n    of the result, improving the confidence that the match is\n    indeed of that PII entity type\n\n    :param text: The actual text that was analyzed\n    :param raw_results: Recognizer results which didn't take\n                        context into consideration\n    :param nlp_artifacts: The nlp artifacts contains elements\n                          such as lemmatized tokens for better\n                          accuracy of the context enhancement process\n    :param recognizers: the list of recognizers\n    :param context: list of context words\n    \"\"\"  # noqa D205 D400\n\n    # create a deep copy of the results object, so we can manipulate it\n    results = copy.deepcopy(raw_results)\n\n    # create recognizer context dictionary\n    recognizers_dict = {recognizer.id: recognizer for recognizer in recognizers}\n\n    # Create empty list in None or lowercase all context words in the list\n    if not context:\n        context = []\n    else:\n        context = [word.lower() for word in context]\n\n    # Sanity\n    if nlp_artifacts is None:\n        logger.warning(\"NLP artifacts were not provided\")\n        return results\n\n    for result in results:\n        recognizer = None\n        # get recognizer matching the result, if found.\n        if (\n            result.recognition_metadata\n            and RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n            in result.recognition_metadata.keys()\n        ):\n            recognizer = recognizers_dict.get(\n                result.recognition_metadata[\n                    RecognizerResult.RECOGNIZER_IDENTIFIER_KEY\n                ]\n            )\n\n        if not recognizer:\n            logger.debug(\n                \"Recognizer name not found as part of the \"\n                \"recognition_metadata dict in the RecognizerResult. \"\n            )\n            continue\n\n        # skip recognizer result if the recognizer doesn't support\n        # context enhancement\n        if not recognizer.context:\n            logger.debug(\n                \"recognizer '%s' does not support context enhancement\",\n                recognizer.name,\n            )\n            continue\n\n        # skip context enhancement if already boosted by recognizer level\n        if result.recognition_metadata.get(\n            RecognizerResult.IS_SCORE_ENHANCED_BY_CONTEXT_KEY\n        ):\n            logger.debug(\"result score already boosted, skipping\")\n            continue\n\n        # extract lemmatized context from the surrounding of the match\n        word = text[result.start : result.end]\n\n        surrounding_words = self._extract_surrounding_words(\n            nlp_artifacts=nlp_artifacts, word=word, start=result.start\n        )\n\n        # combine other sources of context with surrounding words\n        surrounding_words.extend(context)\n\n        supportive_context_word = self._find_supportive_word_in_context(\n            surrounding_words, recognizer.context\n        )\n        if supportive_context_word != \"\":\n            result.score += self.context_similarity_factor\n            result.score = max(result.score, self.min_score_with_context_similarity)\n            result.score = min(result.score, ContextAwareEnhancer.MAX_SCORE)\n\n            # Update the explainability object with context information\n            # helped to improve the score\n            result.analysis_explanation.set_supportive_context_word(\n                supportive_context_word\n            )\n            result.analysis_explanation.set_improved_score(result.score)\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.LocalRecognizer","title":"<code>LocalRecognizer</code>","text":"<p>             Bases: <code>ABC</code>, <code>EntityRecognizer</code></p> <p>PII entity recognizer which runs on the same process as the AnalyzerEngine.</p> Source code in <code>presidio_analyzer/local_recognizer.py</code> <pre><code>class LocalRecognizer(ABC, EntityRecognizer):\n    \"\"\"PII entity recognizer which runs on the same process as the AnalyzerEngine.\"\"\"\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.Pattern","title":"<code>Pattern</code>","text":"<p>A class that represents a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the pattern</p> required <code>regex</code> <code>str</code> <p>the regex pattern to detect</p> required <code>score</code> <code>float</code> <p>the pattern's strength (values varies 0-1)</p> required Source code in <code>presidio_analyzer/pattern.py</code> <pre><code>class Pattern:\n    \"\"\"\n    A class that represents a regex pattern.\n\n    :param name: the name of the pattern\n    :param regex: the regex pattern to detect\n    :param score: the pattern's strength (values varies 0-1)\n    \"\"\"\n\n    def __init__(self, name: str, regex: str, score: float):\n\n        self.name = name\n        self.regex = regex\n        self.score = score\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"\n        Turn this instance into a dictionary.\n\n        :return: a dictionary\n        \"\"\"\n        return_dict = {\"name\": self.name, \"score\": self.score, \"regex\": self.regex}\n        return return_dict\n\n    @classmethod\n    def from_dict(cls, pattern_dict: Dict) -&gt; \"Pattern\":\n        \"\"\"\n        Load an instance from a dictionary.\n\n        :param pattern_dict: a dictionary holding the pattern's parameters\n        :return: a Pattern instance\n        \"\"\"\n        return cls(**pattern_dict)\n\n    def __repr__(self):\n        \"\"\"Return string representation of instance.\"\"\"\n        return json.dumps(self.to_dict())\n\n    def __str__(self):\n        \"\"\"Return string representation of instance.\"\"\"\n        return json.dumps(self.to_dict())\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.Pattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of instance.</p> Source code in <code>presidio_analyzer/pattern.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return string representation of instance.\"\"\"\n    return json.dumps(self.to_dict())\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.Pattern.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of instance.</p> Source code in <code>presidio_analyzer/pattern.py</code> <pre><code>def __str__(self):\n    \"\"\"Return string representation of instance.\"\"\"\n    return json.dumps(self.to_dict())\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.Pattern.from_dict","title":"<code>from_dict(pattern_dict)</code>  <code>classmethod</code>","text":"<p>Load an instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>pattern_dict</code> <code>Dict</code> <p>a dictionary holding the pattern's parameters</p> required <p>Returns:</p> Type Description <code>Pattern</code> <p>a Pattern instance</p> Source code in <code>presidio_analyzer/pattern.py</code> <pre><code>@classmethod\ndef from_dict(cls, pattern_dict: Dict) -&gt; \"Pattern\":\n    \"\"\"\n    Load an instance from a dictionary.\n\n    :param pattern_dict: a dictionary holding the pattern's parameters\n    :return: a Pattern instance\n    \"\"\"\n    return cls(**pattern_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.Pattern.to_dict","title":"<code>to_dict()</code>","text":"<p>Turn this instance into a dictionary.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/pattern.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Turn this instance into a dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return_dict = {\"name\": self.name, \"score\": self.score, \"regex\": self.regex}\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.PatternRecognizer","title":"<code>PatternRecognizer</code>","text":"<p>             Bases: <code>LocalRecognizer</code></p> <p>PII entity recognizer using regular expressions or deny-lists.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>List[Pattern]</code> <p>A list of patterns to detect</p> <code>None</code> <code>deny_list</code> <code>List[str]</code> <p>A list of words to detect, in case our recognizer uses a predefined list of words (deny list)</p> <code>None</code> <code>context</code> <code>List[str]</code> <p>list of context words</p> <code>None</code> <code>deny_list_score</code> <code>float</code> <p>confidence score for a term identified using a deny-list</p> <code>1.0</code> <code>global_regex_flags</code> <code>Optional[int]</code> <p>regex flags to be used in regex matching, including deny-lists.</p> <code>DOTALL | MULTILINE | IGNORECASE</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>class PatternRecognizer(LocalRecognizer):\n    \"\"\"\n    PII entity recognizer using regular expressions or deny-lists.\n\n    :param patterns: A list of patterns to detect\n    :param deny_list: A list of words to detect,\n    in case our recognizer uses a predefined list of words (deny list)\n    :param context: list of context words\n    :param deny_list_score: confidence score for a term\n    identified using a deny-list\n    :param global_regex_flags: regex flags to be used in regex matching,\n    including deny-lists.\n    \"\"\"\n\n    def __init__(\n        self,\n        supported_entity: str,\n        name: str = None,\n        supported_language: str = \"en\",\n        patterns: List[Pattern] = None,\n        deny_list: List[str] = None,\n        context: List[str] = None,\n        deny_list_score: float = 1.0,\n        global_regex_flags: Optional[int] = re.DOTALL | re.MULTILINE | re.IGNORECASE,\n        version: str = \"0.0.1\",\n    ):\n        if not supported_entity:\n            raise ValueError(\"Pattern recognizer should be initialized with entity\")\n\n        if not patterns and not deny_list:\n            raise ValueError(\n                \"Pattern recognizer should be initialized with patterns\"\n                \" or with deny list\"\n            )\n\n        super().__init__(\n            supported_entities=[supported_entity],\n            supported_language=supported_language,\n            name=name,\n            version=version,\n        )\n        if patterns is None:\n            self.patterns = []\n        else:\n            self.patterns = patterns\n        self.context = context\n        self.deny_list_score = deny_list_score\n        self.global_regex_flags = global_regex_flags\n\n        if deny_list:\n            deny_list_pattern = self._deny_list_to_regex(deny_list)\n            self.patterns.append(deny_list_pattern)\n            self.deny_list = deny_list\n        else:\n            self.deny_list = []\n\n    def load(self):  # noqa D102\n        pass\n\n    def analyze(\n        self,\n        text: str,\n        entities: List[str],\n        nlp_artifacts: Optional[NlpArtifacts] = None,\n        regex_flags: Optional[int] = None,\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyzes text to detect PII using regular expressions or deny-lists.\n\n        :param text: Text to be analyzed\n        :param entities: Entities this recognizer can detect\n        :param nlp_artifacts: Output values from the NLP engine\n        :param regex_flags: regex flags to be used in regex matching\n        :return:\n        \"\"\"\n        results = []\n\n        if self.patterns:\n            pattern_result = self.__analyze_patterns(text, regex_flags)\n            results.extend(pattern_result)\n\n        return results\n\n    def _deny_list_to_regex(self, deny_list: List[str]) -&gt; Pattern:\n        \"\"\"\n        Convert a list of words to a matching regex.\n\n        To be analyzed by the analyze method as any other regex patterns.\n\n        :param deny_list: the list of words to detect\n        :return:the regex of the words for detection\n        \"\"\"\n\n        # Escape deny list elements as preparation for regex\n        escaped_deny_list = [re.escape(element) for element in deny_list]\n        regex = r\"(?:^|(?&lt;=\\W))(\" + \"|\".join(escaped_deny_list) + r\")(?:(?=\\W)|$)\"\n        return Pattern(name=\"deny_list\", regex=regex, score=self.deny_list_score)\n\n    def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n        \"\"\"\n        Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the validation was successful.\n        \"\"\"\n        return None\n\n    def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n        \"\"\"\n        Logic to check for result invalidation by running pruning logic.\n\n        For example, each SSN number group should not consist of all the same digits.\n\n        :param pattern_text: the text to validated.\n        Only the part in text that was detected by the regex engine\n        :return: A bool indicating whether the result is invalidated\n        \"\"\"\n        return None\n\n    @staticmethod\n    def build_regex_explanation(\n        recognizer_name: str,\n        pattern_name: str,\n        pattern: str,\n        original_score: float,\n        validation_result: bool,\n        regex_flags: int,\n    ) -&gt; AnalysisExplanation:\n        \"\"\"\n        Construct an explanation for why this entity was detected.\n\n        :param recognizer_name: Name of recognizer detecting the entity\n        :param pattern_name: Regex pattern name which detected the entity\n        :param pattern: Regex pattern logic\n        :param original_score: Score given by the recognizer\n        :param validation_result: Whether validation was used and its result\n        :param regex_flags: Regex flags used in the regex matching\n        :return: Analysis explanation\n        \"\"\"\n        explanation = AnalysisExplanation(\n            recognizer=recognizer_name,\n            original_score=original_score,\n            pattern_name=pattern_name,\n            pattern=pattern,\n            validation_result=validation_result,\n            regex_flags=regex_flags,\n        )\n        return explanation\n\n    def __analyze_patterns(\n        self, text: str, flags: int = None\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Evaluate all patterns in the provided text.\n\n        Including words in the provided deny-list\n\n        :param text: text to analyze\n        :param flags: regex flags\n        :return: A list of RecognizerResult\n        \"\"\"\n        flags = flags if flags else self.global_regex_flags\n        results = []\n        for pattern in self.patterns:\n            match_start_time = datetime.datetime.now()\n            matches = re.finditer(pattern.regex, text, flags=flags)\n            match_time = datetime.datetime.now() - match_start_time\n            logger.debug(\n                \"--- match_time[%s]: %s.%s seconds\",\n                pattern.name,\n                match_time.seconds,\n                match_time.microseconds,\n            )\n\n            for match in matches:\n                start, end = match.span()\n                current_match = text[start:end]\n\n                # Skip empty results\n                if current_match == \"\":\n                    continue\n\n                score = pattern.score\n\n                validation_result = self.validate_result(current_match)\n                description = self.build_regex_explanation(\n                    self.name,\n                    pattern.name,\n                    pattern.regex,\n                    score,\n                    validation_result,\n                    flags,\n                )\n                pattern_result = RecognizerResult(\n                    entity_type=self.supported_entities[0],\n                    start=start,\n                    end=end,\n                    score=score,\n                    analysis_explanation=description,\n                    recognition_metadata={\n                        RecognizerResult.RECOGNIZER_NAME_KEY: self.name,\n                        RecognizerResult.RECOGNIZER_IDENTIFIER_KEY: self.id,\n                    },\n                )\n\n                if validation_result is not None:\n                    if validation_result:\n                        pattern_result.score = EntityRecognizer.MAX_SCORE\n                    else:\n                        pattern_result.score = EntityRecognizer.MIN_SCORE\n\n                invalidation_result = self.invalidate_result(current_match)\n                if invalidation_result is not None and invalidation_result:\n                    pattern_result.score = EntityRecognizer.MIN_SCORE\n\n                if pattern_result.score &gt; EntityRecognizer.MIN_SCORE:\n                    results.append(pattern_result)\n\n                # Update analysis explanation score following validation or invalidation\n                description.score = pattern_result.score\n\n        results = EntityRecognizer.remove_duplicates(results)\n        return results\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"Serialize instance into a dictionary.\"\"\"\n        return_dict = super().to_dict()\n\n        return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n        return_dict[\"deny_list\"] = self.deny_list\n        return_dict[\"context\"] = self.context\n        return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n        del return_dict[\"supported_entities\"]\n\n        return return_dict\n\n    @classmethod\n    def from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n        \"\"\"Create instance from a serialized dict.\"\"\"\n        patterns = entity_recognizer_dict.get(\"patterns\")\n        if patterns:\n            patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n            entity_recognizer_dict[\"patterns\"] = patterns_list\n\n        return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.PatternRecognizer.__analyze_patterns","title":"<code>__analyze_patterns(text, flags=None)</code>","text":"<p>Evaluate all patterns in the provided text.</p> <p>Including words in the provided deny-list</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text to analyze</p> required <code>flags</code> <code>int</code> <p>regex flags</p> <code>None</code> <p>Returns:</p> Type Description <code>List[RecognizerResult]</code> <p>A list of RecognizerResult</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def __analyze_patterns(\n    self, text: str, flags: int = None\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Evaluate all patterns in the provided text.\n\n    Including words in the provided deny-list\n\n    :param text: text to analyze\n    :param flags: regex flags\n    :return: A list of RecognizerResult\n    \"\"\"\n    flags = flags if flags else self.global_regex_flags\n    results = []\n    for pattern in self.patterns:\n        match_start_time = datetime.datetime.now()\n        matches = re.finditer(pattern.regex, text, flags=flags)\n        match_time = datetime.datetime.now() - match_start_time\n        logger.debug(\n            \"--- match_time[%s]: %s.%s seconds\",\n            pattern.name,\n            match_time.seconds,\n            match_time.microseconds,\n        )\n\n        for match in matches:\n            start, end = match.span()\n            current_match = text[start:end]\n\n            # Skip empty results\n            if current_match == \"\":\n                continue\n\n            score = pattern.score\n\n            validation_result = self.validate_result(current_match)\n            description = self.build_regex_explanation(\n                self.name,\n                pattern.name,\n                pattern.regex,\n                score,\n                validation_result,\n                flags,\n            )\n            pattern_result = RecognizerResult(\n                entity_type=self.supported_entities[0],\n                start=start,\n                end=end,\n                score=score,\n                analysis_explanation=description,\n                recognition_metadata={\n                    RecognizerResult.RECOGNIZER_NAME_KEY: self.name,\n                    RecognizerResult.RECOGNIZER_IDENTIFIER_KEY: self.id,\n                },\n            )\n\n            if validation_result is not None:\n                if validation_result:\n                    pattern_result.score = EntityRecognizer.MAX_SCORE\n                else:\n                    pattern_result.score = EntityRecognizer.MIN_SCORE\n\n            invalidation_result = self.invalidate_result(current_match)\n            if invalidation_result is not None and invalidation_result:\n                pattern_result.score = EntityRecognizer.MIN_SCORE\n\n            if pattern_result.score &gt; EntityRecognizer.MIN_SCORE:\n                results.append(pattern_result)\n\n            # Update analysis explanation score following validation or invalidation\n            description.score = pattern_result.score\n\n    results = EntityRecognizer.remove_duplicates(results)\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.PatternRecognizer.analyze","title":"<code>analyze(text, entities, nlp_artifacts=None, regex_flags=None)</code>","text":"<p>Analyzes text to detect PII using regular expressions or deny-lists.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to be analyzed</p> required <code>entities</code> <code>List[str]</code> <p>Entities this recognizer can detect</p> required <code>nlp_artifacts</code> <code>Optional[NlpArtifacts]</code> <p>Output values from the NLP engine</p> <code>None</code> <code>regex_flags</code> <code>Optional[int]</code> <p>regex flags to be used in regex matching</p> <code>None</code> <p>Returns:</p> Type Description <code>List[RecognizerResult]</code> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def analyze(\n    self,\n    text: str,\n    entities: List[str],\n    nlp_artifacts: Optional[NlpArtifacts] = None,\n    regex_flags: Optional[int] = None,\n) -&gt; List[RecognizerResult]:\n    \"\"\"\n    Analyzes text to detect PII using regular expressions or deny-lists.\n\n    :param text: Text to be analyzed\n    :param entities: Entities this recognizer can detect\n    :param nlp_artifacts: Output values from the NLP engine\n    :param regex_flags: regex flags to be used in regex matching\n    :return:\n    \"\"\"\n    results = []\n\n    if self.patterns:\n        pattern_result = self.__analyze_patterns(text, regex_flags)\n        results.extend(pattern_result)\n\n    return results\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.PatternRecognizer.build_regex_explanation","title":"<code>build_regex_explanation(recognizer_name, pattern_name, pattern, original_score, validation_result, regex_flags)</code>  <code>staticmethod</code>","text":"<p>Construct an explanation for why this entity was detected.</p> <p>Parameters:</p> Name Type Description Default <code>recognizer_name</code> <code>str</code> <p>Name of recognizer detecting the entity</p> required <code>pattern_name</code> <code>str</code> <p>Regex pattern name which detected the entity</p> required <code>pattern</code> <code>str</code> <p>Regex pattern logic</p> required <code>original_score</code> <code>float</code> <p>Score given by the recognizer</p> required <code>validation_result</code> <code>bool</code> <p>Whether validation was used and its result</p> required <code>regex_flags</code> <code>int</code> <p>Regex flags used in the regex matching</p> required <p>Returns:</p> Type Description <code>AnalysisExplanation</code> <p>Analysis explanation</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@staticmethod\ndef build_regex_explanation(\n    recognizer_name: str,\n    pattern_name: str,\n    pattern: str,\n    original_score: float,\n    validation_result: bool,\n    regex_flags: int,\n) -&gt; AnalysisExplanation:\n    \"\"\"\n    Construct an explanation for why this entity was detected.\n\n    :param recognizer_name: Name of recognizer detecting the entity\n    :param pattern_name: Regex pattern name which detected the entity\n    :param pattern: Regex pattern logic\n    :param original_score: Score given by the recognizer\n    :param validation_result: Whether validation was used and its result\n    :param regex_flags: Regex flags used in the regex matching\n    :return: Analysis explanation\n    \"\"\"\n    explanation = AnalysisExplanation(\n        recognizer=recognizer_name,\n        original_score=original_score,\n        pattern_name=pattern_name,\n        pattern=pattern,\n        validation_result=validation_result,\n        regex_flags=regex_flags,\n    )\n    return explanation\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.PatternRecognizer.from_dict","title":"<code>from_dict(entity_recognizer_dict)</code>  <code>classmethod</code>","text":"<p>Create instance from a serialized dict.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>@classmethod\ndef from_dict(cls, entity_recognizer_dict: Dict) -&gt; \"PatternRecognizer\":\n    \"\"\"Create instance from a serialized dict.\"\"\"\n    patterns = entity_recognizer_dict.get(\"patterns\")\n    if patterns:\n        patterns_list = [Pattern.from_dict(pat) for pat in patterns]\n        entity_recognizer_dict[\"patterns\"] = patterns_list\n\n    return cls(**entity_recognizer_dict)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.PatternRecognizer.invalidate_result","title":"<code>invalidate_result(pattern_text)</code>","text":"<p>Logic to check for result invalidation by running pruning logic.</p> <p>For example, each SSN number group should not consist of all the same digits.</p> <p>Parameters:</p> Name Type Description Default <code>pattern_text</code> <code>str</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> required <p>Returns:</p> Type Description <code>Optional[bool]</code> <p>A bool indicating whether the result is invalidated</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def invalidate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Logic to check for result invalidation by running pruning logic.\n\n    For example, each SSN number group should not consist of all the same digits.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the result is invalidated\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.PatternRecognizer.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize instance into a dictionary.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Serialize instance into a dictionary.\"\"\"\n    return_dict = super().to_dict()\n\n    return_dict[\"patterns\"] = [pat.to_dict() for pat in self.patterns]\n    return_dict[\"deny_list\"] = self.deny_list\n    return_dict[\"context\"] = self.context\n    return_dict[\"supported_entity\"] = return_dict[\"supported_entities\"][0]\n    del return_dict[\"supported_entities\"]\n\n    return return_dict\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.PatternRecognizer.validate_result","title":"<code>validate_result(pattern_text)</code>","text":"<p>Validate the pattern logic e.g., by running checksum on a detected pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern_text</code> <code>str</code> <p>the text to validated. Only the part in text that was detected by the regex engine</p> required <p>Returns:</p> Type Description <code>Optional[bool]</code> <p>A bool indicating whether the validation was successful.</p> Source code in <code>presidio_analyzer/pattern_recognizer.py</code> <pre><code>def validate_result(self, pattern_text: str) -&gt; Optional[bool]:\n    \"\"\"\n    Validate the pattern logic e.g., by running checksum on a detected pattern.\n\n    :param pattern_text: the text to validated.\n    Only the part in text that was detected by the regex engine\n    :return: A bool indicating whether the validation was successful.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerRegistry","title":"<code>RecognizerRegistry</code>","text":"<p>Detect, register and hold all recognizers to be used by the analyzer.</p> <p>Parameters:</p> Name Type Description Default <code>recognizers</code> <code>Optional[Iterable[EntityRecognizer]]</code> <p>An optional list of recognizers, that will be available instead of the predefined recognizers</p> <code>None</code> <code></code> <code>global_regex_flags</code> <p>regex flags to be used in regex matching, including deny-lists</p> required Source code in <code>presidio_analyzer/recognizer_registry.py</code> <pre><code>class RecognizerRegistry:\n    \"\"\"\n    Detect, register and hold all recognizers to be used by the analyzer.\n\n    :param recognizers: An optional list of recognizers,\n    that will be available instead of the predefined recognizers\n    :param global_regex_flags : regex flags to be used in regex matching,\n    including deny-lists\n\n    \"\"\"\n\n    def __init__(\n        self,\n        recognizers: Optional[Iterable[EntityRecognizer]] = None,\n        global_regex_flags: Optional[int] = re.DOTALL | re.MULTILINE | re.IGNORECASE,\n    ):\n        if recognizers:\n            self.recognizers = recognizers\n        else:\n            self.recognizers = []\n        self.global_regex_flags = global_regex_flags\n\n    def load_predefined_recognizers(\n        self, languages: Optional[List[str]] = None, nlp_engine: NlpEngine = None\n    ) -&gt; None:\n        \"\"\"\n        Load the existing recognizers into memory.\n\n        :param languages: List of languages for which to load recognizers\n        :param nlp_engine: The NLP engine to use.\n        :return: None\n        \"\"\"\n        if not languages:\n            languages = [\"en\"]\n\n        nlp_recognizer = self._get_nlp_recognizer(nlp_engine)\n\n        recognizers_map = {\n            \"en\": [\n                UsBankRecognizer,\n                UsLicenseRecognizer,\n                UsItinRecognizer,\n                UsPassportRecognizer,\n                UsSsnRecognizer,\n                NhsRecognizer,\n                SgFinRecognizer,\n                AuAbnRecognizer,\n                AuAcnRecognizer,\n                AuTfnRecognizer,\n                AuMedicareRecognizer,\n                InPanRecognizer,\n            ],\n            \"es\": [EsNifRecognizer],\n            \"it\": [\n                ItDriverLicenseRecognizer,\n                ItFiscalCodeRecognizer,\n                ItVatCodeRecognizer,\n                ItIdentityCardRecognizer,\n                ItPassportRecognizer,\n            ],\n            \"ALL\": [\n                CreditCardRecognizer,\n                CryptoRecognizer,\n                DateRecognizer,\n                EmailRecognizer,\n                IbanRecognizer,\n                IpRecognizer,\n                MedicalLicenseRecognizer,\n                PhoneRecognizer,\n                UrlRecognizer,\n            ],\n        }\n        for lang in languages:\n            lang_recognizers = [\n                self.__instantiate_recognizer(\n                    recognizer_class=rc, supported_language=lang\n                )\n                for rc in recognizers_map.get(lang, [])\n            ]\n            self.recognizers.extend(lang_recognizers)\n            all_recognizers = [\n                self.__instantiate_recognizer(\n                    recognizer_class=rc, supported_language=lang\n                )\n                for rc in recognizers_map.get(\"ALL\", [])\n            ]\n            self.recognizers.extend(all_recognizers)\n            if nlp_engine:\n                nlp_recognizer_inst = nlp_recognizer(\n                    supported_language=lang,\n                    supported_entities=nlp_engine.get_supported_entities(),\n                )\n            else:\n                nlp_recognizer_inst = nlp_recognizer(supported_language=lang)\n            self.recognizers.append(nlp_recognizer_inst)\n\n    @staticmethod\n    def _get_nlp_recognizer(\n        nlp_engine: NlpEngine,\n    ) -&gt; Type[SpacyRecognizer]:\n        \"\"\"Return the recognizer leveraging the selected NLP Engine.\"\"\"\n\n        if isinstance(nlp_engine, StanzaNlpEngine):\n            return StanzaRecognizer\n        if isinstance(nlp_engine, TransformersNlpEngine):\n            return TransformersRecognizer\n        if not nlp_engine or isinstance(nlp_engine, SpacyNlpEngine):\n            return SpacyRecognizer\n        else:\n            logger.warning(\n                \"nlp engine should be either SpacyNlpEngine,\"\n                \"StanzaNlpEngine or TransformersNlpEngine\"\n            )\n            # Returning default\n            return SpacyRecognizer\n\n    def get_recognizers(\n        self,\n        language: str,\n        entities: Optional[List[str]] = None,\n        all_fields: bool = False,\n        ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n    ) -&gt; List[EntityRecognizer]:\n        \"\"\"\n        Return a list of recognizers which supports the specified name and language.\n\n        :param entities: the requested entities\n        :param language: the requested language\n        :param all_fields: a flag to return all fields of a requested language.\n        :param ad_hoc_recognizers: Additional recognizers provided by the user\n        as part of the request\n        :return: A list of the recognizers which supports the supplied entities\n        and language\n        \"\"\"\n        if language is None:\n            raise ValueError(\"No language provided\")\n\n        if entities is None and all_fields is False:\n            raise ValueError(\"No entities provided\")\n\n        all_possible_recognizers = copy.copy(self.recognizers)\n        if ad_hoc_recognizers:\n            all_possible_recognizers.extend(ad_hoc_recognizers)\n\n        # filter out unwanted recognizers\n        to_return = set()\n        if all_fields:\n            to_return = [\n                rec\n                for rec in all_possible_recognizers\n                if language == rec.supported_language\n            ]\n        else:\n            for entity in entities:\n                subset = [\n                    rec\n                    for rec in all_possible_recognizers\n                    if entity in rec.supported_entities\n                    and language == rec.supported_language\n                ]\n\n                if not subset:\n                    logger.warning(\n                        \"Entity %s doesn't have the corresponding\"\n                        \" recognizer in language : %s\",\n                        entity,\n                        language,\n                    )\n                else:\n                    to_return.update(set(subset))\n\n        logger.debug(\n            \"Returning a total of %s recognizers\",\n            str(len(to_return)),\n        )\n\n        if not to_return:\n            raise ValueError(\"No matching recognizers were found to serve the request.\")\n\n        return list(to_return)\n\n    def add_recognizer(self, recognizer: EntityRecognizer) -&gt; None:\n        \"\"\"\n        Add a new recognizer to the list of recognizers.\n\n        :param recognizer: Recognizer to add\n        \"\"\"\n        if not isinstance(recognizer, EntityRecognizer):\n            raise ValueError(\"Input is not of type EntityRecognizer\")\n\n        self.recognizers.append(recognizer)\n\n    def remove_recognizer(self, recognizer_name: str) -&gt; None:\n        \"\"\"\n        Remove a recognizer based on its name.\n\n        :param recognizer_name: Name of recognizer to remove\n        \"\"\"\n        new_recognizers = [\n            rec for rec in self.recognizers if rec.name != recognizer_name\n        ]\n        logger.info(\n            \"Removed %s recognizers which had the name %s\",\n            str(len(self.recognizers) - len(new_recognizers)),\n            recognizer_name,\n        )\n        self.recognizers = new_recognizers\n\n    def add_pattern_recognizer_from_dict(self, recognizer_dict: Dict) -&gt; None:\n        \"\"\"\n        Load a pattern recognizer from a Dict into the recognizer registry.\n\n        :param recognizer_dict: Dict holding a serialization of an PatternRecognizer\n\n        :example:\n        &gt;&gt;&gt; registry = RecognizerRegistry()\n        &gt;&gt;&gt; recognizer = { \"name\": \"Titles Recognizer\", \"supported_language\": \"de\",\"supported_entity\": \"TITLE\", \"deny_list\": [\"Mr.\",\"Mrs.\"]} # noqa: E501\n        &gt;&gt;&gt; registry.add_pattern_recognizer_from_dict(recognizer)\n        \"\"\"\n\n        recognizer = PatternRecognizer.from_dict(recognizer_dict)\n        self.add_recognizer(recognizer)\n\n    def add_recognizers_from_yaml(self, yml_path: Union[str, Path]) -&gt; None:\n        r\"\"\"\n        Read YAML file and load recognizers into the recognizer registry.\n\n        See example yaml file here:\n        https://github.com/microsoft/presidio/blob/main/presidio-analyzer/conf/example_recognizers.yaml\n\n        :example:\n        &gt;&gt;&gt; yaml_file = \"recognizers.yaml\"\n        &gt;&gt;&gt; registry = RecognizerRegistry()\n        &gt;&gt;&gt; registry.add_recognizers_from_yaml(yaml_file)\n\n        \"\"\"\n\n        try:\n            with open(yml_path, \"r\") as stream:\n                yaml_recognizers = yaml.safe_load(stream)\n\n            for yaml_recognizer in yaml_recognizers[\"recognizers\"]:\n                self.add_pattern_recognizer_from_dict(yaml_recognizer)\n        except IOError as io_error:\n            print(f\"Error reading file {yml_path}\")\n            raise io_error\n        except yaml.YAMLError as yaml_error:\n            print(f\"Failed to parse file {yml_path}\")\n            raise yaml_error\n        except TypeError as yaml_error:\n            print(f\"Failed to parse file {yml_path}\")\n            raise yaml_error\n\n    def __instantiate_recognizer(\n        self, recognizer_class: Type[EntityRecognizer], supported_language: str\n    ):\n        \"\"\"\n        Instantiate a recognizer class given type and input.\n\n        :param recognizer_class: Class object of the recognizer\n        :param supported_language: Language this recognizer should support\n        \"\"\"\n\n        inst = recognizer_class(supported_language=supported_language)\n        if isinstance(inst, PatternRecognizer):\n            inst.global_regex_flags = self.global_regex_flags\n        return inst\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerRegistry.__instantiate_recognizer","title":"<code>__instantiate_recognizer(recognizer_class, supported_language)</code>","text":"<p>Instantiate a recognizer class given type and input.</p> <p>Parameters:</p> Name Type Description Default <code>recognizer_class</code> <code>Type[EntityRecognizer]</code> <p>Class object of the recognizer</p> required <code>supported_language</code> <code>str</code> <p>Language this recognizer should support</p> required Source code in <code>presidio_analyzer/recognizer_registry.py</code> <pre><code>def __instantiate_recognizer(\n    self, recognizer_class: Type[EntityRecognizer], supported_language: str\n):\n    \"\"\"\n    Instantiate a recognizer class given type and input.\n\n    :param recognizer_class: Class object of the recognizer\n    :param supported_language: Language this recognizer should support\n    \"\"\"\n\n    inst = recognizer_class(supported_language=supported_language)\n    if isinstance(inst, PatternRecognizer):\n        inst.global_regex_flags = self.global_regex_flags\n    return inst\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerRegistry.add_pattern_recognizer_from_dict","title":"<code>add_pattern_recognizer_from_dict(recognizer_dict)</code>","text":"<p>Load a pattern recognizer from a Dict into the recognizer registry.</p> <p>:example:</p> <p>registry = RecognizerRegistry() recognizer = { \"name\": \"Titles Recognizer\", \"supported_language\": \"de\",\"supported_entity\": \"TITLE\", \"deny_list\": [\"Mr.\",\"Mrs.\"]} # noqa: E501 registry.add_pattern_recognizer_from_dict(recognizer)</p> <p>Parameters:</p> Name Type Description Default <code>recognizer_dict</code> <code>Dict</code> <p>Dict holding a serialization of an PatternRecognizer</p> required Source code in <code>presidio_analyzer/recognizer_registry.py</code> <pre><code>def add_pattern_recognizer_from_dict(self, recognizer_dict: Dict) -&gt; None:\n    \"\"\"\n    Load a pattern recognizer from a Dict into the recognizer registry.\n\n    :param recognizer_dict: Dict holding a serialization of an PatternRecognizer\n\n    :example:\n    &gt;&gt;&gt; registry = RecognizerRegistry()\n    &gt;&gt;&gt; recognizer = { \"name\": \"Titles Recognizer\", \"supported_language\": \"de\",\"supported_entity\": \"TITLE\", \"deny_list\": [\"Mr.\",\"Mrs.\"]} # noqa: E501\n    &gt;&gt;&gt; registry.add_pattern_recognizer_from_dict(recognizer)\n    \"\"\"\n\n    recognizer = PatternRecognizer.from_dict(recognizer_dict)\n    self.add_recognizer(recognizer)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerRegistry.add_recognizer","title":"<code>add_recognizer(recognizer)</code>","text":"<p>Add a new recognizer to the list of recognizers.</p> <p>Parameters:</p> Name Type Description Default <code>recognizer</code> <code>EntityRecognizer</code> <p>Recognizer to add</p> required Source code in <code>presidio_analyzer/recognizer_registry.py</code> <pre><code>def add_recognizer(self, recognizer: EntityRecognizer) -&gt; None:\n    \"\"\"\n    Add a new recognizer to the list of recognizers.\n\n    :param recognizer: Recognizer to add\n    \"\"\"\n    if not isinstance(recognizer, EntityRecognizer):\n        raise ValueError(\"Input is not of type EntityRecognizer\")\n\n    self.recognizers.append(recognizer)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerRegistry.add_recognizers_from_yaml","title":"<code>add_recognizers_from_yaml(yml_path)</code>","text":"<p>Read YAML file and load recognizers into the recognizer registry.</p> <p>See example yaml file here: https://github.com/microsoft/presidio/blob/main/presidio-analyzer/conf/example_recognizers.yaml</p> <p>:example:</p> <p>yaml_file = \"recognizers.yaml\" registry = RecognizerRegistry() registry.add_recognizers_from_yaml(yaml_file)</p> Source code in <code>presidio_analyzer/recognizer_registry.py</code> <pre><code>def add_recognizers_from_yaml(self, yml_path: Union[str, Path]) -&gt; None:\n    r\"\"\"\n    Read YAML file and load recognizers into the recognizer registry.\n\n    See example yaml file here:\n    https://github.com/microsoft/presidio/blob/main/presidio-analyzer/conf/example_recognizers.yaml\n\n    :example:\n    &gt;&gt;&gt; yaml_file = \"recognizers.yaml\"\n    &gt;&gt;&gt; registry = RecognizerRegistry()\n    &gt;&gt;&gt; registry.add_recognizers_from_yaml(yaml_file)\n\n    \"\"\"\n\n    try:\n        with open(yml_path, \"r\") as stream:\n            yaml_recognizers = yaml.safe_load(stream)\n\n        for yaml_recognizer in yaml_recognizers[\"recognizers\"]:\n            self.add_pattern_recognizer_from_dict(yaml_recognizer)\n    except IOError as io_error:\n        print(f\"Error reading file {yml_path}\")\n        raise io_error\n    except yaml.YAMLError as yaml_error:\n        print(f\"Failed to parse file {yml_path}\")\n        raise yaml_error\n    except TypeError as yaml_error:\n        print(f\"Failed to parse file {yml_path}\")\n        raise yaml_error\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerRegistry.get_recognizers","title":"<code>get_recognizers(language, entities=None, all_fields=False, ad_hoc_recognizers=None)</code>","text":"<p>Return a list of recognizers which supports the specified name and language.</p> <p>Parameters:</p> Name Type Description Default <code>entities</code> <code>Optional[List[str]]</code> <p>the requested entities</p> <code>None</code> <code>language</code> <code>str</code> <p>the requested language</p> required <code>all_fields</code> <code>bool</code> <p>a flag to return all fields of a requested language.</p> <code>False</code> <code>ad_hoc_recognizers</code> <code>Optional[List[EntityRecognizer]]</code> <p>Additional recognizers provided by the user as part of the request</p> <code>None</code> <p>Returns:</p> Type Description <code>List[EntityRecognizer]</code> <p>A list of the recognizers which supports the supplied entities and language</p> Source code in <code>presidio_analyzer/recognizer_registry.py</code> <pre><code>def get_recognizers(\n    self,\n    language: str,\n    entities: Optional[List[str]] = None,\n    all_fields: bool = False,\n    ad_hoc_recognizers: Optional[List[EntityRecognizer]] = None,\n) -&gt; List[EntityRecognizer]:\n    \"\"\"\n    Return a list of recognizers which supports the specified name and language.\n\n    :param entities: the requested entities\n    :param language: the requested language\n    :param all_fields: a flag to return all fields of a requested language.\n    :param ad_hoc_recognizers: Additional recognizers provided by the user\n    as part of the request\n    :return: A list of the recognizers which supports the supplied entities\n    and language\n    \"\"\"\n    if language is None:\n        raise ValueError(\"No language provided\")\n\n    if entities is None and all_fields is False:\n        raise ValueError(\"No entities provided\")\n\n    all_possible_recognizers = copy.copy(self.recognizers)\n    if ad_hoc_recognizers:\n        all_possible_recognizers.extend(ad_hoc_recognizers)\n\n    # filter out unwanted recognizers\n    to_return = set()\n    if all_fields:\n        to_return = [\n            rec\n            for rec in all_possible_recognizers\n            if language == rec.supported_language\n        ]\n    else:\n        for entity in entities:\n            subset = [\n                rec\n                for rec in all_possible_recognizers\n                if entity in rec.supported_entities\n                and language == rec.supported_language\n            ]\n\n            if not subset:\n                logger.warning(\n                    \"Entity %s doesn't have the corresponding\"\n                    \" recognizer in language : %s\",\n                    entity,\n                    language,\n                )\n            else:\n                to_return.update(set(subset))\n\n    logger.debug(\n        \"Returning a total of %s recognizers\",\n        str(len(to_return)),\n    )\n\n    if not to_return:\n        raise ValueError(\"No matching recognizers were found to serve the request.\")\n\n    return list(to_return)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerRegistry.load_predefined_recognizers","title":"<code>load_predefined_recognizers(languages=None, nlp_engine=None)</code>","text":"<p>Load the existing recognizers into memory.</p> <p>Parameters:</p> Name Type Description Default <code>languages</code> <code>Optional[List[str]]</code> <p>List of languages for which to load recognizers</p> <code>None</code> <code>nlp_engine</code> <code>NlpEngine</code> <p>The NLP engine to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>presidio_analyzer/recognizer_registry.py</code> <pre><code>def load_predefined_recognizers(\n    self, languages: Optional[List[str]] = None, nlp_engine: NlpEngine = None\n) -&gt; None:\n    \"\"\"\n    Load the existing recognizers into memory.\n\n    :param languages: List of languages for which to load recognizers\n    :param nlp_engine: The NLP engine to use.\n    :return: None\n    \"\"\"\n    if not languages:\n        languages = [\"en\"]\n\n    nlp_recognizer = self._get_nlp_recognizer(nlp_engine)\n\n    recognizers_map = {\n        \"en\": [\n            UsBankRecognizer,\n            UsLicenseRecognizer,\n            UsItinRecognizer,\n            UsPassportRecognizer,\n            UsSsnRecognizer,\n            NhsRecognizer,\n            SgFinRecognizer,\n            AuAbnRecognizer,\n            AuAcnRecognizer,\n            AuTfnRecognizer,\n            AuMedicareRecognizer,\n            InPanRecognizer,\n        ],\n        \"es\": [EsNifRecognizer],\n        \"it\": [\n            ItDriverLicenseRecognizer,\n            ItFiscalCodeRecognizer,\n            ItVatCodeRecognizer,\n            ItIdentityCardRecognizer,\n            ItPassportRecognizer,\n        ],\n        \"ALL\": [\n            CreditCardRecognizer,\n            CryptoRecognizer,\n            DateRecognizer,\n            EmailRecognizer,\n            IbanRecognizer,\n            IpRecognizer,\n            MedicalLicenseRecognizer,\n            PhoneRecognizer,\n            UrlRecognizer,\n        ],\n    }\n    for lang in languages:\n        lang_recognizers = [\n            self.__instantiate_recognizer(\n                recognizer_class=rc, supported_language=lang\n            )\n            for rc in recognizers_map.get(lang, [])\n        ]\n        self.recognizers.extend(lang_recognizers)\n        all_recognizers = [\n            self.__instantiate_recognizer(\n                recognizer_class=rc, supported_language=lang\n            )\n            for rc in recognizers_map.get(\"ALL\", [])\n        ]\n        self.recognizers.extend(all_recognizers)\n        if nlp_engine:\n            nlp_recognizer_inst = nlp_recognizer(\n                supported_language=lang,\n                supported_entities=nlp_engine.get_supported_entities(),\n            )\n        else:\n            nlp_recognizer_inst = nlp_recognizer(supported_language=lang)\n        self.recognizers.append(nlp_recognizer_inst)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerRegistry.remove_recognizer","title":"<code>remove_recognizer(recognizer_name)</code>","text":"<p>Remove a recognizer based on its name.</p> <p>Parameters:</p> Name Type Description Default <code>recognizer_name</code> <code>str</code> <p>Name of recognizer to remove</p> required Source code in <code>presidio_analyzer/recognizer_registry.py</code> <pre><code>def remove_recognizer(self, recognizer_name: str) -&gt; None:\n    \"\"\"\n    Remove a recognizer based on its name.\n\n    :param recognizer_name: Name of recognizer to remove\n    \"\"\"\n    new_recognizers = [\n        rec for rec in self.recognizers if rec.name != recognizer_name\n    ]\n    logger.info(\n        \"Removed %s recognizers which had the name %s\",\n        str(len(self.recognizers) - len(new_recognizers)),\n        recognizer_name,\n    )\n    self.recognizers = new_recognizers\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult","title":"<code>RecognizerResult</code>","text":"<p>Recognizer Result represents the findings of the detected entity.</p> <p>Result of a recognizer analyzing the text.</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>the type of the entity</p> required <code>start</code> <code>int</code> <p>the start location of the detected entity</p> required <code>end</code> <code>int</code> <p>the end location of the detected entity</p> required <code>score</code> <code>float</code> <p>the score of the detection</p> required <code>analysis_explanation</code> <code>AnalysisExplanation</code> <p>contains the explanation of why this entity was identified</p> <code>None</code> <code>recognition_metadata</code> <code>Dict</code> <p>a dictionary of metadata to be used in recognizer specific cases, for example specific recognized context words and recognizer name</p> <code>None</code> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>class RecognizerResult:\n    \"\"\"\n    Recognizer Result represents the findings of the detected entity.\n\n    Result of a recognizer analyzing the text.\n\n    :param entity_type: the type of the entity\n    :param start: the start location of the detected entity\n    :param end: the end location of the detected entity\n    :param score: the score of the detection\n    :param analysis_explanation: contains the explanation of why this\n                                 entity was identified\n    :param recognition_metadata: a dictionary of metadata to be used in\n    recognizer specific cases, for example specific recognized context words\n    and recognizer name\n    \"\"\"\n\n    # Keys for recognizer metadata\n    RECOGNIZER_NAME_KEY = \"recognizer_name\"\n    RECOGNIZER_IDENTIFIER_KEY = \"recognizer_identifier\"\n\n    # Key of a flag inside recognition_metadata dictionary\n    # which is set to true if the result enhanced by context\n    IS_SCORE_ENHANCED_BY_CONTEXT_KEY = \"is_score_enhanced_by_context\"\n\n    logger = logging.getLogger(\"presidio-analyzer\")\n\n    def __init__(\n        self,\n        entity_type: str,\n        start: int,\n        end: int,\n        score: float,\n        analysis_explanation: AnalysisExplanation = None,\n        recognition_metadata: Dict = None,\n    ):\n\n        self.entity_type = entity_type\n        self.start = start\n        self.end = end\n        self.score = score\n        self.analysis_explanation = analysis_explanation\n\n        if not recognition_metadata:\n            self.logger.debug(\n                \"recognition_metadata should be passed, \"\n                \"containing a recognizer_name value\"\n            )\n\n        self.recognition_metadata = recognition_metadata\n\n    def append_analysis_explanation_text(self, text: str) -&gt; None:\n        \"\"\"Add text to the analysis explanation.\"\"\"\n        if self.analysis_explanation:\n            self.analysis_explanation.append_textual_explanation_line(text)\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"\n        Serialize self to dictionary.\n\n        :return: a dictionary\n        \"\"\"\n        return self.__dict__\n\n    @classmethod\n    def from_json(cls, data: Dict) -&gt; \"RecognizerResult\":\n        \"\"\"\n        Create RecognizerResult from json.\n\n        :param data: e.g. {\n            \"start\": 24,\n            \"end\": 32,\n            \"score\": 0.8,\n            \"entity_type\": \"NAME\"\n        }\n        :return: RecognizerResult\n        \"\"\"\n        score = data.get(\"score\")\n        entity_type = data.get(\"entity_type\")\n        start = data.get(\"start\")\n        end = data.get(\"end\")\n        return cls(entity_type, start, end, score)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the instance.\"\"\"\n        return self.__str__()\n\n    def intersects(self, other: \"RecognizerResult\") -&gt; int:\n        \"\"\"\n        Check if self intersects with a different RecognizerResult.\n\n        :return: If intersecting, returns the number of\n        intersecting characters.\n        If not, returns 0\n        \"\"\"\n        # if they do not overlap the intersection is 0\n        if self.end &lt; other.start or other.end &lt; self.start:\n            return 0\n\n        # otherwise the intersection is min(end) - max(start)\n        return min(self.end, other.end) - max(self.start, other.start)\n\n    def contained_in(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check if self is contained in a different RecognizerResult.\n\n        :return: true if contained\n        \"\"\"\n        return self.start &gt;= other.start and self.end &lt;= other.end\n\n    def contains(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check if one result is contained or equal to another result.\n\n        :param other: another RecognizerResult\n        :return: bool\n        \"\"\"\n        return self.start &lt;= other.start and self.end &gt;= other.end\n\n    def equal_indices(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check if the indices are equal between two results.\n\n        :param other: another RecognizerResult\n        :return:\n        \"\"\"\n        return self.start == other.start and self.end == other.end\n\n    def __gt__(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check if one result is greater by using the results indices in the text.\n\n        :param other: another RecognizerResult\n        :return: bool\n        \"\"\"\n        if self.start == other.start:\n            return self.end &gt; other.end\n        return self.start &gt; other.start\n\n    def __eq__(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check two results are equal by using all class fields.\n\n        :param other: another RecognizerResult\n        :return: bool\n        \"\"\"\n        equal_type = self.entity_type == other.entity_type\n        equal_score = self.score == other.score\n        return self.equal_indices(other) and equal_type and equal_score\n\n    def __hash__(self):\n        \"\"\"\n        Hash the result data by using all class fields.\n\n        :return: int\n        \"\"\"\n        return hash(\n            f\"{str(self.start)} {str(self.end)} {str(self.score)} {self.entity_type}\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the instance.\"\"\"\n        return (\n            f\"type: {self.entity_type}, \"\n            f\"start: {self.start}, \"\n            f\"end: {self.end}, \"\n            f\"score: {self.score}\"\n        )\n\n    def has_conflict(self, other: \"RecognizerResult\") -&gt; bool:\n        \"\"\"\n        Check if two recognizer results are conflicted or not.\n\n        I have a conflict if:\n        1. My indices are the same as the other and my score is lower.\n        2. If my indices are contained in another.\n\n        :param other: RecognizerResult\n        :return:\n        \"\"\"\n        if self.equal_indices(other):\n            return self.score &lt;= other.score\n        return other.contains(self)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check two results are equal by using all class fields.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>RecognizerResult</code> <p>another RecognizerResult</p> required <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def __eq__(self, other: \"RecognizerResult\") -&gt; bool:\n    \"\"\"\n    Check two results are equal by using all class fields.\n\n    :param other: another RecognizerResult\n    :return: bool\n    \"\"\"\n    equal_type = self.entity_type == other.entity_type\n    equal_score = self.score == other.score\n    return self.equal_indices(other) and equal_type and equal_score\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.__gt__","title":"<code>__gt__(other)</code>","text":"<p>Check if one result is greater by using the results indices in the text.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>RecognizerResult</code> <p>another RecognizerResult</p> required <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def __gt__(self, other: \"RecognizerResult\") -&gt; bool:\n    \"\"\"\n    Check if one result is greater by using the results indices in the text.\n\n    :param other: another RecognizerResult\n    :return: bool\n    \"\"\"\n    if self.start == other.start:\n        return self.end &gt; other.end\n    return self.start &gt; other.start\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash the result data by using all class fields.</p> <p>Returns:</p> Type Description <p>int</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def __hash__(self):\n    \"\"\"\n    Hash the result data by using all class fields.\n\n    :return: int\n    \"\"\"\n    return hash(\n        f\"{str(self.start)} {str(self.end)} {str(self.score)} {self.entity_type}\"\n    )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the instance.</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a string representation of the instance.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the instance.</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the instance.\"\"\"\n    return (\n        f\"type: {self.entity_type}, \"\n        f\"start: {self.start}, \"\n        f\"end: {self.end}, \"\n        f\"score: {self.score}\"\n    )\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.append_analysis_explanation_text","title":"<code>append_analysis_explanation_text(text)</code>","text":"<p>Add text to the analysis explanation.</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def append_analysis_explanation_text(self, text: str) -&gt; None:\n    \"\"\"Add text to the analysis explanation.\"\"\"\n    if self.analysis_explanation:\n        self.analysis_explanation.append_textual_explanation_line(text)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.contained_in","title":"<code>contained_in(other)</code>","text":"<p>Check if self is contained in a different RecognizerResult.</p> <p>Returns:</p> Type Description <code>bool</code> <p>true if contained</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def contained_in(self, other: \"RecognizerResult\") -&gt; bool:\n    \"\"\"\n    Check if self is contained in a different RecognizerResult.\n\n    :return: true if contained\n    \"\"\"\n    return self.start &gt;= other.start and self.end &lt;= other.end\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.contains","title":"<code>contains(other)</code>","text":"<p>Check if one result is contained or equal to another result.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>RecognizerResult</code> <p>another RecognizerResult</p> required <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def contains(self, other: \"RecognizerResult\") -&gt; bool:\n    \"\"\"\n    Check if one result is contained or equal to another result.\n\n    :param other: another RecognizerResult\n    :return: bool\n    \"\"\"\n    return self.start &lt;= other.start and self.end &gt;= other.end\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.equal_indices","title":"<code>equal_indices(other)</code>","text":"<p>Check if the indices are equal between two results.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>RecognizerResult</code> <p>another RecognizerResult</p> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def equal_indices(self, other: \"RecognizerResult\") -&gt; bool:\n    \"\"\"\n    Check if the indices are equal between two results.\n\n    :param other: another RecognizerResult\n    :return:\n    \"\"\"\n    return self.start == other.start and self.end == other.end\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Create RecognizerResult from json.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict</code> <p>e.g. { \"start\": 24, \"end\": 32, \"score\": 0.8, \"entity_type\": \"NAME\" }</p> required <p>Returns:</p> Type Description <code>RecognizerResult</code> <p>RecognizerResult</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>@classmethod\ndef from_json(cls, data: Dict) -&gt; \"RecognizerResult\":\n    \"\"\"\n    Create RecognizerResult from json.\n\n    :param data: e.g. {\n        \"start\": 24,\n        \"end\": 32,\n        \"score\": 0.8,\n        \"entity_type\": \"NAME\"\n    }\n    :return: RecognizerResult\n    \"\"\"\n    score = data.get(\"score\")\n    entity_type = data.get(\"entity_type\")\n    start = data.get(\"start\")\n    end = data.get(\"end\")\n    return cls(entity_type, start, end, score)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.has_conflict","title":"<code>has_conflict(other)</code>","text":"<p>Check if two recognizer results are conflicted or not.</p> <p>I have a conflict if: 1. My indices are the same as the other and my score is lower. 2. If my indices are contained in another.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>RecognizerResult</code> <p>RecognizerResult</p> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def has_conflict(self, other: \"RecognizerResult\") -&gt; bool:\n    \"\"\"\n    Check if two recognizer results are conflicted or not.\n\n    I have a conflict if:\n    1. My indices are the same as the other and my score is lower.\n    2. If my indices are contained in another.\n\n    :param other: RecognizerResult\n    :return:\n    \"\"\"\n    if self.equal_indices(other):\n        return self.score &lt;= other.score\n    return other.contains(self)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.intersects","title":"<code>intersects(other)</code>","text":"<p>Check if self intersects with a different RecognizerResult.</p> <p>Returns:</p> Type Description <code>int</code> <p>If intersecting, returns the number of intersecting characters. If not, returns 0</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def intersects(self, other: \"RecognizerResult\") -&gt; int:\n    \"\"\"\n    Check if self intersects with a different RecognizerResult.\n\n    :return: If intersecting, returns the number of\n    intersecting characters.\n    If not, returns 0\n    \"\"\"\n    # if they do not overlap the intersection is 0\n    if self.end &lt; other.start or other.end &lt; self.start:\n        return 0\n\n    # otherwise the intersection is min(end) - max(start)\n    return min(self.end, other.end) - max(self.start, other.start)\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RecognizerResult.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize self to dictionary.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>a dictionary</p> Source code in <code>presidio_analyzer/recognizer_result.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"\n    Serialize self to dictionary.\n\n    :return: a dictionary\n    \"\"\"\n    return self.__dict__\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RemoteRecognizer","title":"<code>RemoteRecognizer</code>","text":"<p>             Bases: <code>ABC</code>, <code>EntityRecognizer</code></p> <p>A configuration for a recognizer that runs on a different process / remote machine.</p> <p>Parameters:</p> Name Type Description Default <code>supported_entities</code> <code>List[str]</code> <p>A list of entities this recognizer can identify</p> required <code>name</code> <code>Optional[str]</code> <p>name of recognizer</p> required <code>supported_language</code> <code>str</code> <p>The language this recognizer can detect entities in</p> required <code>version</code> <code>str</code> <p>Version of this recognizer</p> required Source code in <code>presidio_analyzer/remote_recognizer.py</code> <pre><code>class RemoteRecognizer(ABC, EntityRecognizer):\n    \"\"\"\n    A configuration for a recognizer that runs on a different process / remote machine.\n\n    :param supported_entities: A list of entities this recognizer can identify\n    :param name: name of recognizer\n    :param supported_language: The language this recognizer can detect entities in\n    :param version: Version of this recognizer\n    \"\"\"\n\n    def __init__(\n        self,\n        supported_entities: List[str],\n        name: Optional[str],\n        supported_language: str,\n        version: str,\n        context: Optional[List[str]] = None,\n    ):\n        super().__init__(\n            supported_entities=supported_entities,\n            name=name,\n            supported_language=supported_language,\n            version=version,\n            context=context,\n        )\n\n    @abstractmethod\n    def load(self):  # noqa D102\n        pass\n\n    @abstractmethod\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ):  # noqa ANN201\n        \"\"\"\n        Call an external service for PII detection.\n\n        :param text: text to be analyzed\n        :param entities: Entities that should be looked for\n        :param nlp_artifacts: Additional metadata from the NLP engine\n        :return: List of identified PII entities\n        \"\"\"\n\n        # 1. Call the external service.\n        # 2. Translate results into List[RecognizerResult]\n        pass\n\n    @abstractmethod\n    def get_supported_entities(self) -&gt; List[str]:  # noqa D102\n        pass\n</code></pre>"},{"location":"api/analyzer_python/#presidio_analyzer.RemoteRecognizer.analyze","title":"<code>analyze(text, entities, nlp_artifacts)</code>  <code>abstractmethod</code>","text":"<p>Call an external service for PII detection.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text to be analyzed</p> required <code>entities</code> <code>List[str]</code> <p>Entities that should be looked for</p> required <code>nlp_artifacts</code> <code>NlpArtifacts</code> <p>Additional metadata from the NLP engine</p> required <p>Returns:</p> Type Description <p>List of identified PII entities</p> Source code in <code>presidio_analyzer/remote_recognizer.py</code> <pre><code>@abstractmethod\ndef analyze(\n    self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n):  # noqa ANN201\n    \"\"\"\n    Call an external service for PII detection.\n\n    :param text: text to be analyzed\n    :param entities: Entities that should be looked for\n    :param nlp_artifacts: Additional metadata from the NLP engine\n    :return: List of identified PII entities\n    \"\"\"\n\n    # 1. Call the external service.\n    # 2. Translate results into List[RecognizerResult]\n    pass\n</code></pre>"},{"location":"api/anonymizer_python/","title":"Presidio Anonymizer API Reference","text":"<p>Anonymizer root module.</p>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine","title":"<code>AnonymizerEngine</code>","text":"<p>             Bases: <code>EngineBase</code></p> <p>AnonymizerEngine class.</p> <p>Handles the entire logic of the Presidio-anonymizer. Gets the original text and replaces the PII entities with the desired anonymizers.</p> Source code in <code>presidio_anonymizer/anonymizer_engine.py</code> <pre><code>class AnonymizerEngine(EngineBase):\n    \"\"\"\n    AnonymizerEngine class.\n\n    Handles the entire logic of the Presidio-anonymizer. Gets the original text\n    and replaces the PII entities with the desired anonymizers.\n    \"\"\"\n\n    logger = logging.getLogger(\"presidio-anonymizer\")\n\n    def __init__(self):\n        EngineBase.__init__(self)\n\n    def anonymize(\n            self,\n            text: str,\n            analyzer_results: List[RecognizerResult],\n            operators: Optional[Dict[str, OperatorConfig]] = None,\n    ) -&gt; EngineResult:\n        \"\"\"Anonymize method to anonymize the given text.\n\n        :param text: the text we are anonymizing\n        :param analyzer_results: A list of RecognizerResult class -&gt; The results we\n        received from the analyzer\n        :param operators: The configuration of the anonymizers we would like\n        to use for each entity e.g.: {\"PHONE_NUMBER\":OperatorConfig(\"redact\", {})}\n        received from the analyzer\n        :return: the anonymized text and a list of information about the\n        anonymized entities.\n\n        :example:\n\n        &gt;&gt;&gt; from presidio_anonymizer import AnonymizerEngine\n        &gt;&gt;&gt; from presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n\n        &gt;&gt;&gt; # Initialize the engine with logger.\n        &gt;&gt;&gt; engine = AnonymizerEngine()\n\n        &gt;&gt;&gt; # Invoke the anonymize function with the text, analyzer results and\n        &gt;&gt;&gt; # Operators to define the anonymization type.\n        &gt;&gt;&gt; result = engine.anonymize(\n        &gt;&gt;&gt;     text=\"My name is Bond, James Bond\",\n        &gt;&gt;&gt;     analyzer_results=[RecognizerResult(entity_type=\"PERSON\",\n        &gt;&gt;&gt;                                        start=11,\n        &gt;&gt;&gt;                                        end=15,\n        &gt;&gt;&gt;                                        score=0.8),\n        &gt;&gt;&gt;                       RecognizerResult(entity_type=\"PERSON\",\n        &gt;&gt;&gt;                                        start=17,\n        &gt;&gt;&gt;                                        end=27,\n        &gt;&gt;&gt;                                        score=0.8)],\n        &gt;&gt;&gt;     operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"BIP\"})}\n        &gt;&gt;&gt; )\n\n        &gt;&gt;&gt; print(result)\n        text: My name is BIP, BIP.\n        items:\n        [\n            {'start': 16, 'end': 19, 'entity_type': 'PERSON',\n             'text': 'BIP', 'operator': 'replace'},\n            {'start': 11, 'end': 14, 'entity_type': 'PERSON',\n             'text': 'BIP', 'operator': 'replace'}\n        ]\n\n\n        \"\"\"\n        analyzer_results = self._remove_conflicts_and_get_text_manipulation_data(\n            analyzer_results\n        )\n\n        merged_results = self._merge_entities_with_whitespace_between(\n                text, analyzer_results\n        )\n\n        operators = self.__check_or_add_default_operator(operators)\n\n        return self._operate(text, merged_results, operators, OperatorType.Anonymize)\n\n    def _remove_conflicts_and_get_text_manipulation_data(\n            self, analyzer_results: List[RecognizerResult]\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Iterate the list and create a sorted unique results list from it.\n\n        Only insert results which are:\n        1. Indices are not contained in other result.\n        2. Have the same indices as other results but with larger score.\n        :return: List\n        \"\"\"\n        tmp_analyzer_results = []\n        # This list contains all elements which we need to check a single result\n        # against. If a result is dropped, it can also be dropped from this list\n        # since it is intersecting with another result and we selected the other one.\n        other_elements = analyzer_results.copy()\n        for result in analyzer_results:\n            other_elements.remove(result)\n\n            is_merge_same_entity_type = False\n            for other_element in other_elements:\n                if other_element.entity_type != result.entity_type:\n                    continue\n                if result.intersects(other_element) == 0:\n                    continue\n\n                other_element.start = min(result.start, other_element.start)\n                other_element.end = max(result.end, other_element.end)\n                other_element.score = max(result.score, other_element.score)\n                is_merge_same_entity_type = True\n                break\n            if not is_merge_same_entity_type:\n                other_elements.append(result)\n                tmp_analyzer_results.append(result)\n            else:\n                self.logger.debug(f\"removing element {result} from \"\n                                  f\"results list due to merge\")\n\n        unique_text_metadata_elements = []\n        # This list contains all elements which we need to check a single result\n        # against. If a result is dropped, it can also be dropped from this list\n        # since it is intersecting with another result and we selected the other one.\n        other_elements = tmp_analyzer_results.copy()\n        for result in tmp_analyzer_results:\n            other_elements.remove(result)\n            result_conflicted = self.__is_result_conflicted_with_other_elements(\n                other_elements, result\n            )\n            if not result_conflicted:\n                other_elements.append(result)\n                unique_text_metadata_elements.append(result)\n            else:\n                self.logger.debug(\n                    f\"removing element {result} from results list due to conflict\"\n                )\n        return unique_text_metadata_elements\n\n    def _merge_entities_with_whitespace_between(\n        self,\n        text: str,\n        analyzer_results: List[RecognizerResult]\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"Merge adjacent entities of the same type separated by whitespace.\"\"\"\n        merged_results = []\n        prev_result = None\n        for result in analyzer_results:\n            if prev_result is not None:\n                if prev_result.entity_type == result.entity_type:\n                    if re.search(r'^( )+$', text[prev_result.end:result.start]):\n                        merged_results.remove(prev_result)\n                        result.start = prev_result.start\n            merged_results.append(result)\n            prev_result = result\n        return merged_results\n\n    def get_anonymizers(self) -&gt; List[str]:\n        \"\"\"Return a list of supported anonymizers.\"\"\"\n        names = [p for p in self.operators_factory.get_anonymizers().keys()]\n        return names\n\n    @staticmethod\n    def __is_result_conflicted_with_other_elements(other_elements, result):\n        return any(\n            [result.has_conflict(other_element) for other_element in other_elements]\n        )\n\n    @staticmethod\n    def __check_or_add_default_operator(\n            operators: Dict[str, OperatorConfig]\n    ) -&gt; Dict[str, OperatorConfig]:\n        default_operator = OperatorConfig(DEFAULT)\n        if not operators:\n            return {\"DEFAULT\": default_operator}\n        if not operators.get(\"DEFAULT\"):\n            operators[\"DEFAULT\"] = default_operator\n        return operators\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.anonymize","title":"<code>anonymize(text, analyzer_results, operators=None)</code>","text":"<p>Anonymize method to anonymize the given text.</p> <p>:example:</p> <p>from presidio_anonymizer import AnonymizerEngine from presidio_anonymizer.entities import RecognizerResult, OperatorConfig</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>the text we are anonymizing</p> required <code>analyzer_results</code> <code>List[RecognizerResult]</code> <p>A list of RecognizerResult class -&gt; The results we received from the analyzer</p> required <code>operators</code> <code>Optional[Dict[str, OperatorConfig]]</code> <p>The configuration of the anonymizers we would like to use for each entity e.g.: {\"PHONE_NUMBER\":OperatorConfig(\"redact\", {})} received from the analyzer</p> <code>None</code> <p>Returns:</p> Type Description <code>EngineResult</code> <p>the anonymized text and a list of information about the anonymized entities.</p> Source code in <code>presidio_anonymizer/anonymizer_engine.py</code> <pre><code>def anonymize(\n        self,\n        text: str,\n        analyzer_results: List[RecognizerResult],\n        operators: Optional[Dict[str, OperatorConfig]] = None,\n) -&gt; EngineResult:\n    \"\"\"Anonymize method to anonymize the given text.\n\n    :param text: the text we are anonymizing\n    :param analyzer_results: A list of RecognizerResult class -&gt; The results we\n    received from the analyzer\n    :param operators: The configuration of the anonymizers we would like\n    to use for each entity e.g.: {\"PHONE_NUMBER\":OperatorConfig(\"redact\", {})}\n    received from the analyzer\n    :return: the anonymized text and a list of information about the\n    anonymized entities.\n\n    :example:\n\n    &gt;&gt;&gt; from presidio_anonymizer import AnonymizerEngine\n    &gt;&gt;&gt; from presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n\n    &gt;&gt;&gt; # Initialize the engine with logger.\n    &gt;&gt;&gt; engine = AnonymizerEngine()\n\n    &gt;&gt;&gt; # Invoke the anonymize function with the text, analyzer results and\n    &gt;&gt;&gt; # Operators to define the anonymization type.\n    &gt;&gt;&gt; result = engine.anonymize(\n    &gt;&gt;&gt;     text=\"My name is Bond, James Bond\",\n    &gt;&gt;&gt;     analyzer_results=[RecognizerResult(entity_type=\"PERSON\",\n    &gt;&gt;&gt;                                        start=11,\n    &gt;&gt;&gt;                                        end=15,\n    &gt;&gt;&gt;                                        score=0.8),\n    &gt;&gt;&gt;                       RecognizerResult(entity_type=\"PERSON\",\n    &gt;&gt;&gt;                                        start=17,\n    &gt;&gt;&gt;                                        end=27,\n    &gt;&gt;&gt;                                        score=0.8)],\n    &gt;&gt;&gt;     operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"BIP\"})}\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; print(result)\n    text: My name is BIP, BIP.\n    items:\n    [\n        {'start': 16, 'end': 19, 'entity_type': 'PERSON',\n         'text': 'BIP', 'operator': 'replace'},\n        {'start': 11, 'end': 14, 'entity_type': 'PERSON',\n         'text': 'BIP', 'operator': 'replace'}\n    ]\n\n\n    \"\"\"\n    analyzer_results = self._remove_conflicts_and_get_text_manipulation_data(\n        analyzer_results\n    )\n\n    merged_results = self._merge_entities_with_whitespace_between(\n            text, analyzer_results\n    )\n\n    operators = self.__check_or_add_default_operator(operators)\n\n    return self._operate(text, merged_results, operators, OperatorType.Anonymize)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.anonymize--initialize-the-engine-with-logger","title":"Initialize the engine with logger.","text":"<p>engine = AnonymizerEngine()</p>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.anonymize--invoke-the-anonymize-function-with-the-text-analyzer-results-and","title":"Invoke the anonymize function with the text, analyzer results and","text":""},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.anonymize--operators-to-define-the-anonymization-type","title":"Operators to define the anonymization type.","text":"<p>result = engine.anonymize(     text=\"My name is Bond, James Bond\",     analyzer_results=[RecognizerResult(entity_type=\"PERSON\",                                        start=11,                                        end=15,                                        score=0.8),                       RecognizerResult(entity_type=\"PERSON\",                                        start=17,                                        end=27,                                        score=0.8)],     operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"BIP\"})} )</p> <p>print(result) text: My name is BIP, BIP. items: [     {'start': 16, 'end': 19, 'entity_type': 'PERSON',      'text': 'BIP', 'operator': 'replace'},     {'start': 11, 'end': 14, 'entity_type': 'PERSON',      'text': 'BIP', 'operator': 'replace'} ]</p>"},{"location":"api/anonymizer_python/#presidio_anonymizer.AnonymizerEngine.get_anonymizers","title":"<code>get_anonymizers()</code>","text":"<p>Return a list of supported anonymizers.</p> Source code in <code>presidio_anonymizer/anonymizer_engine.py</code> <pre><code>def get_anonymizers(self) -&gt; List[str]:\n    \"\"\"Return a list of supported anonymizers.\"\"\"\n    names = [p for p in self.operators_factory.get_anonymizers().keys()]\n    return names\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.BatchAnonymizerEngine","title":"<code>BatchAnonymizerEngine</code>","text":"<p>BatchAnonymizerEngine class.</p> <p>A class that provides functionality to anonymize in batches.</p> <p>Parameters:</p> Name Type Description Default <code>anonymizer_engine</code> <code>Optional[AnonymizerEngine]</code> <p>An instance of the AnonymizerEngine class.</p> <code>None</code> Source code in <code>presidio_anonymizer/batch_anonymizer_engine.py</code> <pre><code>class BatchAnonymizerEngine:\n    \"\"\"\n    BatchAnonymizerEngine class.\n\n    A class that provides functionality to anonymize in batches.\n    :param anonymizer_engine: An instance of the AnonymizerEngine class.\n    \"\"\"\n\n    def __init__(self, anonymizer_engine: Optional[AnonymizerEngine] = None):\n        self.anonymizer_engine = anonymizer_engine or AnonymizerEngine()\n\n    def anonymize_list(\n        self,\n        texts: List[Union[str, bool, int, float]],\n        recognizer_results_list: List[List[RecognizerResult]],\n        **kwargs\n    ) -&gt; List[Union[str, object]]:\n        \"\"\"\n        Anonymize a list of strings.\n\n        :param texts: List containing the texts to be anonymized (original texts)\n        :param recognizer_results_list: A list of lists of RecognizerResult,\n        the output of the AnalyzerEngine on each text in the list.\n        :param kwargs: Additional kwargs for the `AnonymizerEngine.anonymize` method\n        \"\"\"\n        return_list = []\n        if not recognizer_results_list:\n            recognizer_results_list = [[] for _ in range(len(texts))]\n        for text, recognizer_results in zip(texts, recognizer_results_list):\n            if type(text) in (str, bool, int, float):\n                res = self.anonymizer_engine.anonymize(\n                    text=str(text), analyzer_results=recognizer_results, **kwargs\n                )\n                return_list.append(res.text)\n            else:\n                return_list.append(text)\n\n        return return_list\n\n    def anonymize_dict(\n        self, analyzer_results: Iterable[DictRecognizerResult], **kwargs\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Anonymize values in a dictionary.\n\n        :param analyzer_results: Iterator of `DictRecognizerResult`\n        containing the output of the AnalyzerEngine.analyze_dict on the input text.\n        :param kwargs: Additional kwargs for the `AnonymizerEngine.anonymize` method\n        \"\"\"\n\n        return_dict = {}\n        for result in analyzer_results:\n            if isinstance(result.value, dict):\n                resp = self.anonymize_dict(\n                    analyzer_results=result.recognizer_results, **kwargs\n                )\n                return_dict[result.key] = resp\n\n            elif isinstance(result.value, str):\n                resp = self.anonymizer_engine.anonymize(\n                    text=result.value,\n                    analyzer_results=result.recognizer_results,\n                    **kwargs\n                )\n                return_dict[result.key] = resp.text\n\n            elif isinstance(result.value, collections.abc.Iterable):\n                anonymize_response = self.anonymize_list(\n                    texts=result.value,\n                    recognizer_results_list=result.recognizer_results,\n                    **kwargs\n                )\n                return_dict[result.key] = anonymize_response\n            else:\n                return_dict[result.key] = result.value\n        return return_dict\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.BatchAnonymizerEngine.anonymize_dict","title":"<code>anonymize_dict(analyzer_results, **kwargs)</code>","text":"<p>Anonymize values in a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>analyzer_results</code> <code>Iterable[DictRecognizerResult]</code> <p>Iterator of <code>DictRecognizerResult</code> containing the output of the AnalyzerEngine.analyze_dict on the input text.</p> required <code>kwargs</code> <p>Additional kwargs for the <code>AnonymizerEngine.anonymize</code> method</p> <code>{}</code> Source code in <code>presidio_anonymizer/batch_anonymizer_engine.py</code> <pre><code>def anonymize_dict(\n    self, analyzer_results: Iterable[DictRecognizerResult], **kwargs\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Anonymize values in a dictionary.\n\n    :param analyzer_results: Iterator of `DictRecognizerResult`\n    containing the output of the AnalyzerEngine.analyze_dict on the input text.\n    :param kwargs: Additional kwargs for the `AnonymizerEngine.anonymize` method\n    \"\"\"\n\n    return_dict = {}\n    for result in analyzer_results:\n        if isinstance(result.value, dict):\n            resp = self.anonymize_dict(\n                analyzer_results=result.recognizer_results, **kwargs\n            )\n            return_dict[result.key] = resp\n\n        elif isinstance(result.value, str):\n            resp = self.anonymizer_engine.anonymize(\n                text=result.value,\n                analyzer_results=result.recognizer_results,\n                **kwargs\n            )\n            return_dict[result.key] = resp.text\n\n        elif isinstance(result.value, collections.abc.Iterable):\n            anonymize_response = self.anonymize_list(\n                texts=result.value,\n                recognizer_results_list=result.recognizer_results,\n                **kwargs\n            )\n            return_dict[result.key] = anonymize_response\n        else:\n            return_dict[result.key] = result.value\n    return return_dict\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.BatchAnonymizerEngine.anonymize_list","title":"<code>anonymize_list(texts, recognizer_results_list, **kwargs)</code>","text":"<p>Anonymize a list of strings.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[Union[str, bool, int, float]]</code> <p>List containing the texts to be anonymized (original texts)</p> required <code>recognizer_results_list</code> <code>List[List[RecognizerResult]]</code> <p>A list of lists of RecognizerResult, the output of the AnalyzerEngine on each text in the list.</p> required <code>kwargs</code> <p>Additional kwargs for the <code>AnonymizerEngine.anonymize</code> method</p> <code>{}</code> Source code in <code>presidio_anonymizer/batch_anonymizer_engine.py</code> <pre><code>def anonymize_list(\n    self,\n    texts: List[Union[str, bool, int, float]],\n    recognizer_results_list: List[List[RecognizerResult]],\n    **kwargs\n) -&gt; List[Union[str, object]]:\n    \"\"\"\n    Anonymize a list of strings.\n\n    :param texts: List containing the texts to be anonymized (original texts)\n    :param recognizer_results_list: A list of lists of RecognizerResult,\n    the output of the AnalyzerEngine on each text in the list.\n    :param kwargs: Additional kwargs for the `AnonymizerEngine.anonymize` method\n    \"\"\"\n    return_list = []\n    if not recognizer_results_list:\n        recognizer_results_list = [[] for _ in range(len(texts))]\n    for text, recognizer_results in zip(texts, recognizer_results_list):\n        if type(text) in (str, bool, int, float):\n            res = self.anonymizer_engine.anonymize(\n                text=str(text), analyzer_results=recognizer_results, **kwargs\n            )\n            return_list.append(res.text)\n        else:\n            return_list.append(text)\n\n    return return_list\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.DeanonymizeEngine","title":"<code>DeanonymizeEngine</code>","text":"<p>             Bases: <code>EngineBase</code></p> <p>Deanonymize text that was previously anonymized.</p> Source code in <code>presidio_anonymizer/deanonymize_engine.py</code> <pre><code>class DeanonymizeEngine(EngineBase):\n    \"\"\"Deanonymize text that was previously anonymized.\"\"\"\n\n    def __init__(self):\n        self.logger = logging.getLogger(\"presidio-anonymizer\")\n        EngineBase.__init__(self)\n\n    def deanonymize(\n        self,\n        text: str,\n        entities: List[OperatorResult],\n        operators: Dict[str, OperatorConfig],\n    ) -&gt; EngineResult:\n        \"\"\"\n        Receive the text, entities and operators to perform deanonymization over.\n\n        :param operators: the operators to apply on the anonymizer result entities\n        :param text: the full text with the encrypted entities\n        :param entities: list of encrypted entities\n        :return: EngineResult - the new text and data about the deanonymized entities.\n        \"\"\"\n        return self._operate(text, entities, operators, OperatorType.Deanonymize)\n\n    def get_deanonymizers(self) -&gt; List[str]:\n        \"\"\"Return a list of supported deanonymizers.\"\"\"\n        names = [p for p in self.operators_factory.get_deanonymizers().keys()]\n        return names\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.DeanonymizeEngine.deanonymize","title":"<code>deanonymize(text, entities, operators)</code>","text":"<p>Receive the text, entities and operators to perform deanonymization over.</p> <p>Parameters:</p> Name Type Description Default <code>operators</code> <code>Dict[str, OperatorConfig]</code> <p>the operators to apply on the anonymizer result entities</p> required <code>text</code> <code>str</code> <p>the full text with the encrypted entities</p> required <code>entities</code> <code>List[OperatorResult]</code> <p>list of encrypted entities</p> required <p>Returns:</p> Type Description <code>EngineResult</code> <p>EngineResult - the new text and data about the deanonymized entities.</p> Source code in <code>presidio_anonymizer/deanonymize_engine.py</code> <pre><code>def deanonymize(\n    self,\n    text: str,\n    entities: List[OperatorResult],\n    operators: Dict[str, OperatorConfig],\n) -&gt; EngineResult:\n    \"\"\"\n    Receive the text, entities and operators to perform deanonymization over.\n\n    :param operators: the operators to apply on the anonymizer result entities\n    :param text: the full text with the encrypted entities\n    :param entities: list of encrypted entities\n    :return: EngineResult - the new text and data about the deanonymized entities.\n    \"\"\"\n    return self._operate(text, entities, operators, OperatorType.Deanonymize)\n</code></pre>"},{"location":"api/anonymizer_python/#presidio_anonymizer.DeanonymizeEngine.get_deanonymizers","title":"<code>get_deanonymizers()</code>","text":"<p>Return a list of supported deanonymizers.</p> Source code in <code>presidio_anonymizer/deanonymize_engine.py</code> <pre><code>def get_deanonymizers(self) -&gt; List[str]:\n    \"\"\"Return a list of supported deanonymizers.\"\"\"\n    names = [p for p in self.operators_factory.get_deanonymizers().keys()]\n    return names\n</code></pre>"},{"location":"api/image_redactor_python/","title":"Presidio Image Redactor API Reference","text":"<p>Image Redactor root module.</p>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BboxProcessor","title":"<code>BboxProcessor</code>","text":"<p>Common module for general bounding box operators.</p> Source code in <code>presidio_image_redactor/bbox.py</code> <pre><code>class BboxProcessor:\n    \"\"\"Common module for general bounding box operators.\"\"\"\n\n    @staticmethod\n    def get_bboxes_from_ocr_results(\n        ocr_results: Dict[str, List[Union[int, str]]],\n    ) -&gt; List[Dict[str, Union[int, float, str]]]:\n        \"\"\"Get bounding boxes on padded image for all detected words from ocr_results.\n\n        :param ocr_results: Raw results from OCR.\n        :return: Bounding box information per word.\n        \"\"\"\n        bboxes = []\n        for i in range(len(ocr_results[\"text\"])):\n            detected_text = ocr_results[\"text\"][i]\n            if detected_text:\n                bbox = {\n                    \"left\": ocr_results[\"left\"][i],\n                    \"top\": ocr_results[\"top\"][i],\n                    \"width\": ocr_results[\"width\"][i],\n                    \"height\": ocr_results[\"height\"][i],\n                    \"conf\": float(ocr_results[\"conf\"][i]),\n                    \"label\": detected_text,\n                }\n                bboxes.append(bbox)\n\n        return bboxes\n\n    @staticmethod\n    def get_bboxes_from_analyzer_results(\n        analyzer_results: List[ImageRecognizerResult],\n    ) -&gt; List[Dict[str, Union[str, float, int]]]:\n        \"\"\"Organize bounding box info from analyzer results.\n\n        :param analyzer_results: Results from using ImageAnalyzerEngine.\n\n        :return: Bounding box info organized.\n        \"\"\"\n        bboxes = []\n        for i in range(len(analyzer_results)):\n            result = analyzer_results[i].to_dict()\n\n            bbox_item = {\n                \"entity_type\": result[\"entity_type\"],\n                \"score\": result[\"score\"],\n                \"left\": result[\"left\"],\n                \"top\": result[\"top\"],\n                \"width\": result[\"width\"],\n                \"height\": result[\"height\"],\n            }\n            bboxes.append(bbox_item)\n\n        return bboxes\n\n    @staticmethod\n    def remove_bbox_padding(\n        analyzer_bboxes: List[Dict[str, Union[str, float, int]]],\n        padding_width: int,\n    ) -&gt; List[Dict[str, int]]:\n        \"\"\"Remove added padding in bounding box coordinates.\n\n        :param analyzer_bboxes: The bounding boxes from analyzer results.\n        :param padding_width: Pixel width used for padding (0 if no padding).\n\n        :return: Bounding box information per word.\n        \"\"\"\n        if padding_width &lt; 0:\n            raise ValueError(\"Padding width must be a non-negative integer.\")\n\n        if len(analyzer_bboxes) &gt; 0:\n            # Get fields\n            has_label = False\n            has_entity_type = False\n            try:\n                _ = analyzer_bboxes[0][\"label\"]\n                has_label = True\n            except KeyError:\n                has_label = False\n            try:\n                _ = analyzer_bboxes[0][\"entity_type\"]\n                has_entity_type = True\n            except KeyError:\n                has_entity_type = False\n\n            # Remove padding from all bounding boxes\n            if has_label is True and has_entity_type is True:\n                bboxes = [\n                    {\n                        \"left\": max(0, bbox[\"left\"] - padding_width),\n                        \"top\": max(0, bbox[\"top\"] - padding_width),\n                        \"width\": bbox[\"width\"],\n                        \"height\": bbox[\"height\"],\n                        \"label\": bbox[\"label\"],\n                        \"entity_type\": bbox[\"entity_type\"]\n                    }\n                    for bbox in analyzer_bboxes\n                ]\n            elif has_label is True and has_entity_type is False:\n                bboxes = [\n                    {\n                        \"left\": max(0, bbox[\"left\"] - padding_width),\n                        \"top\": max(0, bbox[\"top\"] - padding_width),\n                        \"width\": bbox[\"width\"],\n                        \"height\": bbox[\"height\"],\n                        \"label\": bbox[\"label\"]\n                    }\n                    for bbox in analyzer_bboxes\n                ]\n            elif has_label is False and has_entity_type is True:\n                bboxes = [\n                    {\n                        \"left\": max(0, bbox[\"left\"] - padding_width),\n                        \"top\": max(0, bbox[\"top\"] - padding_width),\n                        \"width\": bbox[\"width\"],\n                        \"height\": bbox[\"height\"],\n                        \"entity_type\": bbox[\"entity_type\"]\n                    }\n                    for bbox in analyzer_bboxes\n                ]\n            elif has_label is False and has_entity_type is False:\n                bboxes = [\n                    {\n                        \"left\": max(0, bbox[\"left\"] - padding_width),\n                        \"top\": max(0, bbox[\"top\"] - padding_width),\n                        \"width\": bbox[\"width\"],\n                        \"height\": bbox[\"height\"]\n                    }\n                    for bbox in analyzer_bboxes\n                ]\n        else:\n            bboxes = analyzer_bboxes\n\n        return bboxes\n\n    @staticmethod\n    def match_with_source(\n        all_pos: List[Dict[str, Union[str, int, float]]],\n        pii_source_dict: List[Dict[str, Union[str, int, float]]],\n        detected_pii: Dict[str, Union[str, float, int]],\n        tolerance: int = 50,\n    ) -&gt; Tuple[List[Dict[str, Union[str, int, float]]], bool]:\n        \"\"\"Match returned redacted PII bbox data with some source of truth for PII.\n\n        :param all_pos: Dictionary storing all positives.\n        :param pii_source_dict: List of PII labels for this instance.\n        :param detected_pii: Detected PII (single entity from analyzer_results).\n        :param tolerance: Tolerance for exact coordinates and size data.\n        :return: List of all positive with PII mapped back as possible\n        and whether a match was found.\n        \"\"\"\n        all_pos_match = all_pos.copy()\n\n        # Get info from detected PII (positive)\n        results_left = detected_pii[\"left\"]\n        results_top = detected_pii[\"top\"]\n        results_width = detected_pii[\"width\"]\n        results_height = detected_pii[\"height\"]\n        try:\n            results_score = detected_pii[\"score\"]\n        except KeyError:\n            # Handle matching when no score available\n            results_score = 0\n        match_found = False\n\n        # See what in the ground truth this positive matches\n        for label in pii_source_dict:\n            source_left = label[\"left\"]\n            source_top = label[\"top\"]\n            source_width = label[\"width\"]\n            source_height = label[\"height\"]\n\n            match_left = abs(source_left - results_left) &lt;= tolerance\n            match_top = abs(source_top - results_top) &lt;= tolerance\n            match_width = abs(source_width - results_width) &lt;= tolerance\n            match_height = abs(source_height - results_height) &lt;= tolerance\n            matching = [match_left, match_top, match_width, match_height]\n\n            if False not in matching:\n                # If match is found, carry over ground truth info\n                positive = label\n                positive[\"score\"] = results_score\n                all_pos_match.append(positive)\n                match_found = True\n\n        return all_pos_match, match_found\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BboxProcessor.get_bboxes_from_analyzer_results","title":"<code>get_bboxes_from_analyzer_results(analyzer_results)</code>  <code>staticmethod</code>","text":"<p>Organize bounding box info from analyzer results.</p> <p>Parameters:</p> Name Type Description Default <code>analyzer_results</code> <code>List[ImageRecognizerResult]</code> <p>Results from using ImageAnalyzerEngine.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Union[str, float, int]]]</code> <p>Bounding box info organized.</p> Source code in <code>presidio_image_redactor/bbox.py</code> <pre><code>@staticmethod\ndef get_bboxes_from_analyzer_results(\n    analyzer_results: List[ImageRecognizerResult],\n) -&gt; List[Dict[str, Union[str, float, int]]]:\n    \"\"\"Organize bounding box info from analyzer results.\n\n    :param analyzer_results: Results from using ImageAnalyzerEngine.\n\n    :return: Bounding box info organized.\n    \"\"\"\n    bboxes = []\n    for i in range(len(analyzer_results)):\n        result = analyzer_results[i].to_dict()\n\n        bbox_item = {\n            \"entity_type\": result[\"entity_type\"],\n            \"score\": result[\"score\"],\n            \"left\": result[\"left\"],\n            \"top\": result[\"top\"],\n            \"width\": result[\"width\"],\n            \"height\": result[\"height\"],\n        }\n        bboxes.append(bbox_item)\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BboxProcessor.get_bboxes_from_ocr_results","title":"<code>get_bboxes_from_ocr_results(ocr_results)</code>  <code>staticmethod</code>","text":"<p>Get bounding boxes on padded image for all detected words from ocr_results.</p> <p>Parameters:</p> Name Type Description Default <code>ocr_results</code> <code>Dict[str, List[Union[int, str]]]</code> <p>Raw results from OCR.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Union[int, float, str]]]</code> <p>Bounding box information per word.</p> Source code in <code>presidio_image_redactor/bbox.py</code> <pre><code>@staticmethod\ndef get_bboxes_from_ocr_results(\n    ocr_results: Dict[str, List[Union[int, str]]],\n) -&gt; List[Dict[str, Union[int, float, str]]]:\n    \"\"\"Get bounding boxes on padded image for all detected words from ocr_results.\n\n    :param ocr_results: Raw results from OCR.\n    :return: Bounding box information per word.\n    \"\"\"\n    bboxes = []\n    for i in range(len(ocr_results[\"text\"])):\n        detected_text = ocr_results[\"text\"][i]\n        if detected_text:\n            bbox = {\n                \"left\": ocr_results[\"left\"][i],\n                \"top\": ocr_results[\"top\"][i],\n                \"width\": ocr_results[\"width\"][i],\n                \"height\": ocr_results[\"height\"][i],\n                \"conf\": float(ocr_results[\"conf\"][i]),\n                \"label\": detected_text,\n            }\n            bboxes.append(bbox)\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BboxProcessor.match_with_source","title":"<code>match_with_source(all_pos, pii_source_dict, detected_pii, tolerance=50)</code>  <code>staticmethod</code>","text":"<p>Match returned redacted PII bbox data with some source of truth for PII.</p> <p>Parameters:</p> Name Type Description Default <code>all_pos</code> <code>List[Dict[str, Union[str, int, float]]]</code> <p>Dictionary storing all positives.</p> required <code>pii_source_dict</code> <code>List[Dict[str, Union[str, int, float]]]</code> <p>List of PII labels for this instance.</p> required <code>detected_pii</code> <code>Dict[str, Union[str, float, int]]</code> <p>Detected PII (single entity from analyzer_results).</p> required <code>tolerance</code> <code>int</code> <p>Tolerance for exact coordinates and size data.</p> <code>50</code> <p>Returns:</p> Type Description <code>Tuple[List[Dict[str, Union[str, int, float]]], bool]</code> <p>List of all positive with PII mapped back as possible and whether a match was found.</p> Source code in <code>presidio_image_redactor/bbox.py</code> <pre><code>@staticmethod\ndef match_with_source(\n    all_pos: List[Dict[str, Union[str, int, float]]],\n    pii_source_dict: List[Dict[str, Union[str, int, float]]],\n    detected_pii: Dict[str, Union[str, float, int]],\n    tolerance: int = 50,\n) -&gt; Tuple[List[Dict[str, Union[str, int, float]]], bool]:\n    \"\"\"Match returned redacted PII bbox data with some source of truth for PII.\n\n    :param all_pos: Dictionary storing all positives.\n    :param pii_source_dict: List of PII labels for this instance.\n    :param detected_pii: Detected PII (single entity from analyzer_results).\n    :param tolerance: Tolerance for exact coordinates and size data.\n    :return: List of all positive with PII mapped back as possible\n    and whether a match was found.\n    \"\"\"\n    all_pos_match = all_pos.copy()\n\n    # Get info from detected PII (positive)\n    results_left = detected_pii[\"left\"]\n    results_top = detected_pii[\"top\"]\n    results_width = detected_pii[\"width\"]\n    results_height = detected_pii[\"height\"]\n    try:\n        results_score = detected_pii[\"score\"]\n    except KeyError:\n        # Handle matching when no score available\n        results_score = 0\n    match_found = False\n\n    # See what in the ground truth this positive matches\n    for label in pii_source_dict:\n        source_left = label[\"left\"]\n        source_top = label[\"top\"]\n        source_width = label[\"width\"]\n        source_height = label[\"height\"]\n\n        match_left = abs(source_left - results_left) &lt;= tolerance\n        match_top = abs(source_top - results_top) &lt;= tolerance\n        match_width = abs(source_width - results_width) &lt;= tolerance\n        match_height = abs(source_height - results_height) &lt;= tolerance\n        matching = [match_left, match_top, match_width, match_height]\n\n        if False not in matching:\n            # If match is found, carry over ground truth info\n            positive = label\n            positive[\"score\"] = results_score\n            all_pos_match.append(positive)\n            match_found = True\n\n    return all_pos_match, match_found\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BboxProcessor.remove_bbox_padding","title":"<code>remove_bbox_padding(analyzer_bboxes, padding_width)</code>  <code>staticmethod</code>","text":"<p>Remove added padding in bounding box coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>analyzer_bboxes</code> <code>List[Dict[str, Union[str, float, int]]]</code> <p>The bounding boxes from analyzer results.</p> required <code>padding_width</code> <code>int</code> <p>Pixel width used for padding (0 if no padding).</p> required <p>Returns:</p> Type Description <code>List[Dict[str, int]]</code> <p>Bounding box information per word.</p> Source code in <code>presidio_image_redactor/bbox.py</code> <pre><code>@staticmethod\ndef remove_bbox_padding(\n    analyzer_bboxes: List[Dict[str, Union[str, float, int]]],\n    padding_width: int,\n) -&gt; List[Dict[str, int]]:\n    \"\"\"Remove added padding in bounding box coordinates.\n\n    :param analyzer_bboxes: The bounding boxes from analyzer results.\n    :param padding_width: Pixel width used for padding (0 if no padding).\n\n    :return: Bounding box information per word.\n    \"\"\"\n    if padding_width &lt; 0:\n        raise ValueError(\"Padding width must be a non-negative integer.\")\n\n    if len(analyzer_bboxes) &gt; 0:\n        # Get fields\n        has_label = False\n        has_entity_type = False\n        try:\n            _ = analyzer_bboxes[0][\"label\"]\n            has_label = True\n        except KeyError:\n            has_label = False\n        try:\n            _ = analyzer_bboxes[0][\"entity_type\"]\n            has_entity_type = True\n        except KeyError:\n            has_entity_type = False\n\n        # Remove padding from all bounding boxes\n        if has_label is True and has_entity_type is True:\n            bboxes = [\n                {\n                    \"left\": max(0, bbox[\"left\"] - padding_width),\n                    \"top\": max(0, bbox[\"top\"] - padding_width),\n                    \"width\": bbox[\"width\"],\n                    \"height\": bbox[\"height\"],\n                    \"label\": bbox[\"label\"],\n                    \"entity_type\": bbox[\"entity_type\"]\n                }\n                for bbox in analyzer_bboxes\n            ]\n        elif has_label is True and has_entity_type is False:\n            bboxes = [\n                {\n                    \"left\": max(0, bbox[\"left\"] - padding_width),\n                    \"top\": max(0, bbox[\"top\"] - padding_width),\n                    \"width\": bbox[\"width\"],\n                    \"height\": bbox[\"height\"],\n                    \"label\": bbox[\"label\"]\n                }\n                for bbox in analyzer_bboxes\n            ]\n        elif has_label is False and has_entity_type is True:\n            bboxes = [\n                {\n                    \"left\": max(0, bbox[\"left\"] - padding_width),\n                    \"top\": max(0, bbox[\"top\"] - padding_width),\n                    \"width\": bbox[\"width\"],\n                    \"height\": bbox[\"height\"],\n                    \"entity_type\": bbox[\"entity_type\"]\n                }\n                for bbox in analyzer_bboxes\n            ]\n        elif has_label is False and has_entity_type is False:\n            bboxes = [\n                {\n                    \"left\": max(0, bbox[\"left\"] - padding_width),\n                    \"top\": max(0, bbox[\"top\"] - padding_width),\n                    \"width\": bbox[\"width\"],\n                    \"height\": bbox[\"height\"]\n                }\n                for bbox in analyzer_bboxes\n            ]\n    else:\n        bboxes = analyzer_bboxes\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BilateralFilter","title":"<code>BilateralFilter</code>","text":"<p>             Bases: <code>ImagePreprocessor</code></p> <p>BilateralFilter class.</p> <p>The class applies bilateral filtering to an image. and returns the filtered   image and metadata.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>class BilateralFilter(ImagePreprocessor):\n    \"\"\"BilateralFilter class.\n\n    The class applies bilateral filtering to an image. and returns the filtered\n      image and metadata.\n    \"\"\"\n\n    def __init__(\n        self, diameter: int = 3, sigma_color: int = 40, sigma_space: int = 40\n    ) -&gt; None:\n        \"\"\"Initialize the BilateralFilter class.\n\n        :param diameter: Diameter of each pixel neighborhood.\n        :param sigma_color: value of sigma in the color space.\n        :param sigma_space: value of sigma in the coordinate space.\n        \"\"\"\n        super().__init__(use_greyscale=True)\n\n        self.diameter = diameter\n        self.sigma_color = sigma_color\n        self.sigma_space = sigma_space\n\n    def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n        \"\"\"Preprocess the image to be analyzed.\n\n        :param image: Loaded PIL image.\n\n        :return: The processed image and metadata (diameter, sigma_color, sigma_space).\n        \"\"\"\n        image = self.convert_image_to_array(image)\n\n        # Apply bilateral filtering\n        filtered_image = cv2.bilateralFilter(\n            image,\n            self.diameter,\n            self.sigma_color,\n            self.sigma_space,\n        )\n\n        metadata = {\n            \"diameter\": self.diameter,\n            \"sigma_color\": self.sigma_color,\n            \"sigma_space\": self.sigma_space,\n        }\n\n        return Image.fromarray(filtered_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BilateralFilter.__init__","title":"<code>__init__(diameter=3, sigma_color=40, sigma_space=40)</code>","text":"<p>Initialize the BilateralFilter class.</p> <p>Parameters:</p> Name Type Description Default <code>diameter</code> <code>int</code> <p>Diameter of each pixel neighborhood.</p> <code>3</code> <code>sigma_color</code> <code>int</code> <p>value of sigma in the color space.</p> <code>40</code> <code>sigma_space</code> <code>int</code> <p>value of sigma in the coordinate space.</p> <code>40</code> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def __init__(\n    self, diameter: int = 3, sigma_color: int = 40, sigma_space: int = 40\n) -&gt; None:\n    \"\"\"Initialize the BilateralFilter class.\n\n    :param diameter: Diameter of each pixel neighborhood.\n    :param sigma_color: value of sigma in the color space.\n    :param sigma_space: value of sigma in the coordinate space.\n    \"\"\"\n    super().__init__(use_greyscale=True)\n\n    self.diameter = diameter\n    self.sigma_color = sigma_color\n    self.sigma_space = sigma_space\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.BilateralFilter.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>Preprocess the image to be analyzed.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Loaded PIL image.</p> required <p>Returns:</p> Type Description <code>Tuple[Image, dict]</code> <p>The processed image and metadata (diameter, sigma_color, sigma_space).</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n    \"\"\"Preprocess the image to be analyzed.\n\n    :param image: Loaded PIL image.\n\n    :return: The processed image and metadata (diameter, sigma_color, sigma_space).\n    \"\"\"\n    image = self.convert_image_to_array(image)\n\n    # Apply bilateral filtering\n    filtered_image = cv2.bilateralFilter(\n        image,\n        self.diameter,\n        self.sigma_color,\n        self.sigma_space,\n    )\n\n    metadata = {\n        \"diameter\": self.diameter,\n        \"sigma_color\": self.sigma_color,\n        \"sigma_space\": self.sigma_space,\n    }\n\n    return Image.fromarray(filtered_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ContrastSegmentedImageEnhancer","title":"<code>ContrastSegmentedImageEnhancer</code>","text":"<p>             Bases: <code>ImagePreprocessor</code></p> <p>Class containing all logic to perform contrastive segmentation.</p> <p>Contrastive segmentation is a preprocessing step that aims to enhance the text in an image by increasing the contrast between the text and the background. The parameters used to run the preprocessing are selected based on the contrast level of the image.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>class ContrastSegmentedImageEnhancer(ImagePreprocessor):\n    \"\"\"Class containing all logic to perform contrastive segmentation.\n\n    Contrastive segmentation is a preprocessing step that aims to enhance the\n    text in an image by increasing the contrast between the text and the\n    background. The parameters used to run the preprocessing are selected based\n    on the contrast level of the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        bilateral_filter: Optional[BilateralFilter] = None,\n        adaptive_threshold: Optional[SegmentedAdaptiveThreshold] = None,\n        image_rescaling: Optional[ImageRescaling] = None,\n        low_contrast_threshold: int = 40,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\n\n        :param bilateral_filter: Optional BilateralFilter instance.\n        :param adaptive_threshold: Optional AdaptiveThreshold instance.\n        :param image_rescaling: Optional ImageRescaling instance.\n        :param low_contrast_threshold: Threshold for low contrast images.\n        \"\"\"\n\n        super().__init__(use_greyscale=True)\n        if not bilateral_filter:\n            self.bilateral_filter = BilateralFilter()\n        else:\n            self.bilateral_filter = bilateral_filter\n\n        if not adaptive_threshold:\n            self.adaptive_threshold = SegmentedAdaptiveThreshold()\n        else:\n            self.adaptive_threshold = adaptive_threshold\n\n        if not image_rescaling:\n            self.image_rescaling = ImageRescaling()\n        else:\n            self.image_rescaling = image_rescaling\n\n        self.low_contrast_threshold = low_contrast_threshold\n\n    def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n        \"\"\"Preprocess the image to be analyzed.\n\n        :param image: Loaded PIL image.\n\n        :return: The processed image and metadata (background color, scale percentage,\n             contrast level, and C value).\n        \"\"\"\n        image = self.convert_image_to_array(image)\n\n        # Apply bilateral filtering\n        filtered_image, _ = self.bilateral_filter.preprocess_image(image)\n\n        # Convert to grayscale\n        pil_filtered_image = Image.fromarray(np.uint8(filtered_image))\n        pil_grayscale_image = pil_filtered_image.convert(\"L\")\n        grayscale_image = np.asarray(pil_grayscale_image)\n\n        # Improve contrast\n        adjusted_image, _, adjusted_contrast = self._improve_contrast(grayscale_image)\n\n        # Adaptive Thresholding\n        adaptive_threshold_image, _ = self.adaptive_threshold.preprocess_image(\n            adjusted_image\n        )\n        # Increase contrast\n        _, threshold_image = cv2.threshold(\n            np.asarray(adaptive_threshold_image),\n            0,\n            255,\n            cv2.THRESH_BINARY | cv2.THRESH_OTSU,\n        )\n\n        # Rescale image\n        rescaled_image, scale_metadata = self.image_rescaling.preprocess_image(\n            threshold_image\n        )\n\n        return rescaled_image, scale_metadata\n\n    def _improve_contrast(self, image: np.ndarray) -&gt; Tuple[np.ndarray, str, str]:\n        \"\"\"Improve the contrast of an image based on its initial contrast level.\n\n        :param image: Input image.\n\n        :return: A tuple containing the improved image, the initial contrast level,\n             and the adjusted contrast level.\n        \"\"\"\n        contrast, mean_intensity = self._get_image_contrast(image)\n\n        if contrast &lt;= self.low_contrast_threshold:\n            alpha = 1.5\n            beta = -mean_intensity * alpha\n            adjusted_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n            adjusted_contrast, _ = self._get_image_contrast(adjusted_image)\n        else:\n            adjusted_image = image\n            adjusted_contrast = contrast\n        return adjusted_image, contrast, adjusted_contrast\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ContrastSegmentedImageEnhancer.__init__","title":"<code>__init__(bilateral_filter=None, adaptive_threshold=None, image_rescaling=None, low_contrast_threshold=40)</code>","text":"<p>Initialize the class.</p> <p>Parameters:</p> Name Type Description Default <code>bilateral_filter</code> <code>Optional[BilateralFilter]</code> <p>Optional BilateralFilter instance.</p> <code>None</code> <code>adaptive_threshold</code> <code>Optional[SegmentedAdaptiveThreshold]</code> <p>Optional AdaptiveThreshold instance.</p> <code>None</code> <code>image_rescaling</code> <code>Optional[ImageRescaling]</code> <p>Optional ImageRescaling instance.</p> <code>None</code> <code>low_contrast_threshold</code> <code>int</code> <p>Threshold for low contrast images.</p> <code>40</code> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def __init__(\n    self,\n    bilateral_filter: Optional[BilateralFilter] = None,\n    adaptive_threshold: Optional[SegmentedAdaptiveThreshold] = None,\n    image_rescaling: Optional[ImageRescaling] = None,\n    low_contrast_threshold: int = 40,\n) -&gt; None:\n    \"\"\"Initialize the class.\n\n    :param bilateral_filter: Optional BilateralFilter instance.\n    :param adaptive_threshold: Optional AdaptiveThreshold instance.\n    :param image_rescaling: Optional ImageRescaling instance.\n    :param low_contrast_threshold: Threshold for low contrast images.\n    \"\"\"\n\n    super().__init__(use_greyscale=True)\n    if not bilateral_filter:\n        self.bilateral_filter = BilateralFilter()\n    else:\n        self.bilateral_filter = bilateral_filter\n\n    if not adaptive_threshold:\n        self.adaptive_threshold = SegmentedAdaptiveThreshold()\n    else:\n        self.adaptive_threshold = adaptive_threshold\n\n    if not image_rescaling:\n        self.image_rescaling = ImageRescaling()\n    else:\n        self.image_rescaling = image_rescaling\n\n    self.low_contrast_threshold = low_contrast_threshold\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ContrastSegmentedImageEnhancer.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>Preprocess the image to be analyzed.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Loaded PIL image.</p> required <p>Returns:</p> Type Description <code>Tuple[Image, dict]</code> <p>The processed image and metadata (background color, scale percentage, contrast level, and C value).</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n    \"\"\"Preprocess the image to be analyzed.\n\n    :param image: Loaded PIL image.\n\n    :return: The processed image and metadata (background color, scale percentage,\n         contrast level, and C value).\n    \"\"\"\n    image = self.convert_image_to_array(image)\n\n    # Apply bilateral filtering\n    filtered_image, _ = self.bilateral_filter.preprocess_image(image)\n\n    # Convert to grayscale\n    pil_filtered_image = Image.fromarray(np.uint8(filtered_image))\n    pil_grayscale_image = pil_filtered_image.convert(\"L\")\n    grayscale_image = np.asarray(pil_grayscale_image)\n\n    # Improve contrast\n    adjusted_image, _, adjusted_contrast = self._improve_contrast(grayscale_image)\n\n    # Adaptive Thresholding\n    adaptive_threshold_image, _ = self.adaptive_threshold.preprocess_image(\n        adjusted_image\n    )\n    # Increase contrast\n    _, threshold_image = cv2.threshold(\n        np.asarray(adaptive_threshold_image),\n        0,\n        255,\n        cv2.THRESH_BINARY | cv2.THRESH_OTSU,\n    )\n\n    # Rescale image\n    rescaled_image, scale_metadata = self.image_rescaling.preprocess_image(\n        threshold_image\n    )\n\n    return rescaled_image, scale_metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine","title":"<code>DicomImagePiiVerifyEngine</code>","text":"<p>             Bases: <code>ImagePiiVerifyEngine</code>, <code>DicomImageRedactorEngine</code></p> <p>Class to handle verification and evaluation for DICOM de-identification.</p> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>class DicomImagePiiVerifyEngine(ImagePiiVerifyEngine, DicomImageRedactorEngine):\n    \"\"\"Class to handle verification and evaluation for DICOM de-identification.\"\"\"\n\n    def __init__(\n        self,\n        ocr_engine: Optional[OCR] = None,\n        image_analyzer_engine: Optional[ImageAnalyzerEngine] = None,\n    ):\n        \"\"\"Initialize DicomImagePiiVerifyEngine object.\n\n        :param ocr_engine: OCR engine to use.\n        :param image_analyzer_engine: Image analyzer engine to use.\n        \"\"\"\n        # Initialize OCR engine\n        if not ocr_engine:\n            self.ocr_engine = TesseractOCR()\n        else:\n            self.ocr_engine = ocr_engine\n\n        # Initialize image analyzer engine\n        if not image_analyzer_engine:\n            self.image_analyzer_engine = ImageAnalyzerEngine()\n        else:\n            self.image_analyzer_engine = image_analyzer_engine\n\n        # Initialize bbox processor\n        self.bbox_processor = BboxProcessor()\n\n    def verify_dicom_instance(\n        self,\n        instance: pydicom.dataset.FileDataset,\n        padding_width: int = 25,\n        display_image: bool = True,\n        show_text_annotation: bool = True,\n        use_metadata: bool = True,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; Tuple[Optional[PIL.Image.Image], dict, list]:\n        \"\"\"Verify PII on a single DICOM instance.\n\n        :param instance: Loaded DICOM instance including pixel data and metadata.\n        :param padding_width: Padding width to use when running OCR.\n        :param display_image: If the verificationimage is displayed and returned.\n        :param show_text_annotation: True to display entity type when displaying\n        image with bounding boxes.\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in ImageAnalyzerEngine.\n\n        :return: Image with boxes identifying PHI, OCR results,\n        and analyzer results.\n        \"\"\"\n        instance_copy = deepcopy(instance)\n\n        try:\n            instance_copy.PixelData\n        except AttributeError:\n            raise AttributeError(\"Provided DICOM instance lacks pixel data.\")\n\n        # Load image for processing\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            # Convert DICOM to PNG and add padding for OCR (during analysis)\n            is_greyscale = self._check_if_greyscale(instance_copy)\n            image = self._rescale_dcm_pixel_array(instance_copy, is_greyscale)\n            self._save_pixel_array_as_png(image, is_greyscale, \"tmp_dcm\", tmpdirname)\n\n            png_filepath = f\"{tmpdirname}/tmp_dcm.png\"\n            loaded_image = Image.open(png_filepath)\n            image = self._add_padding(loaded_image, is_greyscale, padding_width)\n\n        # Get OCR results\n        perform_ocr_kwargs, ocr_threshold = self.image_analyzer_engine._parse_ocr_kwargs(ocr_kwargs)  # noqa: E501\n        ocr_results = self.ocr_engine.perform_ocr(image, **perform_ocr_kwargs)\n        if ocr_threshold:\n            ocr_results = self.image_analyzer_engine.threshold_ocr_result(\n                ocr_results,\n                ocr_threshold\n            )\n        ocr_bboxes = self.bbox_processor.get_bboxes_from_ocr_results(\n            ocr_results\n        )\n\n        # Get analyzer results\n        analyzer_results = self._get_analyzer_results(\n            image, instance, use_metadata, ocr_kwargs, ad_hoc_recognizers,\n            **text_analyzer_kwargs\n        )\n        analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n            analyzer_results\n        )\n\n        # Prepare for plotting\n        pii_bboxes = self.image_analyzer_engine.get_pii_bboxes(\n            ocr_bboxes,\n            analyzer_bboxes\n        )\n        if is_greyscale:\n            use_greyscale_cmap = True\n        else:\n            use_greyscale_cmap = False\n\n        # Get image with verification boxes\n        verify_image = (\n            self.image_analyzer_engine.add_custom_bboxes(\n                image, pii_bboxes, show_text_annotation, use_greyscale_cmap\n            )\n            if display_image\n            else None\n        )\n\n        return verify_image, ocr_bboxes, analyzer_bboxes\n\n    def eval_dicom_instance(\n        self,\n        instance: pydicom.dataset.FileDataset,\n        ground_truth: dict,\n        padding_width: int = 25,\n        tolerance: int = 50,\n        display_image: bool = False,\n        use_metadata: bool = True,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; Tuple[Optional[PIL.Image.Image], dict]:\n        \"\"\"Evaluate performance for a single DICOM instance.\n\n        :param instance: Loaded DICOM instance including pixel data and metadata.\n        :param ground_truth: Dictionary containing ground truth labels for the instance.\n        :param padding_width: Padding width to use when running OCR.\n        :param tolerance: Pixel distance tolerance for matching to ground truth.\n        :param display_image: If the verificationimage is displayed and returned.\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in ImageAnalyzerEngine.\n\n        :return: Evaluation comparing redactor engine results vs ground truth.\n        \"\"\"\n        # Verify detected PHI\n        verify_image, ocr_results, analyzer_results = self.verify_dicom_instance(\n            instance,\n            padding_width,\n            display_image,\n            use_metadata,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n        formatted_ocr_results = self.bbox_processor.get_bboxes_from_ocr_results(\n            ocr_results\n        )\n        detected_phi = self.bbox_processor.get_bboxes_from_analyzer_results(\n            analyzer_results\n        )\n\n        # Remove duplicate entities in results\n        detected_phi = self._remove_duplicate_entities(detected_phi)\n\n        # Get correct PHI text (all TP and FP)\n        all_pos = self._label_all_positives(\n            ground_truth, formatted_ocr_results, detected_phi, tolerance\n        )\n\n        # Calculate evaluation metrics\n        precision = self.calculate_precision(ground_truth, all_pos)\n        recall = self.calculate_recall(ground_truth, all_pos)\n\n        eval_results = {\n            \"all_positives\": all_pos,\n            \"ground_truth\": ground_truth,\n            \"precision\": precision,\n            \"recall\": recall,\n        }\n\n        return verify_image, eval_results\n\n    @staticmethod\n    def _remove_duplicate_entities(\n        results: List[dict], dup_pix_tolerance: int = 5\n    ) -&gt; List[dict]:\n        \"\"\"Handle when a word is detected multiple times as different types of entities.\n\n        :param results: List of detected PHI with bbox info.\n        :param dup_pix_tolerance: Pixel difference tolerance for identifying duplicates.\n        :return: Detected PHI with no duplicate entities.\n        \"\"\"\n        dups = []\n        sorted(results, key=lambda x: x['score'], reverse=True)\n        results_no_dups = []\n        dims = [\"left\", \"top\", \"width\", \"height\"]\n\n        # Check for duplicates\n        for i in range(len(results) - 1):\n            i_dims = {dim: results[i][dim] for dim in dims}\n\n            # Ignore if we've already detected this dup combination\n            for other in range(i + 1, len(results)):\n                if i not in results_no_dups:\n                    other_dims = {dim: results[other][dim] for dim in dims}\n                    matching_dims = {\n                        dim: abs(i_dims[dim] - other_dims[dim]) &lt;= dup_pix_tolerance\n                        for dim in dims\n                    }\n                    matching = list(matching_dims.values())\n\n                    if all(matching):\n                        lower_scored_index = other if \\\n                            results[other]['score'] &lt; results[i]['score'] else i\n                        dups.append(lower_scored_index)\n\n        # Remove duplicates\n        for i in range(len(results)):\n            if i not in dups:\n                results_no_dups.append(results[i])\n\n        return results_no_dups\n\n    def _label_all_positives(\n        self,\n        gt_labels_dict: dict,\n        ocr_results: List[dict],\n        detected_phi: List[dict],\n        tolerance: int = 50,\n    ) -&gt; List[dict]:\n        \"\"\"Label all entities detected as PHI by using ground truth and OCR results.\n\n        All positives (detected_phi) do not contain PHI labels and are thus\n        difficult to work with intuitively. This method maps back to the\n        actual PHI to each detected sensitive entity.\n\n        :param gt_labels_dict: Dictionary with ground truth labels for a\n        single DICOM instance.\n        :param ocr_results: All detected text.\n        :param detected_phi: Formatted analyzer_results.\n        :param tolerance: Tolerance for exact coordinates and size data.\n        :return: List of all positives, labeled.\n        \"\"\"\n        all_pos = []\n\n        # Cycle through each positive (TP or FP)\n        for analyzer_result in detected_phi:\n\n            # See if there are any ground truth matches\n            all_pos, gt_match_found = self.bbox_processor.match_with_source(\n                all_pos, gt_labels_dict, analyzer_result, tolerance\n            )\n\n            # If not, check back with OCR\n            if not gt_match_found:\n                all_pos, _ = self.bbox_processor.match_with_source(\n                    all_pos, ocr_results, analyzer_result, tolerance\n                )\n\n        # Remove any duplicates\n        all_pos = self._remove_duplicate_entities(all_pos)\n\n        return all_pos\n\n    @staticmethod\n    def calculate_precision(gt: List[dict], all_pos: List[dict]) -&gt; float:\n        \"\"\"Calculate precision.\n\n        :param gt: List of ground truth labels.\n        :param all_pos: All Detected PHI (mapped back to have actual PHI text).\n        :return: Precision value.\n        \"\"\"\n        # Find True Positive (TP) and precision\n        tp = [i for i in all_pos if i in gt]\n        try:\n            precision = len(tp) / len(all_pos)\n        except ZeroDivisionError:\n            precision = 0\n\n        return precision\n\n    @staticmethod\n    def calculate_recall(gt: List[dict], all_pos: List[dict]) -&gt; float:\n        \"\"\"Calculate recall.\n\n        :param gt: List of ground truth labels.\n        :param all_pos: All Detected PHI (mapped back to have actual PHI text).\n        :return: Recall value.\n        \"\"\"\n        # Find True Positive (TP) and precision\n        tp = [i for i in all_pos if i in gt]\n        try:\n            recall = len(tp) / len(gt)\n        except ZeroDivisionError:\n            recall = 0\n\n        return recall\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.__init__","title":"<code>__init__(ocr_engine=None, image_analyzer_engine=None)</code>","text":"<p>Initialize DicomImagePiiVerifyEngine object.</p> <p>Parameters:</p> Name Type Description Default <code>ocr_engine</code> <code>Optional[OCR]</code> <p>OCR engine to use.</p> <code>None</code> <code>image_analyzer_engine</code> <code>Optional[ImageAnalyzerEngine]</code> <p>Image analyzer engine to use.</p> <code>None</code> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>def __init__(\n    self,\n    ocr_engine: Optional[OCR] = None,\n    image_analyzer_engine: Optional[ImageAnalyzerEngine] = None,\n):\n    \"\"\"Initialize DicomImagePiiVerifyEngine object.\n\n    :param ocr_engine: OCR engine to use.\n    :param image_analyzer_engine: Image analyzer engine to use.\n    \"\"\"\n    # Initialize OCR engine\n    if not ocr_engine:\n        self.ocr_engine = TesseractOCR()\n    else:\n        self.ocr_engine = ocr_engine\n\n    # Initialize image analyzer engine\n    if not image_analyzer_engine:\n        self.image_analyzer_engine = ImageAnalyzerEngine()\n    else:\n        self.image_analyzer_engine = image_analyzer_engine\n\n    # Initialize bbox processor\n    self.bbox_processor = BboxProcessor()\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.calculate_precision","title":"<code>calculate_precision(gt, all_pos)</code>  <code>staticmethod</code>","text":"<p>Calculate precision.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>List[dict]</code> <p>List of ground truth labels.</p> required <code>all_pos</code> <code>List[dict]</code> <p>All Detected PHI (mapped back to have actual PHI text).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Precision value.</p> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>@staticmethod\ndef calculate_precision(gt: List[dict], all_pos: List[dict]) -&gt; float:\n    \"\"\"Calculate precision.\n\n    :param gt: List of ground truth labels.\n    :param all_pos: All Detected PHI (mapped back to have actual PHI text).\n    :return: Precision value.\n    \"\"\"\n    # Find True Positive (TP) and precision\n    tp = [i for i in all_pos if i in gt]\n    try:\n        precision = len(tp) / len(all_pos)\n    except ZeroDivisionError:\n        precision = 0\n\n    return precision\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.calculate_recall","title":"<code>calculate_recall(gt, all_pos)</code>  <code>staticmethod</code>","text":"<p>Calculate recall.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>List[dict]</code> <p>List of ground truth labels.</p> required <code>all_pos</code> <code>List[dict]</code> <p>All Detected PHI (mapped back to have actual PHI text).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Recall value.</p> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>@staticmethod\ndef calculate_recall(gt: List[dict], all_pos: List[dict]) -&gt; float:\n    \"\"\"Calculate recall.\n\n    :param gt: List of ground truth labels.\n    :param all_pos: All Detected PHI (mapped back to have actual PHI text).\n    :return: Recall value.\n    \"\"\"\n    # Find True Positive (TP) and precision\n    tp = [i for i in all_pos if i in gt]\n    try:\n        recall = len(tp) / len(gt)\n    except ZeroDivisionError:\n        recall = 0\n\n    return recall\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.eval_dicom_instance","title":"<code>eval_dicom_instance(instance, ground_truth, padding_width=25, tolerance=50, display_image=False, use_metadata=True, ocr_kwargs=None, ad_hoc_recognizers=None, **text_analyzer_kwargs)</code>","text":"<p>Evaluate performance for a single DICOM instance.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>FileDataset</code> <p>Loaded DICOM instance including pixel data and metadata.</p> required <code>ground_truth</code> <code>dict</code> <p>Dictionary containing ground truth labels for the instance.</p> required <code>padding_width</code> <code>int</code> <p>Padding width to use when running OCR.</p> <code>25</code> <code>tolerance</code> <code>int</code> <p>Pixel distance tolerance for matching to ground truth.</p> <code>50</code> <code>display_image</code> <code>bool</code> <p>If the verificationimage is displayed and returned.</p> <code>False</code> <code>use_metadata</code> <code>bool</code> <p>Whether to redact text in the image that are present in the metadata.</p> <code>True</code> <code>ocr_kwargs</code> <code>Optional[dict]</code> <p>Additional params for OCR methods.</p> <code>None</code> <code>ad_hoc_recognizers</code> <code>Optional[List[PatternRecognizer]]</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <code>None</code> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in ImageAnalyzerEngine.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Optional[Image], dict]</code> <p>Evaluation comparing redactor engine results vs ground truth.</p> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>def eval_dicom_instance(\n    self,\n    instance: pydicom.dataset.FileDataset,\n    ground_truth: dict,\n    padding_width: int = 25,\n    tolerance: int = 50,\n    display_image: bool = False,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Tuple[Optional[PIL.Image.Image], dict]:\n    \"\"\"Evaluate performance for a single DICOM instance.\n\n    :param instance: Loaded DICOM instance including pixel data and metadata.\n    :param ground_truth: Dictionary containing ground truth labels for the instance.\n    :param padding_width: Padding width to use when running OCR.\n    :param tolerance: Pixel distance tolerance for matching to ground truth.\n    :param display_image: If the verificationimage is displayed and returned.\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in ImageAnalyzerEngine.\n\n    :return: Evaluation comparing redactor engine results vs ground truth.\n    \"\"\"\n    # Verify detected PHI\n    verify_image, ocr_results, analyzer_results = self.verify_dicom_instance(\n        instance,\n        padding_width,\n        display_image,\n        use_metadata,\n        ocr_kwargs=ocr_kwargs,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n    formatted_ocr_results = self.bbox_processor.get_bboxes_from_ocr_results(\n        ocr_results\n    )\n    detected_phi = self.bbox_processor.get_bboxes_from_analyzer_results(\n        analyzer_results\n    )\n\n    # Remove duplicate entities in results\n    detected_phi = self._remove_duplicate_entities(detected_phi)\n\n    # Get correct PHI text (all TP and FP)\n    all_pos = self._label_all_positives(\n        ground_truth, formatted_ocr_results, detected_phi, tolerance\n    )\n\n    # Calculate evaluation metrics\n    precision = self.calculate_precision(ground_truth, all_pos)\n    recall = self.calculate_recall(ground_truth, all_pos)\n\n    eval_results = {\n        \"all_positives\": all_pos,\n        \"ground_truth\": ground_truth,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n\n    return verify_image, eval_results\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImagePiiVerifyEngine.verify_dicom_instance","title":"<code>verify_dicom_instance(instance, padding_width=25, display_image=True, show_text_annotation=True, use_metadata=True, ocr_kwargs=None, ad_hoc_recognizers=None, **text_analyzer_kwargs)</code>","text":"<p>Verify PII on a single DICOM instance.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>FileDataset</code> <p>Loaded DICOM instance including pixel data and metadata.</p> required <code>padding_width</code> <code>int</code> <p>Padding width to use when running OCR.</p> <code>25</code> <code>display_image</code> <code>bool</code> <p>If the verificationimage is displayed and returned.</p> <code>True</code> <code>show_text_annotation</code> <code>bool</code> <p>True to display entity type when displaying image with bounding boxes.</p> <code>True</code> <code>use_metadata</code> <code>bool</code> <p>Whether to redact text in the image that are present in the metadata.</p> <code>True</code> <code>ocr_kwargs</code> <code>Optional[dict]</code> <p>Additional params for OCR methods.</p> <code>None</code> <code>ad_hoc_recognizers</code> <code>Optional[List[PatternRecognizer]]</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <code>None</code> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in ImageAnalyzerEngine.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Optional[Image], dict, list]</code> <p>Image with boxes identifying PHI, OCR results, and analyzer results.</p> Source code in <code>presidio_image_redactor/dicom_image_pii_verify_engine.py</code> <pre><code>def verify_dicom_instance(\n    self,\n    instance: pydicom.dataset.FileDataset,\n    padding_width: int = 25,\n    display_image: bool = True,\n    show_text_annotation: bool = True,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Tuple[Optional[PIL.Image.Image], dict, list]:\n    \"\"\"Verify PII on a single DICOM instance.\n\n    :param instance: Loaded DICOM instance including pixel data and metadata.\n    :param padding_width: Padding width to use when running OCR.\n    :param display_image: If the verificationimage is displayed and returned.\n    :param show_text_annotation: True to display entity type when displaying\n    image with bounding boxes.\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in ImageAnalyzerEngine.\n\n    :return: Image with boxes identifying PHI, OCR results,\n    and analyzer results.\n    \"\"\"\n    instance_copy = deepcopy(instance)\n\n    try:\n        instance_copy.PixelData\n    except AttributeError:\n        raise AttributeError(\"Provided DICOM instance lacks pixel data.\")\n\n    # Load image for processing\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        # Convert DICOM to PNG and add padding for OCR (during analysis)\n        is_greyscale = self._check_if_greyscale(instance_copy)\n        image = self._rescale_dcm_pixel_array(instance_copy, is_greyscale)\n        self._save_pixel_array_as_png(image, is_greyscale, \"tmp_dcm\", tmpdirname)\n\n        png_filepath = f\"{tmpdirname}/tmp_dcm.png\"\n        loaded_image = Image.open(png_filepath)\n        image = self._add_padding(loaded_image, is_greyscale, padding_width)\n\n    # Get OCR results\n    perform_ocr_kwargs, ocr_threshold = self.image_analyzer_engine._parse_ocr_kwargs(ocr_kwargs)  # noqa: E501\n    ocr_results = self.ocr_engine.perform_ocr(image, **perform_ocr_kwargs)\n    if ocr_threshold:\n        ocr_results = self.image_analyzer_engine.threshold_ocr_result(\n            ocr_results,\n            ocr_threshold\n        )\n    ocr_bboxes = self.bbox_processor.get_bboxes_from_ocr_results(\n        ocr_results\n    )\n\n    # Get analyzer results\n    analyzer_results = self._get_analyzer_results(\n        image, instance, use_metadata, ocr_kwargs, ad_hoc_recognizers,\n        **text_analyzer_kwargs\n    )\n    analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n        analyzer_results\n    )\n\n    # Prepare for plotting\n    pii_bboxes = self.image_analyzer_engine.get_pii_bboxes(\n        ocr_bboxes,\n        analyzer_bboxes\n    )\n    if is_greyscale:\n        use_greyscale_cmap = True\n    else:\n        use_greyscale_cmap = False\n\n    # Get image with verification boxes\n    verify_image = (\n        self.image_analyzer_engine.add_custom_bboxes(\n            image, pii_bboxes, show_text_annotation, use_greyscale_cmap\n        )\n        if display_image\n        else None\n    )\n\n    return verify_image, ocr_bboxes, analyzer_bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine","title":"<code>DicomImageRedactorEngine</code>","text":"<p>             Bases: <code>ImageRedactorEngine</code></p> <p>Performs OCR + PII detection + bounding box redaction.</p> <p>Parameters:</p> Name Type Description Default <code>image_analyzer_engine</code> <code>ImageAnalyzerEngine</code> <p>Engine which performs OCR + PII detection.</p> <code>None</code> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>class DicomImageRedactorEngine(ImageRedactorEngine):\n    \"\"\"Performs OCR + PII detection + bounding box redaction.\n\n    :param image_analyzer_engine: Engine which performs OCR + PII detection.\n    \"\"\"\n\n    def redact_and_return_bbox(\n        self,\n        image: pydicom.dataset.FileDataset,\n        fill: str = \"contrast\",\n        padding_width: int = 25,\n        crop_ratio: float = 0.75,\n        use_metadata: bool = True,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; Tuple[pydicom.dataset.FileDataset, List[Dict[str, int]]]:\n        \"\"\"Redact method to redact the given DICOM image and return redacted bboxes.\n\n        Please note, this method duplicates the image, creates a\n        new instance and manipulates it.\n\n        :param image: Loaded DICOM instance including pixel data and metadata.\n        :param fill: Fill setting to use for redaction box (\"contrast\" or \"background\").\n        :param padding_width: Padding width to use when running OCR.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        :return: DICOM instance with redacted pixel data.\n        \"\"\"\n        # Check input\n        if type(image) not in [pydicom.dataset.FileDataset, pydicom.dataset.Dataset]:\n            raise TypeError(\"The provided image must be a loaded DICOM instance.\")\n        try:\n            image.PixelData\n        except AttributeError as e:\n            raise AttributeError(f\"Provided DICOM instance lacks pixel data: {e}\")\n        except PermissionError as e:\n            raise PermissionError(f\"Unable to access pixel data (may not exist): {e}\")\n        except IsADirectoryError as e:\n            raise IsADirectoryError(f\"DICOM instance is a directory: {e}\")\n\n        instance = deepcopy(image)\n\n        # Load image for processing\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            # Convert DICOM to PNG and add padding for OCR (during analysis)\n            is_greyscale = self._check_if_greyscale(instance)\n            image = self._rescale_dcm_pixel_array(instance, is_greyscale)\n            image_name = str(uuid.uuid4())\n            self._save_pixel_array_as_png(image, is_greyscale, image_name, tmpdirname)\n\n            png_filepath = f\"{tmpdirname}/{image_name}.png\"\n            loaded_image = Image.open(png_filepath)\n            image = self._add_padding(loaded_image, is_greyscale, padding_width)\n\n        # Detect PII\n        analyzer_results = self._get_analyzer_results(\n            image,\n            instance,\n            use_metadata,\n            ocr_kwargs,\n            ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n        # Redact all bounding boxes from DICOM file\n        analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n            analyzer_results\n        )\n        bboxes = self.bbox_processor.remove_bbox_padding(analyzer_bboxes, padding_width)\n        redacted_image = self._add_redact_box(instance, bboxes, crop_ratio, fill)\n\n        return redacted_image, bboxes\n\n    def redact(\n        self,\n        image: pydicom.dataset.FileDataset,\n        fill: str = \"contrast\",\n        padding_width: int = 25,\n        crop_ratio: float = 0.75,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; pydicom.dataset.FileDataset:\n        \"\"\"Redact method to redact the given DICOM image.\n\n        Please note, this method duplicates the image, creates a\n        new instance and manipulates it.\n\n        :param image: Loaded DICOM instance including pixel data and metadata.\n        :param fill: Fill setting to use for redaction box (\"contrast\" or \"background\").\n        :param padding_width: Padding width to use when running OCR.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        :return: DICOM instance with redacted pixel data.\n        \"\"\"\n        redacted_image, _ = self.redact_and_return_bbox(\n            image=image,\n            fill=fill,\n            padding_width=padding_width,\n            crop_ratio=crop_ratio,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n        return redacted_image\n\n    def redact_from_file(\n        self,\n        input_dicom_path: str,\n        output_dir: str,\n        padding_width: int = 25,\n        crop_ratio: float = 0.75,\n        fill: str = \"contrast\",\n        use_metadata: bool = True,\n        save_bboxes: bool = False,\n        verbose: bool = True,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; None:\n        \"\"\"Redact method to redact from a given file.\n\n        Please notice, this method duplicates the file, creates\n        new instance and manipulate them.\n\n        :param input_dicom_path: String path to DICOM image.\n        :param output_dir: String path to parent output directory.\n        :param padding_width : Padding width to use when running OCR.\n        :param fill: Color setting to use for redaction box\n        (\"contrast\" or \"background\").\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param save_bboxes: True if we want to save boundings boxes.\n        :param verbose: True to print where redacted file was written to.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n        \"\"\"\n        # Verify the given paths\n        if Path(input_dicom_path).is_dir() is True:\n            raise TypeError(\"input_dicom_path must be file (not dir)\")\n        if Path(input_dicom_path).is_file() is False:\n            raise TypeError(\"input_dicom_path must be a valid file\")\n        if Path(output_dir).is_file() is True:\n            raise TypeError(\n                \"output_dir must be a directory (does not need to exist yet)\"\n            )\n\n        # Create duplicate\n        dst_path = self._copy_files_for_processing(input_dicom_path, output_dir)\n\n        # Process DICOM file\n        output_location = self._redact_single_dicom_image(\n            dcm_path=dst_path,\n            crop_ratio=crop_ratio,\n            fill=fill,\n            padding_width=padding_width,\n            use_metadata=use_metadata,\n            overwrite=True,\n            dst_parent_dir=\".\",\n            save_bboxes=save_bboxes,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n        if verbose:\n            print(f\"Output written to {output_location}\")\n\n        return None\n\n    def redact_from_directory(\n        self,\n        input_dicom_path: str,\n        output_dir: str,\n        padding_width: int = 25,\n        crop_ratio: float = 0.75,\n        fill: str = \"contrast\",\n        use_metadata: bool = True,\n        save_bboxes: bool = False,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; None:\n        \"\"\"Redact method to redact from a directory of files.\n\n        Please notice, this method duplicates the files, creates\n        new instances and manipulate them.\n\n        :param input_dicom_path: String path to directory of DICOM images.\n        :param output_dir: String path to parent output directory.\n        :param padding_width : Padding width to use when running OCR.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Color setting to use for redaction box\n        (\"contrast\" or \"background\").\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param save_bboxes: True if we want to save boundings boxes.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n        \"\"\"\n        # Verify the given paths\n        if Path(input_dicom_path).is_dir() is False:\n            raise TypeError(\"input_dicom_path must be a valid directory\")\n        if Path(input_dicom_path).is_file() is True:\n            raise TypeError(\"input_dicom_path must be a directory (not file)\")\n        if Path(output_dir).is_file() is True:\n            raise TypeError(\n                \"output_dir must be a directory (does not need to exist yet)\"\n            )\n\n        # Create duplicates\n        dst_path = self._copy_files_for_processing(input_dicom_path, output_dir)\n\n        # Process DICOM files\n        output_location = self._redact_multiple_dicom_images(\n            dcm_dir=dst_path,\n            crop_ratio=crop_ratio,\n            fill=fill,\n            padding_width=padding_width,\n            use_metadata=use_metadata,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            overwrite=True,\n            dst_parent_dir=\".\",\n            save_bboxes=save_bboxes,\n            ocr_kwargs=ocr_kwargs,\n            **text_analyzer_kwargs,\n        )\n\n        print(f\"Output written to {output_location}\")\n\n        return None\n\n    @staticmethod\n    def _get_all_dcm_files(dcm_dir: Path) -&gt; List[Path]:\n        \"\"\"Return paths to all DICOM files in a directory and its sub-directories.\n\n        :param dcm_dir: pathlib Path to a directory containing at least one .dcm file.\n\n        :return: List of pathlib Path objects.\n        \"\"\"\n        # Define applicable extensions\n        extensions = [\"[dD][cC][mM]\", \"[dD][iI][cC][oO][mM]\"]\n\n        # Get all files with any applicable extension\n        all_files = []\n        for extension in extensions:\n            p = dcm_dir.glob(f\"**/*.{extension}\")\n            files = [x for x in p if x.is_file()]\n            all_files += files\n\n        return all_files\n\n    @staticmethod\n    def _check_if_greyscale(instance: pydicom.dataset.FileDataset) -&gt; bool:\n        \"\"\"Check if a DICOM image is in greyscale.\n\n        :param instance: A single DICOM instance.\n\n        :return: FALSE if the Photometric Interpretation is RGB.\n        \"\"\"\n        # Check if image is grayscale using the Photometric Interpretation element\n        try:\n            color_scale = instance.PhotometricInterpretation\n        except AttributeError:\n            color_scale = None\n        is_greyscale = color_scale in [\"MONOCHROME1\", \"MONOCHROME2\"]\n\n        return is_greyscale\n\n    @staticmethod\n    def _rescale_dcm_pixel_array(\n        instance: pydicom.dataset.FileDataset, is_greyscale: bool\n    ) -&gt; np.ndarray:\n        \"\"\"Rescale DICOM pixel_array.\n\n        :param instance: A singe DICOM instance.\n        :param is_greyscale: FALSE if the Photometric Interpretation is RGB.\n\n        :return: Rescaled DICOM pixel_array.\n        \"\"\"\n        # Normalize contrast\n        if \"WindowWidth\" in instance:\n            if is_greyscale:\n                image_2d = apply_voi_lut(instance.pixel_array, instance)\n            else:\n                image_2d = instance.pixel_array\n        else:\n            image_2d = instance.pixel_array\n\n        # Convert to float to avoid overflow or underflow losses.\n        image_2d_float = image_2d.astype(float)\n\n        if not is_greyscale:\n            image_2d_scaled = image_2d_float\n        else:\n            # Rescaling grey scale between 0-255\n            image_2d_scaled = (\n                np.maximum(image_2d_float, 0) / image_2d_float.max()\n            ) * 255.0\n\n        # Convert to uint\n        image_2d_scaled = np.uint8(image_2d_scaled)\n\n        return image_2d_scaled\n\n    @staticmethod\n    def _save_pixel_array_as_png(\n        pixel_array: np.array,\n        is_greyscale: bool,\n        output_file_name: str = \"example\",\n        output_dir: str = \"temp_dir\",\n    ) -&gt; None:\n        \"\"\"Save the pixel data from a loaded DICOM instance as PNG.\n\n        :param pixel_array: Pixel data from the instance.\n        :param is_greyscale: True if image is greyscale.\n        :param output_file_name: Name of output file (no file extension).\n        :param output_dir: String path to output directory.\n        \"\"\"\n        shape = pixel_array.shape\n\n        # Write the PNG file\n        os.makedirs(output_dir, exist_ok=True)\n        if is_greyscale:\n            with open(f\"{output_dir}/{output_file_name}.png\", \"wb\") as png_file:\n                w = png.Writer(shape[1], shape[0], greyscale=True)\n                w.write(png_file, pixel_array)\n        else:\n            with open(f\"{output_dir}/{output_file_name}.png\", \"wb\") as png_file:\n                w = png.Writer(shape[1], shape[0], greyscale=False)\n                # Semi-flatten the pixel array to RGB representation in 2D\n                pixel_array = np.reshape(pixel_array, (shape[0], shape[1] * 3))\n                w.write(png_file, pixel_array)\n\n        return None\n\n    @classmethod\n    def _convert_dcm_to_png(cls, filepath: Path, output_dir: str = \"temp_dir\") -&gt; tuple:\n        \"\"\"Convert DICOM image to PNG file.\n\n        :param filepath: pathlib Path to a single dcm file.\n        :param output_dir: String path to output directory.\n\n        :return: Shape of pixel array and if image mode is greyscale.\n        \"\"\"\n        ds = pydicom.dcmread(filepath)\n\n        # Check if image is grayscale using the Photometric Interpretation element\n        is_greyscale = cls._check_if_greyscale(ds)\n\n        # Rescale pixel array\n        image = cls._rescale_dcm_pixel_array(ds, is_greyscale)\n        shape = image.shape\n\n        # Write to PNG file\n        cls._save_pixel_array_as_png(image, is_greyscale, filepath.stem, output_dir)\n\n        return shape, is_greyscale\n\n    @staticmethod\n    def _get_bg_color(\n        image: Image.Image, is_greyscale: bool, invert: bool = False\n    ) -&gt; Union[int, Tuple[int, int, int]]:\n        \"\"\"Select most common color as background color.\n\n        :param image: Loaded PIL image.\n        :param colorscale: Colorscale of image (e.g., 'grayscale', 'RGB')\n        :param invert: TRUE if you want to get the inverse of the bg color.\n\n        :return: Background color.\n        \"\"\"\n        # Invert colors if invert flag is True\n        if invert:\n            if image.mode == \"RGBA\":\n                # Handle transparency as needed\n                r, g, b, a = image.split()\n                rgb_image = Image.merge(\"RGB\", (r, g, b))\n                inverted_image = ImageOps.invert(rgb_image)\n                r2, g2, b2 = inverted_image.split()\n\n                image = Image.merge(\"RGBA\", (r2, g2, b2, a))\n\n            else:\n                image = ImageOps.invert(image)\n\n        # Get background color\n        if is_greyscale:\n            # Select most common color as color\n            bg_color = int(np.bincount(list(image.getdata())).argmax())\n        else:\n            # Reduce size of image to 1 pixel to get dominant color\n            tmp_image = image.copy()\n            tmp_image = tmp_image.resize((1, 1), resample=0)\n            bg_color = tmp_image.getpixel((0, 0))\n\n        return bg_color\n\n    @staticmethod\n    def _get_array_corners(pixel_array: np.ndarray, crop_ratio: float) -&gt; np.ndarray:\n        \"\"\"Crop a pixel array to just return the corners in a single array.\n\n        :param pixel_array: Numpy array containing the pixel data.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n\n        :return: Cropped input array.\n        \"\"\"\n        if crop_ratio &gt;= 1.0 or crop_ratio &lt;= 0:\n            raise ValueError(\"crop_ratio must be between 0 and 1\")\n\n        # Set dimensions\n        width = pixel_array.shape[0]\n        height = pixel_array.shape[1]\n        crop_width = int(np.floor(width * crop_ratio / 2))\n        crop_height = int(np.floor(height * crop_ratio / 2))\n\n        # Get coordinates for corners\n        # (left, top, right, bottom)\n        box_top_left = (0, 0, crop_width, crop_height)\n        box_top_right = (width - crop_width, 0, width, crop_height)\n        box_bottom_left = (0, height - crop_height, crop_width, height)\n        box_bottom_right = (width - crop_width, height - crop_height, width, height)\n        boxes = [box_top_left, box_top_right, box_bottom_left, box_bottom_right]\n\n        # Only keep box pixels\n        cropped_pixel_arrays = [\n            pixel_array[box[0] : box[2], box[1] : box[3]] for box in boxes\n        ]\n\n        # Combine the cropped pixel arrays\n        cropped_array = np.vstack(cropped_pixel_arrays)\n\n        return cropped_array\n\n    @classmethod\n    def _get_most_common_pixel_value(\n        cls,\n        instance: pydicom.dataset.FileDataset,\n        crop_ratio: float,\n        fill: str = \"contrast\",\n    ) -&gt; Union[int, Tuple[int, int, int]]:\n        \"\"\"Find the most common pixel value.\n\n        :param instance: A singe DICOM instance.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Determines how box color is selected.\n        'contrast' - Masks stand out relative to background.\n        'background' - Masks are same color as background.\n\n        :return: Most or least common pixel value (depending on fill).\n        \"\"\"\n        # Crop down to just only look at image corners\n        cropped_array = cls._get_array_corners(instance.pixel_array, crop_ratio)\n\n        # Get flattened pixel array\n        flat_pixel_array = np.array(cropped_array).flatten()\n\n        is_greyscale = cls._check_if_greyscale(instance)\n        if is_greyscale:\n            # Get most common value\n            values, counts = np.unique(flat_pixel_array, return_counts=True)\n            flat_pixel_array = np.array(flat_pixel_array)\n            common_value = values[np.argmax(counts)]\n        else:\n            raise TypeError(\n                \"Most common pixel value retrieval is only supported for greyscale images at this point.\"  # noqa: E501\n            )\n\n        # Invert color as necessary\n        if fill.lower() in [\"contrast\", \"invert\", \"inverted\", \"inverse\"]:\n            pixel_value = np.max(flat_pixel_array) - common_value\n        elif fill.lower() in [\"background\", \"bg\"]:\n            pixel_value = common_value\n\n        return pixel_value\n\n    @classmethod\n    def _add_padding(\n        cls,\n        image: Image.Image,\n        is_greyscale: bool,\n        padding_width: int,\n    ) -&gt; Image.Image:\n        \"\"\"Add border to image using most common color.\n\n        :param image: Loaded PIL image.\n        :param is_greyscale: Whether image is in grayscale or not.\n        :param padding_width: Pixel width of padding (uniform).\n\n        :return: PIL image with padding.\n        \"\"\"\n        # Check padding width value\n        if padding_width &lt;= 0:\n            raise ValueError(\"Enter a positive value for padding\")\n        elif padding_width &gt;= 100:\n            raise ValueError(\n                \"Excessive padding width entered. Please use a width under 100 pixels.\"  # noqa: E501\n            )\n\n        # Select most common color as border color\n        border_color = cls._get_bg_color(image, is_greyscale)\n\n        # Add padding\n        right = padding_width\n        left = padding_width\n        top = padding_width\n        bottom = padding_width\n\n        width, height = image.size\n\n        new_width = width + right + left\n        new_height = height + top + bottom\n\n        image_with_padding = Image.new(\n            image.mode, (new_width, new_height), border_color\n        )\n        image_with_padding.paste(image, (left, top))\n\n        return image_with_padding\n\n    @staticmethod\n    def _copy_files_for_processing(src_path: str, dst_parent_dir: str) -&gt; Path:\n        \"\"\"Copy DICOM files. All processing should be done on the copies.\n\n        :param src_path: String path to DICOM file or directory containing DICOM files.\n        :param dst_parent_dir: String path to parent directory of output location.\n\n        :return: Output location of the file(s).\n        \"\"\"\n        # Identify output path\n        tail = list(Path(src_path).parts)[-1]\n        dst_path = Path(dst_parent_dir, tail)\n\n        # Copy file(s)\n        if Path(src_path).is_dir() is True:\n            try:\n                shutil.copytree(src_path, dst_path)\n            except FileExistsError:\n                raise FileExistsError(\n                    \"Destination files already exist. Please clear the destination files or specify a different dst_parent_dir.\"  # noqa: E501\n                )\n        elif Path(src_path).is_file() is True:\n            # Create the output dir manually if working with a single file\n            os.makedirs(Path(dst_path).parent, exist_ok=True)\n            shutil.copyfile(src_path, dst_path)\n        else:\n            raise FileNotFoundError(f\"{src_path} does not exist\")\n\n        return dst_path\n\n    @staticmethod\n    def _get_text_metadata(\n        instance: pydicom.dataset.FileDataset,\n    ) -&gt; Tuple[list, list, list]:\n        \"\"\"Retrieve all text metadata from the DICOM image.\n\n        :param instance: Loaded DICOM instance.\n\n        :return: List of all the instance's element values (excluding pixel data),\n        bool for if the element is specified as being a name,\n        bool for if the element is specified as being related to the patient.\n        \"\"\"\n        metadata_text = list()\n        is_name = list()\n        is_patient = list()\n\n        for element in instance:\n            # Save all metadata except the DICOM image itself\n            if element.name != \"Pixel Data\":\n                # Save the metadata\n                metadata_text.append(element.value)\n\n                # Track whether this particular element is a name\n                if \"name\" in element.name.lower():\n                    is_name.append(True)\n                else:\n                    is_name.append(False)\n\n                # Track whether this particular element is directly tied to the patient\n                if \"patient\" in element.name.lower():\n                    is_patient.append(True)\n                else:\n                    is_patient.append(False)\n            else:\n                metadata_text.append(\"\")\n                is_name.append(False)\n                is_patient.append(False)\n\n        return metadata_text, is_name, is_patient\n\n    @staticmethod\n    def augment_word(word: str, case_sensitive: bool = False) -&gt; list:\n        \"\"\"Apply multiple types of casing to the provided string.\n\n        :param words: String containing the word or term of interest.\n        :param case_sensitive: True if we want to preserve casing.\n\n        :return: List of the same string with different casings and spacing.\n        \"\"\"\n        word_list = []\n        if word != \"\":\n            # Replacing separator character with space, if any\n            text_no_separator = word.replace(\"^\", \" \")\n            text_no_separator = text_no_separator.replace(\"-\", \" \")\n            text_no_separator = \" \".join(text_no_separator.split())\n\n            if case_sensitive:\n                word_list.append(text_no_separator)\n                word_list.extend(\n                    [\n                        text_no_separator.split(\" \"),\n                    ]\n                )\n            else:\n                # Capitalize all characters in string\n                text_upper = text_no_separator.upper()\n\n                # Lowercase all characters in string\n                text_lower = text_no_separator.lower()\n\n                # Capitalize first letter in each part of string\n                text_title = text_no_separator.title()\n\n                # Append iterations\n                word_list.extend(\n                    [text_no_separator, text_upper, text_lower, text_title]\n                )\n\n                # Adding each term as a separate item in the list\n                word_list.extend(\n                    [\n                        text_no_separator.split(\" \"),\n                        text_upper.split(\" \"),\n                        text_lower.split(\" \"),\n                        text_title.split(\" \"),\n                    ]\n                )\n\n            # Flatten list\n            flat_list = []\n            for item in word_list:\n                if isinstance(item, list):\n                    flat_list.extend(item)\n                else:\n                    flat_list.append(item)\n\n            # Remove any duplicates and empty strings\n            word_list = list(set(flat_list))\n            word_list = list(filter(None, word_list))\n\n        return word_list\n\n    @classmethod\n    def _process_names(cls, text_metadata: list, is_name: list) -&gt; list:\n        \"\"\"Process names to have multiple iterations in our PHI list.\n\n        :param metadata_text: List of all the instance's element values\n        (excluding pixel data).\n        :param is_name: True if the element is specified as being a name.\n\n        :return: Metadata text with additional name iterations appended.\n        \"\"\"\n        phi_list = text_metadata.copy()\n\n        for i in range(0, len(text_metadata)):\n            if is_name[i] is True:\n                original_text = str(text_metadata[i])\n                phi_list += cls.augment_word(original_text)\n\n        return phi_list\n\n    @staticmethod\n    def _add_known_generic_phi(phi_list: list) -&gt; list:\n        \"\"\"Add known potential generic PHI values.\n\n        :param phi_list: List of PHI to use with Presidio ad-hoc recognizer.\n\n        :return: Same list with added known values.\n        \"\"\"\n        known_generic_phi = [\"[M]\", \"[F]\", \"[X]\", \"[U]\", \"M\", \"F\", \"X\", \"U\"]\n        phi_list.extend(known_generic_phi)\n\n        return phi_list\n\n    @classmethod\n    def _make_phi_list(\n        cls,\n        original_metadata: List[Union[pydicom.multival.MultiValue, list, tuple]],\n        is_name: List[bool],\n        is_patient: List[bool],\n    ) -&gt; list:\n        \"\"\"Make the list of PHI to use in Presidio ad-hoc recognizer.\n\n        :param original_metadata: List of all the instance's element values\n        (excluding pixel data).\n        :param is_name: True if the element is specified as being a name.\n        :param is_patient: True if the element is specified as being\n        related to the patient.\n\n        :return: List of PHI (str) to use with Presidio ad-hoc recognizer.\n        \"\"\"\n        # Process names\n        phi_list = cls._process_names(original_metadata, is_name)\n\n        # Add known potential phi values\n        phi_list = cls._add_known_generic_phi(phi_list)\n\n        # Flatten any nested lists\n        for phi in phi_list:\n            if type(phi) in [pydicom.multival.MultiValue, list, tuple]:\n                for item in phi:\n                    phi_list.append(item)\n                phi_list.remove(phi)\n\n        # Convert all items to strings\n        phi_str_list = [str(phi) for phi in phi_list]\n\n        # Remove duplicates\n        phi_str_list = list(set(phi_str_list))\n\n        return phi_str_list\n\n    @classmethod\n    def _set_bbox_color(\n        cls, instance: pydicom.dataset.FileDataset, fill: str\n    ) -&gt; Union[int, Tuple[int, int, int]]:\n        \"\"\"Set the bounding box color.\n\n        :param instance: A single DICOM instance.\n        :param fill: Determines how box color is selected.\n        'contrast' - Masks stand out relative to background.\n        'background' - Masks are same color as background.\n\n        :return: int or tuple of int values determining masking box color.\n        \"\"\"\n        # Check if we want the box color to contrast with the background\n        if fill.lower() in [\"contrast\", \"invert\", \"inverted\", \"inverse\"]:\n            invert_flag = True\n        elif fill.lower() in [\"background\", \"bg\"]:\n            invert_flag = False\n        else:\n            raise ValueError(\"fill must be 'contrast' or 'background'\")\n\n        # Temporarily save as PNG to get color\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            dst_path = Path(f\"{tmpdirname}/temp.dcm\")\n            instance.save_as(dst_path)\n            _, is_greyscale = cls._convert_dcm_to_png(dst_path, output_dir=tmpdirname)\n\n            png_filepath = f\"{tmpdirname}/{dst_path.stem}.png\"\n            loaded_image = Image.open(png_filepath)\n            box_color = cls._get_bg_color(loaded_image, is_greyscale, invert_flag)\n\n        return box_color\n\n    @staticmethod\n    def _check_if_compressed(instance: pydicom.dataset.FileDataset) -&gt; bool:\n        \"\"\"Check if the pixel data is compressed.\n\n        :param instance: DICOM instance.\n\n        :return: Boolean for whether the pixel data is compressed.\n        \"\"\"\n        # Calculate expected bytes\n        rows = instance.Rows\n        columns = instance.Columns\n        samples_per_pixel = instance.SamplesPerPixel\n        bits_allocated = instance.BitsAllocated\n        try:\n            number_of_frames = instance[0x0028, 0x0008].value\n        except KeyError:\n            number_of_frames = 1\n        expected_num_bytes = (\n            rows * columns * number_of_frames * samples_per_pixel * (bits_allocated / 8)\n        )\n\n        # Compare expected vs actual\n        is_compressed = (int(expected_num_bytes)) &gt; len(instance.PixelData)\n\n        return is_compressed\n\n    @staticmethod\n    def _compress_pixel_data(\n        instance: pydicom.dataset.FileDataset,\n    ) -&gt; pydicom.dataset.FileDataset:\n        \"\"\"Recompress pixel data that was decompressed during redaction.\n\n        :param instance: Loaded DICOM instance.\n\n        :return: Instance with compressed pixel data.\n        \"\"\"\n        compression_method = pydicom.uid.RLELossless\n\n        # Temporarily change syntax to an \"uncompressed\" method\n        instance.file_meta.TransferSyntaxUID = pydicom.uid.UID(\"1.2.840.10008.1.2\")\n\n        # Compress and update syntax\n        instance.compress(compression_method, encoding_plugin=\"gdcm\")\n        instance.file_meta.TransferSyntaxUID = compression_method\n\n        return instance\n\n    @staticmethod\n    def _check_if_has_image_icon_sequence(\n        instance: pydicom.dataset.FileDataset,\n    ) -&gt; bool:\n        \"\"\"Check if there is an image icon sequence tag in the metadata.\n\n        This leads to pixel data being present in multiple locations.\n\n        :param instance: DICOM instance.\n\n        :return: Boolean for whether the instance has an image icon sequence tag.\n        \"\"\"\n        has_image_icon_sequence = False\n        try:\n            _ = instance[0x0088, 0x0200]\n            has_image_icon_sequence = True\n        except KeyError:\n            has_image_icon_sequence = False\n\n        return has_image_icon_sequence\n\n    @classmethod\n    def _add_redact_box(\n        cls,\n        instance: pydicom.dataset.FileDataset,\n        bounding_boxes_coordinates: list,\n        crop_ratio: float,\n        fill: str = \"contrast\",\n    ) -&gt; pydicom.dataset.FileDataset:\n        \"\"\"Add redaction bounding boxes on a DICOM instance.\n\n        :param instance: A single DICOM instance.\n        :param bounding_boxes_coordinates: Bounding box coordinates.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Determines how box color is selected.\n        'contrast' - Masks stand out relative to background.\n        'background' - Masks are same color as background.\n\n        :return: A new dicom instance with redaction bounding boxes.\n        \"\"\"\n        # Copy instance\n        redacted_instance = deepcopy(instance)\n        is_compressed = cls._check_if_compressed(redacted_instance)\n        has_image_icon_sequence = cls._check_if_has_image_icon_sequence(\n            redacted_instance\n        )\n\n        # Select masking box color\n        is_greyscale = cls._check_if_greyscale(instance)\n        if is_greyscale:\n            box_color = cls._get_most_common_pixel_value(instance, crop_ratio, fill)\n        else:\n            box_color = cls._set_bbox_color(redacted_instance, fill)\n\n        # Apply mask\n        for i in range(0, len(bounding_boxes_coordinates)):\n            bbox = bounding_boxes_coordinates[i]\n            top = bbox[\"top\"]\n            left = bbox[\"left\"]\n            width = bbox[\"width\"]\n            height = bbox[\"height\"]\n            redacted_instance.pixel_array[\n                top : top + height, left : left + width\n            ] = box_color\n\n        redacted_instance.PixelData = redacted_instance.pixel_array.tobytes()\n\n        # If original pixel data is compressed, recompress after redaction\n        if is_compressed or has_image_icon_sequence:\n            # Temporary \"fix\" to manually set all YBR photometric interp as YBR_FULL\n            if \"YBR\" in redacted_instance.PhotometricInterpretation:\n                redacted_instance.PhotometricInterpretation = \"YBR_FULL\"\n            redacted_instance = cls._compress_pixel_data(redacted_instance)\n\n        return redacted_instance\n\n    def _get_analyzer_results(\n        self,\n        image: Image.Image,\n        instance: pydicom.dataset.FileDataset,\n        use_metadata: bool,\n        ocr_kwargs: Optional[dict],\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]],\n        **text_analyzer_kwargs,\n    ) -&gt; List[ImageRecognizerResult]:\n        \"\"\"Analyze image with selected redaction approach.\n\n        :param image: DICOM pixel data as PIL image.\n        :param instance: DICOM instance (with metadata).\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine (e.g., allow_list).\n\n        :return: Analyzer results.\n        \"\"\"\n        # Check the ad-hoc recognizers list\n        self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n        # Create custom recognizer using DICOM metadata\n        if use_metadata:\n            original_metadata, is_name, is_patient = self._get_text_metadata(instance)\n            phi_list = self._make_phi_list(original_metadata, is_name, is_patient)\n            deny_list_recognizer = PatternRecognizer(\n                supported_entity=\"PERSON\", deny_list=phi_list\n            )\n\n            if type(ad_hoc_recognizers) is None:\n                ad_hoc_recognizers = [deny_list_recognizer]\n            elif type(ad_hoc_recognizers) is list:\n                ad_hoc_recognizers.append(deny_list_recognizer)\n\n        # Detect PII\n        if ad_hoc_recognizers is None:\n            analyzer_results = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                **text_analyzer_kwargs,\n            )\n        else:\n            analyzer_results = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                ad_hoc_recognizers=ad_hoc_recognizers,\n                **text_analyzer_kwargs,\n            )\n\n        return analyzer_results\n\n    @staticmethod\n    def _save_bbox_json(output_dcm_path: str, bboxes: List[Dict[str, int]]) -&gt; None:\n        \"\"\"Save the redacted bounding box info as a json file.\n\n        :param output_dcm_path: Path to the redacted DICOM file.\n\n        :param bboxes: Bounding boxes used in redaction.\n        \"\"\"\n        output_json_path = Path(output_dcm_path).with_suffix(\".json\")\n\n        with open(output_json_path, \"w\") as write_file:\n            json.dump(bboxes, write_file, indent=4)\n\n    def _redact_single_dicom_image(\n        self,\n        dcm_path: str,\n        crop_ratio: float,\n        fill: str,\n        padding_width: int,\n        use_metadata: bool,\n        overwrite: bool,\n        dst_parent_dir: str,\n        save_bboxes: bool,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; str:\n        \"\"\"Redact text PHI present on a DICOM image.\n\n        :param dcm_path: String path to the DICOM file.\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Color setting to use for bounding boxes\n        (\"contrast\" or \"background\").\n        :param padding_width: Pixel width of padding (uniform).\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param overwrite: Only set to True if you are providing the\n        duplicated DICOM path in dcm_path.\n        :param dst_parent_dir: String path to parent directory of where to store copies.\n        :param save_bboxes: True if we want to save boundings boxes.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        :return: Path to the output DICOM file.\n        \"\"\"\n        # Ensure we are working on a single file\n        if Path(dcm_path).is_dir():\n            raise FileNotFoundError(\"Please ensure dcm_path is a single file\")\n        elif Path(dcm_path).is_file() is False:\n            raise FileNotFoundError(f\"{dcm_path} does not exist\")\n\n        # Copy file before processing if overwrite==False\n        if overwrite is False:\n            dst_path = self._copy_files_for_processing(dcm_path, dst_parent_dir)\n        else:\n            dst_path = dcm_path\n\n        # Load instance\n        instance = pydicom.dcmread(dst_path)\n\n        try:\n            instance.PixelData\n        except AttributeError:\n            raise AttributeError(\"Provided DICOM file lacks pixel data.\")\n\n        # Load image for processing\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            # Convert DICOM to PNG and add padding for OCR (during analysis)\n            _, is_greyscale = self._convert_dcm_to_png(dst_path, output_dir=tmpdirname)\n            png_filepath = f\"{tmpdirname}/{dst_path.stem}.png\"\n            loaded_image = Image.open(png_filepath)\n            image = self._add_padding(loaded_image, is_greyscale, padding_width)\n\n        # Detect PII\n        analyzer_results = self._get_analyzer_results(\n            image,\n            instance,\n            use_metadata,\n            ocr_kwargs,\n            ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n        # Redact all bounding boxes from DICOM file\n        analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n            analyzer_results\n        )\n        bboxes = self.bbox_processor.remove_bbox_padding(analyzer_bboxes, padding_width)\n        redacted_dicom_instance = self._add_redact_box(\n            instance, bboxes, crop_ratio, fill\n        )\n        redacted_dicom_instance.save_as(dst_path)\n\n        # Save redacted bboxes\n        if save_bboxes:\n            self._save_bbox_json(dst_path, bboxes)\n\n        return dst_path\n\n    def _redact_multiple_dicom_images(\n        self,\n        dcm_dir: str,\n        crop_ratio: float,\n        fill: str,\n        padding_width: int,\n        use_metadata: bool,\n        overwrite: bool,\n        dst_parent_dir: str,\n        save_bboxes: bool,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; str:\n        \"\"\"Redact text PHI present on all DICOM images in a directory.\n\n        :param dcm_dir: String path to directory containing DICOM files (can be nested).\n        :param crop_ratio: Portion of image to consider when selecting\n        most common pixel value as the background color value.\n        :param fill: Color setting to use for bounding boxes\n        (\"contrast\" or \"background\").\n        :param padding_width: Pixel width of padding (uniform).\n        :param use_metadata: Whether to redact text in the image that\n        are present in the metadata.\n        :param overwrite: Only set to True if you are providing\n        the duplicated DICOM dir in dcm_dir.\n        :param dst_parent_dir: String path to parent directory of where to store copies.\n        :param save_bboxes: True if we want to save boundings boxes.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        Return:\n            dst_dir (str): Path to the output DICOM directory.\n        \"\"\"\n        # Ensure we are working on a directory (can have sub-directories)\n        if Path(dcm_dir).is_file():\n            raise FileNotFoundError(\"Please ensure dcm_path is a directory\")\n        elif Path(dcm_dir).is_dir() is False:\n            raise FileNotFoundError(f\"{dcm_dir} does not exist\")\n\n        # List of files to process directly\n        if overwrite is False:\n            dst_dir = self._copy_files_for_processing(dcm_dir, dst_parent_dir)\n        else:\n            dst_dir = dcm_dir\n\n        # Process each DICOM file directly\n        all_dcm_files = self._get_all_dcm_files(Path(dst_dir))\n        for dst_path in all_dcm_files:\n            self._redact_single_dicom_image(\n                dst_path,\n                crop_ratio,\n                fill,\n                padding_width,\n                use_metadata,\n                overwrite,\n                dst_parent_dir,\n                save_bboxes,\n                ocr_kwargs=ocr_kwargs,\n                ad_hoc_recognizers=ad_hoc_recognizers,\n                **text_analyzer_kwargs,\n            )\n\n        return dst_dir\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine.augment_word","title":"<code>augment_word(word, case_sensitive=False)</code>  <code>staticmethod</code>","text":"<p>Apply multiple types of casing to the provided string.</p> <p>Parameters:</p> Name Type Description Default <code>words</code> <p>String containing the word or term of interest.</p> required <code>case_sensitive</code> <code>bool</code> <p>True if we want to preserve casing.</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>List of the same string with different casings and spacing.</p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>@staticmethod\ndef augment_word(word: str, case_sensitive: bool = False) -&gt; list:\n    \"\"\"Apply multiple types of casing to the provided string.\n\n    :param words: String containing the word or term of interest.\n    :param case_sensitive: True if we want to preserve casing.\n\n    :return: List of the same string with different casings and spacing.\n    \"\"\"\n    word_list = []\n    if word != \"\":\n        # Replacing separator character with space, if any\n        text_no_separator = word.replace(\"^\", \" \")\n        text_no_separator = text_no_separator.replace(\"-\", \" \")\n        text_no_separator = \" \".join(text_no_separator.split())\n\n        if case_sensitive:\n            word_list.append(text_no_separator)\n            word_list.extend(\n                [\n                    text_no_separator.split(\" \"),\n                ]\n            )\n        else:\n            # Capitalize all characters in string\n            text_upper = text_no_separator.upper()\n\n            # Lowercase all characters in string\n            text_lower = text_no_separator.lower()\n\n            # Capitalize first letter in each part of string\n            text_title = text_no_separator.title()\n\n            # Append iterations\n            word_list.extend(\n                [text_no_separator, text_upper, text_lower, text_title]\n            )\n\n            # Adding each term as a separate item in the list\n            word_list.extend(\n                [\n                    text_no_separator.split(\" \"),\n                    text_upper.split(\" \"),\n                    text_lower.split(\" \"),\n                    text_title.split(\" \"),\n                ]\n            )\n\n        # Flatten list\n        flat_list = []\n        for item in word_list:\n            if isinstance(item, list):\n                flat_list.extend(item)\n            else:\n                flat_list.append(item)\n\n        # Remove any duplicates and empty strings\n        word_list = list(set(flat_list))\n        word_list = list(filter(None, word_list))\n\n    return word_list\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine.redact","title":"<code>redact(image, fill='contrast', padding_width=25, crop_ratio=0.75, ocr_kwargs=None, ad_hoc_recognizers=None, **text_analyzer_kwargs)</code>","text":"<p>Redact method to redact the given DICOM image.</p> <p>Please note, this method duplicates the image, creates a new instance and manipulates it.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FileDataset</code> <p>Loaded DICOM instance including pixel data and metadata.</p> required <code>fill</code> <code>str</code> <p>Fill setting to use for redaction box (\"contrast\" or \"background\").</p> <code>'contrast'</code> <code>padding_width</code> <code>int</code> <p>Padding width to use when running OCR.</p> <code>25</code> <code>crop_ratio</code> <code>float</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <code>0.75</code> <code>ocr_kwargs</code> <code>Optional[dict]</code> <p>Additional params for OCR methods.</p> <code>None</code> <code>ad_hoc_recognizers</code> <code>Optional[List[PatternRecognizer]]</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <code>None</code> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FileDataset</code> <p>DICOM instance with redacted pixel data.</p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact(\n    self,\n    image: pydicom.dataset.FileDataset,\n    fill: str = \"contrast\",\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; pydicom.dataset.FileDataset:\n    \"\"\"Redact method to redact the given DICOM image.\n\n    Please note, this method duplicates the image, creates a\n    new instance and manipulates it.\n\n    :param image: Loaded DICOM instance including pixel data and metadata.\n    :param fill: Fill setting to use for redaction box (\"contrast\" or \"background\").\n    :param padding_width: Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: DICOM instance with redacted pixel data.\n    \"\"\"\n    redacted_image, _ = self.redact_and_return_bbox(\n        image=image,\n        fill=fill,\n        padding_width=padding_width,\n        crop_ratio=crop_ratio,\n        ocr_kwargs=ocr_kwargs,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n\n    return redacted_image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine.redact_and_return_bbox","title":"<code>redact_and_return_bbox(image, fill='contrast', padding_width=25, crop_ratio=0.75, use_metadata=True, ocr_kwargs=None, ad_hoc_recognizers=None, **text_analyzer_kwargs)</code>","text":"<p>Redact method to redact the given DICOM image and return redacted bboxes.</p> <p>Please note, this method duplicates the image, creates a new instance and manipulates it.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>FileDataset</code> <p>Loaded DICOM instance including pixel data and metadata.</p> required <code>fill</code> <code>str</code> <p>Fill setting to use for redaction box (\"contrast\" or \"background\").</p> <code>'contrast'</code> <code>padding_width</code> <code>int</code> <p>Padding width to use when running OCR.</p> <code>25</code> <code>crop_ratio</code> <code>float</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <code>0.75</code> <code>use_metadata</code> <code>bool</code> <p>Whether to redact text in the image that are present in the metadata.</p> <code>True</code> <code>ocr_kwargs</code> <code>Optional[dict]</code> <p>Additional params for OCR methods.</p> <code>None</code> <code>ad_hoc_recognizers</code> <code>Optional[List[PatternRecognizer]]</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <code>None</code> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[FileDataset, List[Dict[str, int]]]</code> <p>DICOM instance with redacted pixel data.</p> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact_and_return_bbox(\n    self,\n    image: pydicom.dataset.FileDataset,\n    fill: str = \"contrast\",\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    use_metadata: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Tuple[pydicom.dataset.FileDataset, List[Dict[str, int]]]:\n    \"\"\"Redact method to redact the given DICOM image and return redacted bboxes.\n\n    Please note, this method duplicates the image, creates a\n    new instance and manipulates it.\n\n    :param image: Loaded DICOM instance including pixel data and metadata.\n    :param fill: Fill setting to use for redaction box (\"contrast\" or \"background\").\n    :param padding_width: Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: DICOM instance with redacted pixel data.\n    \"\"\"\n    # Check input\n    if type(image) not in [pydicom.dataset.FileDataset, pydicom.dataset.Dataset]:\n        raise TypeError(\"The provided image must be a loaded DICOM instance.\")\n    try:\n        image.PixelData\n    except AttributeError as e:\n        raise AttributeError(f\"Provided DICOM instance lacks pixel data: {e}\")\n    except PermissionError as e:\n        raise PermissionError(f\"Unable to access pixel data (may not exist): {e}\")\n    except IsADirectoryError as e:\n        raise IsADirectoryError(f\"DICOM instance is a directory: {e}\")\n\n    instance = deepcopy(image)\n\n    # Load image for processing\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        # Convert DICOM to PNG and add padding for OCR (during analysis)\n        is_greyscale = self._check_if_greyscale(instance)\n        image = self._rescale_dcm_pixel_array(instance, is_greyscale)\n        image_name = str(uuid.uuid4())\n        self._save_pixel_array_as_png(image, is_greyscale, image_name, tmpdirname)\n\n        png_filepath = f\"{tmpdirname}/{image_name}.png\"\n        loaded_image = Image.open(png_filepath)\n        image = self._add_padding(loaded_image, is_greyscale, padding_width)\n\n    # Detect PII\n    analyzer_results = self._get_analyzer_results(\n        image,\n        instance,\n        use_metadata,\n        ocr_kwargs,\n        ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n\n    # Redact all bounding boxes from DICOM file\n    analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n        analyzer_results\n    )\n    bboxes = self.bbox_processor.remove_bbox_padding(analyzer_bboxes, padding_width)\n    redacted_image = self._add_redact_box(instance, bboxes, crop_ratio, fill)\n\n    return redacted_image, bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine.redact_from_directory","title":"<code>redact_from_directory(input_dicom_path, output_dir, padding_width=25, crop_ratio=0.75, fill='contrast', use_metadata=True, save_bboxes=False, ocr_kwargs=None, ad_hoc_recognizers=None, **text_analyzer_kwargs)</code>","text":"<p>Redact method to redact from a directory of files.</p> <p>Please notice, this method duplicates the files, creates new instances and manipulate them.</p> <p>Parameters:</p> Name Type Description Default <code>input_dicom_path</code> <code>str</code> <p>String path to directory of DICOM images.</p> required <code>output_dir</code> <code>str</code> <p>String path to parent output directory.</p> required <code></code> <code>padding_width</code> <p>Padding width to use when running OCR.</p> required <code>crop_ratio</code> <code>float</code> <p>Portion of image to consider when selecting most common pixel value as the background color value.</p> <code>0.75</code> <code>fill</code> <code>str</code> <p>Color setting to use for redaction box (\"contrast\" or \"background\").</p> <code>'contrast'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to redact text in the image that are present in the metadata.</p> <code>True</code> <code>save_bboxes</code> <code>bool</code> <p>True if we want to save boundings boxes.</p> <code>False</code> <code>ocr_kwargs</code> <code>Optional[dict]</code> <p>Additional params for OCR methods.</p> <code>None</code> <code>ad_hoc_recognizers</code> <code>Optional[List[PatternRecognizer]]</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <code>None</code> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <code>{}</code> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact_from_directory(\n    self,\n    input_dicom_path: str,\n    output_dir: str,\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    fill: str = \"contrast\",\n    use_metadata: bool = True,\n    save_bboxes: bool = False,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; None:\n    \"\"\"Redact method to redact from a directory of files.\n\n    Please notice, this method duplicates the files, creates\n    new instances and manipulate them.\n\n    :param input_dicom_path: String path to directory of DICOM images.\n    :param output_dir: String path to parent output directory.\n    :param padding_width : Padding width to use when running OCR.\n    :param crop_ratio: Portion of image to consider when selecting\n    most common pixel value as the background color value.\n    :param fill: Color setting to use for redaction box\n    (\"contrast\" or \"background\").\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param save_bboxes: True if we want to save boundings boxes.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n    \"\"\"\n    # Verify the given paths\n    if Path(input_dicom_path).is_dir() is False:\n        raise TypeError(\"input_dicom_path must be a valid directory\")\n    if Path(input_dicom_path).is_file() is True:\n        raise TypeError(\"input_dicom_path must be a directory (not file)\")\n    if Path(output_dir).is_file() is True:\n        raise TypeError(\n            \"output_dir must be a directory (does not need to exist yet)\"\n        )\n\n    # Create duplicates\n    dst_path = self._copy_files_for_processing(input_dicom_path, output_dir)\n\n    # Process DICOM files\n    output_location = self._redact_multiple_dicom_images(\n        dcm_dir=dst_path,\n        crop_ratio=crop_ratio,\n        fill=fill,\n        padding_width=padding_width,\n        use_metadata=use_metadata,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        overwrite=True,\n        dst_parent_dir=\".\",\n        save_bboxes=save_bboxes,\n        ocr_kwargs=ocr_kwargs,\n        **text_analyzer_kwargs,\n    )\n\n    print(f\"Output written to {output_location}\")\n\n    return None\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DicomImageRedactorEngine.redact_from_file","title":"<code>redact_from_file(input_dicom_path, output_dir, padding_width=25, crop_ratio=0.75, fill='contrast', use_metadata=True, save_bboxes=False, verbose=True, ocr_kwargs=None, ad_hoc_recognizers=None, **text_analyzer_kwargs)</code>","text":"<p>Redact method to redact from a given file.</p> <p>Please notice, this method duplicates the file, creates new instance and manipulate them.</p> <p>Parameters:</p> Name Type Description Default <code>input_dicom_path</code> <code>str</code> <p>String path to DICOM image.</p> required <code>output_dir</code> <code>str</code> <p>String path to parent output directory.</p> required <code></code> <code>padding_width</code> <p>Padding width to use when running OCR.</p> required <code>fill</code> <code>str</code> <p>Color setting to use for redaction box (\"contrast\" or \"background\").</p> <code>'contrast'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to redact text in the image that are present in the metadata.</p> <code>True</code> <code>save_bboxes</code> <code>bool</code> <p>True if we want to save boundings boxes.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>True to print where redacted file was written to.</p> <code>True</code> <code>ocr_kwargs</code> <code>Optional[dict]</code> <p>Additional params for OCR methods.</p> <code>None</code> <code>ad_hoc_recognizers</code> <code>Optional[List[PatternRecognizer]]</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <code>None</code> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <code>{}</code> Source code in <code>presidio_image_redactor/dicom_image_redactor_engine.py</code> <pre><code>def redact_from_file(\n    self,\n    input_dicom_path: str,\n    output_dir: str,\n    padding_width: int = 25,\n    crop_ratio: float = 0.75,\n    fill: str = \"contrast\",\n    use_metadata: bool = True,\n    save_bboxes: bool = False,\n    verbose: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; None:\n    \"\"\"Redact method to redact from a given file.\n\n    Please notice, this method duplicates the file, creates\n    new instance and manipulate them.\n\n    :param input_dicom_path: String path to DICOM image.\n    :param output_dir: String path to parent output directory.\n    :param padding_width : Padding width to use when running OCR.\n    :param fill: Color setting to use for redaction box\n    (\"contrast\" or \"background\").\n    :param use_metadata: Whether to redact text in the image that\n    are present in the metadata.\n    :param save_bboxes: True if we want to save boundings boxes.\n    :param verbose: True to print where redacted file was written to.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n    \"\"\"\n    # Verify the given paths\n    if Path(input_dicom_path).is_dir() is True:\n        raise TypeError(\"input_dicom_path must be file (not dir)\")\n    if Path(input_dicom_path).is_file() is False:\n        raise TypeError(\"input_dicom_path must be a valid file\")\n    if Path(output_dir).is_file() is True:\n        raise TypeError(\n            \"output_dir must be a directory (does not need to exist yet)\"\n        )\n\n    # Create duplicate\n    dst_path = self._copy_files_for_processing(input_dicom_path, output_dir)\n\n    # Process DICOM file\n    output_location = self._redact_single_dicom_image(\n        dcm_path=dst_path,\n        crop_ratio=crop_ratio,\n        fill=fill,\n        padding_width=padding_width,\n        use_metadata=use_metadata,\n        overwrite=True,\n        dst_parent_dir=\".\",\n        save_bboxes=save_bboxes,\n        ocr_kwargs=ocr_kwargs,\n        ad_hoc_recognizers=ad_hoc_recognizers,\n        **text_analyzer_kwargs,\n    )\n\n    if verbose:\n        print(f\"Output written to {output_location}\")\n\n    return None\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DocumentIntelligenceOCR","title":"<code>DocumentIntelligenceOCR</code>","text":"<p>             Bases: <code>OCR</code></p> <p>OCR class that uses Azure AI Document Intelligence OCR engine.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Optional[str]</code> <p>The API key</p> <code>None</code> <code>endpoint</code> <code>Optional[str]</code> <p>The API endpoint</p> <code>None</code> <code>model_id</code> <code>Optional[str]</code> <p>Which model to use  For details, see https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/</p> <code>'prebuilt-document'</code> Source code in <code>presidio_image_redactor/document_intelligence_ocr.py</code> <pre><code>class DocumentIntelligenceOCR(OCR):\n    \"\"\"OCR class that uses Azure AI Document Intelligence OCR engine.\n\n    :param key: The API key\n    :param endpoint: The API endpoint\n    :param model_id: Which model to use\n\n    For details, see\n    https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/\n    \"\"\"\n\n    SUPPORTED_MODELS = [\n        \"prebuilt-document\",\n        \"prebuilt-read\",\n        \"prebuilt-layout\",\n        \"prebuilt-contract\",\n        \"prebuilt-healthInsuranceCard.us\",\n        \"prebuilt-invoice\",\n        \"prebuilt-receipt\",\n        \"prebuilt-idDocument\",\n        \"prebuilt-businessCard\"\n    ]\n\n    def __init__(self,\n                 endpoint: Optional[str] = None,\n                 key: Optional[str] = None,\n                 model_id: Optional[str] = \"prebuilt-document\"):\n        if model_id not in DocumentIntelligenceOCR.SUPPORTED_MODELS:\n            raise ValueError(\"Unsupported model id: %s\" % model_id)\n\n        # If endpoint and/or key are not passed, attempt to get from environment\n        # variables\n        if not endpoint:\n            endpoint = os.getenv(\"DOCUMENT_INTELLIGENCE_ENDPOINT\")\n\n        if not key:\n            key = os.getenv(\"DOCUMENT_INTELLIGENCE_KEY\")\n\n        if not key or not endpoint:\n            raise ValueError(\"Endpoint and key must be specified\")\n\n        self.client = DocumentAnalysisClient(\n            endpoint=endpoint,\n            credential=AzureKeyCredential(key)\n        )\n        self.model_id = model_id\n\n    @staticmethod\n    def _polygon_to_bbox(polygon : Sequence[Point]) -&gt; tuple:\n        \"\"\"Convert polygon to a tuple of left/top/width/height.\n\n        The returned bounding box should entirely cover the passed polygon.\n\n        :param polygon: A sequence of points\n\n        :return a tuple of left/top/width/height in pixel dimensions\n\n        \"\"\"\n        # We need at least two points for a valid bounding box.\n        if len(polygon) &lt; 2:\n            return (0, 0, 0, 0)\n\n        left = min([int(p.x) for p in polygon])\n        top = min([int(p.y) for p in polygon])\n        right = max([int(p.x) for p in polygon])\n        bottom = max([int(p.y) for p in polygon])\n        width = right - left\n        height = bottom - top\n        return (left, top, width, height)\n\n    @staticmethod\n    def _page_to_bboxes(page: DocumentPage) -&gt; dict:\n        \"\"\"Convert bounding boxes to uniform format.\n\n        Presidio supports tesseract format of output only, so we format in the same\n        way.\n        Expected format looks like:\n        {\n            \"left\": [123, 345],\n            \"top\": [0, 15],\n            \"width\": [100, 75],\n            \"height\": [25, 30],\n            \"conf\": [\"1\", \"0.87\"],\n            \"text\": [\"JOHN\", \"DOE\"],\n        }\n\n        :param page: The documentpage object from the DI client library\n\n        :return dictionary in the expected format for presidio\n        \"\"\"\n        bounds = [DocumentIntelligenceOCR._polygon_to_bbox(word.polygon)\n                  for word in page.words]\n\n        return {\n            \"left\": [box[0] for box in bounds],\n            \"top\": [box[1] for box in bounds],\n            \"width\": [box[2] for box in bounds],\n            \"height\": [box[3] for box in bounds],\n            \"conf\": [w.confidence for w in page.words],\n            \"text\": [w.content for w in page.words]\n        }\n\n    def get_imgbytes(self, image: Union[bytes, np.ndarray, Image.Image]) -&gt; bytes:\n        \"\"\"Retrieve the image bytes from the image object.\n\n        :param image:  Any of bytes/numpy array /PIL image object\n\n        :return raw image bytes\n        \"\"\"\n        if isinstance(image, bytes):\n            return image\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n            # Fallthrough to process PIL image\n        if isinstance(image, Image.Image):\n            # Image is a PIL image, write to bytes stream\n            ostream = BytesIO()\n            image.save(ostream, 'PNG')\n            imgbytes = ostream.getvalue()\n        elif isinstance(image, str):\n            # image is a filename\n            imgbytes = open(image, \"rb\")\n        else:\n            raise ValueError(\"Unsupported image type: %s\" % type(image))\n        return imgbytes\n\n    def analyze_document(self, imgbytes : bytes, **kwargs) -&gt; AnalyzedDocument:\n        \"\"\"Analyze the document and return the result.\n\n        :param imgbytes: The bytes to send to the API endpoint\n        :param kwargs: additional arguments for begin_analyze_document\n\n        :return the result of the poller, an AnalyzedDocument object.\n        \"\"\"\n        poller = self.client.begin_analyze_document(self.model_id, imgbytes, **kwargs)\n        return poller.result()\n\n    def perform_ocr(self, image: object, **kwargs) -&gt; dict:\n        \"\"\"Perform OCR on the image.\n\n        :param image: PIL Image/numpy array or file path(str) to be processed\n        :param kwargs: Additional values for begin_analyze_document\n\n        :return: results dictionary containing bboxes and text for each detected word\n        \"\"\"\n        imgbytes = self.get_imgbytes(image)\n        result = self.analyze_document(imgbytes, **kwargs)\n\n        # Currently cannot handle more than one page.\n        if not (len(result.pages) == 1):\n            raise ValueError(\"DocumentIntelligenceOCR only supports 1 page documents\")\n\n        return DocumentIntelligenceOCR._page_to_bboxes(result.pages[0])\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DocumentIntelligenceOCR.analyze_document","title":"<code>analyze_document(imgbytes, **kwargs)</code>","text":"<p>Analyze the document and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>imgbytes</code> <code>bytes</code> <p>The bytes to send to the API endpoint</p> required <code>kwargs</code> <p>additional arguments for begin_analyze_document</p> <code>{}</code> Source code in <code>presidio_image_redactor/document_intelligence_ocr.py</code> <pre><code>def analyze_document(self, imgbytes : bytes, **kwargs) -&gt; AnalyzedDocument:\n    \"\"\"Analyze the document and return the result.\n\n    :param imgbytes: The bytes to send to the API endpoint\n    :param kwargs: additional arguments for begin_analyze_document\n\n    :return the result of the poller, an AnalyzedDocument object.\n    \"\"\"\n    poller = self.client.begin_analyze_document(self.model_id, imgbytes, **kwargs)\n    return poller.result()\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DocumentIntelligenceOCR.get_imgbytes","title":"<code>get_imgbytes(image)</code>","text":"<p>Retrieve the image bytes from the image object.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[bytes, ndarray, Image]</code> <p>Any of bytes/numpy array /PIL image object</p> required Source code in <code>presidio_image_redactor/document_intelligence_ocr.py</code> <pre><code>def get_imgbytes(self, image: Union[bytes, np.ndarray, Image.Image]) -&gt; bytes:\n    \"\"\"Retrieve the image bytes from the image object.\n\n    :param image:  Any of bytes/numpy array /PIL image object\n\n    :return raw image bytes\n    \"\"\"\n    if isinstance(image, bytes):\n        return image\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n        # Fallthrough to process PIL image\n    if isinstance(image, Image.Image):\n        # Image is a PIL image, write to bytes stream\n        ostream = BytesIO()\n        image.save(ostream, 'PNG')\n        imgbytes = ostream.getvalue()\n    elif isinstance(image, str):\n        # image is a filename\n        imgbytes = open(image, \"rb\")\n    else:\n        raise ValueError(\"Unsupported image type: %s\" % type(image))\n    return imgbytes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.DocumentIntelligenceOCR.perform_ocr","title":"<code>perform_ocr(image, **kwargs)</code>","text":"<p>Perform OCR on the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>object</code> <p>PIL Image/numpy array or file path(str) to be processed</p> required <code>kwargs</code> <p>Additional values for begin_analyze_document</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>results dictionary containing bboxes and text for each detected word</p> Source code in <code>presidio_image_redactor/document_intelligence_ocr.py</code> <pre><code>def perform_ocr(self, image: object, **kwargs) -&gt; dict:\n    \"\"\"Perform OCR on the image.\n\n    :param image: PIL Image/numpy array or file path(str) to be processed\n    :param kwargs: Additional values for begin_analyze_document\n\n    :return: results dictionary containing bboxes and text for each detected word\n    \"\"\"\n    imgbytes = self.get_imgbytes(image)\n    result = self.analyze_document(imgbytes, **kwargs)\n\n    # Currently cannot handle more than one page.\n    if not (len(result.pages) == 1):\n        raise ValueError(\"DocumentIntelligenceOCR only supports 1 page documents\")\n\n    return DocumentIntelligenceOCR._page_to_bboxes(result.pages[0])\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine","title":"<code>ImageAnalyzerEngine</code>","text":"<p>ImageAnalyzerEngine class.</p> <p>Parameters:</p> Name Type Description Default <code>analyzer_engine</code> <code>Optional[AnalyzerEngine]</code> <p>The Presidio AnalyzerEngine instance to be used to detect PII in text</p> <code>None</code> <code>ocr</code> <code>Optional[OCR]</code> <p>the OCR object to be used to detect text in images.</p> <code>None</code> <code>image_preprocessor</code> <code>Optional[ImagePreprocessor]</code> <p>The ImagePreprocessor object to be used to preprocess the image</p> <code>None</code> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>class ImageAnalyzerEngine:\n    \"\"\"ImageAnalyzerEngine class.\n\n    :param analyzer_engine: The Presidio AnalyzerEngine instance\n        to be used to detect PII in text\n    :param ocr: the OCR object to be used to detect text in images.\n    :param image_preprocessor: The ImagePreprocessor object to be\n        used to preprocess the image\n    \"\"\"\n\n    def __init__(\n        self,\n        analyzer_engine: Optional[AnalyzerEngine] = None,\n        ocr: Optional[OCR] = None,\n        image_preprocessor: Optional[ImagePreprocessor] = None,\n    ):\n        if not analyzer_engine:\n            analyzer_engine = AnalyzerEngine()\n        self.analyzer_engine = analyzer_engine\n\n        if not ocr:\n            ocr = TesseractOCR()\n        self.ocr = ocr\n\n        if not image_preprocessor:\n            image_preprocessor = ImagePreprocessor()\n        self.image_preprocessor = image_preprocessor\n\n    def analyze(\n        self, image: object, ocr_kwargs: Optional[dict] = None, **text_analyzer_kwargs\n    ) -&gt; List[ImageRecognizerResult]:\n        \"\"\"Analyse method to analyse the given image.\n\n        :param image: PIL Image/numpy array or file path(str) to be processed.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        :return: List of the extract entities with image bounding boxes.\n        \"\"\"\n        # Perform OCR\n        perform_ocr_kwargs, ocr_threshold = self._parse_ocr_kwargs(ocr_kwargs)\n        image, preprocessing_metadata = self.image_preprocessor.preprocess_image(image)\n        ocr_result = self.ocr.perform_ocr(image, **perform_ocr_kwargs)\n        ocr_result = self.remove_space_boxes(ocr_result)\n\n        if preprocessing_metadata and (\"scale_factor\" in preprocessing_metadata):\n            ocr_result = self._scale_bbox_results(\n                ocr_result, preprocessing_metadata[\"scale_factor\"]\n            )\n\n        # Apply OCR confidence threshold if it is passed in\n        if ocr_threshold:\n            ocr_result = self.threshold_ocr_result(ocr_result, ocr_threshold)\n\n        # Analyze text\n        text = self.ocr.get_text_from_ocr_dict(ocr_result)\n\n        analyzer_result = self.analyzer_engine.analyze(\n            text=text, language=\"en\", **text_analyzer_kwargs\n        )\n        allow_list = self._check_for_allow_list(text_analyzer_kwargs)\n        bboxes = self.map_analyzer_results_to_bounding_boxes(\n            analyzer_result, ocr_result, text, allow_list\n        )\n\n        return bboxes\n\n    @staticmethod\n    def threshold_ocr_result(ocr_result: dict, ocr_threshold: float) -&gt; dict:\n        \"\"\"Filter out OCR results below confidence threshold.\n\n        :param ocr_result: OCR results (raw).\n        :param ocr_threshold: Threshold value between -1 and 100.\n\n        :return: OCR results with low confidence items removed.\n        \"\"\"\n        if ocr_threshold &lt; -1 or ocr_threshold &gt; 100:\n            raise ValueError(\"ocr_threshold must be between -1 and 100\")\n\n        # Get indices of items above threshold\n        idx = list()\n        for i, val in enumerate(ocr_result[\"conf\"]):\n            if float(val) &gt;= ocr_threshold:\n                idx.append(i)\n\n        # Only retain high confidence items\n        filtered_ocr_result = {}\n        for key in list(ocr_result.keys()):\n            filtered_ocr_result[key] = [ocr_result[key][i] for i in idx]\n\n        return filtered_ocr_result\n\n    @staticmethod\n    def remove_space_boxes(ocr_result: dict) -&gt; dict:\n        \"\"\"Remove OCR bboxes that are for spaces.\n\n        :param ocr_result: OCR results (raw or thresholded).\n        :return: OCR results with empty words removed.\n        \"\"\"\n        # Get indices of items with no text\n        idx = list()\n        for i, text in enumerate(ocr_result[\"text\"]):\n            is_not_space = text.isspace() is False\n            if text != \"\" and is_not_space:\n                idx.append(i)\n\n        # Only retain items with text\n        filtered_ocr_result = {}\n        for key in list(ocr_result.keys()):\n            filtered_ocr_result[key] = [ocr_result[key][i] for i in idx]\n\n        return filtered_ocr_result\n\n    @staticmethod\n    def map_analyzer_results_to_bounding_boxes(\n        text_analyzer_results: List[RecognizerResult],\n        ocr_result: dict,\n        text: str,\n        allow_list: List[str],\n    ) -&gt; List[ImageRecognizerResult]:\n        \"\"\"Map extracted PII entities to image bounding boxes.\n\n        Matching is based on the position of the recognized entity from analyzer\n        and word (in ocr dict) in the text.\n\n        :param text_analyzer_results: PII entities recognized by presidio analyzer\n        :param ocr_result: dict results with words and bboxes from OCR\n        :param text: text the results are based on\n        :param allow_list: List of words to not redact\n\n        return: list of extracted entities with image bounding boxes\n        \"\"\"\n        if (not ocr_result) or (not text_analyzer_results):\n            return []\n\n        bboxes = []\n        proc_indexes = 0\n        indexes = len(text_analyzer_results)\n\n        pos = 0\n        iter_ocr = enumerate(ocr_result[\"text\"])\n        for index, word in iter_ocr:\n            if not word:\n                pos += 1\n            else:\n                for element in text_analyzer_results:\n                    text_element = text[element.start : element.end]\n                    # check position and text of ocr word matches recognized entity\n                    if (\n                        max(pos, element.start) &lt; min(element.end, pos + len(word))\n                    ) and ((text_element in word) or (word in text_element)):\n                        yes_make_bbox_for_word = (\n                            (word is not None)\n                            and (word != \"\")\n                            and (word.isspace() is False)\n                            and (word not in allow_list)\n                        )\n                        # Do not add bbox for standalone spaces / empty strings\n                        if yes_make_bbox_for_word:\n                            bboxes.append(\n                                ImageRecognizerResult(\n                                    element.entity_type,\n                                    element.start,\n                                    element.end,\n                                    element.score,\n                                    ocr_result[\"left\"][index],\n                                    ocr_result[\"top\"][index],\n                                    ocr_result[\"width\"][index],\n                                    ocr_result[\"height\"][index],\n                                )\n                            )\n\n                            # add bounding boxes for all words in ocr dict\n                            # contained within the text of recognized entity\n                            # based on relative position in the full text\n                            while pos + len(word) &lt; element.end:\n                                prev_word = word\n                                index, word = next(iter_ocr)\n                                yes_make_bbox_for_word = (\n                                    (word is not None)\n                                    and (word != \"\")\n                                    and (word.isspace() is False)\n                                    and (word not in allow_list)\n                                )\n                                if yes_make_bbox_for_word:\n                                    bboxes.append(\n                                        ImageRecognizerResult(\n                                            element.entity_type,\n                                            element.start,\n                                            element.end,\n                                            element.score,\n                                            ocr_result[\"left\"][index],\n                                            ocr_result[\"top\"][index],\n                                            ocr_result[\"width\"][index],\n                                            ocr_result[\"height\"][index],\n                                        )\n                                    )\n                                pos += len(prev_word) + 1\n                            proc_indexes += 1\n\n                if proc_indexes == indexes:\n                    break\n                pos += len(word) + 1\n\n        return bboxes\n\n    @staticmethod\n    def _scale_bbox_results(\n        ocr_result: Dict[str, List[Union[int, str]]], scale_factor: float\n    ) -&gt; Dict[str, float]:\n        \"\"\"Scale down the bounding box results based on a scale percentage.\n\n        :param ocr_result: OCR results (raw).\n        :param scale_percent: Scale percentage for resizing the bounding box.\n\n        :return: OCR results (scaled).\n        \"\"\"\n        scaled_results = deepcopy(ocr_result)\n        coordinate_keys = [\"left\", \"top\"]\n        dimension_keys = [\"width\", \"height\"]\n\n        for coord_key in coordinate_keys:\n            scaled_results[coord_key] = [\n                int(np.ceil((x) / (scale_factor))) for x in scaled_results[coord_key]\n            ]\n\n        for dim_key in dimension_keys:\n            scaled_results[dim_key] = [\n                max(1, int(np.ceil(x / (scale_factor))))\n                for x in scaled_results[dim_key]\n            ]\n        return scaled_results\n\n    @staticmethod\n    def _remove_bbox_padding(\n        analyzer_bboxes: List[Dict[str, Union[str, float, int]]],\n        padding_width: int,\n    ) -&gt; List[Dict[str, int]]:\n        \"\"\"Remove added padding in bounding box coordinates.\n\n        :param analyzer_bboxes: The bounding boxes from analyzer results.\n        :param padding_width: Pixel width used for padding (0 if no padding).\n\n        :return: Bounding box information per word.\n        \"\"\"\n\n        unpadded_results = deepcopy(analyzer_bboxes)\n        if padding_width &lt; 0:\n            raise ValueError(\"Padding width must be a non-negative integer.\")\n\n        coordinate_keys = [\"left\", \"top\"]\n        for coord_key in coordinate_keys:\n            unpadded_results[coord_key] = [\n                max(0, x - padding_width) for x in unpadded_results[coord_key]\n            ]\n\n        return unpadded_results\n\n    @staticmethod\n    def _parse_ocr_kwargs(ocr_kwargs: dict) -&gt; Tuple[dict, float]:\n        \"\"\"Parse the OCR-related kwargs.\n\n        :param ocr_kwargs: Parameters for OCR operations.\n\n        :return: Params for ocr.perform_ocr and ocr_threshold\n        \"\"\"\n        ocr_threshold = None\n        if ocr_kwargs is not None:\n            if \"ocr_threshold\" in ocr_kwargs:\n                ocr_threshold = ocr_kwargs[\"ocr_threshold\"]\n                ocr_kwargs = {\n                    key: value\n                    for key, value in ocr_kwargs.items()\n                    if key != \"ocr_threshold\"\n                }\n        else:\n            ocr_kwargs = {}\n\n        return ocr_kwargs, ocr_threshold\n\n    @staticmethod\n    def _check_for_allow_list(text_analyzer_kwargs: dict) -&gt; List[str]:\n        \"\"\"Check the text_analyzer_kwargs for an allow_list.\n\n        :param text_analyzer_kwargs: Text analyzer kwargs.\n        :return: The allow list if it exists.\n        \"\"\"\n        allow_list = []\n        if text_analyzer_kwargs is not None:\n            if \"allow_list\" in text_analyzer_kwargs:\n                allow_list = text_analyzer_kwargs[\"allow_list\"]\n\n        return allow_list\n\n    @staticmethod\n    def fig2img(fig: matplotlib.figure.Figure) -&gt; Image:\n        \"\"\"Convert a Matplotlib figure to a PIL Image and return it.\n\n        :param fig: Matplotlib figure.\n\n        :return: Image of figure.\n        \"\"\"\n        buf = io.BytesIO()\n        fig.savefig(buf)\n        buf.seek(0)\n        img = Image.open(buf)\n\n        return img\n\n    @staticmethod\n    def get_pii_bboxes(\n        ocr_bboxes: List[dict],\n        analyzer_bboxes: List[dict]\n    ) -&gt; List[dict]:\n        \"\"\"Get a list of bboxes with is_PII property.\n\n        :param ocr_bboxes: Bboxes from OCR results.\n        :param analyzer_bboxes: Bboxes from analyzer results.\n\n        :return: All bboxes with appropriate label for whether it is PHI or not.\n        \"\"\"\n        bboxes = []\n        for ocr_bbox in ocr_bboxes:\n            has_match = False\n\n            # Check if we have the same bbox in analyzer results\n            for analyzer_bbox in analyzer_bboxes:\n                has_same_position = (ocr_bbox[\"left\"] == analyzer_bbox[\"left\"] and ocr_bbox[\"top\"] == analyzer_bbox[\"top\"])  # noqa: E501\n                has_same_dimension = (ocr_bbox[\"width\"] == analyzer_bbox[\"width\"] and ocr_bbox[\"height\"] == analyzer_bbox[\"height\"])  # noqa: E501\n                is_same = (has_same_position is True and has_same_dimension is True)\n\n                if is_same is True:\n                    current_bbox = analyzer_bbox\n                    current_bbox[\"is_PII\"] = True\n                    has_match = True\n                    break\n\n            if has_match is False:\n                current_bbox = ocr_bbox\n                current_bbox[\"is_PII\"] = False\n\n            bboxes.append(current_bbox)\n\n        return bboxes\n\n    @classmethod\n    def add_custom_bboxes(\n        cls,\n        image: Image,\n        bboxes: List[dict],\n        show_text_annotation: bool = True,\n        use_greyscale_cmap: bool = False\n    ) -&gt; Image:\n        \"\"\"Add custom bounding boxes to image.\n\n        :param image: Standard image of DICOM pixels.\n        :param bboxes: List of bounding boxes to display (with is_PII field).\n        :param gt_bboxes: Ground truth bboxes (list of dictionaries).\n        :param show_text_annotation: True if you want text annotation for\n        PHI status to display.\n        :param use_greyscale_cmap: Use greyscale color map.\n        :return: Image with bounding boxes drawn on.\n        \"\"\"\n        image_custom = ImageChops.duplicate(image)\n        image_x, image_y = image_custom.size\n\n        fig, ax = plt.subplots()\n        image_r = 70\n        fig.set_size_inches(image_x / image_r, image_y / image_r)\n\n        if len(bboxes) == 0:\n            ax.imshow(image_custom)\n            return image_custom\n        else:\n            for box in bboxes:\n                try:\n                    entity_type = box[\"entity_type\"]\n                except KeyError:\n                    entity_type = \"UNKNOWN\"\n\n                try:\n                    if box[\"is_PII\"]:\n                        bbox_color = \"r\"\n                    else:\n                        bbox_color = \"b\"\n                except KeyError:\n                    bbox_color = \"b\"\n\n                # Get coordinates and dimensions\n                x0 = box[\"left\"]\n                y0 = box[\"top\"]\n                x1 = x0 + box[\"width\"]\n                y1 = y0 + box[\"height\"]\n                rect = matplotlib.patches.Rectangle(\n                    (x0, y0),\n                    x1 - x0, y1 - y0,\n                    edgecolor=bbox_color,\n                    facecolor=\"none\"\n                )\n                ax.add_patch(rect)\n                if show_text_annotation:\n                    ax.annotate(\n                        entity_type,\n                        xy=(x0 - 3, y0 - 3),\n                        xycoords=\"data\",\n                        bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"0.9\"),\n                    )\n            if use_greyscale_cmap:\n                ax.imshow(image_custom, cmap=\"gray\")\n            else:\n                ax.imshow(image_custom)\n            im_from_fig = cls.fig2img(fig)\n            im_resized = im_from_fig.resize((image_x, image_y))\n\n        return im_resized\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.add_custom_bboxes","title":"<code>add_custom_bboxes(image, bboxes, show_text_annotation=True, use_greyscale_cmap=False)</code>  <code>classmethod</code>","text":"<p>Add custom bounding boxes to image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Standard image of DICOM pixels.</p> required <code>bboxes</code> <code>List[dict]</code> <p>List of bounding boxes to display (with is_PII field).</p> required <code>gt_bboxes</code> <p>Ground truth bboxes (list of dictionaries).</p> required <code>show_text_annotation</code> <code>bool</code> <p>True if you want text annotation for PHI status to display.</p> <code>True</code> <code>use_greyscale_cmap</code> <code>bool</code> <p>Use greyscale color map.</p> <code>False</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image with bounding boxes drawn on.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@classmethod\ndef add_custom_bboxes(\n    cls,\n    image: Image,\n    bboxes: List[dict],\n    show_text_annotation: bool = True,\n    use_greyscale_cmap: bool = False\n) -&gt; Image:\n    \"\"\"Add custom bounding boxes to image.\n\n    :param image: Standard image of DICOM pixels.\n    :param bboxes: List of bounding boxes to display (with is_PII field).\n    :param gt_bboxes: Ground truth bboxes (list of dictionaries).\n    :param show_text_annotation: True if you want text annotation for\n    PHI status to display.\n    :param use_greyscale_cmap: Use greyscale color map.\n    :return: Image with bounding boxes drawn on.\n    \"\"\"\n    image_custom = ImageChops.duplicate(image)\n    image_x, image_y = image_custom.size\n\n    fig, ax = plt.subplots()\n    image_r = 70\n    fig.set_size_inches(image_x / image_r, image_y / image_r)\n\n    if len(bboxes) == 0:\n        ax.imshow(image_custom)\n        return image_custom\n    else:\n        for box in bboxes:\n            try:\n                entity_type = box[\"entity_type\"]\n            except KeyError:\n                entity_type = \"UNKNOWN\"\n\n            try:\n                if box[\"is_PII\"]:\n                    bbox_color = \"r\"\n                else:\n                    bbox_color = \"b\"\n            except KeyError:\n                bbox_color = \"b\"\n\n            # Get coordinates and dimensions\n            x0 = box[\"left\"]\n            y0 = box[\"top\"]\n            x1 = x0 + box[\"width\"]\n            y1 = y0 + box[\"height\"]\n            rect = matplotlib.patches.Rectangle(\n                (x0, y0),\n                x1 - x0, y1 - y0,\n                edgecolor=bbox_color,\n                facecolor=\"none\"\n            )\n            ax.add_patch(rect)\n            if show_text_annotation:\n                ax.annotate(\n                    entity_type,\n                    xy=(x0 - 3, y0 - 3),\n                    xycoords=\"data\",\n                    bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"0.9\"),\n                )\n        if use_greyscale_cmap:\n            ax.imshow(image_custom, cmap=\"gray\")\n        else:\n            ax.imshow(image_custom)\n        im_from_fig = cls.fig2img(fig)\n        im_resized = im_from_fig.resize((image_x, image_y))\n\n    return im_resized\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.analyze","title":"<code>analyze(image, ocr_kwargs=None, **text_analyzer_kwargs)</code>","text":"<p>Analyse method to analyse the given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>object</code> <p>PIL Image/numpy array or file path(str) to be processed.</p> required <code>ocr_kwargs</code> <code>Optional[dict]</code> <p>Additional params for OCR methods.</p> <code>None</code> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[ImageRecognizerResult]</code> <p>List of the extract entities with image bounding boxes.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>def analyze(\n    self, image: object, ocr_kwargs: Optional[dict] = None, **text_analyzer_kwargs\n) -&gt; List[ImageRecognizerResult]:\n    \"\"\"Analyse method to analyse the given image.\n\n    :param image: PIL Image/numpy array or file path(str) to be processed.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: List of the extract entities with image bounding boxes.\n    \"\"\"\n    # Perform OCR\n    perform_ocr_kwargs, ocr_threshold = self._parse_ocr_kwargs(ocr_kwargs)\n    image, preprocessing_metadata = self.image_preprocessor.preprocess_image(image)\n    ocr_result = self.ocr.perform_ocr(image, **perform_ocr_kwargs)\n    ocr_result = self.remove_space_boxes(ocr_result)\n\n    if preprocessing_metadata and (\"scale_factor\" in preprocessing_metadata):\n        ocr_result = self._scale_bbox_results(\n            ocr_result, preprocessing_metadata[\"scale_factor\"]\n        )\n\n    # Apply OCR confidence threshold if it is passed in\n    if ocr_threshold:\n        ocr_result = self.threshold_ocr_result(ocr_result, ocr_threshold)\n\n    # Analyze text\n    text = self.ocr.get_text_from_ocr_dict(ocr_result)\n\n    analyzer_result = self.analyzer_engine.analyze(\n        text=text, language=\"en\", **text_analyzer_kwargs\n    )\n    allow_list = self._check_for_allow_list(text_analyzer_kwargs)\n    bboxes = self.map_analyzer_results_to_bounding_boxes(\n        analyzer_result, ocr_result, text, allow_list\n    )\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.fig2img","title":"<code>fig2img(fig)</code>  <code>staticmethod</code>","text":"<p>Convert a Matplotlib figure to a PIL Image and return it.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Matplotlib figure.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image of figure.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@staticmethod\ndef fig2img(fig: matplotlib.figure.Figure) -&gt; Image:\n    \"\"\"Convert a Matplotlib figure to a PIL Image and return it.\n\n    :param fig: Matplotlib figure.\n\n    :return: Image of figure.\n    \"\"\"\n    buf = io.BytesIO()\n    fig.savefig(buf)\n    buf.seek(0)\n    img = Image.open(buf)\n\n    return img\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.get_pii_bboxes","title":"<code>get_pii_bboxes(ocr_bboxes, analyzer_bboxes)</code>  <code>staticmethod</code>","text":"<p>Get a list of bboxes with is_PII property.</p> <p>Parameters:</p> Name Type Description Default <code>ocr_bboxes</code> <code>List[dict]</code> <p>Bboxes from OCR results.</p> required <code>analyzer_bboxes</code> <code>List[dict]</code> <p>Bboxes from analyzer results.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>All bboxes with appropriate label for whether it is PHI or not.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@staticmethod\ndef get_pii_bboxes(\n    ocr_bboxes: List[dict],\n    analyzer_bboxes: List[dict]\n) -&gt; List[dict]:\n    \"\"\"Get a list of bboxes with is_PII property.\n\n    :param ocr_bboxes: Bboxes from OCR results.\n    :param analyzer_bboxes: Bboxes from analyzer results.\n\n    :return: All bboxes with appropriate label for whether it is PHI or not.\n    \"\"\"\n    bboxes = []\n    for ocr_bbox in ocr_bboxes:\n        has_match = False\n\n        # Check if we have the same bbox in analyzer results\n        for analyzer_bbox in analyzer_bboxes:\n            has_same_position = (ocr_bbox[\"left\"] == analyzer_bbox[\"left\"] and ocr_bbox[\"top\"] == analyzer_bbox[\"top\"])  # noqa: E501\n            has_same_dimension = (ocr_bbox[\"width\"] == analyzer_bbox[\"width\"] and ocr_bbox[\"height\"] == analyzer_bbox[\"height\"])  # noqa: E501\n            is_same = (has_same_position is True and has_same_dimension is True)\n\n            if is_same is True:\n                current_bbox = analyzer_bbox\n                current_bbox[\"is_PII\"] = True\n                has_match = True\n                break\n\n        if has_match is False:\n            current_bbox = ocr_bbox\n            current_bbox[\"is_PII\"] = False\n\n        bboxes.append(current_bbox)\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.map_analyzer_results_to_bounding_boxes","title":"<code>map_analyzer_results_to_bounding_boxes(text_analyzer_results, ocr_result, text, allow_list)</code>  <code>staticmethod</code>","text":"<p>Map extracted PII entities to image bounding boxes.</p> <p>Matching is based on the position of the recognized entity from analyzer and word (in ocr dict) in the text.</p> <p>Parameters:</p> Name Type Description Default <code>text_analyzer_results</code> <code>List[RecognizerResult]</code> <p>PII entities recognized by presidio analyzer</p> required <code>ocr_result</code> <code>dict</code> <p>dict results with words and bboxes from OCR</p> required <code>text</code> <code>str</code> <p>text the results are based on</p> required <code>allow_list</code> <code>List[str]</code> <p>List of words to not redact  return: list of extracted entities with image bounding boxes</p> required Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@staticmethod\ndef map_analyzer_results_to_bounding_boxes(\n    text_analyzer_results: List[RecognizerResult],\n    ocr_result: dict,\n    text: str,\n    allow_list: List[str],\n) -&gt; List[ImageRecognizerResult]:\n    \"\"\"Map extracted PII entities to image bounding boxes.\n\n    Matching is based on the position of the recognized entity from analyzer\n    and word (in ocr dict) in the text.\n\n    :param text_analyzer_results: PII entities recognized by presidio analyzer\n    :param ocr_result: dict results with words and bboxes from OCR\n    :param text: text the results are based on\n    :param allow_list: List of words to not redact\n\n    return: list of extracted entities with image bounding boxes\n    \"\"\"\n    if (not ocr_result) or (not text_analyzer_results):\n        return []\n\n    bboxes = []\n    proc_indexes = 0\n    indexes = len(text_analyzer_results)\n\n    pos = 0\n    iter_ocr = enumerate(ocr_result[\"text\"])\n    for index, word in iter_ocr:\n        if not word:\n            pos += 1\n        else:\n            for element in text_analyzer_results:\n                text_element = text[element.start : element.end]\n                # check position and text of ocr word matches recognized entity\n                if (\n                    max(pos, element.start) &lt; min(element.end, pos + len(word))\n                ) and ((text_element in word) or (word in text_element)):\n                    yes_make_bbox_for_word = (\n                        (word is not None)\n                        and (word != \"\")\n                        and (word.isspace() is False)\n                        and (word not in allow_list)\n                    )\n                    # Do not add bbox for standalone spaces / empty strings\n                    if yes_make_bbox_for_word:\n                        bboxes.append(\n                            ImageRecognizerResult(\n                                element.entity_type,\n                                element.start,\n                                element.end,\n                                element.score,\n                                ocr_result[\"left\"][index],\n                                ocr_result[\"top\"][index],\n                                ocr_result[\"width\"][index],\n                                ocr_result[\"height\"][index],\n                            )\n                        )\n\n                        # add bounding boxes for all words in ocr dict\n                        # contained within the text of recognized entity\n                        # based on relative position in the full text\n                        while pos + len(word) &lt; element.end:\n                            prev_word = word\n                            index, word = next(iter_ocr)\n                            yes_make_bbox_for_word = (\n                                (word is not None)\n                                and (word != \"\")\n                                and (word.isspace() is False)\n                                and (word not in allow_list)\n                            )\n                            if yes_make_bbox_for_word:\n                                bboxes.append(\n                                    ImageRecognizerResult(\n                                        element.entity_type,\n                                        element.start,\n                                        element.end,\n                                        element.score,\n                                        ocr_result[\"left\"][index],\n                                        ocr_result[\"top\"][index],\n                                        ocr_result[\"width\"][index],\n                                        ocr_result[\"height\"][index],\n                                    )\n                                )\n                            pos += len(prev_word) + 1\n                        proc_indexes += 1\n\n            if proc_indexes == indexes:\n                break\n            pos += len(word) + 1\n\n    return bboxes\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.remove_space_boxes","title":"<code>remove_space_boxes(ocr_result)</code>  <code>staticmethod</code>","text":"<p>Remove OCR bboxes that are for spaces.</p> <p>Parameters:</p> Name Type Description Default <code>ocr_result</code> <code>dict</code> <p>OCR results (raw or thresholded).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>OCR results with empty words removed.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@staticmethod\ndef remove_space_boxes(ocr_result: dict) -&gt; dict:\n    \"\"\"Remove OCR bboxes that are for spaces.\n\n    :param ocr_result: OCR results (raw or thresholded).\n    :return: OCR results with empty words removed.\n    \"\"\"\n    # Get indices of items with no text\n    idx = list()\n    for i, text in enumerate(ocr_result[\"text\"]):\n        is_not_space = text.isspace() is False\n        if text != \"\" and is_not_space:\n            idx.append(i)\n\n    # Only retain items with text\n    filtered_ocr_result = {}\n    for key in list(ocr_result.keys()):\n        filtered_ocr_result[key] = [ocr_result[key][i] for i in idx]\n\n    return filtered_ocr_result\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageAnalyzerEngine.threshold_ocr_result","title":"<code>threshold_ocr_result(ocr_result, ocr_threshold)</code>  <code>staticmethod</code>","text":"<p>Filter out OCR results below confidence threshold.</p> <p>Parameters:</p> Name Type Description Default <code>ocr_result</code> <code>dict</code> <p>OCR results (raw).</p> required <code>ocr_threshold</code> <code>float</code> <p>Threshold value between -1 and 100.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>OCR results with low confidence items removed.</p> Source code in <code>presidio_image_redactor/image_analyzer_engine.py</code> <pre><code>@staticmethod\ndef threshold_ocr_result(ocr_result: dict, ocr_threshold: float) -&gt; dict:\n    \"\"\"Filter out OCR results below confidence threshold.\n\n    :param ocr_result: OCR results (raw).\n    :param ocr_threshold: Threshold value between -1 and 100.\n\n    :return: OCR results with low confidence items removed.\n    \"\"\"\n    if ocr_threshold &lt; -1 or ocr_threshold &gt; 100:\n        raise ValueError(\"ocr_threshold must be between -1 and 100\")\n\n    # Get indices of items above threshold\n    idx = list()\n    for i, val in enumerate(ocr_result[\"conf\"]):\n        if float(val) &gt;= ocr_threshold:\n            idx.append(i)\n\n    # Only retain high confidence items\n    filtered_ocr_result = {}\n    for key in list(ocr_result.keys()):\n        filtered_ocr_result[key] = [ocr_result[key][i] for i in idx]\n\n    return filtered_ocr_result\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePiiVerifyEngine","title":"<code>ImagePiiVerifyEngine</code>","text":"<p>             Bases: <code>ImageRedactorEngine</code></p> <p>ImagePiiVerifyEngine class only supporting Pii verification currently.</p> Source code in <code>presidio_image_redactor/image_pii_verify_engine.py</code> <pre><code>class ImagePiiVerifyEngine(ImageRedactorEngine):\n    \"\"\"ImagePiiVerifyEngine class only supporting Pii verification currently.\"\"\"\n\n    def verify(\n        self,\n        image: Image,\n        is_greyscale: bool = False,\n        display_image: bool = True,\n        show_text_annotation: bool = True,\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs\n    ) -&gt; Image:\n        \"\"\"Annotate image with the detect PII entity.\n\n        Please notice, this method duplicates the image, creates a\n        new instance and manipulate it.\n\n        :param image: PIL Image to be processed.\n        :param is_greyscale: Whether the image is greyscale or not.\n        :param display_image: If the verificationimage is displayed and returned.\n        :param show_text_annotation: True to display entity type when displaying\n        image with bounding boxes.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in ImageAnalyzerEngine.\n\n        :return: the annotated image\n        \"\"\"\n        image = ImageChops.duplicate(image)\n\n        # Check the ad-hoc recognizers list\n        self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n        # Detect text\n        perform_ocr_kwargs, ocr_threshold = self.image_analyzer_engine._parse_ocr_kwargs(ocr_kwargs)  # noqa: E501\n        ocr_results = self.image_analyzer_engine.ocr.perform_ocr(\n            image,\n            **perform_ocr_kwargs\n        )\n        if ocr_threshold:\n            ocr_results = self.image_analyzer_engine.threshold_ocr_result(\n                ocr_results,\n                ocr_threshold\n            )\n        ocr_bboxes = self.bbox_processor.get_bboxes_from_ocr_results(ocr_results)\n\n        # Detect PII\n        if ad_hoc_recognizers is None:\n            analyzer_results = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                **text_analyzer_kwargs,\n            )\n        else:\n            analyzer_results = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                ad_hoc_recognizers=ad_hoc_recognizers,\n                **text_analyzer_kwargs,\n            )\n        analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n            analyzer_results\n        )\n\n        # Prepare for plotting\n        pii_bboxes = self.image_analyzer_engine.get_pii_bboxes(\n            ocr_bboxes,\n            analyzer_bboxes\n        )\n        if is_greyscale:\n            use_greyscale_cmap = True\n        else:\n            use_greyscale_cmap = False\n\n        # Get image with verification boxes\n        verify_image = (\n            self.image_analyzer_engine.add_custom_bboxes(\n                image, pii_bboxes, show_text_annotation, use_greyscale_cmap\n            )\n            if display_image\n            else None\n        )\n\n        return verify_image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePiiVerifyEngine.verify","title":"<code>verify(image, is_greyscale=False, display_image=True, show_text_annotation=True, ocr_kwargs=None, ad_hoc_recognizers=None, **text_analyzer_kwargs)</code>","text":"<p>Annotate image with the detect PII entity.</p> <p>Please notice, this method duplicates the image, creates a new instance and manipulate it.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>PIL Image to be processed.</p> required <code>is_greyscale</code> <code>bool</code> <p>Whether the image is greyscale or not.</p> <code>False</code> <code>display_image</code> <code>bool</code> <p>If the verificationimage is displayed and returned.</p> <code>True</code> <code>show_text_annotation</code> <code>bool</code> <p>True to display entity type when displaying image with bounding boxes.</p> <code>True</code> <code>ocr_kwargs</code> <code>Optional[dict]</code> <p>Additional params for OCR methods.</p> <code>None</code> <code>ad_hoc_recognizers</code> <code>Optional[List[PatternRecognizer]]</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <code>None</code> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in ImageAnalyzerEngine.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Image</code> <p>the annotated image</p> Source code in <code>presidio_image_redactor/image_pii_verify_engine.py</code> <pre><code>def verify(\n    self,\n    image: Image,\n    is_greyscale: bool = False,\n    display_image: bool = True,\n    show_text_annotation: bool = True,\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs\n) -&gt; Image:\n    \"\"\"Annotate image with the detect PII entity.\n\n    Please notice, this method duplicates the image, creates a\n    new instance and manipulate it.\n\n    :param image: PIL Image to be processed.\n    :param is_greyscale: Whether the image is greyscale or not.\n    :param display_image: If the verificationimage is displayed and returned.\n    :param show_text_annotation: True to display entity type when displaying\n    image with bounding boxes.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in ImageAnalyzerEngine.\n\n    :return: the annotated image\n    \"\"\"\n    image = ImageChops.duplicate(image)\n\n    # Check the ad-hoc recognizers list\n    self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n    # Detect text\n    perform_ocr_kwargs, ocr_threshold = self.image_analyzer_engine._parse_ocr_kwargs(ocr_kwargs)  # noqa: E501\n    ocr_results = self.image_analyzer_engine.ocr.perform_ocr(\n        image,\n        **perform_ocr_kwargs\n    )\n    if ocr_threshold:\n        ocr_results = self.image_analyzer_engine.threshold_ocr_result(\n            ocr_results,\n            ocr_threshold\n        )\n    ocr_bboxes = self.bbox_processor.get_bboxes_from_ocr_results(ocr_results)\n\n    # Detect PII\n    if ad_hoc_recognizers is None:\n        analyzer_results = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            **text_analyzer_kwargs,\n        )\n    else:\n        analyzer_results = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n    analyzer_bboxes = self.bbox_processor.get_bboxes_from_analyzer_results(\n        analyzer_results\n    )\n\n    # Prepare for plotting\n    pii_bboxes = self.image_analyzer_engine.get_pii_bboxes(\n        ocr_bboxes,\n        analyzer_bboxes\n    )\n    if is_greyscale:\n        use_greyscale_cmap = True\n    else:\n        use_greyscale_cmap = False\n\n    # Get image with verification boxes\n    verify_image = (\n        self.image_analyzer_engine.add_custom_bboxes(\n            image, pii_bboxes, show_text_annotation, use_greyscale_cmap\n        )\n        if display_image\n        else None\n    )\n\n    return verify_image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePreprocessor","title":"<code>ImagePreprocessor</code>","text":"<p>ImagePreprocessor class.</p> <p>Parent class for image preprocessing objects.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>class ImagePreprocessor:\n    \"\"\"ImagePreprocessor class.\n\n    Parent class for image preprocessing objects.\n    \"\"\"\n\n    def __init__(self, use_greyscale: bool = True) -&gt; None:\n        \"\"\"Initialize the ImagePreprocessor class.\n\n        :param use_greyscale: Whether to convert the image to greyscale.\n        \"\"\"\n        self.use_greyscale = use_greyscale\n\n    def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n        \"\"\"Preprocess the image to be analyzed.\n\n        :param image: Loaded PIL image.\n\n        :return: The processed image and any metadata regarding the\n             preprocessing approach.\n        \"\"\"\n        return image, {}\n\n    def convert_image_to_array(self, image: Image.Image) -&gt; np.ndarray:\n        \"\"\"Convert PIL image to numpy array.\n\n        :param image: Loaded PIL image.\n        :param convert_to_greyscale: Whether to convert the image to greyscale.\n\n        :return: image pixels as a numpy array.\n\n        \"\"\"\n\n        if isinstance(image, np.ndarray):\n            img = image\n        else:\n            if self.use_greyscale:\n                image = image.convert(\"L\")\n            img = np.asarray(image)\n        return img\n\n    @staticmethod\n    def _get_bg_color(\n        image: Image.Image, is_greyscale: bool, invert: bool = False\n    ) -&gt; Union[int, Tuple[int, int, int]]:\n        \"\"\"Select most common color as background color.\n\n        :param image: Loaded PIL image.\n        :param is_greyscale: Whether the image is greyscale.\n        :param invert: TRUE if you want to get the inverse of the bg color.\n\n        :return: Background color.\n        \"\"\"\n        # Invert colors if invert flag is True\n        if invert:\n            if image.mode == \"RGBA\":\n                # Handle transparency as needed\n                r, g, b, a = image.split()\n                rgb_image = Image.merge(\"RGB\", (r, g, b))\n                inverted_image = PIL.ImageOps.invert(rgb_image)\n                r2, g2, b2 = inverted_image.split()\n\n                image = Image.merge(\"RGBA\", (r2, g2, b2, a))\n\n            else:\n                image = PIL.ImageOps.invert(image)\n\n        # Get background color\n        if is_greyscale:\n            # Select most common color as color\n            bg_color = int(np.bincount(image.flatten()).argmax())\n        else:\n            # Reduce size of image to 1 pixel to get dominant color\n            tmp_image = image.copy()\n            tmp_image = tmp_image.resize((1, 1), resample=0)\n            bg_color = tmp_image.getpixel((0, 0))\n\n        return bg_color\n\n    @staticmethod\n    def _get_image_contrast(image: np.ndarray) -&gt; Tuple[float, float]:\n        \"\"\"Compute the contrast level and mean intensity of an image.\n\n        :param image: Input image pixels (as a numpy array).\n\n        :return: A tuple containing the contrast level and mean intensity of the image.\n        \"\"\"\n        contrast = np.std(image)\n        mean_intensity = np.mean(image)\n        return contrast, mean_intensity\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePreprocessor.__init__","title":"<code>__init__(use_greyscale=True)</code>","text":"<p>Initialize the ImagePreprocessor class.</p> <p>Parameters:</p> Name Type Description Default <code>use_greyscale</code> <code>bool</code> <p>Whether to convert the image to greyscale.</p> <code>True</code> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def __init__(self, use_greyscale: bool = True) -&gt; None:\n    \"\"\"Initialize the ImagePreprocessor class.\n\n    :param use_greyscale: Whether to convert the image to greyscale.\n    \"\"\"\n    self.use_greyscale = use_greyscale\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePreprocessor.convert_image_to_array","title":"<code>convert_image_to_array(image)</code>","text":"<p>Convert PIL image to numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Loaded PIL image.</p> required <code>convert_to_greyscale</code> <p>Whether to convert the image to greyscale.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>image pixels as a numpy array.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def convert_image_to_array(self, image: Image.Image) -&gt; np.ndarray:\n    \"\"\"Convert PIL image to numpy array.\n\n    :param image: Loaded PIL image.\n    :param convert_to_greyscale: Whether to convert the image to greyscale.\n\n    :return: image pixels as a numpy array.\n\n    \"\"\"\n\n    if isinstance(image, np.ndarray):\n        img = image\n    else:\n        if self.use_greyscale:\n            image = image.convert(\"L\")\n        img = np.asarray(image)\n    return img\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImagePreprocessor.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>Preprocess the image to be analyzed.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Loaded PIL image.</p> required <p>Returns:</p> Type Description <code>Tuple[Image, dict]</code> <p>The processed image and any metadata regarding the preprocessing approach.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n    \"\"\"Preprocess the image to be analyzed.\n\n    :param image: Loaded PIL image.\n\n    :return: The processed image and any metadata regarding the\n         preprocessing approach.\n    \"\"\"\n    return image, {}\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageRedactorEngine","title":"<code>ImageRedactorEngine</code>","text":"<p>ImageRedactorEngine performs OCR + PII detection + bounding box redaction.</p> <p>Parameters:</p> Name Type Description Default <code>image_analyzer_engine</code> <code>ImageAnalyzerEngine</code> <p>Engine which performs OCR + PII detection.</p> <code>None</code> Source code in <code>presidio_image_redactor/image_redactor_engine.py</code> <pre><code>class ImageRedactorEngine:\n    \"\"\"ImageRedactorEngine performs OCR + PII detection + bounding box redaction.\n\n    :param image_analyzer_engine: Engine which performs OCR + PII detection.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_analyzer_engine: ImageAnalyzerEngine = None,\n    ):\n        if not image_analyzer_engine:\n            self.image_analyzer_engine = ImageAnalyzerEngine()\n        else:\n            self.image_analyzer_engine = image_analyzer_engine\n\n        self.bbox_processor = BboxProcessor()\n\n    def redact(\n        self,\n        image: Image,\n        fill: Union[int, Tuple[int, int, int]] = (0, 0, 0),\n        ocr_kwargs: Optional[dict] = None,\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n        **text_analyzer_kwargs,\n    ) -&gt; Image:\n        \"\"\"Redact method to redact the given image.\n\n        Please notice, this method duplicates the image, creates a new instance and\n        manipulate it.\n        :param image: PIL Image to be processed.\n        :param fill: colour to fill the shape - int (0-255) for\n        grayscale or Tuple(R, G, B) for RGB.\n        :param ocr_kwargs: Additional params for OCR methods.\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        :param text_analyzer_kwargs: Additional values for the analyze method\n        in AnalyzerEngine.\n\n        :return: the redacted image\n        \"\"\"\n\n        image = ImageChops.duplicate(image)\n\n        # Check the ad-hoc recognizers list\n        self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n        # Detect PII\n        if ad_hoc_recognizers is None:\n            bboxes = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                **text_analyzer_kwargs,\n            )\n        else:\n            bboxes = self.image_analyzer_engine.analyze(\n                image,\n                ocr_kwargs=ocr_kwargs,\n                ad_hoc_recognizers=ad_hoc_recognizers,\n                **text_analyzer_kwargs,\n            )\n\n        draw = ImageDraw.Draw(image)\n\n        for box in bboxes:\n            x0 = box.left\n            y0 = box.top\n            x1 = x0 + box.width\n            y1 = y0 + box.height\n            draw.rectangle([x0, y0, x1, y1], fill=fill)\n\n        return image\n\n    @staticmethod\n    def _check_ad_hoc_recognizer_list(\n        ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    ):\n        \"\"\"Check if the provided ad-hoc recognizer list is valid.\n\n        :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n        for ad-hoc recognizer.\n        \"\"\"\n        if isinstance(ad_hoc_recognizers, (list, type(None))):\n            if isinstance(ad_hoc_recognizers, list):\n                if len(ad_hoc_recognizers) &gt;= 1:\n                    are_recognizers = all(\n                        isinstance(\n                            x, presidio_analyzer.pattern_recognizer.PatternRecognizer\n                        )\n                        for x in ad_hoc_recognizers\n                    )\n                    if are_recognizers is False:\n                        raise TypeError(\n                            \"\"\"All items in ad_hoc_recognizers list must be\n                            PatternRecognizer objects\"\"\"\n                        )\n                else:\n                    raise TypeError(\n                        \"ad_hoc_recognizers must be None or list of PatternRecognizer\"\n                    )\n        else:\n            raise TypeError(\n                \"ad_hoc_recognizers must be None or list of PatternRecognizer\"\n            )\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageRedactorEngine.redact","title":"<code>redact(image, fill=(0, 0, 0), ocr_kwargs=None, ad_hoc_recognizers=None, **text_analyzer_kwargs)</code>","text":"<p>Redact method to redact the given image.</p> <p>Please notice, this method duplicates the image, creates a new instance and manipulate it.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>PIL Image to be processed.</p> required <code>fill</code> <code>Union[int, Tuple[int, int, int]]</code> <p>colour to fill the shape - int (0-255) for grayscale or Tuple(R, G, B) for RGB.</p> <code>(0, 0, 0)</code> <code>ocr_kwargs</code> <code>Optional[dict]</code> <p>Additional params for OCR methods.</p> <code>None</code> <code>ad_hoc_recognizers</code> <code>Optional[List[PatternRecognizer]]</code> <p>List of PatternRecognizer objects to use for ad-hoc recognizer.</p> <code>None</code> <code>text_analyzer_kwargs</code> <p>Additional values for the analyze method in AnalyzerEngine.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Image</code> <p>the redacted image</p> Source code in <code>presidio_image_redactor/image_redactor_engine.py</code> <pre><code>def redact(\n    self,\n    image: Image,\n    fill: Union[int, Tuple[int, int, int]] = (0, 0, 0),\n    ocr_kwargs: Optional[dict] = None,\n    ad_hoc_recognizers: Optional[List[PatternRecognizer]] = None,\n    **text_analyzer_kwargs,\n) -&gt; Image:\n    \"\"\"Redact method to redact the given image.\n\n    Please notice, this method duplicates the image, creates a new instance and\n    manipulate it.\n    :param image: PIL Image to be processed.\n    :param fill: colour to fill the shape - int (0-255) for\n    grayscale or Tuple(R, G, B) for RGB.\n    :param ocr_kwargs: Additional params for OCR methods.\n    :param ad_hoc_recognizers: List of PatternRecognizer objects to use\n    for ad-hoc recognizer.\n    :param text_analyzer_kwargs: Additional values for the analyze method\n    in AnalyzerEngine.\n\n    :return: the redacted image\n    \"\"\"\n\n    image = ImageChops.duplicate(image)\n\n    # Check the ad-hoc recognizers list\n    self._check_ad_hoc_recognizer_list(ad_hoc_recognizers)\n\n    # Detect PII\n    if ad_hoc_recognizers is None:\n        bboxes = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            **text_analyzer_kwargs,\n        )\n    else:\n        bboxes = self.image_analyzer_engine.analyze(\n            image,\n            ocr_kwargs=ocr_kwargs,\n            ad_hoc_recognizers=ad_hoc_recognizers,\n            **text_analyzer_kwargs,\n        )\n\n    draw = ImageDraw.Draw(image)\n\n    for box in bboxes:\n        x0 = box.left\n        y0 = box.top\n        x1 = x0 + box.width\n        y1 = y0 + box.height\n        draw.rectangle([x0, y0, x1, y1], fill=fill)\n\n    return image\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageRescaling","title":"<code>ImageRescaling</code>","text":"<p>             Bases: <code>ImagePreprocessor</code></p> <p>ImageRescaling class. Rescales images based on their size.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>class ImageRescaling(ImagePreprocessor):\n    \"\"\"ImageRescaling class. Rescales images based on their size.\"\"\"\n\n    def __init__(\n        self,\n        small_size: int = 1048576,\n        large_size: int = 4000000,\n        factor: int = 2,\n        interpolation: int = cv2.INTER_AREA,\n    ) -&gt; None:\n        \"\"\"Initialize the ImageRescaling class.\n\n        :param small_size: Threshold for small image size.\n        :param large_size: Threshold for large image size.\n        :param factor: Scaling factor for resizing.\n        :param interpolation: Interpolation method for resizing.\n        \"\"\"\n        super().__init__(use_greyscale=True)\n\n        self.small_size = small_size\n        self.large_size = large_size\n        self.factor = factor\n        self.interpolation = interpolation\n\n    def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n        \"\"\"Preprocess the image to be analyzed.\n\n        :param image: Loaded PIL image.\n\n        :return: The processed image and metadata (scale_factor).\n        \"\"\"\n\n        scale_factor = 1\n        if image.size &lt; self.small_size:\n            scale_factor = self.factor\n        elif image.size &gt; self.large_size:\n            scale_factor = 1 / self.factor\n\n        width = int(image.shape[1] * scale_factor)\n        height = int(image.shape[0] * scale_factor)\n        dimensions = (width, height)\n\n        # resize image\n        rescaled_image = cv2.resize(image, dimensions, interpolation=self.interpolation)\n        metadata = {\"scale_factor\": scale_factor}\n        return Image.fromarray(rescaled_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageRescaling.__init__","title":"<code>__init__(small_size=1048576, large_size=4000000, factor=2, interpolation=cv2.INTER_AREA)</code>","text":"<p>Initialize the ImageRescaling class.</p> <p>Parameters:</p> Name Type Description Default <code>small_size</code> <code>int</code> <p>Threshold for small image size.</p> <code>1048576</code> <code>large_size</code> <code>int</code> <p>Threshold for large image size.</p> <code>4000000</code> <code>factor</code> <code>int</code> <p>Scaling factor for resizing.</p> <code>2</code> <code>interpolation</code> <code>int</code> <p>Interpolation method for resizing.</p> <code>INTER_AREA</code> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def __init__(\n    self,\n    small_size: int = 1048576,\n    large_size: int = 4000000,\n    factor: int = 2,\n    interpolation: int = cv2.INTER_AREA,\n) -&gt; None:\n    \"\"\"Initialize the ImageRescaling class.\n\n    :param small_size: Threshold for small image size.\n    :param large_size: Threshold for large image size.\n    :param factor: Scaling factor for resizing.\n    :param interpolation: Interpolation method for resizing.\n    \"\"\"\n    super().__init__(use_greyscale=True)\n\n    self.small_size = small_size\n    self.large_size = large_size\n    self.factor = factor\n    self.interpolation = interpolation\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.ImageRescaling.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>Preprocess the image to be analyzed.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Loaded PIL image.</p> required <p>Returns:</p> Type Description <code>Tuple[Image, dict]</code> <p>The processed image and metadata (scale_factor).</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Tuple[Image.Image, dict]:\n    \"\"\"Preprocess the image to be analyzed.\n\n    :param image: Loaded PIL image.\n\n    :return: The processed image and metadata (scale_factor).\n    \"\"\"\n\n    scale_factor = 1\n    if image.size &lt; self.small_size:\n        scale_factor = self.factor\n    elif image.size &gt; self.large_size:\n        scale_factor = 1 / self.factor\n\n    width = int(image.shape[1] * scale_factor)\n    height = int(image.shape[0] * scale_factor)\n    dimensions = (width, height)\n\n    # resize image\n    rescaled_image = cv2.resize(image, dimensions, interpolation=self.interpolation)\n    metadata = {\"scale_factor\": scale_factor}\n    return Image.fromarray(rescaled_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.OCR","title":"<code>OCR</code>","text":"<p>             Bases: <code>ABC</code></p> <p>OCR class that performs OCR on a given image.</p> Source code in <code>presidio_image_redactor/ocr.py</code> <pre><code>class OCR(ABC):\n    \"\"\"OCR class that performs OCR on a given image.\"\"\"\n\n    @abstractmethod\n    def perform_ocr(self, image: object, **kwargs) -&gt; dict:\n        \"\"\"Perform OCR on a given image.\n\n        :param image: PIL Image/numpy array or file path(str) to be processed\n        :param kwargs: Additional values for perform OCR method\n\n        :return: results dictionary containing bboxes and text for each detected word\n        \"\"\"\n        pass\n\n    @staticmethod\n    def get_text_from_ocr_dict(ocr_result: dict, separator: str = \" \") -&gt; str:\n        \"\"\"Combine the text from the OCR dict to full text.\n\n        :param ocr_result: dictionary containing the ocr results per word\n        :param separator: separator to use when joining the words\n\n        return: str containing the full extracted text as string\n        \"\"\"\n        if not ocr_result:\n            return \"\"\n        else:\n            return separator.join(ocr_result[\"text\"])\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.OCR.get_text_from_ocr_dict","title":"<code>get_text_from_ocr_dict(ocr_result, separator=' ')</code>  <code>staticmethod</code>","text":"<p>Combine the text from the OCR dict to full text.</p> <p>Parameters:</p> Name Type Description Default <code>ocr_result</code> <code>dict</code> <p>dictionary containing the ocr results per word</p> required <code>separator</code> <code>str</code> <p>separator to use when joining the words  return: str containing the full extracted text as string</p> <code>' '</code> Source code in <code>presidio_image_redactor/ocr.py</code> <pre><code>@staticmethod\ndef get_text_from_ocr_dict(ocr_result: dict, separator: str = \" \") -&gt; str:\n    \"\"\"Combine the text from the OCR dict to full text.\n\n    :param ocr_result: dictionary containing the ocr results per word\n    :param separator: separator to use when joining the words\n\n    return: str containing the full extracted text as string\n    \"\"\"\n    if not ocr_result:\n        return \"\"\n    else:\n        return separator.join(ocr_result[\"text\"])\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.OCR.perform_ocr","title":"<code>perform_ocr(image, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Perform OCR on a given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>object</code> <p>PIL Image/numpy array or file path(str) to be processed</p> required <code>kwargs</code> <p>Additional values for perform OCR method</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>results dictionary containing bboxes and text for each detected word</p> Source code in <code>presidio_image_redactor/ocr.py</code> <pre><code>@abstractmethod\ndef perform_ocr(self, image: object, **kwargs) -&gt; dict:\n    \"\"\"Perform OCR on a given image.\n\n    :param image: PIL Image/numpy array or file path(str) to be processed\n    :param kwargs: Additional values for perform OCR method\n\n    :return: results dictionary containing bboxes and text for each detected word\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.SegmentedAdaptiveThreshold","title":"<code>SegmentedAdaptiveThreshold</code>","text":"<p>             Bases: <code>ImagePreprocessor</code></p> <p>SegmentedAdaptiveThreshold class.</p> <p>The class applies adaptive thresholding to an image and returns the thresholded image and metadata. The parameters used to run the adaptivethresholding are selected based on the contrast level of the image.</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>class SegmentedAdaptiveThreshold(ImagePreprocessor):\n    \"\"\"SegmentedAdaptiveThreshold class.\n\n    The class applies adaptive thresholding to an image\n    and returns the thresholded image and metadata.\n    The parameters used to run the adaptivethresholding are selected based on\n    the contrast level of the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int = 5,\n        contrast_threshold: int = 40,\n        c_low_contrast: int = 10,\n        c_high_contrast: int = 40,\n        bg_threshold: int = 122,\n    ) -&gt; None:\n        \"\"\"Initialize the SegmentedAdaptiveThreshold class.\n\n        :param block_size: Size of the neighborhood area for threshold calculation.\n        :param contrast_threshold: Threshold for low contrast images.\n        :param C_low_contrast: Constant added to the mean for low contrast images.\n        :param C_high_contrast: Constant added to the mean for high contrast images.\n        :param bg_threshold: Threshold for background color.\n        \"\"\"\n\n        super().__init__(use_greyscale=True)\n        self.block_size = block_size\n        self.c_low_contrast = c_low_contrast\n        self.c_high_contrast = c_high_contrast\n        self.bg_threshold = bg_threshold\n        self.contrast_threshold = contrast_threshold\n\n    def preprocess_image(\n        self, image: Union[Image.Image, np.ndarray]\n    ) -&gt; Tuple[Image.Image, dict]:\n        \"\"\"Preprocess the image.\n\n        :param image: Loaded PIL image.\n\n        :return: The processed image and metadata (C, background_color, contrast).\n        \"\"\"\n        if isinstance(image, np.ndarray):\n            image = self.convert_image_to_array(image)\n\n        # Determine background color\n        background_color = self._get_bg_color(image, True)\n        contrast, _ = self._get_image_contrast(image)\n\n        c = (\n            self.c_low_contrast\n            if contrast &lt;= self.contrast_threshold\n            else self.c_high_contrast\n        )\n\n        if background_color &lt; self.bg_threshold:\n            adaptive_threshold_image = cv2.adaptiveThreshold(\n                image,\n                255,\n                cv2.ADAPTIVE_THRESH_MEAN_C,\n                cv2.THRESH_BINARY_INV,\n                self.block_size,\n                -c,\n            )\n        else:\n            adaptive_threshold_image = cv2.adaptiveThreshold(\n                image,\n                255,\n                cv2.ADAPTIVE_THRESH_MEAN_C,\n                cv2.THRESH_BINARY,\n                self.block_size,\n                c,\n            )\n\n        metadata = {\"C\": c, \"background_color\": background_color, \"contrast\": contrast}\n        return Image.fromarray(adaptive_threshold_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.SegmentedAdaptiveThreshold.__init__","title":"<code>__init__(block_size=5, contrast_threshold=40, c_low_contrast=10, c_high_contrast=40, bg_threshold=122)</code>","text":"<p>Initialize the SegmentedAdaptiveThreshold class.</p> <p>Parameters:</p> Name Type Description Default <code>block_size</code> <code>int</code> <p>Size of the neighborhood area for threshold calculation.</p> <code>5</code> <code>contrast_threshold</code> <code>int</code> <p>Threshold for low contrast images.</p> <code>40</code> <code>C_low_contrast</code> <p>Constant added to the mean for low contrast images.</p> required <code>C_high_contrast</code> <p>Constant added to the mean for high contrast images.</p> required <code>bg_threshold</code> <code>int</code> <p>Threshold for background color.</p> <code>122</code> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def __init__(\n    self,\n    block_size: int = 5,\n    contrast_threshold: int = 40,\n    c_low_contrast: int = 10,\n    c_high_contrast: int = 40,\n    bg_threshold: int = 122,\n) -&gt; None:\n    \"\"\"Initialize the SegmentedAdaptiveThreshold class.\n\n    :param block_size: Size of the neighborhood area for threshold calculation.\n    :param contrast_threshold: Threshold for low contrast images.\n    :param C_low_contrast: Constant added to the mean for low contrast images.\n    :param C_high_contrast: Constant added to the mean for high contrast images.\n    :param bg_threshold: Threshold for background color.\n    \"\"\"\n\n    super().__init__(use_greyscale=True)\n    self.block_size = block_size\n    self.c_low_contrast = c_low_contrast\n    self.c_high_contrast = c_high_contrast\n    self.bg_threshold = bg_threshold\n    self.contrast_threshold = contrast_threshold\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.SegmentedAdaptiveThreshold.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>Preprocess the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[Image, ndarray]</code> <p>Loaded PIL image.</p> required <p>Returns:</p> Type Description <code>Tuple[Image, dict]</code> <p>The processed image and metadata (C, background_color, contrast).</p> Source code in <code>presidio_image_redactor/image_processing_engine.py</code> <pre><code>def preprocess_image(\n    self, image: Union[Image.Image, np.ndarray]\n) -&gt; Tuple[Image.Image, dict]:\n    \"\"\"Preprocess the image.\n\n    :param image: Loaded PIL image.\n\n    :return: The processed image and metadata (C, background_color, contrast).\n    \"\"\"\n    if isinstance(image, np.ndarray):\n        image = self.convert_image_to_array(image)\n\n    # Determine background color\n    background_color = self._get_bg_color(image, True)\n    contrast, _ = self._get_image_contrast(image)\n\n    c = (\n        self.c_low_contrast\n        if contrast &lt;= self.contrast_threshold\n        else self.c_high_contrast\n    )\n\n    if background_color &lt; self.bg_threshold:\n        adaptive_threshold_image = cv2.adaptiveThreshold(\n            image,\n            255,\n            cv2.ADAPTIVE_THRESH_MEAN_C,\n            cv2.THRESH_BINARY_INV,\n            self.block_size,\n            -c,\n        )\n    else:\n        adaptive_threshold_image = cv2.adaptiveThreshold(\n            image,\n            255,\n            cv2.ADAPTIVE_THRESH_MEAN_C,\n            cv2.THRESH_BINARY,\n            self.block_size,\n            c,\n        )\n\n    metadata = {\"C\": c, \"background_color\": background_color, \"contrast\": contrast}\n    return Image.fromarray(adaptive_threshold_image), metadata\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.TesseractOCR","title":"<code>TesseractOCR</code>","text":"<p>             Bases: <code>OCR</code></p> <p>OCR class that performs OCR on a given image.</p> Source code in <code>presidio_image_redactor/tesseract_ocr.py</code> <pre><code>class TesseractOCR(OCR):\n    \"\"\"OCR class that performs OCR on a given image.\"\"\"\n\n    def perform_ocr(self, image: object, **kwargs) -&gt; dict:\n        \"\"\"Perform OCR on a given image.\n\n        :param image: PIL Image/numpy array or file path(str) to be processed\n        :param kwargs: Additional values for OCR image_to_data\n\n        :return: results dictionary containing bboxes and text for each detected word\n        \"\"\"\n        output_type = pytesseract.Output.DICT\n        return pytesseract.image_to_data(image, output_type=output_type, **kwargs)\n</code></pre>"},{"location":"api/image_redactor_python/#presidio_image_redactor.TesseractOCR.perform_ocr","title":"<code>perform_ocr(image, **kwargs)</code>","text":"<p>Perform OCR on a given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>object</code> <p>PIL Image/numpy array or file path(str) to be processed</p> required <code>kwargs</code> <p>Additional values for OCR image_to_data</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>results dictionary containing bboxes and text for each detected word</p> Source code in <code>presidio_image_redactor/tesseract_ocr.py</code> <pre><code>def perform_ocr(self, image: object, **kwargs) -&gt; dict:\n    \"\"\"Perform OCR on a given image.\n\n    :param image: PIL Image/numpy array or file path(str) to be processed\n    :param kwargs: Additional values for OCR image_to_data\n\n    :return: results dictionary containing bboxes and text for each detected word\n    \"\"\"\n    output_type = pytesseract.Output.DICT\n    return pytesseract.image_to_data(image, output_type=output_type, **kwargs)\n</code></pre>"},{"location":"image-redactor/","title":"Presidio Image Redactor","text":"<p>Please notice, this package is still in beta and not production ready.</p>"},{"location":"image-redactor/#description","title":"Description","text":"<p>The Presidio Image Redactor is a Python based module for detecting and redacting PII text entities in images. </p> <p>This module may also be used on medical DICOM images. The <code>DicomImageRedactorEngine</code> class may be used to redact text PII present as pixels in DICOM images. </p> <p>Note</p> <p>This class only redacts pixel data and does not scrub text PII which may exist in the DICOM metadata.  We highly recommend using the DICOM image redactor engine to redact text from images BEFORE scrubbing metadata PII.*</p>"},{"location":"image-redactor/#installation","title":"Installation","text":"<p>Pre-requisites:</p> <ul> <li>Install Tesseract OCR by following the   instructions on how to install it for your operating system.</li> </ul> <p>Attention</p> <p>For best performance, please use the most up-to-date version of Tesseract OCR. Presidio was tested with v5.2.0.</p> Using pipUsing DockerFrom source <p>Note</p> <p>Consider installing the Presidio python packages on a virtual environment like venv or conda.</p> <p>To get started with Presidio-image-redactor, download the package and the <code>en_core_web_lg</code> spaCy model:</p> <pre><code>pip install presidio-image-redactor\npython -m spacy download en_core_web_lg\n</code></pre> <p>Note</p> <p>This requires Docker to be installed. Download Docker.</p> <pre><code># Download image from Dockerhub\ndocker pull mcr.microsoft.com/presidio-image-redactor\n\n# Run the container with the default port\ndocker run -d -p 5003:3000 mcr.microsoft.com/presidio-image-redactor:latest\n</code></pre> <p>First, clone the Presidio repo. See here for instructions.</p> <p>Then, build the presidio-image-redactor container:</p> <pre><code>cd presidio-image-redactor\ndocker build . -t presidio/presidio-image-redactor\n</code></pre>"},{"location":"image-redactor/#getting-started-standard-image-types","title":"Getting started (standard image types)","text":"PythonAs an HTTP server <p>Once the Presidio-image-redactor package is installed, run this simple script:</p> <pre><code>from PIL import Image\nfrom presidio_image_redactor import ImageRedactorEngine\n\n# Get the image to redact using PIL lib (pillow)\nimage = Image.open(\"./docs/image-redactor/ocr_text.png\")\n\n# Initialize the engine\nengine = ImageRedactorEngine()\n\n# Redact the image with pink color\nredacted_image = engine.redact(image, (255, 192, 203))\n\n# save the redacted image \nredacted_image.save(\"new_image.png\")\n# uncomment to open the image for viewing\n# redacted_image.show()\n</code></pre> <p>You can run presidio image redactor as an http server using either python runtime or using a docker container.</p> <p>Python script example can be found under: /presidio/e2e-tests/tests/test_image_redactor.py</p>"},{"location":"image-redactor/#using-docker-container","title":"Using docker container","text":"<pre><code>cd presidio-image-redactor\ndocker run -p 5003:3000 presidio-image-redactor \n</code></pre>"},{"location":"image-redactor/#using-python-runtime","title":"Using python runtime","text":"<p>Note</p> <p>This requires the Presidio Github repository to be cloned.</p> <pre><code>cd presidio-image-redactor\npython app.py\n# use ocr_test.png as the image to redact, and 255 as the color fill. \n# out.png is the new redacted image received from the server.\ncurl -XPOST \"http://localhost:3000/redact\" -H \"content-type: multipart/form-data\" -F \"image=@ocr_test.png\" -F \"data=\\\"{'color_fill':'255'}\\\"\" &gt; out.png\n</code></pre>"},{"location":"image-redactor/#getting-started-dicom-images","title":"Getting started (DICOM images)","text":"Python <p>Once the Presidio-image-redactor package is installed, run this simple script:</p> <pre><code>import pydicom\nfrom presidio_image_redactor import DicomImageRedactorEngine\n\n# Set input and output paths\ninput_path = \"path/to/your/dicom/file.dcm\"\noutput_dir = \"./output\"\n\n# Initialize the engine\nengine = DicomImageRedactorEngine()\n\n# Option 1: Redact from a loaded DICOM image\ndicom_image = pydicom.dcmread(input_path)\nredacted_dicom_image = engine.redact(dicom_image, fill=\"contrast\")\n\n# Option 2: Redact from a loaded DICOM image and return redacted regions\nredacted_dicom_image, bboxes = engine.redact_and_return_bbox(dicom_image, fill=\"contrast\")\n\n# Option 3: Redact from DICOM file and save redacted regions as json file\nengine.redact_from_file(input_path, output_dir, padding_width=25, fill=\"contrast\", save_bboxes=True)\n\n# Option 4: Redact from directory and save redacted regions as json files\nocr_kwargs = {\"ocr_threshold\": 50}\nengine.redact_from_directory(\"path/to/your/dicom\", output_dir, fill=\"background\", save_bboxes=True, ocr_kwargs=ocr_kwargs)\n</code></pre>"},{"location":"image-redactor/#getting-started-using-the-document-intelligence-ocr-engine","title":"Getting started using the document intelligence OCR engine","text":"<p>Presidio offers two engines for OCR based PII removal. The first is the default engine which uses Tesseract OCR. The second is the Document Intelligence OCR engine which uses Azure's Document Intelligence service, which requires an Azure subscription. The following sections describe how to setup and use the Document Intelligence OCR engine.</p> <p>You will need to register with Azure to get an API key and endpoint.  Perform the steps in the \"Prerequisites\" section of this page.  Once your resource deploys, copy your endpoint and key values and save them for the next step.</p> <p>The most basic usage of the engine can be setup like the following in python <pre><code>diOCR = DocumentIntelligenceOCR(endpoint=\"&lt;your_endpoint&gt;\", key=\"&lt;your_key&gt;\")\n</code></pre></p> <p>The DocumentIntelligenceOCR can also attempt to pull your endpoint and key values from environment variables. <pre><code>$ export DOCUMENT_INTELLIGENCE_ENDPOINT=&lt;your_endpoint&gt;\n$ export DOCUMENT_INTELLIGENCE_KEY=&lt;your_key&gt;\n</code></pre></p>"},{"location":"image-redactor/#document-intelligence-model-support","title":"Document Intelligence Model Support","text":"<p>There are numerous document processing models available, and currently we only support the most basic usage of the model.  For an overview of the functionalities offered by Document Intelligence, see this page. Presidio offers only word-level processing on the result for PII redaction purposes, as all prebuilt document models support this interface. Different models support additional structured support for tables, paragraphs, key-value pairs, fields and other types of metadata in the response. </p> <p>Additional metadata can be sent to the Document Intelligence API call, such as pages, locale, and features, which are documented here. You are encouraged to test each model to see which fits best to your use case.</p>"},{"location":"image-redactor/#creating-an-image-redactor-engine-in-python","title":"Creating an image redactor engine in Python:","text":"<pre><code>diOCR = DocumentIntelligenceOCR()\nia_engine = ImageAnalyzerEngine(ocr=di_ocr)\nmy_engine = ImageRedactorEngine(image_analyzer_engine=ia_engine)\n</code></pre>"},{"location":"image-redactor/#testing-document-inteligence","title":"Testing Document Inteligence","text":"<p>Follow the steps of running the tests</p> <p>The test suite has a series of tests which are only exercised when the appropriate environment variables are populated.  To run the test suite, to test the DocumentIntelligenceOCR engine, call the tests like this:</p> <pre><code>$ export DOCUMENT_INTELLIGENCE_ENDPOINT=&lt;your_endpoint&gt;\n$ export DOCUMENT_INTELLIGENCE_KEY=&lt;your_key&gt;\n$ pytest\n</code></pre>"},{"location":"image-redactor/#evaluating-de-identification-performance","title":"Evaluating de-identification performance","text":"<p>If you are interested in evaluating the performance of the DICOM de-identification against ground truth labels, please see the evaluating DICOM de-identification page.</p>"},{"location":"image-redactor/#side-note-for-windows","title":"Side note for Windows","text":"<p>If you are using a Windows machine, you may run into issues if file paths are too long. Unfortunatley, this is not rare when working with DICOM images that are often nested in directories with descriptive names.</p> <p>To avoid errors where the code may not recognize a path as existing due to the length of the characters in the file path, please enable long paths on your system.</p>"},{"location":"image-redactor/#dicom-data-citation","title":"DICOM Data Citation","text":"<p>The DICOM data used for unit and integration testing for <code>DicomImageRedactorEngine</code> are stored in this repository with permission from the original dataset owners. Please see the dataset information as follows:</p> <p>Rutherford, M., Mun, S.K., Levine, B., Bennett, W.C., Smith, K., Farmer, P., Jarosz, J., Wagner, U., Farahani, K., Prior, F. (2021). A DICOM dataset for evaluation of medical image de-identification (Pseudo-PHI-DICOM-Data) [Data set]. The Cancer Imaging Archive. DOI: https://doi.org/10.7937/s17z-r072</p>"},{"location":"image-redactor/#api-reference","title":"API reference","text":"<p>the API Spec for the Image Redactor REST API reference details and Image Redactor Python API for Python API reference</p>"},{"location":"image-redactor/evaluating_dicom_redaction/","title":"Evaluating DICOM de-identification","text":""},{"location":"image-redactor/evaluating_dicom_redaction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Ground truth format</li> <li>Creating ground truth files</li> <li>Evaluating de-identification performance</li> </ul>"},{"location":"image-redactor/evaluating_dicom_redaction/#introduction","title":"Introduction","text":"<p>We can evaluate the performance of the <code>DicomImageRedactorEngine</code> DICOM de-identification by using the <code>DicomImagePiiVerifyEngine</code>. The evaluation results consist of:</p> <ul> <li>Image with bounding boxes identifying detected Personal Health Information (PHI)</li> <li>All positives (True Positives and False Positives)</li> <li>Precision</li> <li>Recall</li> </ul>"},{"location":"image-redactor/evaluating_dicom_redaction/#ground-truth-format","title":"Ground truth format","text":"<p>Ground truth labels are stored as <code>.json</code> files containing filename as the highest level keys. Each filename object consists of an item for each individual entity.</p> <pre><code>{\n    \"your/dicom/dir/file_0.dcm\": [\n        {\n            \"label\": \"DAVIDSON\",\n            \"left\": 25,\n            \"top\": 25,\n            \"width\": 241,\n            \"height\": 37\n        },\n        {\n            \"label\": \"DOUGLAS\",\n            \"left\": 287,\n            \"top\": 25,\n            \"width\": 230,\n            \"height\": 36\n        },\n        {\n            \"label\": \"[M]\",\n            \"left\": 535,\n            \"top\": 25,\n            \"width\": 60,\n            \"height\": 45\n        },\n        {\n            \"label\": \"01.09.2012\",\n            \"left\": 613,\n            \"top\": 26,\n            \"width\": 226,\n            \"height\": 35\n        },\n        {\n            \"label\": \"06.16.1976\",\n            \"left\": 170,\n            \"top\": 72,\n            \"width\": 218,\n            \"height\": 35\n        }\n    ],\n    \"your/dicom/dir/file_1.dcm\": [\n        ...\n    ]\n}\n</code></pre> <p>Return to the Table of Contents</p>"},{"location":"image-redactor/evaluating_dicom_redaction/#creating-ground-truth-files","title":"Creating ground truth files","text":"<p>The <code>DicomImagePiiVerifyEngine</code> class can be used to assist in ground truth label generation. Use the following code snippet to generate the verification image, OCR results, and NER (analyzer) results.</p> <pre><code>import pydicom\nfrom presidio_image_redactor import DicomImagePiiVerifyEngine\n\n# Initialize engine\ndicom_engine = DicomImagePiiVerifyEngine()\n\n# Choose your file to create ground truth for\nfilename = \"path/to/your/file.dcm\"\ninstance = pydicom.dcmread(filename)\npadding_width = 25\n\n# Get OCR and NER results\nverification_image, ocr_results, analyzer_results = dicom_engine.verify_dicom_instance(instance, padding_width)\n\n# Format results for more direct comparison\nocr_results_formatted = dicom_engine.bbox_processor.get_bboxes_from_ocr_results(ocr_results)\nanalyzer_results_formatted = dicom_engine.bbox_processor.get_bboxes_from_analyzer_results(analyzer_results)\n</code></pre> <p>By looking at the output of <code>verify_dicom_instance</code>, we can create a ground truth labels json.</p> <p>Save <code>analyzer_results_formatted</code> as a json file and then perform the following</p> <ol> <li>Group the results into a new item with the file name set as the key.</li> <li>For each item in this group:     a. Remove the \"entity_type\" field and value.     b. Add a new \"label\" field with the value set to the ground truth text PHI with matching coordinate as you can see in the formatted OCR results and verification image.</li> </ol> <p>Then check that your ground truth json contains all the text PHI you can visually confirm in the DICOM image. If something is not detected by the OCR or NER, you will need to manually add the item yourself.</p> <p>Pixel position and size data can be obtained using any labeling software or imaging processing software (e.g., MS Paint) on the verification image.</p> <p>Note: When manually specifying pixel position, make sure to account for any padding introduced in the OCR process (default padding added is 25 pixels).</p>"},{"location":"image-redactor/evaluating_dicom_redaction/#example","title":"Example","text":"<p>Let's say we ran the above code block and see the following for <code>ocr_results_formatted</code> and <code>analyzer_results_formatted</code>.</p> <pre><code>// OCR Results (formatted)\n[\n    {\n        \"left\": 25,\n        \"top\": 25,\n        \"width\": 241,\n        \"height\": 37,\n        \"conf\": 95.833916,\n        \"label\": \"DAVIDSON\"\n    },\n    {\n        \"left\": 287,\n        \"top\": 25,\n        \"width\": 230,\n        \"height\": 36,\n        \"conf\": 93.292221,\n        \"label\": \"DOUGLAS\"\n    }\n]\n\n// Analyzer Results (formatted)\n[\n    {\n        \"entity_type\": \"PERSON\",\n        \"score\": 1.0,\n        \"left\": 25,\n        \"top\": 25,\n        \"width\": 241,\n        \"height\": 37\n    },\n    {\n        \"entity_type\": \"PERSON\",\n        \"score\": 1.0,\n        \"left\": 287,\n        \"top\": 25,\n        \"width\": 230,\n        \"height\": 36\n    }\n]\n</code></pre> <p>Looking at the position and size values of the ground truth and detected text from the analyzer results, we can see that the first item in the analyzer results is likely \"DAVIDSON\" and the second is likely \"DOUGLAS\".</p> <p>With this, we set our ground truth json to the following:</p> <pre><code>// Ground truth json\n{\n    \"path/to/your/file.dcm\": [\n        {\n            \"label\": \"DAVIDSON\",\n            \"left\": 25,\n            \"top\": 25,\n            \"width\": 241,\n            \"height\": 37\n        },\n        {\n            \"label\": \"DOUGLAS\",\n            \"left\": 287,\n            \"top\": 25,\n            \"width\": 230,\n            \"height\": 36\n        }\n    ]\n}\n</code></pre> <p>Return to the Table of Contents</p>"},{"location":"image-redactor/evaluating_dicom_redaction/#evaluating-de-identification-performance","title":"Evaluating de-identification performance","text":"<p>The <code>DicomImagePiiVerifyEngine</code> can be used to evaluate DICOM de-identification performance.</p> <pre><code># Load ground truth for one file\nwith open(gt_path) as json_file:\n    all_ground_truth = json.load(json_file)\nground_truth = all_ground_truth[file_of_interest]\n\n# Select your DICOM instance\ninstance = pydicom.dcmread(file_of_interest)\n\n# Evaluate the DICOM de-identification performance\n_, eval_results = dicom_engine.eval_dicom_instance(instance, ground_truth)\n</code></pre> <p>You can also set optional arguments to see the effect of padding width, ground-truth matching tolerance, and OCR confidence threshold (e.g., <code>ocr_kwargs={\"ocr_threshold\": 50}</code>).</p> <p>For a full demonstration, please see the evaluation notebook.</p> <p>Return to the Table of Contents</p>"},{"location":"samples/","title":"Samples","text":"Topic Data Type Resource Sample Usage Text Python Notebook Presidio Basic Usage Notebook Usage Text Python Notebook Customizing Presidio Analyzer Usage Semi-structured Python Notebook Analyzing structured / semi-structured data in batch Usage Text Python Notebook Encrypting and Decrypting identified entities Usage Text Python Notebook Getting the identified entity value using a custom Operator Usage text Python Notebook Anonymizing known values Usage Images Python Notebook Redacting Text PII from DICOM images Usage Images Python Notebook Using an allow list with image redaction Usage PDF Python Notebook Annotating PII in a PDF Usage Images Python Notebook Plot custom bounding boxes Usage Text Python Notebook Integrating with external services Usage Text Python file Remote Recognizer Usage Text Python file Azure AI Language as a Remote Recognizer Usage CSV Python file Analyze and Anonymize CSV file Usage Text Python Using Flair as an external PII model Usage Text Python file Using Transformers as an external PII model Usage Text Python file Passing a lambda as a Presidio anonymizer using Faker Usage REST API (postman) Presidio as a REST endpoint Deployment App Service Presidio with App Service Deployment Kubernetes Presidio with Kubernetes Deployment Spark/Azure Databricks Presidio with Spark Deployment Azure Data Factory with App Service ETL for small dataset Deployment Azure Data Factory with Databricks ETL for large datasets ADF Pipeline Azure Data Factory Add Presidio as an HTTP service to your Azure Data Factory ADF Pipeline Azure Data Factory Add Presidio on Databricks to your Azure Data Factory Demo Streamlit app Create a simple demo app using Streamlit"},{"location":"samples/deployments/","title":"Example deployments","text":"<ul> <li>Azure App Service</li> <li>Kubernetes</li> <li>Spark/Azure Databricks</li> <li>Azure Data Factory</li> </ul>"},{"location":"samples/deployments/app-service/","title":"Deploy presidio services to an Azure App Service","text":"<p>Presidio containers can be hosted on an Azure App Service. Azure App Service provides a managed production environment, which supports docker containers and devops optimizations. It is a global scale service with built in security and compliance features that fits multiple cloud workloads. The presidio team uses Azure App Service for both its development environment and the presidio demo website.</p>"},{"location":"samples/deployments/app-service/#deploy-presidio-services-to-azure","title":"Deploy Presidio services to Azure","text":"<p>Use the following button to deploy presidio services to your Azure subscription.</p> <p></p>"},{"location":"samples/deployments/app-service/#deploy-using-command-line-script","title":"Deploy using command-line script","text":"<p>The following script can be used alternatively to the ARM template deployment above. It sets up the same components which are required for each of the presidio services (analyzer and anonymizer) as the template.</p>"},{"location":"samples/deployments/app-service/#basic-setup","title":"Basic setup","text":"<pre><code>RESOURCE_GROUP=&lt;resource group name&gt;\nAPP_SERVICE_NAME=&lt;name of app service&gt;\nLOCATION=&lt;location&gt;\nAPP_SERVICE_SKU=&lt;sku&gt;\n\nIMAGE_NAME=mcr.microsoft.com/presidio-analyzer\n# the following parameters are only required if you build and deploy your own containers from a private registry\nACR_USER_NAME=&lt;user name&gt;\nACR_USER_PASSWORD=&lt;password&gt;\n\n# create the resource group\naz group create --name $RESOURCE_GROUP\n# create the app service plan\naz appservice plan create --name $APP_SERVICE_NAME-plan --resource-group $RESOURCE_GROUP  \\\n--is-linux --location $LOCATION --sku $APP_SERVICE_SKU\n# create the web app using the official presidio images\naz webapp create --name $APP_SERVICE_NAME --plan $APP_SERVICE_NAME-plan \\\n--resource-group $RESOURCE_GROUP -i $IMAGE_NAME\n\n# or alternatively, if building presidio and deploying from a private container registry\naz webapp create --name $APP_SERVICE_NAME --plan $APP_SERVICE_NAME-plan \\\n--resource-group $RESOURCE_GROUP -i $IMAGE_NAME -s $ACR_USER_NAME -w $ACR_USER_PASSWORD\n</code></pre>"},{"location":"samples/deployments/app-service/#blocking-network-access","title":"Blocking network access","text":"<p>Use the following script to restrict network access for a specific ip such as your computer, a front-end website or an API management.</p> <pre><code>FRONT_END_IP_RANGE=[front end ip range]\naz webapp config access-restriction add --resource-group $RESOURCE_GROUP --name $APP_SERVICE_NAME \\\n  --rule-name 'Front-end allow rule' --action Allow --ip-address $FRONT_END_IP_RANGE --priority 100\n</code></pre> <p>Further network isolation, using virtual networks, is possible using an Isolated tier of Azure App Service.</p>"},{"location":"samples/deployments/app-service/#configure-app-service-logging","title":"Configure App Service Logging","text":""},{"location":"samples/deployments/app-service/#logging-to-the-app-service-file-system","title":"Logging to the App Service File System","text":"<pre><code>az webapp log config --name $APP_SERVICE_NAME --resource-group $RESOURCE_GROUP \\\n--application-logging filesystem --detailed-error-messages true \\\n--docker-container-logging filesystem --level information\n</code></pre>"},{"location":"samples/deployments/app-service/#logging-to-log-analytics-workspace","title":"Logging to Log Analytics Workspace","text":"<pre><code>LOG_ANALYTICS_WORKSPACE_RESROUCE_GROUP=&lt;resource group of log analytics&gt;\nLOG_ANALYTICS_WORKSPACE_NAME=&lt;log analytics name&gt;\n\n# create a log analytics workspace\naz monitor log-analytics workspace create --resource-group $LOG_ANALYTICS_WORKSPACE_RESROUCE_GROUP --workspace-name $LOG_ANALYTICS_WORKSPACE_NAME\n\n# query the log analytics workspace id\nLOG_ANALYTICS_WORKSPACE_ID=$(az monitor log-analytics workspace show --resource-group $LOG_ANALYTICS_WORKSPACE_RESROUCE_GROUP --workspace-name $LOG_ANALYTICS_WORKSPACE_NAME --query id -o tsv)\n# query the app service id\nAPP_SERVICE_ID=$(az monitor log-analytics workspace show --resource-group $RESOURCE_GROUP --name $APP_SERVICE_NAME --query id -o tsv)\n\n# create the diagnostics settings\naz monitor diagnostic-settings create --name $APP_SERVICE_NAME-diagnostics --resource /\n$APP_SERVICE_ID --logs   '[{\"category\": \"AppServicePlatformLogs\",\"enabled\": true}, {\"category\": \"AppServiceConsoleLogs\", \"enabled\": true}]' --metrics '[{\"category\": \"AllMetrics\",\"enabled\": true}]' --workspace $LOG_ANALYTICS_WORKSPACE_ID\n</code></pre>"},{"location":"samples/deployments/app-service/#using-an-arm-template","title":"Using an ARM template","text":"<p>Alternatlively, you can use the provided ARM template which can deploy either both or any of the presidio services. Note that while Log Analytics integration with Azure App Service is in preview, the ARM template deployment will not create a Log Analytics resource or configure the diagnostics settings from the App Service to a Log Analytics workspace. To deploy the app services using the provided ARM template, fill in the provided values.json file with the required values and run the following script.</p> <pre><code>az deployment group create --resource-group $RESOURCE_GROUP --template-file presidio-services.json --parameters @values.json\n</code></pre>"},{"location":"samples/deployments/data-factory/","title":"Anonymize PII entities with Azure Data Factory","text":"<p>You can build data anonymization ETL pipelines using Azure Data Factory (ADF) and Presidio. This section provides instructions on how to leverage presidio in Azure Data Factory:</p> <ol> <li>Complete samples - Setup Azure Data Factory and Presidio in a single deployment and run the samples.</li> <li>Using Azure Data Factory Template Gallery to anonymize text files - If you already have an instance of Azure Data Factory and would like to use the built in data-anonymization-with-presidio template to anonymize text files.</li> <li>Using Azure Data Factory Template Gallery to anonymize datasets - If you already have an instance of Azure Data Factory and would like to use the built in data-anonymization-with-presidio template to anonymize csv datasets.</li> </ol>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-databricks/","title":"Anonymize PII entities in datasets using Azure Data Factory template and Presidio on Databricks","text":"<p>This sample uses the built in data anonymization template of Azure Data Factory (which is a part of the Template Gallery) to copy a csv dataset from one location to another, while anonymizing PII data from a text column in the dataset. It leverages the code for using Presidio on Azure Databricks to call Presidio as a Databricks notebook job in the Azure Data Factory (ADF) pipeline to transform the input dataset before mergine the results to an Azure Blob Storage.</p> <p>Note that this solution is capabale of transforming large datasets. For smaller, text based input you may want to work with the Data Anonymization with Presidio as an HTTP service template which offers an easier deployment for Presidio.</p> <p>The sample deploys the following Azure Services:</p> <ul> <li>Azure Storage - The target storage account where data will be persisted.</li> <li>Azure Databricks - Host presidio to anonymize the data.</li> </ul> <p>Additionaly you should already have an instance of Azure Data Factory which hosts and orchestrates the transformation pipeline and a storage account which holds the source files.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-databricks/#about-this-solution-template","title":"About this Solution Template","text":"<p>This template gets the files from your source file-based store. It then anonymizes the content and uploads each of them to the destination store.</p> <p>The template contains three activities:</p> <ul> <li>AnonymizeSource runs the presidio notebook on the input file to create an output folder with csv parts.</li> <li>MergeAnonymizedToTarget merges the csv parts from databricks output folder to a single csv on the target storage container.</li> <li>DeleteAnonymized deletes the temporary output folder.</li> </ul> <p>The template defines four parameters:</p> <ul> <li>SourceStore_Location is the container name of your source store where you want to move files from (STORAGE_CONTAINER_NAME).</li> <li>DestinationStore_Name is the container name in the target storage account which is provisioned by the ARM template.</li> <li>SourceFile_Name is the name of the input file.</li> <li>TextColumn_Name is the name of the column to be anonymized.</li> </ul>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-databricks/#how-to-use-this-solution-template","title":"How to use this Solution Template","text":"<p>To use this template you should first setup the required infrastructure for the sample to run, then setup the template in Azure Data Factory.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-databricks/#setup-presidio","title":"Setup Presidio","text":"<p>Provision and setup the datbricks cluster by following the Deploy and Setup steps in presidio-spark sample. Take a note of the authentication token and do not follow the \"Running the sample\" steps.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-databricks/#setup-azure-data-factory","title":"Setup Azure Data Factory","text":"<ol> <li> <p>Go to the Data anonymization with Presidio on Databricks template. Select the AnonymizedCSV connection (Azure Storage) and select \"New\" from the drop down menu. </p> </li> <li> <p>Name the service \"PresidioStorage\" and select the storage account that was created in the previous steps from your subscription. Note that Target source was also selecte as the sample uses the same storage account for both source and target. </p> </li> <li> <p>Select the Anonymize Source connection (Databricks) and select \"New\" from the drop down menu. </p> </li> <li> <p>Name the service \"PresidioDatabricks\" and select the Azure Databricks workspace that was created in the previous steps from your subscription. Follow through the steps to input the authentication token which was generated in the previous step, or create a new one by following this guide. Select presidio_cluster to run the job. </p> </li> <li> <p>Select Use this template tab</p> </li> <li> <p>You'll see the pipeline, as in the following example: </p> </li> <li> <p>Upload a csv file to the storage container and select Debug, enter the Parameters, and then select Finish. The parameters are the container where you want to move files from, the container name where you want to move the anonymized files to, the csv file name and the name of a text column in the csv file. </p> </li> <li> <p>Review the result. </p> </li> </ol>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-http/","title":"Anonymize PII entities in text using Azure Data Factory template and Presidio as an HTTP service","text":"<p>This sample uses the built in data anonymization template of Azure Data Factory which is a part of the Template Gallery to move a set of text files from one location to another while anonymizing their content. It leverages the code for using Presidio on Azure App Service to call Presidio as an HTTP REST endpoint in the Azure Data Factory (ADF) pipeline while parsing and storing each file as an Azure Blob Storage.</p> <p>Note that given the solution architecture which call presidio services using HTTP, this sample should be used for up to 5000 files, each up to 200KB in size. The restrictions are based on ADF lookup-activity which is used to iterate the files in the storage container (up to 5000 records), and having Presidio as an HTTP endpoint with text being sent over network to be anonymized.  For larger sets please work with the Data Anonymization with Presidio on Databricks template.</p> <p>The sample deploys the following Azure Services:</p> <ul> <li>Azure KeyVault - Holds the access keys for Azure Storage to avoid having keys and secrets in the code.</li> <li>Azure Storage - The target storage account where data will be persisted.</li> <li>Azure App Service - Host presidio to anonymize the data.</li> </ul> <p>Additionaly you should already have an instance of Azure Data Factory which host and orchestrate the transformation pipeline and a storage account which holds the source files.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-http/#about-this-solution-template","title":"About this Solution Template","text":"<p>This template gets the files from your source file-based store. It then anonymizes the content and uploads each of them to the destination store.</p> <p>The template contains eight activities:</p> <ul> <li>GetMetadata gets the list of objects including the files and subfolders from your folder on source store. It will not retrieve the objects recursively.</li> <li>Filter filter the objects list from GetMetadata activity to select the files only.</li> <li>GetSASToken gets the target storage account SAS token from the Azure Key Vault.</li> <li>ForEach gets the file list from the Filter activity and then iterates over the list and passes each file to the Anonymization activities.</li> <li>LoadFileContent loads the content of a text file to an ADF Lookup.</li> <li>PresidioAnalyze sends the text content to Presidio Analyzer.</li> <li>PresidioAnonymize sends the text and the analysis response to Presidio Anonymizer to get the anonymized text.</li> <li>UploadBlob uses Azure Blob Storage REST API to upload the anonymized text to the target container.</li> </ul> <p>The template defines four parameters:</p> <ul> <li>SourceStore_Location is the container name of your source store where you want to move files from.</li> <li>DestinationStore_Name is the name of the target storage account which is provisioned by the ARM template.</li> <li>DestinationStore_Location is the container name of your destination store where you want to move files to. it has a default value of a container which was created during provisioning of the ARM template (presidio).</li> <li>KeyVault_Name is the name of the Azure Key Vault which is provisioned by the ARM template.</li> <li>Analyzer_Url is the URL for the Analyzer App Service which is provisioned by the ARM template.</li> <li>Anonymizer_Url is the URL for the Anonymizer App Service which is provisioned by the ARM template.</li> </ul>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-http/#how-to-use-this-solution-template","title":"How to use this Solution Template","text":"<p>To use this template you should first setup the required infrastructure for the sample to run, then setup the template in Azure Data Factory.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-http/#setup-presidio","title":"Setup Presidio","text":"<p>Create the Azure App Service, the storage accounts and an Azure Key Vault by clicking the Deploy-to-Azure button, or by running the following script to provision the provided ARM template.</p> <p></p> <pre><code>RESOURCE_GROUP=[Name of resource group]\nLOCATION=[location of resources]\n\naz group create --name $RESOURCE_GROUP --location $LOCATION\naz deployment group create -g $RESOURCE_GROUP --template-file ./arm-templates/azure-deploy-adf-template-gallery-http.json\n</code></pre> <p>Note that:</p> <ul> <li>A SAS token keys is created and read from Azure Storage and then imported to Azure Key Vault. Using ARM template built in functions: listAccountSas. This token is time limited.</li> <li>An access policy grants the Azure Data Factory managed identity access to the Azure Key Vault. You should provide your ADF client principal ID by following this guide.</li> </ul>"},{"location":"samples/deployments/data-factory/presidio-data-factory-template-gallery-http/#setup-azure-data-factory","title":"Setup Azure Data Factory","text":"<ol> <li> <p>Go to the Data anonymization with Presidio as an HTTP service template. Select existing connection or create a New connection to your source file store where you want to move files from. Be aware that DataSource_Folder and DataSource_File are reference to the same connection of your source file store. </p> </li> <li> <p>Select Use this template tab</p> </li> <li> <p>You'll see the pipeline, as in the following example: </p> </li> <li> <p>Select Debug, enter the Parameters, and then select Finish. The parameters are the container where you want to move files from and the container path where you want to move the anonymized files to. </p> </li> <li> <p>Review the result. </p> </li> </ol>"},{"location":"samples/deployments/data-factory/presidio-data-factory/","title":"Anonymize PII entities in an Azure Data Factory ETL Pipeline","text":"<p>The following samples showcase two scenarios which use Azure Data Factory (ADF) to move a set of JSON objects from an online location to an Azure Storage while anonymizing their content. The first sample leverages the code for using Presidio on Azure App Service to call Presidio as an HTTP REST endpoint in the ADF pipeline while parsing and storing each file as an Azure Blob Storage. The second sample leverage the code for using Presidio on spark to run over a set of files on an Azure Blob Storage to anonymnize their content, in the case of having a large data set that requires the scale of databricks.</p> <p>The samples deploy and use the following Azure Services:</p> <ul> <li>Azure Data Factory - Host and orchestrate the transformation pipeline.</li> <li>Azure KeyVault - Holds the access keys for Azure Storage to avoid having keys and secrets in the code.</li> <li>Azure Storage - Persistence layer of this sample.</li> <li>Azure Databricks/ Azure App Service - Host presidio to anonymize the data.</li> </ul> <p>The input file used by the samples is hosted on presidio-research repository. It is setup as a variable on the provided ARM template and used by Azure Data Factory as the input source.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#option-1-presidio-as-an-http-rest-endpoint","title":"Option 1: Presidio as an HTTP REST endpoint","text":"<p>By using Presidio as an HTTP endpoint, the user can select which infrastructure best suits their requirements. in this sample, Presidio is deployed to an Azure App Service, but other deployment targets can be used, such as kubernetes.</p> <p></p>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#deploy-the-arm-template","title":"Deploy the ARM template","text":"<p>Create the Azure App Service and the ADF pipeline by clicking the Deploy-to-Azure button, or by running the following script to provision the provided ARM template.</p> <p></p> <pre><code>RESOURCE_GROUP=[Name of resource group]\nLOCATION=[location of resources]\n\naz group create --name $RESOURCE_GROUP --location $LOCATION\naz deployment group create -g $RESOURCE_GROUP --template-file ./arm-templates/azure-deploy-adf-app-service.json\n</code></pre> <p>Note that:</p> <ul> <li>A SAS token keys is created and read from Azure Storage and then imported to Azure Key Vault. Using ARM template built in functions: listAccountSas.</li> <li>An access policy grants the Azure Data Factory managed identity access to the Azure Key Vault by using ARM template reference function to the Data Factory object and acquire its identity.principalId property. This is enabled by setting the data factory ARM resource's identity attribute to managed identity (SystemAssigned).</li> </ul>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#about-this-solution-template","title":"About this Solution Template","text":"<p>This template gets a collection of JSON documents from a file on GitHub. It then extracts one of the text fields of the document, anonymizes the content and uploads it as a text file to the destination store.</p> <p>The template contains seven activities:</p> <ul> <li>GetDataSet-\u200aCopy the dataset from GitHub to the first folder on the Azure Storage blob container (/dataset).</li> <li>LoadSet-\u200aLoads the dataset into the Azure Data Factory memory for processing in a for-each loop.</li> <li>GetSASToken\u200a-\u200aGet the SAS token from Azure Key Vault. This will be used later for writing to the blob container.</li> <li>SaveBlobs\u200a-\u200aIs a For-Each loop activity. It includes a clause which is executed for each document in the array.</li> <li>PresidioAnalyze\u200a-\u200aSends the text to presidio analyzer endpoint.</li> <li>PresidioAnonymize\u200a-\u200aSends the response from presidio analyzer to presidio anonymizer endpoint.</li> <li>UploadBlob\u200a-\u200aSaves the anonymized response from presidio to a randomly named text file on the target Azure Blob Storage.</li> </ul>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#option-2-presidio-on-azure-databricks","title":"Option 2: Presidio on Azure Databricks","text":"<p>By using Presidio as a Notebook step in ADF, we allow Databricks to scale presidio according to the cluster capabilities and the input dataset. Using presidio as a native python package in pyspark can unlock more analysis and de-identifiaction scenarios.</p> <p></p>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#pre-requisite-deploy-azure-databricks","title":"Pre-requisite - Deploy Azure Databricks","text":"<p>Provision and setup the datbricks cluster by following the steps in presidio-spark sample. Note the output key and export it as DATABRICKS_TOKEN environment variable.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#deploy-the-arm-template_1","title":"Deploy the ARM template","text":"<p>Create the rest of the services by running the following script which uses the provided ARM template.</p> <pre><code>RESOURCE_GROUP=[Name of resource group]\nLOCATION=[location of resources]\nDATABRICKS_HOST=https://$DATABRICKS_WORKSPACE_URL\nDATABRICKS_CLUSTER_ID=$(databricks clusters get --cluster-name presidio_cluster | jq -r .cluster_id)\nDATABRICKS_NOTEBOOK_LOCATION=\"/notebooks/01_transform_presidio\"\n\naz deployment group create -g $RESOURCE_GROUP --template-file ./arm-templates/azure-deploy-adf-databricks.json --parameters Databricks_accessToken=$DATABRICKS_TOKEN Databricks_clusterId=$DATABRICKS_CLUSTER_ID Databricks_notebookLocation=$DATABRICKS_NOTEBOOK_LOCATION Databricks_workSpaceUrl=$DATABRICKS_HOST AzureBlobStorage_accountName=$STORAGE_ACCOUNT_NAME AzureBlobStorage_cotainerName=$STORAGE_CONTAINER_NAME\n</code></pre> <p>Note that: Two keys are read from Azure Storage and imported to Azure Key Vault, the account Access Token and a SAS token, using ARM template built in functions: listAccountSas and listKeys.</p>"},{"location":"samples/deployments/data-factory/presidio-data-factory/#about-this-solution-template_1","title":"About this Solution Template","text":"<p>This template gets a collection of JSON documents from a file on GitHub. It then extracts one of the text fields of the document and saves it to a text file on a temporary folder in the storage account (un-anonymized content). It then runs a spark notebook job that anonymizes the content of the files in that folder and saves the result as csv files on the destination store.</p> <p>The template contains seven activities:</p> <ul> <li>GetDataSet\u200a-\u200aCopy the dataset from GitHub to the first folder on the Azure Storage blob container (/dataset).</li> <li>GetSASToken\u200a-\u200aGet the SAS token from Azure Key Vault. This will be used later for writing to the blob container.</li> <li>LoadSet\u200a-\u200a Loads the dataset into the Azure Data Factory memory for processing in a for-each loop.</li> <li>SaveBlobs\u200a-\u200aIs a For-Each loop activity. It includes a clause which is executed for each document in the array.</li> <li>UploadBlob\u200a-\u200aSaves the text file on a temporary container on the target Azure Blob Storage</li> <li>GetSecret\u200a-\u200aGet the storage account secret from Azure Key Vault. This will be used later for accessing the blob container from databricks</li> <li>Presidio-Anonymize\u200a-\u200aIs a databricks spark job which runs presidio on the temporary storage container. the result of this job is a new container (/output) with csv files that contain the anonymized text.</li> </ul>"},{"location":"samples/deployments/k8s/","title":"Deploy presidio to Kubernetes","text":"<p>You can install Presidio locally using KIND, as a service in Kubernetes or AKS.</p> <ul> <li>Deploy locally using KIND</li> <li>Deploy with Kubernetes</li> <li>Prerequisites</li> <li>Step by Step Deployment with customizable parameters</li> </ul>"},{"location":"samples/deployments/k8s/#deploy-locally-with-kind","title":"Deploy locally with KIND","text":"<p>KIND (Kubernetes IN Docker).</p> <ol> <li> <p>Install Docker.</p> </li> <li> <p>Clone Presidio.</p> </li> <li> <p>Run the following script, which will use KIND (Kubernetes emulation in Docker)</p> </li> </ol> <pre><code>cd docs/samples/deployments/k8s/deployment/\n./run-with-kind.sh\n</code></pre> <ol> <li>Wait and verify all pods are running:</li> </ol> <pre><code>kubectl get pod -n presidio\n</code></pre> <ol> <li>Port forwarding of HTTP requests to the API micro-service will be done automatically. In order to run manual:</li> </ol> <pre><code>kubectl port-forward &lt;presidio-analyzer-pod-name&gt; 8080:8080 -n presidio\n</code></pre>"},{"location":"samples/deployments/k8s/#presidio-as-a-service-with-kubernetes","title":"Presidio As a Service with Kubernetes","text":""},{"location":"samples/deployments/k8s/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Kubernetes 1.18+ cluster with RBAC enabled. If you are using AKS RBAC is enabled by default.</li> </ol> <p>!!! note: Note       Note the pod's resources requirements (CPU and memory) and plan the cluster accordingly.</p> <ol> <li> <p>kubectl installed. Verify you can communicate with the cluster by running:</p> <pre><code>kubectl version\n</code></pre> </li> <li> <p>Local helm client.</p> </li> <li>Optional - Container Registry - such as ACR. Only needed if you are using your own presidio images and not the default ones from from Microsoft syndicates container catalog</li> <li>Recent presidio repo is cloned on your local machine.</li> </ol>"},{"location":"samples/deployments/k8s/#step-by-step-deployment-with-customizable-parameters","title":"Step by step deployment with customizable parameters","text":"<ol> <li> <p>Install Helm with RBAC.</p> </li> <li> <p>Optional - Ingress controller for presidio API, e.g., NGINX.</p> </li> </ol> <p>Note: Presidio is not deployed with an ingress controller by default.    to change this behavior, deploy the helm chart with <code>ingress.enabled=true</code> and specify they type of ingress controller to be used with <code>ingress.class=nginx</code> (supported classes are: <code>nginx</code>).</p> <ol> <li>Deploy from <code>/docs/samples/deployments/k8s/charts/presidio</code></li> </ol> <pre><code># Based on the DOCKER_REGISTRY and PRESIDIO_LABEL from the previous steps\nhelm install --name demo --set registry=${DOCKER_REGISTRY},tag=${PRESIDIO_LABEL} . --namespace presidio\n</code></pre>"},{"location":"samples/deployments/spark/","title":"Anonymize PII using Presidio on Spark","text":"<p>You can leverages presidio to perform data anonymization as part of spark notebooks.</p> <p>The following sample uses Azure Databricks and simple text files hosted on Azure Blob Storage. However, it can easily change to fit any other scenario which requires PII analysis or anonymization as part of spark jobs.</p> <p>Note that this code works for Databricks runtime 8.1 (spark 3.1.1) and the libraries described here.</p>"},{"location":"samples/deployments/spark/#the-basics-of-working-with-presidio-in-spark","title":"The basics of working with Presidio in Spark","text":"<p>A typical use case of Presidio in Spark is transforming a text column in a data frame, by anonymizing its content. The following code sample, a part of transform presidio notebook, is the basis of the e2e sample which uses Azure Databricks as the Spark environment.</p> <pre><code>anonymized_column = \"value\" # name of column to anonymize\nanalyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n\n# broadcast the engines to the cluster nodes\nbroadcasted_analyzer = sc.broadcast(analyzer)\nbroadcasted_anonymizer = sc.broadcast(anonymizer)\n\n# define a pandas UDF function and a series function over it.\ndef anonymize_text(text: str) -&gt; str:\n    analyzer = broadcasted_analyzer.value\n    anonymizer = broadcasted_anonymizer.value\n    analyzer_results = analyzer.analyze(text=text, language=\"en\")\n    anonymized_results = anonymizer.anonymize(\n        text=text,\n        analyzer_results=analyzer_results,\n        operators={\n            \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"&lt;ANONYMIZED&gt;\"})\n        },\n    )\n    return anonymized_results.text\n\n\ndef anonymize_series(s: pd.Series) -&gt; pd.Series:\n    return s.apply(anonymize_text)\n\n\n# define a the function as pandas UDF\nanonymize = pandas_udf(anonymize_series, returnType=StringType())\n\n# apply the udf\nanonymized_df = input_df.withColumn(\n    anonymized_column, anonymize(col(anonymized_column))\n)\n</code></pre>"},{"location":"samples/deployments/spark/#pre-requisites","title":"Pre-requisites","text":"<p>If you do not have an instance of Azure Databricks, follow through with the following steps to provision and setup the required infrastrucutre.</p> <p>If you do have a Databricks workspace and a cluster you wish to configure to run Presidio, jump over to the Configure an existing cluster section.</p>"},{"location":"samples/deployments/spark/#deploy-infrastructure","title":"Deploy Infrastructure","text":"<p>Provision the Azure resources by running the following script.</p> <pre><code>export RESOURCE_GROUP=[resource group name]\nexport STORAGE_ACCOUNT_NAME=[storage account name]\nexport STORAGE_CONTAINER_NAME=[blob container name]\nexport DATABRICKS_WORKSPACE_NAME=[databricks workspace name]\nexport DATABRICKS_SKU=[basic/standard/premium]\nexport LOCATION=[location]\n\n# Create the resource group\naz group create --name $RESOURCE_GROUP --location $LOCATION\n\n# Use ARM template to build the resources and get back the workspace URL\ndeployment_response=$(az deployment group create -g $RESOURCE_GROUP --template-file ./docs/samples/deployments/spark/arm-template/databricks.json  --parameters location=$LOCATION workspaceName=$DATABRICKS_WORKSPACE_NAME storageAccountName=$STORAGE_ACCOUNT_NAME containerName=$STORAGE_CONTAINER_NAME)\n\nexport DATABRICKS_WORKSPACE_URL=$(echo $deployment_response | jq -r \".properties.outputs.workspaceUrl.value\")\nexport DATABRICKS_WORKSPACE_ID=$(echo $deployment_response | jq -r \".properties.outputs.workspaceId.value\")\n</code></pre>"},{"location":"samples/deployments/spark/#setup-databricks","title":"Setup Databricks","text":"<p>The following script will setup a new cluster in the databricks workspace and prepare it to run presidio anonymization jobs. Once finished, the script will output an access key which you can use when working with databricks cli.</p> <pre><code>sh ./scripts/configure_databricks.sh\n</code></pre>"},{"location":"samples/deployments/spark/#configure-an-existing-cluster","title":"Configure an existing cluster","text":"<p>Only follow through with the steps in this section if you have an existing databricks workspace and clsuter you wish to configure to run presidio. If you've followed through with the \"Deploy Infrastructure\" and \"Setup Databricks\" sections you do not have to run the script in this section.</p>"},{"location":"samples/deployments/spark/#set-up-secret-scope-and-secrets-for-storage-account","title":"Set up secret scope and secrets for storage account","text":"<p>Add an Azure Storage account key to secret scope.</p> <pre><code>STORAGE_PRIMARY_KEY=[Primary key of storage account]\n\ndatabricks secrets create-scope --scope storage_scope --initial-manage-principal users\ndatabricks secrets put --scope storage_scope --key storage_account_access_key --string-value \"$STORAGE_PRIMARY_KEY\"\n</code></pre>"},{"location":"samples/deployments/spark/#upload-or-update-cluster-init-scripts","title":"Upload or update cluster init scripts","text":"<p>Presidio libraries are loaded to the cluster on init. Upload the cluster setup script or add its content to the existing cluster's init script.</p> <pre><code>databricks fs cp \"./setup/startup.sh\" \"dbfs:/FileStore/dependencies/startup.sh\"\n</code></pre> <p>Setup the cluster to run the init script.</p>"},{"location":"samples/deployments/spark/#upload-presidio-notebooks","title":"Upload presidio notebooks","text":"<pre><code>databricks workspace import_dir \"./notebooks\" \"/notebooks\" --overwrite\n</code></pre>"},{"location":"samples/deployments/spark/#update-cluster-environment","title":"Update cluster environment","text":"<p>Add the following environment variables to your databricks cluster:</p> <pre><code>\"STORAGE_MOUNT_NAME\": \"/mnt/files\"\n\"STORAGE_CONTAINER_NAME\": [Blob container name]\n\"STORAGE_ACCOUNT_NAME\": [Storage account name]\n</code></pre>"},{"location":"samples/deployments/spark/#mount-the-storage-container","title":"Mount the storage container","text":"<p>Run the notebook 00_setup to mount the storage account to databricks.</p>"},{"location":"samples/deployments/spark/#running-the-sample","title":"Running the sample","text":""},{"location":"samples/deployments/spark/#configure-presidio-transformation-notebook","title":"Configure Presidio transformation notebook","text":"<p>From Databricks workspace, under notebooks folder, open the provided 01_transform_presidio notebook and attach it to the cluster preisidio_cluster. Run the first code-cell and note the following parameters on the top end of the notebook (notebook widgets) and set them accordingly</p> <ul> <li>Input File Format - text (selected).</li> <li>Input path - a folder on the container where input files are found.</li> <li>Output Folder - a folder on the container where output files will be written to.</li> <li>Column to Anonymize - value (selected).</li> </ul>"},{"location":"samples/deployments/spark/#run-the-notebook","title":"Run the notebook","text":"<p>Upload a text file to the blob storage input folder, using any preferd method (Azure Portal, Azure Storage Explorer, Azure CLI).</p> <pre><code>az storage blob upload --account-name $STORAGE_ACCOUNT_NAME  --container $STORAGE_CONTAINER_NAME --file ./[file name] --name input/[file name]\n</code></pre> <p>Run the notebook cells, the output should be csv files which contain two columns, the original file name, and the anonymized content of that file.</p>"},{"location":"samples/docker/","title":"Using Presidio in Docker","text":""},{"location":"samples/docker/#description","title":"Description","text":"<p>Presidio can expose REST endpoints for each service using Flask and Docker. Follow the installation guide to learn how to install and run presidio-analyzer and presidio-anonymizer using docker.</p>"},{"location":"samples/docker/#postman-collection","title":"Postman collection","text":"<p>This repository contains a postman collection with sample REST API request for each service. Follow this tutorial to learn how to export the sample requests into postman</p> <ol> <li>Download Presidio Analyzer postman requests</li> <li>Download Presidio Anonymizer postman requests</li> </ol>"},{"location":"samples/docker/#sample-api-calls","title":"Sample API Calls","text":""},{"location":"samples/docker/#simple-text-analysis","title":"Simple Text Analysis","text":"<pre><code>curl -X POST http://localhost:5002/analyze -H \"Content-type: application/json\" --data \"{ \\\"text\\\": \\\"John Smith drivers license is AC432223\\\", \\\"language\\\" : \\\"en\\\"}\"\n</code></pre>"},{"location":"samples/docker/#simple-text-anonymization","title":"Simple Text Anonymization","text":"<pre><code>curl -X POST http://localhost:5001/anonymize -H \"Content-type: application/json\" --data \"{\\\"text\\\": \\\"hello world, my name is Jane Doe. My number is: 034453334\\\", \\\"analyzer_results\\\": [{\\\"start\\\": 24, \\\"end\\\": 32, \\\"score\\\": 0.8, \\\"entity_type\\\": \\\"NAME\\\"}, { \\\"start\\\": 48, \\\"end\\\": 57,  \\\"score\\\": 0.95,\\\"entity_type\\\": \\\"PHONE_NUMBER\\\" }],  \\\"anonymizers\\\": {\\\"DEFAULT\\\": { \\\"type\\\": \\\"replace\\\", \\\"new_value\\\": \\\"ANONYMIZED\\\" },\\\"PHONE_NUMBER\\\": { \\\"type\\\": \\\"mask\\\", \\\"masking_char\\\": \\\"*\\\", \\\"chars_to_mask\\\": 4, \\\"from_end\\\": true }}}\"\n</code></pre>"},{"location":"samples/python/Anonymizing%20known%20values/","title":"Anonymizing known values","text":"<pre><code># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n\n!python -m spacy download en_core_web_lg\n</code></pre> <p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/Anonymizing%20known%20values.ipynb</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, PatternRecognizer\nfrom presidio_anonymizer import AnonymizerEngine\n</code></pre> <pre><code># Get known values as a deny-list\nknown_names_list = [\"George\", \"Abraham\", \"Theodore\", \"Bill\", \"Barack\", \"Donald\", \"Joe\"]\n</code></pre> <pre><code># Create a PatternRecognizer for the deny list\ndeny_list_recognizer = PatternRecognizer(supported_entity=\"PRESIDENT_FIRST_NAME\", deny_list=known_names_list)\n</code></pre> <pre><code>registry = RecognizerRegistry()\nregistry.add_recognizer(deny_list_recognizer)\n\nanalyzer = AnalyzerEngine(registry=registry)\n\nanonymizer = AnonymizerEngine()\n</code></pre> <pre><code>text=\"George Washington was the first US president\"\n\nresults = analyzer.analyze(text=text, language=\"en\")\n\nprint(\"Identified entities:\")\nprint(results)\nprint(\"\")\nanonymized = anonymizer.anonymize(text=text, analyzer_results=results)\nprint(f\"Anonymized text:\\n{anonymized.text}\")\n</code></pre> <pre>\n<code>Identified entities:\n[type: PRESIDENT_FIRST_NAME, start: 0, end: 6, score: 1.0]\n\nAnonymized text:\n&lt;PRESIDENT_FIRST_NAME&gt; Washington was the first US president\n</code>\n</pre> <pre><code>person1 = {\"name\": \"Martin Smith\", \n           \"special_value\":\"145A\", \n           \"free_text\": \"Martin Smith, id 145A, likes playing basketball\"}\nperson2 = {\"name\":\"Deb Schmidt\", \n           \"special_value\":\"256B\", \n           \"free_text\": \"Deb Schmidt, id 256B likes playing soccer\"}\nperson3 = {\"name\":\"R2D2\", \n           \"special_value\":\"X1T2\", \n           \"free_text\": \"X1T2 is R2D2's special value\"}\n\ndataset = [person1, person2, person3]\ndataset\n</code></pre> <pre>\n<code>[{'name': 'Martin Smith',\n  'special_value': '145A',\n  'free_text': 'Martin Smith, id 145A, likes playing basketball'},\n {'name': 'Deb Schmidt',\n  'special_value': '256B',\n  'free_text': 'Deb Schmidt, id 256B likes playing soccer'},\n {'name': 'R2D2',\n  'special_value': 'X1T2',\n  'free_text': \"X1T2 is R2D2's special value\"}]</code>\n</pre> <p>We're interested in anonymizing the free text using the values contained in <code>name</code> and <code>special_value</code>. Since these values are only available in the context of one record, we use the ad-hoc recognizer capability in Presidio, instead of a generic deny-list <code>PatternRecognizer</code> added to Presidio's <code>RecognizerRegistry</code>.</p> <pre><code># Go over dataset\nfor person in dataset:\n\n    # Get the different known values\n    name = person['name']\n    special_val = person['special_value']\n\n    # Get the free text to anonymize\n    free_text = person['free_text']\n\n    # Create ad-hoc recognizers\n    ad_hoc_name_recognizer = PatternRecognizer(supported_entity=\"name\", deny_list = [name])\n    ad_hoc_id_recognizer = PatternRecognizer(supported_entity=\"special_value\", deny_list = [special_val])\n\n    # Run the analyze method with ad_hoc_recognizers:\n    analyzer_results = analyzer.analyze(text=free_text, \n                                        language=\"en\", \n                                        ad_hoc_recognizers=[ad_hoc_name_recognizer, ad_hoc_id_recognizer])\n\n    # Anonymize results\n    anonymized = anonymizer.anonymize(text=free_text, analyzer_results=analyzer_results)\n    print(anonymized.text)\n\n    # Store output in original dataset\n    person[\"anonymized_free_text\"] = anonymized.text\n</code></pre> <pre>\n<code>&lt;name&gt;, id &lt;special_value&gt;, likes playing basketball\n&lt;name&gt;, id &lt;special_value&gt; likes playing soccer\n&lt;special_value&gt; is &lt;name&gt;'s special value\n</code>\n</pre> <pre><code># Dataset now contains the anonymiezd version as well\ndataset\n</code></pre> <pre>\n<code>[{'name': 'Martin Smith',\n  'special_value': '145A',\n  'free_text': 'Martin Smith, id 145A, likes playing basketball',\n  'anonymized_free_text': '&lt;name&gt;, id &lt;special_value&gt;, likes playing basketball'},\n {'name': 'Deb Schmidt',\n  'special_value': '256B',\n  'free_text': 'Deb Schmidt, id 256B likes playing soccer',\n  'anonymized_free_text': '&lt;name&gt;, id &lt;special_value&gt; likes playing soccer'},\n {'name': 'R2D2',\n  'special_value': 'X1T2',\n  'free_text': \"X1T2 is R2D2's special value\",\n  'anonymized_free_text': \"&lt;special_value&gt; is &lt;name&gt;'s special value\"}]</code>\n</pre> <p>Note that in these examples we're only using the custom recognizers we created. We can also add our custom recognizers to the existing recognizers in presidio, by calling <code>registry.load_predefined_recognizers()</code>:</p> <pre><code>registry = RecognizerRegistry()\n\n# Load existing recognizer\nregistry.load_predefined_recognizers()\n\n# Add our custom one\nregistry.add_recognizer(deny_list_recognizer)\n\n# Initialize AnalyzerEngine\nanalyzer = AnalyzerEngine(registry=registry)\n</code></pre> <pre><code>analyzer.analyze(\"George Washington was the first president of the United States\", language=\"en\")\n</code></pre> <pre>\n<code>[type: PRESIDENT_FIRST_NAME, start: 0, end: 6, score: 1.0,\n type: PERSON, start: 0, end: 17, score: 0.85,\n type: LOCATION, start: 45, end: 62, score: 0.85]</code>\n</pre> <p>Since George is also a name, it was detected twice, once as a PERSON entity, and once as a custom entity.</p> <p>Read more:</p> <ul> <li>For more info on Presidio Analyzer, see this documentation</li> <li>For more info on Presidio Anonymize, see this documentation</li> <li>To further customize the anonymization type, see this tutorial</li> </ul> <pre><code>\n</code></pre>"},{"location":"samples/python/Anonymizing%20known%20values/#anonymizing-known-values","title":"Anonymizing known values","text":"<p>In addition to statistical and pattern based approaches, Presidio also supports the identification and anonymization of known values, using the deny-list mechanism. In this example we'll cover two cases: 1. The known values are known a-priori (e.g., we have a list of names) 2. The known values are only known in the context of a request (e.g., we have the name of a person as the filename)</p>"},{"location":"samples/python/Anonymizing%20known%20values/#example-1-values-are-known-a-priori","title":"Example 1: values are known a-priori","text":"<p>Assume you have a list of potential PII values, you can create a recognizer which would detect them every time they appear in the text. For this case, we can create a deny-list based recognizer, and add it to presidio's <code>RecognizerRegistry</code>:</p>"},{"location":"samples/python/Anonymizing%20known%20values/#example-2-values-are-only-known-in-the-context-of-the-request","title":"Example 2: values are only known in the context of the request","text":"<p>In some cases, we know the potential PII values only in the context of a specific text. Examples could be: 1. Detect PII entities in free text columns in tabular databases, where other columns have entity values we can leverage 2. Detect PII in a file having the filename or other metadata holding potential PII values (e.g. Smith.csv) 3. Anonymize medical images which contain metadata 4. Anonymize financial forms when the actual PII data is known</p> <p>In this case we can use a functionality called ad-hoc recognizers. Here's a simple example:</p>"},{"location":"samples/python/GPT3_synth_data/","title":"GPT3 synth data","text":"<pre><code># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!pip install openai pandas\n!python -m spacy download en_core_web_lg\n</code></pre> <p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/GPT3_synth_data.ipynb</p> <pre><code>import pprint\nfrom dotenv import load_dotenv\nimport os\nimport pandas as pd\nimport openai\n\nload_dotenv()\n\nopenai.api_key = os.getenv(\"OPENAI_KEY\") #Or put explicitly in notebook. Find out more here: https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key\n</code></pre> <pre><code>def call_completion_model(prompt:str, model:str=\"text-davinci-003\", max_tokens:int=512) -&amp;gt;str:\n    \"\"\"Creates a request for the OpenAI Completion service and returns the response.\n\n    :param prompt: The prompt for the completion model\n    :param model: OpenAI model name\n    :param max_tokens: Model's max tokens parameter\n    \"\"\"\n\n    response = openai.Completion.create(\n        model=model,\n        prompt= prompt,\n        max_tokens=max_tokens\n    )\n\n    return response['choices'][0].text\n</code></pre> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\nanalyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n\nsample = \"\"\"\nHello, my name is David Johnson and I live in Maine.\nMy credit card number is 4095-2609-9393-4932 and my crypto wallet id is 16Yeky6GMjeNkAiNcBY7ZhrLoMSgg1BoyZ.\n\nOn September 18 I visited microsoft.com and sent an email to test@presidio.site,  from the IP 192.168.0.1.\n\nMy passport: 191280342 and my phone number: (212) 555-1234.\n\nThis is a valid International Bank Account Number: IL150120690000003111111 . Can you please check the status on bank account 954567876544?\n\nKate's social security number is 078-05-1126.  Her driver license? it is 1234567A.\n\"\"\"\n\nresults = analyzer.analyze(sample, language=\"en\")\nanonymized = anonymizer.anonymize(text=sample, analyzer_results=results)\nanonymized_text = anonymized.text\nprint(anonymized_text)\n</code></pre> <pre>\n<code>\nHello, my name is &lt;PERSON&gt; and I live in &lt;LOCATION&gt;.\nMy credit card number is &lt;CREDIT_CARD&gt; and my crypto wallet id is &lt;CRYPTO&gt;.\n\nOn &lt;DATE_TIME&gt; I visited &lt;URL&gt; and sent an email to &lt;EMAIL_ADDRESS&gt;,  from the IP &lt;IP_ADDRESS&gt;.\n\nMy passport: &lt;US_PASSPORT&gt; and my phone number: &lt;PHONE_NUMBER&gt;.\n\nThis is a valid International Bank Account Number: &lt;IBAN_CODE&gt; . Can you please check the status on bank account &lt;US_BANK_NUMBER&gt;?\n\n&lt;PERSON&gt;'s social security number is &lt;US_SSN&gt;.  Her driver license? it is &lt;US_DRIVER_LICENSE&gt;.\n\n</code>\n</pre> <pre><code>def create_prompt(anonymized_text: str) -&amp;gt; str:\n    \"\"\"\n    Create the prompt with instructions to GPT-3.\n\n    :param anonymized_text: Text with placeholders instead of PII values, e.g. My name is &lt;person&gt;.\n    \"\"\"\n\n    prompt = f\"\"\"\n    Your role is to create synthetic text based on de-identified text with placeholders instead of personally identifiable information.\n    Replace the placeholders (e.g. , , {{DATE}}, {{ip_address}}) with fake values.\n\n    Instructions:\n\n    Use completely random numbers, so every digit is drawn between 0 and 9.\n    Use realistic names that come from diverse genders, ethnicities and countries.\n    If there are no placeholders, return the text as is and provide an answer.\n    input: How do I change the limit on my credit card {{credit_card_number}}?\n    output: How do I change the limit on my credit card 2539 3519 2345 1555?\n    input: {anonymized_text}\n    output:\n    \"\"\"\n    return prompt\n</code></pre> <pre><code>print(\"This is the prompt with de-identified values:\")\nprint(create_prompt(anonymized_text))\n</code></pre> <pre>\n<code>This is the prompt with de-identified values:\n\n    Your role is to create synthetic text based on de-identified text with placeholders instead of personally identifiable information.\n    Replace the placeholders (e.g. , , {DATE}, {ip_address}) with fake values.\n\n    Instructions:\n\n    Use completely random numbers, so every digit is drawn between 0 and 9.\n    Use realistic names that come from diverse genders, ethnicities and countries.\n    If there are no placeholders, return the text as is and provide an answer.\n    If there is any additional PII/PHI in the sentence, replace it too.\n\n    input: How do I change the limit on my credit card {credit_card_number}?\n    output: How do I change the limit on my credit card 2539 3519 2345 1555?\n    input: \nHello, my name is &lt;PERSON&gt; and I live in &lt;LOCATION&gt;.\nMy credit card number is &lt;CREDIT_CARD&gt; and my crypto wallet id is &lt;CRYPTO&gt;.\n\nOn &lt;DATE_TIME&gt; I visited &lt;URL&gt; and sent an email to &lt;EMAIL_ADDRESS&gt;,  from the IP &lt;IP_ADDRESS&gt;.\n\nMy passport: &lt;US_PASSPORT&gt; and my phone number: &lt;PHONE_NUMBER&gt;.\n\nThis is a valid International Bank Account Number: &lt;IBAN_CODE&gt; . Can you please check the status on bank account &lt;US_BANK_NUMBER&gt;?\n\n&lt;PERSON&gt;'s social security number is &lt;US_SSN&gt;.  Her driver license? it is &lt;US_DRIVER_LICENSE&gt;.\n\n    output:\n\n</code>\n</pre> <pre><code>gpt_res = call_completion_model(create_prompt(anonymized_text))\n</code></pre> <pre><code>print(gpt_res)\n</code></pre> <pre>\n<code> Hello, my name is Kalyn Carranza and I live in Kenya.\nMy credit card number is 9871 8047 5709 9872 and my crypto wallet id is FV7POW12.\n\nOn 12th August 2021, 12:31pm I visited http://www.example.com and sent an email to vrystrkbnb@example.com,  from the IP 109.98.184.102.\n\nMy passport: 975-56-3481 and my phone number: 501-678-2198.\n\nThis is a valid International Bank Account Number: GB89 CPFX 4256 1763 8810 31 . Can you please check the status on bank account 855-31-6381?\n\nKalyn Carranza's social security number is 270-10-8543. Her driver license? it is 1183 2066 152 66.\n</code>\n</pre> <pre><code>import urllib\n\ntemplates = []\n\nurl = \"https://raw.githubusercontent.com/microsoft/presidio-research/master/presidio_evaluator/data_generator/raw_data/templates.txt\"\nfor line in urllib.request.urlopen(url):\n    templates.append(line.decode('utf-8')) \n</code></pre> <pre><code>print(\"Example templates:\")\ntemplates[:5]\n</code></pre> <pre>\n<code>Example templates:\n</code>\n</pre> <pre>\n<code>['I want to increase limit on my card # {{credit_card_number}} for certain duration of time. is it possible?\\n',\n 'My credit card {{credit_card_number}} has been lost, Can I request you to block it.\\n',\n 'Need to change billing date of my card {{credit_card_number}}\\n',\n 'I want to update my primary and secondary address to the same: {{address}}\\n',\n \"In case of my child's account, we need to add {{person}} as guardian\\n\"]</code>\n</pre> <pre><code>import time\npp = pprint.PrettyPrinter(indent=2, width=110)\nsentences = []\nfor template in templates:\n    synth_sentence = call_completion_model(create_prompt(template))\n    sentence_dict = {\"original\": template, \"synthetic\":synth_sentence.strip()}\n    sentences.append(sentence_dict)\n    pp.pprint(sentence_dict)\n    time.sleep(3) # wait to not get blocked by service (only applicable for the free tier)\n    print(\"--------------\")\n</code></pre> <pre>\n<code>{ 'original': 'I want to increase limit on my card # {{credit_card_number}} for certain duration of time. is '\n              'it possible?\\n',\n  'synthetic': 'I want to increase limit on my card # 2539 3519 2345 1555 for certain duration of time. Is '\n               'it possible?'}\n--------------\n{ 'original': 'My credit card {{credit_card_number}} has been lost, Can I request you to block it.\\n',\n  'synthetic': 'My credit card 2539 3519 2345 1555 has been lost, Can I request you to block it.'}\n--------------\n{ 'original': 'Need to change billing date of my card {{credit_card_number}}\\n',\n  'synthetic': 'Need to change billing date of my card 2539 3519 2345 1555?'}\n--------------\n{ 'original': 'I want to update my primary and secondary address to the same: {{address}}\\n',\n  'synthetic': 'I want to update my primary and secondary address to the same: 3241 Tulip Lane, London, NW9 '\n               '5RX.'}\n--------------\n{ 'original': \"In case of my child's account, we need to add {{person}} as guardian\\n\",\n  'synthetic': \"In case of my child's account, we need to add Abigail Jones as guardian.\"}\n--------------\n{ 'original': 'Are there any charges applied for money transfer from {{iban}} to other bank accounts\\n',\n  'synthetic': 'Are there any charges applied for money transfer from GB55TRRX92363841637179 to other bank '\n               'accounts?'}\n--------------\n{ 'original': 'Are there any charges applied to withdraw money from ATM with the card '\n              '{{credit_card_number}}\\n',\n  'synthetic': 'Are there any charges applied to withdraw money from ATM with the card 2539 3519 2345 1555?'}\n--------------\n{ 'original': 'Not getting bank documents to my address. Can you please validate the following? '\n              '{{address}}\\n',\n  'synthetic': 'Not getting bank documents to my address. Can you please validate the following? 943 Elm '\n               'Street, Fort Wayne, IN 46825.'}\n--------------\n{ 'original': 'Please update the billing address with {{address}} for this card: {{credit_card_number}}\\n',\n  'synthetic': 'Please update the billing address with 3989 Elm Street, San Francisco, CA 94103 for this '\n               'card: 2539 3519 2345 1555.'}\n--------------\n{ 'original': 'Need to see last 10 transaction of card {{credit_card_number}}\\n',\n  'synthetic': 'Need to see last 10 transactions of card 2539 3519 2345 1555'}\n--------------\n{ 'original': 'I have lost my card {{credit_card_number}}. Could you please block my credit card ASAP ? My '\n              'name is {{person}}.\\n',\n  'synthetic': 'I have lost my card 2539 3519 2345 1555. Could you please block my credit card ASAP ? My '\n               'name is John Smith.'}\n--------------\n{ 'original': \"My card {{credit_card_number}} is expiring this month. Please let me know process to it's \"\n              'extend validity.\\n',\n  'synthetic': \"My card 2539 3519 2345 1555 is expiring this month. Please let me know process to it's \"\n               'extend validity.'}\n--------------\n{ 'original': \"I have done an online order but didn't get any message on my registered {{phone_number}}. \"\n              'Could you please look into it ?\\n',\n  'synthetic': \"I have done an online order but didn't get any message on my registered (502) 183-3752. \"\n               'Could you please look into it?'}\n--------------\n{ 'original': 'What is procedure to redeem points won on credit card {{credit_card_number}} transactions ?\\n',\n  'synthetic': 'What is procedure to redeem points won on credit card 2539 3519 2345 1555 transactions?'}\n--------------\n{ 'original': 'My card {{credit_card_number}} expires soon \ufffd when will I get a new one?\\n',\n  'synthetic': 'My card 2539 3519 2345 1555 expires soon \ufffd when will I get a new one?'}\n--------------\n{ 'original': 'How do I check my balance on my credit card?\\n',\n  'synthetic': 'How do I check my balance on my credit card?'}\n--------------\n{ 'original': 'Could I change the payment due date of my credit card?\\n',\n  'synthetic': 'Could I change the payment due date of my credit card?'}\n--------------\n{ 'original': 'How can I request a new credit card pin ?\\n',\n  'synthetic': 'How can I request a new credit card pin?'}\n--------------\n{ 'original': 'Can I withdraw cash using my card {{credit_card_number}} at aTM center ?\\n',\n  'synthetic': 'Can I withdraw cash using my card 2539 3519 2345 1555 at ATM center?'}\n--------------\n{ 'original': 'How do I change the address linked to my credit card to {{address}}?\\n',\n  'synthetic': 'How do I change the address linked to my credit card to 1325 Valley View Rd, Yelm, WA 98597?'}\n--------------\n{ 'original': 'How do I open my credit card statement?\\n',\n  'synthetic': 'How do I open my credit card statement?'}\n--------------\n{'original': \"I'm originally from {{country}}\\n\", 'synthetic': \"I'm originally from Canada.\"}\n--------------\n{ 'original': 'I will be travelling to {{country}} next week, so I need my passport to be ready by then\\n',\n  'synthetic': 'I will be travelling to Mexico next week, so I need my passport to be ready by then.'}\n--------------\n{'original': \"Who's coming to {{country}} with me?\\n\", 'synthetic': \"Who's coming to Mexico with me?\"}\n--------------\n{'original': '{{country}} was super fun to visit!\\n', 'synthetic': 'France was super fun to visit!'}\n--------------\n{ 'original': 'Could you please email me the statement for last month , my credit card number is '\n              '{{credit_card_number}}?\\n',\n  'synthetic': 'Could you please email me the statement for last month, my credit card number is 2539 3519 '\n               '2345 1555?'}\n--------------\n{ 'original': 'Could you please send me the last billed amount for cc {{credit_card_number}} on my e-mail '\n              '{{email}}?\\n',\n  'synthetic': 'Could you please send me the last billed amount for cc 2539 3519 2345 1555 on my email '\n               'sue.perkins@example.com?'}\n--------------\n{ 'original': 'How do I change my address to {{address}} for post mail?\\n',\n  'synthetic': 'How do I change my address to 2514 Golden Street, San Diego, CA 90203 for post mail?'}\n--------------\n{ 'original': 'My name appears incorrectly on credit card statement could you please correct it to '\n              '{{prefix_male}} {{name_male}}?\\n',\n  'synthetic': 'My name appears incorrectly on credit card statement could you please correct it to Mr. '\n               'Gregor?'}\n--------------\n{ 'original': 'My name appears incorrectly on credit card statement could you please correct it to '\n              '{{prefix_female}} {{name_female}}?\\n',\n  'synthetic': 'My name appears incorrectly on credit card statement could you please correct it to Ms. '\n               'Emma?'}\n--------------\n{ 'original': 'card number {{credit_card_number}} is lost, can you please send a new one to {{address}}? I '\n              'am in {{city}} for a business trip\\n',\n  'synthetic': 'card number 2539 3519 2345 1555 is lost, can you please send a new one to 16 Meadow Lane, '\n               'Hicksville, NY 11801? I am in New York City for a business trip.'}\n--------------\n{ 'original': \"Please transfer all funds from my account to this hackers' {{email}}\\n\",\n  'synthetic': \"Please transfer all funds from my account to this hackers' mitchellejess@gmail.com\"}\n--------------\n{ 'original': \"I can't browse to your site, keep getting address {{ip_address}} blocked error\\n\",\n  'synthetic': \"I can't browse to your site, keep getting address 135.238.125.181 blocked error.\"}\n--------------\n{ 'original': 'My religion does not allow speaking to bots, they are evil and hacked by the Devil\\n',\n  'synthetic': 'My religion does not allow speaking to bots, they are evil and hacked by the Devil'}\n--------------\n{ 'original': \"Excuse me, Sir bot, but I really don't like this tone\\n\",\n  'synthetic': \"Excuse me, Sir bot, but I really don't like this tone\"}\n--------------\n{ 'original': \"WHAT ??? I DON'T KNOW WHAT TO PRESS NEXT!!! ? !! ?!\\n\",\n  'synthetic': \"WHAT ??? I DON'T KNOW WHAT TO PRESS NEXT!!! ? !! ?!\"}\n--------------\n{ 'original': \"Please have the manager call me at {{phone_number}} I'd like to join accounts with ms. \"\n              '{{first_name}}\\n',\n  'synthetic': \"Please have the manager call me at 443-214-5568 I'd like to join accounts with ms. Tanya.\"}\n--------------\n{ 'original': 'Inject SELECT * FROM Users WHERE client_ip = ?%//!%20\\\\|{{ip_address}}|%20/\\n',\n  'synthetic': 'Inject SELECT * FROM Users WHERE client_ip = 192.162.94.21'}\n--------------\n{ 'original': '{{first_name}}, can I please speak to your boss?\\n',\n  'synthetic': 'Abigail, can I please speak to your boss?'}\n--------------\n{ 'original': 'May I request to have the statement sent to {{address}}?\\n',\n  'synthetic': 'May I request to have the statement sent to 14 Maple Street, Chicago, IL 60647?'}\n--------------\n{ 'original': \"Will my account stay active? It's under my partner's name {{person}}\\n\",\n  'synthetic': \"Will my account stay active? It's under my partner's name John Smith.\"}\n--------------\n{'original': 'What are my options?\\n', 'synthetic': 'What are my options?'}\n--------------\n{ 'original': 'Bot: Where would you like this to be sent to? User: {{address}}\\n',\n  'synthetic': 'Bot: Where would you like this to be sent to? User: 369 Center Street, New York, NY 10018'}\n--------------\n{ 'original': \"Bot: What's the name on the account? User: {{person}}\\n\",\n  'synthetic': \"Bot: What's the name on the account? User: John Smith\"}\n--------------\n{ 'original': 'I would like to stop receiving messages to {{phone_number}}\\n',\n  'synthetic': 'I would like to stop receiving messages to 714-572-8764.'}\n--------------\n{'original': 'CAN I SPEAK TO A REAL PERSON?!?!\\n', 'synthetic': 'CAN I SPEAK TO A REAL PERSON?!?!'}\n--------------\n{ 'original': \"I'll meet you at {{address}} after the concert.\\n\",\n  'synthetic': \"I'll meet you at 4465 West Street after the concert.\"}\n--------------\n{ 'original': 'I would like to remove my kid {{first_name}} from the will. How do I do that?\\n',\n  'synthetic': 'I would like to remove my kid Sarah from the will. How do I do that?'}\n--------------\n{ 'original': 'The name in the account is not correct, please change it to {{person}}\\n',\n  'synthetic': 'The name in the account is not correct, please change it to Mary Smith.'}\n--------------\n{ 'original': 'Hello I moved, please update my new address is {{address}}\\n',\n  'synthetic': 'Hello I moved, please update my new address is 14 Benson Street, Dolton, Illinois, 60419.'}\n--------------\n{ 'original': 'I need to add my addresses, here they are: {{address}}, and {{address}}\\n',\n  'synthetic': 'I need to add my addresses, here they are: 4284 Taloncourt Road, Lake Travis, TX 76884, and '\n               '12 Needle Street, Lawrenceburg, IN 47025.'}\n--------------\n{ 'original': 'Please send my portfolio to this email {{email}}\\n',\n  'synthetic': 'Please send my portfolio to this email johnsmith@example.com'}\n--------------\n{ 'original': 'Hello, this is {{prefix_male}} {{name_male}}. Who are you?\\n',\n  'synthetic': 'Hello, this is Mr. Jackson. Who are you?'}\n--------------\n{ 'original': 'I want to add {{person}} as a beneficiary to my account\\n',\n  'synthetic': 'I want to add Mary Smith as a beneficiary to my account.'}\n--------------\n{ 'original': 'I want to cancel my card {{credit_card_number}} because I lost it\\n',\n  'synthetic': 'I want to cancel my card 2539 3519 2345 1555 because I lost it'}\n--------------\n{ 'original': 'Please block card no {{credit_card_number}}\\n',\n  'synthetic': 'Please block card no 2539 3519 2345 1555'}\n--------------\n{ 'original': 'What is the limit for card {{credit_card_number}}?\\n',\n  'synthetic': 'What is the limit for card 2539 3519 2345 1555?'}\n--------------\n{ 'original': 'Can someone call me on {{phone_number}}? I have some questions about opening an account.\\n',\n  'synthetic': 'Can someone call me on 639-243-7534? I have some questions about opening an account.'}\n--------------\n{'original': 'My name is {{first_name}}\\n', 'synthetic': 'My name is Olivia.'}\n--------------\n{ 'original': \"I'm moving out of the country, so please cancel my subscription\\n\",\n  'synthetic': \"I'm moving out of the country, so please cancel my subscription.\"}\n--------------\n{ 'original': 'My name is {{person}} but everyone calls me {{first_name}}\\n',\n  'synthetic': 'My name is Zara Smith but everyone calls me Zara.'}\n--------------\n{ 'original': \"Please tell me your date of birth. It's {{date_of_birth}}\\n\",\n  'synthetic': \"Please tell me your date of birth. It's May 18th 1996.\"}\n--------------\n{ 'original': 'You said your email is {{email}}. Is that correct?\\n',\n  'synthetic': 'You said your email is jane.doe99@example.com. Is that correct?'}\n--------------\n{ 'original': 'I once lived in {{address}}. I now live in {{address}}\\n',\n  'synthetic': 'I once lived in 189 Maple Street. I now live in 45 McLeod Avenue.'}\n--------------\n{ 'original': \"I'd like to call a taxi to {{address}}. Please call me when you're here.\\n\",\n  'synthetic': \"I'd like to call a taxi to 125 Washington Street. Please call me when you're here.\"}\n--------------\n{ 'original': 'Please charge my credit card. Number is {{credit_card_number}}\\n',\n  'synthetic': 'Please charge my credit card. Number is 9147 3623 4882 7568.'}\n--------------\n{'original': \"What's your email? {{email}}\\n\", 'synthetic': \"What's your email? janet.smith@example.com\"}\n--------------\n{ 'original': \"What's your credit card? {{credit_card_number}}\\n\",\n  'synthetic': \"What's your credit card? 6527 4235 1093 1522?\"}\n--------------\n{'original': \"What's your name? {{person}}\\n\", 'synthetic': \"What's your name? Ibrahim Karim\"}\n--------------\n{'original': \"What's your last name? {{last_name}}\\n\", 'synthetic': \"What's your last name? Lopez\"}\n--------------\n{ 'original': 'How can we reach you? You can call {{phone_number}}\\n',\n  'synthetic': 'How can we reach you? You can call 029 7783 8252.'}\n--------------\n{ 'original': \"I'd like it to be sent to {{address}}\\n\",\n  'synthetic': \"I'd like it to be sent to 1234 Smith Street, Chicago, IL 60616.\"}\n--------------\n{'original': 'Meet me at {{address}}\\n', 'synthetic': 'Meet me at 328 Anderson Street, Boston, MA 02109.'}\n--------------\n{ 'original': 'The restaurant is located at {{address}}. It serves great {{nationality}} food.\\n',\n  'synthetic': 'The restaurant is located at 1214 W. Main Ave. It serves great Italian food.'}\n--------------\n{ 'original': \"So where are we meeting? There's this nice new {{nationality}} place downtown. Cool, what's \"\n              \"the address? Oh do they serve vegan stuff? It's in {{address}}\\n\",\n  'synthetic': \"So where are we meeting? There's this nice new Mexican place downtown. Cool, what's the \"\n               \"address? Oh do they serve vegan stuff? It's in 9083 Holcomb Street.\"}\n--------------\n{ 'original': \"Hi {{first_name}}, I'm contacting you about a problem I have with sending a wire transfer \"\n              'using this IBAN {{iban}}\\n',\n  'synthetic': \"Hi Kim, I'm contacting you about a problem I have with sending a wire transfer using this \"\n               'IBAN DE56 8123 6888 0139 8956 61.'}\n--------------\n{ 'original': 'She was born on {{date_of_birth}}. Her maiden name is {{last_name}}\\n',\n  'synthetic': 'She was born on 02/15/1965. Her maiden name is Garcia.'}\n--------------\n{'original': 'Sometimes people call me {{first_name}}\\n', 'synthetic': 'Sometimes people call me Evan'}\n--------------\n{'original': \"Maybe it's under {{person}}\\n\", 'synthetic': \"Maybe it's under Malik Robinson\"}\n--------------\n{'original': \"It's like that since {{date_of_birth}}\\n\", 'synthetic': \"It's like that since 06/14/1988\"}\n--------------\n{ 'original': 'Just posted a photo {{url}}\\n',\n  'synthetic': 'Just posted a photo http://www.example.com/q3jx1zgca2.'}\n--------------\n{'original': 'My website is {{url}}\\n', 'synthetic': 'My website is www.synthetictestwebsite.com'}\n--------------\n{'original': 'My IBAN is {{iban}}\\n', 'synthetic': 'My IBAN is FR14 2004 1010 0505 0001 3M02 606.'}\n--------------\n{ 'original': \"I've shared files with you {{url}}\\n\",\n  'synthetic': \"I've shared files with you https://www.myfilesharing.com/agfsosid90\"}\n--------------\n{'original': 'I work for {{organization}}\\n', 'synthetic': 'I work for CorporationX.'}\n--------------\n{ 'original': '{{person}} from {{organization}} is the keynote speaker\\n',\n  'synthetic': 'John Smith from Harvard University is the keynote speaker.'}\n--------------\n{'original': '{{first_name}} is from {{organization}}\\n', 'synthetic': 'Maxwell is from Acme Corporation.'}\n--------------\n{ 'original': 'The address of {{organization}} is {{address}}\\n',\n  'synthetic': 'The address of Acme Corporation is 1020 Grand Avenue, Los Angeles, CA 90015.'}\n--------------\n{ 'original': 'His social security number is {{ssn}}\\n',\n  'synthetic': 'His social security number is 271-97-6469.'}\n--------------\n{'original': \"Here's my SSN: {{ssn}}\\n\", 'synthetic': \"Here's my SSN: 697-18-3566\"}\n--------------\n{ 'original': '{{first_name_nonbinary}} is a very sympathetic person. They are also good listeners.\\n',\n  'synthetic': 'Piper is a very sympathetic person. They are also good listeners.'}\n--------------\n{ 'original': '{{first_name}} is very reliable. You can always depend on him.\\n',\n  'synthetic': 'Fred is very reliable. You can always depend on him.'}\n--------------\n{'original': 'Why is {{first_name}} so impulsive?\\n', 'synthetic': 'Why is Daniella so impulsive?'}\n--------------\n{ 'original': '{{person}} will be talking in the conference\\n',\n  'synthetic': 'Sally Smith will be talking in the conference.'}\n--------------\n{'original': 'have you heard {{person}} speak yet?\\n', 'synthetic': 'Have you heard Keira Emerson speak yet?'}\n--------------\n{ 'original': 'Have you been to a {{person}} concert before?\\n',\n  'synthetic': 'Have you been to a John Smith concert before?'}\n--------------\n{ 'original': \"I'm so jealous! said {{first_name}} to {{first_name}}\\n\",\n  'synthetic': \"I'm so jealous! said Veronika to Trevor.\"}\n--------------\n{ 'original': 'The true gender of {{first_name}} has been under debate for years, but the riff and building '\n              'energy is a rock masterpiece regardless.\\n',\n  'synthetic': 'The true gender of Miguel has been under debate for years, but the riff and building energy '\n               'is a rock masterpiece regardless.'}\n--------------\n{ 'original': 'For my take on {{prefix_female}} {{last_name_female}}, see Guilty Pleasures: 5 Musicians Of '\n              \"The 70s You're Supposed To Hate (But Secretly Love)\\n\",\n  'synthetic': \"For my take on Mrs. Miller, see Guilty Pleasures: 5 Musicians Of The 70s You're Supposed To \"\n               'Hate (But Secretly Love)'}\n--------------\n{ 'original': \"Unlike the {{last_name}} novel, it's not about necrophilia. What it is about, I suppose is \"\n              \"anyone's guess. A brilliant piece of baroque pop.\\n\",\n  'synthetic': \"Unlike the Smith novel, it's not about necrophilia. What it is about, I suppose is anyone's \"\n               'guess. A brilliant piece of baroque pop.'}\n--------------\n{ 'original': \"One of the most depressing songs on the list. He's injured from the waist down from \"\n              \"{{country}}, but {{first_name}} just has to get laid. Don't go to town, {{first_name}}!\\n\",\n  'synthetic': \"One of the most depressing songs on the list. He's injured from the waist down from Uganda, \"\n               \"but Stacey just has to get laid. Don't go to town, Stacey!\"}\n--------------\n{ 'original': 'Is there a better crafted pop song on this list? {{last_name}} and {{last_name}} were '\n              'precision engineers.\\n',\n  'synthetic': 'Is there a better crafted pop song on this list? Smith and Jones were precision engineers.'}\n--------------\n{ 'original': 'C\\'mon, sing it with me: \"You picked a fine time to leave me {{first_name}}, four hungry '\n              'children and a crop in the field...\"\\n',\n  'synthetic': 'C\\'mon, sing it with me: \"You picked a fine time to leave me Lucas, four hungry children and '\n               'a crop in the field...\"'}\n--------------\n{ 'original': \"A tribute to {{person}} \u2013 sadly, she wasn't impressed.\\n\",\n  'synthetic': \"A tribute to Paula Harris \u2013 sadly, she wasn't impressed.\"}\n--------------\n{ 'original': \"When they weren't singing about Hobbits, satanic felines and interstellar journeys, they were \"\n              \"singing about the verses from {{person}}'s Cautionary Tales. Is there a better example of \"\n              'unbridled creativity than early {{last_name}}?\\n',\n  'synthetic': \"When they weren't singing about Hobbits, satanic felines and interstellar journeys, they \"\n               \"were singing about the verses from Sally Smith's Cautionary Tales. Is there a better example \"\n               \"of unbridled creativity than early Jones'?\"}\n--------------\n{ 'original': 'A great song made even greater by a mandolin coda (not by {{person}}).\\n',\n  'synthetic': 'A great song made even greater by a mandolin coda (not by Justin Bieber).'}\n--------------\n{ 'original': '{{name_male}} listed his top 20 songs for Entertainment Weekly and had the balls to list this '\n              'song at #15. (What did he put at #1 you ask? Answer:\"Tube Snake Boogie\" by {{person}} \u2013 go '\n              'figure)\\n',\n  'synthetic': 'John listed his top 20 songs for Entertainment Weekly and had the balls to list this song at '\n               '#15. (What did he put at #1 you ask? Answer:\"Tube Snake Boogie\" by ZZ Top \u2013 go figure)'}\n--------------\n{ 'original': \"From the film {{nationality}} graffiti (also features {{person}}. What's not to love?\\n\",\n  'synthetic': \"From the film Canadian graffiti (also features Zakari Rigori. What's not to love?\"}\n--------------\n{ 'original': 'You can tell {{first_name}} was a huge {{person}} fan. Written when he was {{age}}.\\n',\n  'synthetic': 'You can tell Isabella was a huge basketball fan. Written when she was 19.'}\n--------------\n{ 'original': \"This song by ex-Zombie {{last_name}} is a perfect example of why you shouldn't concentrate on \"\n              'the order of this list. An argument could be made that this should be at number one, and I '\n              \"wouldn't argue with it.\\n\",\n  'synthetic': \"This song by ex-Zombie Williams is a perfect example of why you shouldn't concentrate on the \"\n               'order of this list. An argument could be made that this should be at number one, and I '\n               \"wouldn't argue with it.\"}\n--------------\n{ 'original': 'The title refers to {{street_name}} street in {{city}}. It was on this street that many of '\n              'the clubs where Metallica first played were situated. \"Battery is found in me\" shows that '\n              'these early shows on {{street_name}} Street were important to them. Battery is where \"lunacy '\n              'finds you\" and you \"smash through the boundaries.\"\\n',\n  'synthetic': 'The title refers to Washington Street in Chicago. It was on this street that many of the '\n               'clubs where Metallica first played were situated. \"Battery is found in me\" shows that these '\n               'early shows on Washington Street were important to them. Battery is where \"lunacy finds you\" '\n               'and you \"smash through the boundaries.\"'}\n--------------\n{ 'original': 'Blink-182 pay tribute here to the {{country}}. Producer {{person}} explained to Fuse TV: \"We '\n              \"all liked the idea of writing a song about our state, where we live and love. To me it's the \"\n              'most beautiful place in the world, this song was us giving credit to how lucky we are to have '\n              'lived here and grown up here, raising families here, the whole thing.\"\\n',\n  'synthetic': 'Blink-182 pay tribute here to the United States. Producer Kim Johnson explained to Fuse TV: '\n               '\"We all liked the idea of writing a song about our state, where we live and love. To me '\n               \"it's the most beautiful place in the world, this song was us giving credit to how lucky we \"\n               'are to have lived here and grown up here, raising families here, the whole thing.\"'}\n--------------\n{ 'original': 'It may be too that {{last_name}} was influenced by an earlier song, \"Carry Me Back To '\n              '{{country}},\" which was arranged and sung by {{person}} in {{year}} (though {{last_name}}\\'s '\n              'song was actually about a boat!).\\n',\n  'synthetic': 'It may be too that Jenkins was influenced by an earlier song, \"Carry Me Back To Trinidad,\" '\n               \"which was arranged and sung by Mark Twain in 1901 (though Jenkins's song was actually about \"\n               'a boat!).'}\n--------------\n{ 'original': 'The {{person}} version recorded for {{organization}} became the first celebrity recording by '\n              'a classical musician to sell one million copies. The song was awarded the seventh gold disc '\n              'ever granted.\\n',\n  'synthetic': 'The Helen Smith version recorded for Universal Music Group became the first celebrity '\n               'recording by a classical musician to sell one million copies. The song was awarded the '\n               'seventh gold disc ever granted.'}\n--------------\n{ 'original': 'In {{country}} they have company songs, musical expressions of employee loyalty sung by '\n              'salarymen. Unfortunately, as regular RR commenter {{person}} points out, \"most are '\n              'horrible\".\\n',\n  'synthetic': 'In Japan they have company songs, musical expressions of employee loyalty sung by salarymen. '\n               'Unfortunately, as regular RR commenter Hannah Jackson points out, \"most are horrible\".'}\n--------------\n{ 'original': '\"The big three\" of The Big Three Killed My Baby are the car manufacturers that dominate the '\n              \"economy of the White Stripes' home city {{city}}: {{organization}}, {{organization}} and \"\n              '{{organization}}. \"Don\\'t feed me planned obsolescence,\" says {{person}} in an '\n              'uncharacteristically political song, lamenting the demise of the unions in the 60s.\\n',\n  'synthetic': '\"The big three\" of The Big Three Killed My Baby are the car manufacturers that dominate the '\n               'economy of the White Stripes\\' home city Detroit: Ford, General Motors and Chrysler. \"Don\\'t '\n               'feed me planned obsolescence,\" says Thomas Johnson in an uncharacteristically political '\n               'song, lamenting the demise of the unions in the 60s.'}\n--------------\n{ 'original': '{{organization}} songwriter {{name_female}} employs corporate lingo in the first verse of her '\n              '{{organization}} resignation Letter\\n',\n  'synthetic': 'Acme Corporation songwriter Hannah Smith employs corporate lingo in the first verse of her '\n               'Acme Corporation resignation Letter.'}\n--------------\n{ 'original': 'Mission Statement: This non-profit founded by radio executives \"serves as an advocate for the '\n              'value of music\" and \"supports its songwriters, composers and publishers by taking care of an '\n              'important aspect of their careers \u2013 getting paid,\" according to the {{organization}} website. '\n              'They offer blanket music licenses to businesses and organizations that allow them to play '\n              'nearly 13 million musical works.\\n',\n  'synthetic': 'Mission Statement: This non-profit founded by radio executives \"serves as an advocate for '\n               'the value of music\" and \"supports its songwriters, composers and publishers by taking care '\n               'of an important aspect of their careers \u2013 getting paid,\" according to the Music Alliance '\n               'website. They offer blanket music licenses to businesses and organizations that allow them '\n               'to play nearly 13 million musical works.'}\n--------------\n{ 'original': 'The {{organization}} Orchestra was founded in {{year}}. Since then, it has grown from a '\n              'volunteer community orchestra to a fully professional orchestra serving Southern '\n              '{{country}}\\n',\n  'synthetic': 'The Chaz Symphony Orchestra was founded in 2015. Since then, it has grown from a volunteer '\n               'community orchestra to a fully professional orchestra serving Southern France.'}\n--------------\n{ 'original': 'Celebrating its 10th year in {{city}}, {{organization}} is a 501(c)3 that invites songwriters '\n              'from around the world to {{city}} to share the universal language of music in collaborations '\n              'designed to bridge cultures, build friendships and cultivate peace.\\n',\n  'synthetic': 'Celebrating its 10th year in Honolulu, Artslana is a 501(c)3 that invites songwriters from '\n               'around the world to Honolulu to share the universal language of music in collaborations '\n               'designed to bridge cultures, build friendships and cultivate peace.'}\n--------------\n{ 'original': '{{organization}} is the brainchild of our 3 founders: {{last_name}}, {{last_name}} and '\n              '{{last_name}}.  The idea was born (on the beach) while they were constructing a website to be '\n              'the basis of another start-up idea.\\n',\n  'synthetic': 'Acme Co. is the brainchild of our 3 founders: Smith, Nguyen and Takahashi. The idea was born '\n               '(on the beach) while they were constructing a website to be the basis of another start-up '\n               'idea.'}\n--------------\n{ 'original': '{{organization}} is an {{nationality}} multinational investment bank and financial services '\n              'company\\n',\n  'synthetic': 'Goldman Sachs is an American multinational investment bank and financial services company.'}\n--------------\n{ 'original': 'Zoolander is a {{year}} {{nationality}} action-comedy film directed by {{person}} and '\n              'starring {{last_name}}\\n',\n  'synthetic': 'Zoolander is a 2001 American action-comedy film directed by Ben Stiller and starring '\n               'Stiller.'}\n--------------\n{ 'original': 'During {{year}}, {{organization}} invested heavily in new microprocessor designs fostering '\n              'the rapid growth of the computer industry.\\n',\n  'synthetic': 'During 2019, Acme Technologies invested heavily in new microprocessor designs fostering the '\n               'rapid growth of the computer industry.'}\n--------------\n{ 'original': 'On {{date_time}}, the {{nationality}} government formally began the process of withdrawal by '\n              'invoking Article 50 of the Treaty on European Union\\n',\n  'synthetic': 'On June 23rd, 2020, the British government formally began the process of withdrawal by '\n               'invoking Article 50 of the Treaty on European Union.'}\n--------------\n{ 'original': '{{first_name}} shouted at {{first_name}}: \"What are you doing here?\"\\n',\n  'synthetic': 'Shadeed shouted at Latonya: \"What are you doing here?\"'}\n--------------\n{ 'original': '{{first_name}} spent a year at {{organization}} as the assistant to {{person}}, and the '\n              'following year at {{organization}} in {{city}}, which later became {{organization}} in '\n              '{{year}}.\\n',\n  'synthetic': 'Nina spent a year at Kingsley Academy as the assistant to Dean Roberts, and the following '\n               'year at Brighty Design in San Francisco, which later became Thompkins Designs in 2005.'}\n--------------\n{ 'original': '{{last_name}} began writing as a teenager, publishing her first story, \"The Dimensions of a '\n              'Shadow\", in {{year}} while studying English and journalism at the University of {{city}}.\\n',\n  'synthetic': 'Smith began writing as a teenager, publishing her first story, \"The Dimensions of a Shadow\", '\n               'in 1994 while studying English and journalism at the University of Chicago.'}\n--------------\n{ 'original': \"My driver's license number is {{us_driver_license}}\\n\",\n  'synthetic': \"My driver's license number is VFJ600DN5051EA1N\"}\n--------------\n{ 'original': '{{person}}\\\\n\\\\n{{building_number}} {{street_name}}\\\\n {{secondary_address}}\\\\n {{city}}\\\\n '\n              '{{country}} {{postcode}}\\n',\n  'synthetic': 'John Smith \\n123 Main Street \\nApartment 6 \\nNew York City \\nUSA 12254'}\n--------------\n{ 'original': '{{person}}\\\\n\\\\n{{building_number}} {{street_name}}\\\\n {{secondary_address}}\\\\n '\n              '{{city}}\\\\n\\\\n {{country}} {{postcode}}\\n',\n  'synthetic': 'John Smith\\n    843 Easton Street\\n    Suite 3\\n    Hamden\\n   \\nUnited States 12345'}\n--------------\n{ 'original': '{{prefix_female}} {{name_female}} {{secondary_address}} {{building_number}} '\n              '{{street_name}}\\\\n{{city}} {{state_abbr}} {{zipcode}}\\n',\n  'synthetic': 'Ms. Allison Grobman Apt 12 975 Hamilton Street\\\\\\\\New York NY 11111'}\n--------------\n{ 'original': '{{building_number}} {{street_name}}\\\\n {{secondary_address}}\\\\n {{city}}\\\\n {{country}} '\n              '{{postcode}}\\n',\n  'synthetic': '456 Maple Street\\\\n Apt 5D\\\\n San Francisco \\\\n USA 94110'}\n--------------\n{ 'original': 'The corner of {street_name} and {street_name}\\n',\n  'synthetic': 'The corner of Johnson Street and Monroe Avenue'}\n--------------\n{ 'original': 'The restaurant is at {{building_number}} {{street_name}}\\n',\n  'synthetic': 'The restaurant is at 1243 Willow Street.'}\n--------------\n{'original': 'My friend lives in {{city}}\\n', 'synthetic': 'My friend lives in Cairo.'}\n--------------\n{ 'original': '{{first_name}} lives on {{street_name}} street.\\n',\n  'synthetic': 'Maria lives on Liberty Street.'}\n--------------\n{ 'original': '{{name}} lives at {{building_number}} {{street_name}}, {{city}}\\n',\n  'synthetic': 'John Smith lives at 635 Poplar Street, Houston'}\n--------------\n{ 'original': '{{first_name_male}} had given {{first_name}} his address: {{building_number}} '\n              '{{street_name}}\\n',\n  'synthetic': 'Adam had given Sarah his address: 44 Apple Street'}\n--------------\n{ 'original': '{{first_name_male}} had given {{first_name}} his address: {{building_number}} '\n              '{{street_name}}, {{city}}\\n',\n  'synthetic': 'David had given Emma his address: 515 Elm Street, Camden.'}\n--------------\n{ 'original': 'What is your address? it is {{address}}\\n',\n  'synthetic': 'What is your address? it is 3498 Allensby Street, Los Angeles, CA 90011.'}\n--------------\n{'original': 'We moved here from {{city}}\\n', 'synthetic': 'We moved here from Paris.'}\n--------------\n{'original': 'We moved here from {{country}}\\n', 'synthetic': 'We moved here from Venezuela.'}\n--------------\n{ 'original': '{{person}}\\\\n\\\\n{{building_number}} {{street_name}}\\\\n {{secondary_address}}\\\\n {{city}}\\\\n '\n              '{{country}} {{postcode}}\\\\n{{phone_number}}-Office\\\\,{{phone_number}}-Fax\\n',\n  'synthetic': 'Jessica Thompson\\n'\n               '    8745 West Drive\\n'\n               '    Suite 1402\\n'\n               '    Brooklyn, NY USA 12009\\n'\n               '    789-534-9921-Office, 567-945-0023-Fax'}\n--------------\n{ 'original': '{{person}}\\\\n{{job}}\\\\n{{organization}}\\\\n{{address}}\\n',\n  'synthetic': 'John Smith\\\\nAccountant\\\\nGlobalTech Solutions\\\\n25 Speedwell Street, Richmond, VA 23223'}\n--------------\n{ 'original': 'Our offices are located at {{address}}\\n',\n  'synthetic': 'Our offices are located at 1234 Main St, Los Angeles, CA 91234.'}\n--------------\n{ 'original': 'Please return to {{address}} in case of an issue.\\n',\n  'synthetic': 'Please return to 123 Cherry Street, El Monte, CA 91731 in case of an issue.'}\n--------------\n{ 'original': '{{organization}}\\\\n\\\\n{{address}}\\n',\n  'synthetic': 'ABC Inc.\\n     1234 Main Street, Anytown, ST 12345'}\n--------------\n{ 'original': 'The {{organization}} office is at {{address}}\\n',\n  'synthetic': 'The ABC Corporation office is at 123 Redwood Street, Anytown, USA.'}\n--------------\n{ 'original': '{{name}}\\\\n{{organization}}\\\\n{{address}}\\\\n{{phone_number}} office\\\\n{{phone_number}} '\n              'fax\\\\n{{phone_number}} mobile\\\\n\\n',\n  'synthetic': 'Larry Fernandez\\\\nStar Enterprises\\\\n421 5th Avenue, Los Angeles, CA 90012\\\\n123-456-7890 '\n               'office\\\\n456-789-0123 fax\\\\n256-454-2397 mobile\\\\n'}\n--------------\n{ 'original': '{{name}}\\\\n{{organization}}\\\\n{{address}}\\\\nMobile: {{phone_number}}\\\\nDesk: '\n              '{{phone_number}}\\\\nFax: {{phone_number}}\\\\n\\n',\n  'synthetic': 'John Brown\\n'\n               '    ABC Consulting\\n'\n               '    5th St., Suite 116, LA CA 90004\\n'\n               '    Mobile: 213-294-4497\\n'\n               '    Desk: 424-348-1275\\n'\n               '    Fax: 323-456-2545'}\n--------------\n{ 'original': 'Billing address: {{name}}\\\\n    {{building_number}} {{street_name}} '\n              '{{secondary_address}}\\\\n   {{city}}\\\\n    {{state_abbr}}\\\\n    {{zipcode}}\\\\n\\n',\n  'synthetic': 'Billing address: Mariam Rajput\\n'\n               '    576 Broadway Street Apartment A8\\n'\n               '   Sacramento\\n'\n               '    CA\\n'\n               '    95349'}\n--------------\n{ 'original': \"As promised, here's {{first_name}}'s address:\\\\n\\\\n{{address}}\\n\",\n  'synthetic': \"As promised, here's Aamir's address:\\\\n\\\\n2166 Sesame Street, Fortaleza, Cear\u00e1, Brazil.\"}\n--------------\n{ 'original': '&gt;{{name}}\\\\n&gt;{{organization}}\\\\n&gt;{{person}}\\\\n&gt;{{building_number}} '\n              '{{street_name}}\\\\n&gt;{{secondary_address}}\\\\n&gt;{{city}}\\\\n&gt;{{country}} {{postcode}}\\n',\n  'synthetic': '&gt;Freda Chen\\n'\n               '&gt;Example Inc.\\n'\n               '&gt;John Doe\\n'\n               '&gt;50 Main Street\\n'\n               '&gt;Apt. 2\\n'\n               '&gt;New York\\n'\n               '&gt;United States 10005'}\n--------------\n{ 'original': '??? {{name}}\\\\n??? {{organization}}\\\\n??? {{building_number}} {{street_name}}\\\\n??? '\n              '{{secondary_address}}\\\\n??? {{city}}\\\\n??? {{country}} {{postcode}}\\n',\n  'synthetic': 'John Smith\\n'\n               '     ABC Corporation\\n'\n               '     192 Main Street\\n'\n               '     Suite 100\\n'\n               '     Austin\\n'\n               '     United States 78701'}\n--------------\n{ 'original': '&gt; \\\\n&gt; {{name}}\\\\n&gt; {{organization}}\\\\n&gt; {{person}}\\\\n&gt; {{building_number}} '\n              '{{street_name}}\\\\n&gt; {{secondary_address}}\\\\n&gt; {{city}}\\\\n&gt; {{country}} {{postcode}}\\n',\n  'synthetic': '&gt; John Doe\\n'\n               '    &gt; ABC Corp\\n'\n               '    &gt; Jane Smith\\n'\n               '    &gt; 123 Main Street\\n'\n               '    &gt; APT 8\\n'\n               '    &gt; San Francisco\\n'\n               '    &gt; United States 12345'}\n--------------\n{ 'original': 'Pedestrians must enter on {{street_name}} St. the first three months\\n',\n  'synthetic': 'Pedestrians must enter on Jericho Avenue St. the first three months'}\n--------------\n{ 'original': 'When: {{date_time}}\\\\nWhere: {{city}} Country Club.\\n',\n  'synthetic': 'When: 05/01/2020 10:00am\\\\nWhere: Richmond Country Club.'}\n--------------\n{ 'original': \"We'll meet {{day_of_week}} at {{organization}}, {{building_number}} {{street_name}}, \"\n              '{{city}}\\n',\n  'synthetic': \"We'll meet Monday at Smartdel Solutions, 145 King Street, San Diego.\"}\n--------------\n{ 'original': 'They had 6: {{first_name}}, {{first_name}}, {{first_name}}, {{first_name}}, {{first_name}} '\n              'and {{first_name}}.\\n',\n  'synthetic': 'They had 6: Sarah, Micheal, Kanak, Hana, Mei and Dan.'}\n--------------\n{'original': 'She moved here from {{country}}\\n', 'synthetic': 'She moved here from Mexico.'}\n--------------\n{'original': 'My zip code is {{zipcode}}\\n', 'synthetic': 'My zip code is 47713.'}\n--------------\n{'original': 'ZIP: {{zipcode}}\\n', 'synthetic': 'ZIP: 08547'}\n--------------\n{'original': 'The bus station is on {{street_name}}\\n', 'synthetic': 'The bus station is on Wilson Avenue.'}\n--------------\n{ 'original': \"They're not answering at {{phone_number}}\\n\",\n  'synthetic': \"They're not answering at 654-339-1013.\"}\n--------------\n{ 'original': 'God gave rock and roll to you, gave rock and roll to you, put it in the soul of everyone.\\n',\n  'synthetic': 'God gave rock and roll to you, gave rock and roll to you, put it in the soul of everyone.'}\n--------------\n{'original': '3... 2... 1... liftoff!\\n', 'synthetic': '3... 2... 1... liftoff!'}\n--------------\n{ 'original': 'My great great grandfather was called {{name_male}}, and my great great grandmother was '\n              'called {{name_female}}\\n',\n  'synthetic': 'My great great grandfather was called Michael, and my great great grandmother was called '\n               'Emma.'}\n--------------\n{'original': 'She named him {{first_name_male}}\\n', 'synthetic': 'She named him Juan.'}\n--------------\n{ 'original': 'Name:    {{name}}\\\\nAddress:     {{address}}\\n',\n  'synthetic': 'Name:    Amari Walters\\\\nAddress:     32 Webster Street, Salem, MA 01819'}\n--------------\n{ 'original': 'Follow up with {{name}} in a couple of months.\\n',\n  'synthetic': 'Follow up with Beatriz Lawrence in a couple of months.'}\n--------------\n{ 'original': '{{prefix_male}} {{last_name_male}} is a {{age}} year old man who grew up in {{city}}.\\n',\n  'synthetic': 'Mr.Williams is a 28 year old man who grew up in Dallas.'}\n--------------\n{ 'original': 'Date: {{date_time}}\\\\nName: {{name}}\\\\nPhone: {{phone_number}}\\n',\n  'synthetic': 'Date: 01/03/2021 13:45 \\\\nName: Pratima Joshi \\\\nPhone: 467-562-8954'}\n--------------\n{ 'original': '{{first_name}}: \"Who are you?\"\\\\n{{first_name_female}}:\"I\\'m {{first_name}}\\'s daughter\".\\n',\n  'synthetic': 'Bob: \"Who are you?\"\\\\nMaria:\"I\\'m Bob\\'s daughter\".'}\n--------------\n{ 'original': 'At my suggestion, one morning over breakfast, she agreed, and on the last Sunday before Labor '\n              'Day we returned to {{city}} by helicopter.\\n',\n  'synthetic': 'At my suggestion, one morning over breakfast, she agreed, and on the last Sunday before '\n               'Labor Day we returned to Paris by helicopter.'}\n--------------\n{ 'original': \"It was a done thing between him and {{first_name}}'s kid; and everybody thought so.\\n\",\n  'synthetic': \"It was a done thing between him and Jeffery's kid; and everybody thought so.\"}\n--------------\n{ 'original': 'Capitalized words like Wisdom and Discipline are often mistaken with names.\\n',\n  'synthetic': 'Capitalized words like Wisdom and Discipline are often mistaken with names.'}\n--------------\n{ 'original': 'The letter arrived at {{address}} last night.\\n',\n  'synthetic': 'The letter arrived at 1143 Orange Street last night.'}\n--------------\n{ 'original': 'The Princess Royal arrived at {{city}} this morning from {{country}}.\\n',\n  'synthetic': 'The Princess Royal arrived at London this morning from France.'}\n--------------\n{'original': \"I'm in {{city}}, at the conference\\n\", 'synthetic': \"I'm in Toronto, at the conference.\"}\n--------------\n{ 'original': '{{name}}, the {{job}}, said: \"I\\'m glad to hear that this has been withdrawn \u2013 quite why they '\n              'thought this would go down well is beyond me.\"\\n',\n  'synthetic': 'Gloria Green, the Nurse Practitioner, said: \"I\\'m glad to hear that this has been withdrawn '\n               '\u2013 quite why they thought this would go down well is beyond me.\"'}\n--------------\n{ 'original': '\"I\\'m glad to hear that {{country}} is moving in that direction,\" says {{last_name}}.\\n',\n  'synthetic': '\"I\\'m glad to hear that Canada is moving in that direction,\" says Smith.'}\n--------------\n{ 'original': 'I am {{nation_woman}} but I live in {{country}}.\\n',\n  'synthetic': 'I am Marianna Montenegro but I live in Ukraine.'}\n--------------\n{'original': 'We are proud {{nation_plural}}\\n', 'synthetic': 'We are proud Americans.'}\n--------------\n{ 'original': \"{{person}}'s killers sentenced to life in prison\\n\",\n  'synthetic': \"John Smith's killers sentenced to life in prison\"}\n--------------\n{ 'original': \"{{country}} leader gives 'kill without warning' order\\n\",\n  'synthetic': \"Brazilian leader gives 'kill without warning' order\"}\n--------------\n{ 'original': 'The {{nationality}} Border Force have detained top-flight tennis player {{name_female}} over '\n              'visa disputes.\\n',\n  'synthetic': 'The British Border Force have detained top-flight tennis player Maria Rodriguez over visa '\n               'disputes.'}\n--------------\n{ 'original': 'You will be responsible for the husbandry and care of a large variety of species including '\n              'lemurs, antelope, camels, and more\\n',\n  'synthetic': 'You will be responsible for the husbandry and care of a large variety of species including '\n               'lemurs, antelope, camels, and more.'}\n--------------\n{ 'original': '{{name}}\\\\n\\\\n{{job}}\\\\n\\\\nPersonal '\n              'Info:\\\\nPhone:\\\\n{{phone_number}}\\\\n\\\\nE-mail:\\\\n{{email}}\\\\n\\\\nWebsite:\\\\n{{url}}\\\\n\\\\nAddress:\\\\n{{address}}.\\n',\n  'synthetic': 'Robert James\\\\n\\\\nSoftware Engineer\\\\n\\\\nPersonal '\n               'Info:\\\\nPhone:\\\\n555-847-8915\\\\n\\\\nE-mail:\\\\nrobertjames@example.com\\\\n\\\\nWebsite:\\\\nwww.example.com\\\\n\\\\nAddress:\\\\n277 '\n               'Park Ave North, Denver, CO 80100.'}\n--------------\n{ 'original': '{{name}}\\\\n\\\\n{{city}}\\\\n{{country}}\\n',\n  'synthetic': 'John Smith\\n     Los Angeles\\n     United States'}\n--------------\n{ 'original': 'Title VII of the Civil Rights Act of {{year}} protects individuals against employment '\n              'discrimination on the basis of race and color as well as national origin, sex, or religion.\\n',\n  'synthetic': 'Title VII of the Civil Rights Act of 1964 protects individuals against employment '\n               'discrimination on the basis of race and color as well as national origin, sex, or religion.'}\n--------------\n{ 'original': 'Energetic and driven salesperson with 8+ years of professional experience in inbound and '\n              'outbound sales. Awarded Salesperson of the Month three times. Helped increase inbound sales '\n              'by 16% within the first year of employment. Looking to support {{organization}} in {{city}} '\n              '{{zipcode}} in its mission to become a market-leading solution.\\n',\n  'synthetic': 'Energetic and driven salesperson with 8+ years of professional experience in inbound and '\n               'outbound sales. Awarded Salesperson of the Month three times. Helped increase inbound sales '\n               'by 16% within the first year of employment. Looking to support Acme Corporation in Los '\n               'Angeles 90018 in its mission to become a market-leading solution.'}\n--------------\n{ 'original': 'The bus drops you off at {{building_number}} {{street_name}} St.\\n',\n  'synthetic': 'The bus drops you off at 2774 Chestnut St.'}\n--------------\n{ 'original': 'Ask the driver to stop at the corner of {{street_name}} St. and {{street_name}} St.\\n',\n  'synthetic': 'Ask the driver to stop at the corner of Maple St. and Sycamore St.'}\n--------------\n{ 'original': 'He lives on the north side of {{street_name}}.\\n',\n  'synthetic': 'He lives on the north side of Abbey Road.'}\n--------------\n{ 'original': \"I used to work for {{organization}} as {{job}}, but quit a few months ago. Now I'm \"\n              'unemployed.\\n',\n  'synthetic': \"I used to work for ABC Corporation as Software Engineer, but quit a few months ago. Now I'm \"\n               'unemployed.'}\n--------------\n{'original': '{{city}} bridge is falling down.\\n', 'synthetic': 'Berlin bridge is falling down.'}\n--------------\n{ 'original': '{{name}} of {{organization}} is the CEO of the year. ABC Business considered several other '\n              \"influential CEOs for this year's honor, including {{name}} of {{organization}}, \"\n              \"{{organization}}'s {{name}}, {{name}} of {{organization}}'s, {{name}} of {{organization}}, \"\n              \"and {{organization}}'s {{name}}.\\n\",\n  'synthetic': 'Franklin Smith of Technology Solutions International is the CEO of the year. ABC Business '\n               \"considered several other influential CEOs for this year's honor, including Madison Chang of \"\n               \"Radiance Digital, Radiance Digital's Ashleigh Jones, Cash Huang of Clark &amp; Partner's, Sierra \"\n               \"Urbina of Intelicity, and Clark &amp; Partners's Asher Kenney.\"}\n--------------\n{ 'original': '{{organization}} is a design agency based in {{city}}.\\n',\n  'synthetic': 'Maestro Design Inc. is a design agency based in Amsterdam.'}\n--------------\n{ 'original': 'Action &amp; Adventure, Animation, Comedy, Kids &amp; Family, Mystery &amp; Suspense\\\\nDirected By:    '\n              '{{name}}\\n',\n  'synthetic': 'Action &amp; Adventure, Animation, Comedy, Kids &amp; Family, Mystery &amp; Suspense\\n'\n               'Directed By: Esther Jones'}\n--------------\n{ 'original': '{{first_name}}: What a wife.\\\\n{{first_name}}: Remember me, {{first_name}}? When I killed '\n              'your brother, I talked just like this!\\\\n{{first_name}}: You saved my life! How can I ever '\n              'repay you?\\n',\n  'synthetic': 'Emma: What a wife.\\n'\n               '    Emma: Remember me, Emma? When I killed your brother, I talked just like this!\\n'\n               '    Emma: You saved my life! How can I ever repay you?'}\n--------------\n{'original': 'He just turned {{age}} years old\\n', 'synthetic': 'He just turned 7 years old'}\n--------------\n{ 'original': \"I'm {{name}}, originally from {{city}}, and i'm {{age}} y/o.\\n\",\n  'synthetic': \"I'm Emily Evanston, originally from London, and I'm 24 y/o.\"}\n--------------\n{ 'original': 'Patient is a {{age}}-year-old male with a history of headaches\\n',\n  'synthetic': 'Patient is a 35-year-old male with a history of headaches'}\n--------------\n{'original': 'I just turned {{age}}\\n', 'synthetic': 'I just turned 24.'}\n--------------\n{'original': 'My father retired at the age of {{age}}\\n', 'synthetic': 'My father retired at the age of 60.'}\n--------------\n{ 'original': 'This {{age}} year old female complaining of stomach pain.\\n',\n  'synthetic': 'This 28 year old female complaining of stomach pain.'}\n--------------\n{ 'original': \"My birthday is on the weekend. I'll turn {{age}}.\\n\",\n  'synthetic': \"My birthday is on the weekend. I'll turn 20.\"}\n--------------\n{'original': 'My brother just turned {{age}}\\n', 'synthetic': 'My brother just turned 18.'}\n--------------\n{ 'original': '{{prefix}} {{last_name}} flew to {{city}} on {{day_of_week}} morning.',\n  'synthetic': 'Dr. Nguyen flew to Los Angeles on Tuesday morning.'}\n--------------\n</code>\n</pre> <p>This notebook demonstrates how to leverage OpenAI models for fake/surrogate data generation. It uses Presidio to first de-identify data (as de-identification might be required prior to passing the model to OpenAI), and then uses OpenAI completion models to create synthetic/fake/surrogate data based on real data. OpenAI models would also potentially remove additional PII entities, if those are not detected by Presidio.</p> <p>Some impressions: 1. GPT-3 sometimes gives additonal output, especially if the text is a question or concerning a human/bot interaction. Engineering the prompt can mitigate some of these issues. Potential post-processing might be required. 2. GPT-3 sometimes creates fake values even in the absence of placeholders. 3. GPT-3 re-uses context from other sentences, which could cause phone numbers are sometimes generated using a credit card pattern or other similar mistakes. 4. Co-references are sometimes missed (i.e. two name placeholders that should be filled with the same name, or referencing he/she to a male/female name)</p>"},{"location":"samples/python/GPT3_synth_data/#use-presidio-gpt-3-to-turn-real-text-into-fake-text","title":"Use Presidio + GPT-3 to turn real text into fake text","text":"<p>This notebook uses Presidio to turn text with PII into text where PII entities are replaced with placeholders, e.g. \"<code>My name is David</code>\" turns into \"<code>My name is {{PERSON}}</code>\". Then, it calls the OpenAI GPT-3 API to create a fake record which is based on the original one.</p> <p>Flow: 1. <code>My friend David lives in Paris. He likes it.</code> 1. <code>My friend {{PERSON}} lives in {{CITY}}. He likes it.</code> 1. <code>My friend Lucy lives in Beirut. She likes it.</code></p> <p>Note that OpenAI completion models could possibly detect PII values and replace them in one call, but it is suggested to validate that all PII entities are indeed detected.</p>"},{"location":"samples/python/GPT3_synth_data/#imports-and-set-up-openai-key","title":"Imports and set up OpenAI Key","text":""},{"location":"samples/python/GPT3_synth_data/#define-request-for-the-openai-completion-service","title":"Define request for the OpenAI Completion service","text":""},{"location":"samples/python/GPT3_synth_data/#de-identify-data-using-presidio-analyzer-and-anonymizer","title":"De-identify data using Presidio Analyzer and Anonymizer","text":""},{"location":"samples/python/GPT3_synth_data/#create-prompt-instructions-text-to-manipulate","title":"Create prompt (instructions + text to manipulate)","text":""},{"location":"samples/python/GPT3_synth_data/#call-gpt-3","title":"Call GPT-3","text":""},{"location":"samples/python/GPT3_synth_data/#alternatively-run-on-a-list-of-template-sentences","title":"Alternatively, run on a list of template sentences:","text":""},{"location":"samples/python/batch_processing/","title":"Batch processing","text":"<pre><code># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</code></pre> <p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/batch_processing.ipynb</p> <pre><code>from typing import List, Optional, Dict, Union, Iterator, Iterable\nimport collections\nfrom dataclasses import dataclass\nimport pprint\n\nimport pandas as pd\n\nfrom presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult\nfrom presidio_anonymizer import AnonymizerEngine, BatchAnonymizerEngine\nfrom presidio_anonymizer.entities import EngineResult\n</code></pre> <pre><code>columns = [\"name phrase\", \"phone number phrase\", \"integer\", \"boolean\" ]\nsample_data = [\n        ('Charlie likes this', 'Please call 212-555-1234 after 2pm', 1, True),\n        ('You should talk to Mike', 'his number is 978-428-7111', 2, False),\n        ('Mary had a little startup', 'Phone number: 202-342-1234', 3, False)\n]\n</code></pre> <pre><code># Create Pandas DataFrame\ndf  = pd.DataFrame(sample_data,columns=columns)\n\ndf\n</code></pre> name phrase phone number phrase integer boolean 0 Charlie likes this Please call 212-555-1234 after 2pm 1 True 1 You should talk to Mike his number is 978-428-7111 2 False 2 Mary had a little startup Phone number: 202-342-1234 3 False <pre><code># DataFrame to dict\ndf_dict = df.to_dict(orient=\"list\")\n</code></pre> <pre><code>pprint.pprint(df_dict)\n</code></pre> <pre>\n<code>{'boolean': [True, False, False],\n 'integer': [1, 2, 3],\n 'name phrase': ['Charlie likes this',\n                 'You should talk to Mike',\n                 'Mary had a little startup'],\n 'phone number phrase': ['Please call 212-555-1234 after 2pm',\n                         'his number is 978-428-7111',\n                         'Phone number: 202-342-1234']}\n</code>\n</pre> <pre><code>analyzer = AnalyzerEngine()\nbatch_analyzer = BatchAnalyzerEngine(analyzer_engine=analyzer)\nbatch_anonymizer = BatchAnonymizerEngine()\n</code></pre> <pre><code>analyzer_results = batch_analyzer.analyze_dict(df_dict, language=\"en\")\nanalyzer_results = list(analyzer_results)\nanalyzer_results\n</code></pre> <pre>\n<code>[DictAnalyzerResult(key='name phrase', value=['Charlie likes this', 'You should talk to Mike', 'Mary had a little startup'], recognizer_results=[[type: PERSON, start: 0, end: 7, score: 0.85], [type: PERSON, start: 19, end: 23, score: 0.85], [type: PERSON, start: 0, end: 4, score: 0.85]]),\n DictAnalyzerResult(key='phone number phrase', value=['Please call 212-555-1234 after 2pm', 'his number is 978-428-7111', 'Phone number: 202-342-1234'], recognizer_results=[[type: DATE_TIME, start: 31, end: 34, score: 0.85, type: PHONE_NUMBER, start: 12, end: 24, score: 0.75], [type: PHONE_NUMBER, start: 14, end: 26, score: 0.75], [type: PHONE_NUMBER, start: 14, end: 26, score: 0.75]]),\n DictAnalyzerResult(key='integer', value=[1, 2, 3], recognizer_results=[[], [], []]),\n DictAnalyzerResult(key='boolean', value=[True, False, False], recognizer_results=[[], [], []])]</code>\n</pre> <pre><code>anonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results)\n</code></pre> <pre><code>scrubbed_df = pd.DataFrame(anonymizer_results)\n</code></pre> <pre><code>scrubbed_df\n</code></pre> name phrase phone number phrase integer boolean 0 &lt;PERSON&gt; likes this Please call &lt;PHONE_NUMBER&gt; after &lt;DATE_TIME&gt; 1 True 1 You should talk to &lt;PERSON&gt; his number is &lt;PHONE_NUMBER&gt; 2 False 2 &lt;PERSON&gt; had a little startup Phone number: &lt;PHONE_NUMBER&gt; 3 False <pre><code>nested_dict = {\n    \"key_a\": {\"key_a1\": \"My phone number is 212-121-1424\"},\n    \"key_b\": {\"www.abc.com\"},\n    \"key_c\": 3,\n    \"names\": [\"James Bond\", \"Clark Kent\", \"Hakeem Olajuwon\", \"No name here!\"]\n}\n\npprint.pprint(nested_dict)\n</code></pre> <pre>\n<code>{'key_a': {'key_a1': 'My phone number is 212-121-1424'},\n 'key_b': {'www.abc.com'},\n 'key_c': 3,\n 'names': ['James Bond', 'Clark Kent', 'Hakeem Olajuwon', 'No name here!']}\n</code>\n</pre> <pre><code># Analyze dict\nanalyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\")\n\n# Anonymize dict\nanonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results)\npprint.pprint(anonymizer_results)\n</code></pre> <pre>\n<code>{'key_a': {'key_a1': 'My phone number is &lt;PHONE_NUMBER&gt;'},\n 'key_b': ['&lt;URL&gt;'],\n 'key_c': 3,\n 'names': ['&lt;PERSON&gt;', '&lt;PERSON&gt;', '&lt;PERSON&gt;', 'No name here!']}\n</code>\n</pre> <pre><code>keys_to_skip=[\"key_a1\", \"names\"]\nanalyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\", keys_to_skip=keys_to_skip)\n\n# Anonymize dict\nanonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results)\npprint.pprint(anonymizer_results)\n</code></pre> <pre>\n<code>{'key_a': {'key_a1': 'My phone number is 212-121-1424'},\n 'key_b': ['&lt;URL&gt;'],\n 'key_c': 3,\n 'names': ['James Bond', 'Clark Kent', 'Hakeem Olajuwon', 'No name here!']}\n</code>\n</pre> <pre><code>keys_to_skip = [\"key_a.key_a1\"]\n\nanalyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\", keys_to_skip=keys_to_skip)\n\n# Anonymize dict\nanonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results)\npprint.pprint(anonymizer_results)\n</code></pre> <pre>\n<code>{'key_a': {'key_a1': 'My phone number is 212-121-1424'},\n 'key_b': ['&lt;URL&gt;'],\n 'key_c': 3,\n 'names': ['&lt;PERSON&gt;', '&lt;PERSON&gt;', '&lt;PERSON&gt;', 'No name here!']}\n</code>\n</pre>"},{"location":"samples/python/batch_processing/#run-presidio-on-structured-semi-structured-data","title":"Run Presidio on structured / semi-structured data","text":"<p>This sample shows how Presidio could be potentially extended to handle the anonymization of a table or data frame. It introduces methods for the analysis and anonymization of both lists and dicts. </p> <p>Note: this sample input here is a Pandas DataFrame and a JSON file, but it can be used in other scenarios such as querying SQL data or using Spark DataFrames.</p>"},{"location":"samples/python/batch_processing/#set-up-imports","title":"Set up imports","text":""},{"location":"samples/python/batch_processing/#example-using-sample-tabular-data","title":"Example using sample tabular data","text":""},{"location":"samples/python/batch_processing/#example-using-json","title":"Example using JSON","text":""},{"location":"samples/python/batch_processing/#ignoring-specific-keys","title":"Ignoring specific keys","text":""},{"location":"samples/python/batch_processing/#ignoring-nested-keys","title":"Ignoring nested keys","text":""},{"location":"samples/python/batch_processing/#note","title":"Note!","text":"<p>JSON files with objects within lists, e.g.: <pre><code>{\n  \"key\": [\n    {\n      \"key2\": \"Peter Parker\"\n    },\n    {\n      \"key3\": \"555-1234\"\n    }\n  ]\n}\n</code></pre></p> <p>Are not yet supported. Consider breaking the JSON to parts if needed.</p>"},{"location":"samples/python/customizing_presidio_analyzer/","title":"Customizing presidio analyzer","text":"<p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/customizing_presidio_analyzer.ipynb</p> <pre><code># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</code></pre> <pre><code>from typing import List\nimport pprint\n\nfrom presidio_analyzer import AnalyzerEngine, PatternRecognizer, EntityRecognizer, Pattern, RecognizerResult\nfrom presidio_analyzer.recognizer_registry import RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\nfrom presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n</code></pre> <pre><code>titles_list = [\"Sir\", \"Ma'am\", \"Madam\", \"Mr.\", \"Mrs.\", \"Ms.\", \"Miss\", \"Dr.\", \"Professor\"]\n</code></pre> <p>Second, let's create a <code>PatternRecognizer</code> which would scan for those titles, by passing a <code>deny_list</code>:</p> <pre><code>titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\", deny_list=titles_list)\n</code></pre> <p>At this point we can call our recognizer directly:</p> <pre><code>text1 = \"I suspect Professor Plum, in the Dining Room, with the candlestick\"\nresult = titles_recognizer.analyze(text1, entities=[\"TITLE\"])\nprint(f\"Result:\\n {result}\")\n</code></pre> <pre>\n<code>Result:\n [type: TITLE, start: 10, end: 19, score: 1.0]\n</code>\n</pre> <p>Finally, let's add this new recognizer to the list of recognizers used by the Presidio <code>AnalyzerEngine</code>:</p> <pre><code>analyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(titles_recognizer)\n</code></pre> <p>When initializing the <code>AnalyzerEngine</code>, Presidio loads all available recognizers, including the <code>NlpEngine</code> used to detect entities, and extract tokens, lemmas and other linguistic features.</p> <p>Let's run the analyzer with the new recognizer in place:</p> <pre><code>results = analyzer.analyze(text=text1, language=\"en\")\n</code></pre> <pre><code>print(\"Results:\")\nprint(results)\n</code></pre> <pre>\n<code>Results:\n[type: TITLE, start: 10, end: 19, score: 1.0, type: PERSON, start: 20, end: 24, score: 0.85]\n</code>\n</pre> <p>As expected, both the name \"Plum\" and the title were identified as PII:</p> <pre><code>print(\"Identified these PII entities:\")\nfor result in results:\n    print(f\"- {text1[result.start:result.end]} as {result.entity_type}\")\n</code></pre> <pre>\n<code>Identified these PII entities:\n- Professor as TITLE\n- Plum as PERSON\n</code>\n</pre> <pre><code># Define the regex pattern in a Presidio `Pattern` object:\nnumbers_pattern = Pattern(name=\"numbers_pattern\",regex=\"\\d+\", score = 0.5)\n\n# Define the recognizer with one or more patterns\nnumber_recognizer = PatternRecognizer(supported_entity=\"NUMBER\", patterns = [numbers_pattern])\n</code></pre> <p>Testing the recognizer itself:</p> <pre><code>text2 = \"I live in 510 Broad st.\"\n\nnumbers_result = number_recognizer.analyze(text=text2, entities=[\"NUMBER\"])\nprint(\"Result:\")\nprint(numbers_result)\n</code></pre> <pre>\n<code>Result:\n[type: NUMBER, start: 10, end: 13, score: 0.5]\n</code>\n</pre> <p>It's important to mention that recognizers is likely to have errors, both false-positive and false-negative, which would impact the entire performance of Presidio. Consider testing each recognizer on a representative dataset prior to integrating it into Presidio. For more info, see the best practices for developing recognizers documentation.</p> <pre><code>class MyRecognizer(EntityRecognizer):\n\n    def load(self) -&amp;gt; None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts) -&amp;gt; List[RecognizerResult]:\n        \"\"\"\n        Logic for detecting a specific PII\n        \"\"\"\n        pass\n</code></pre> <p>For example, detecting numbers in either numerical or alphabetic (e.g. Forty five) form:</p> <pre><code>class NumbersRecognizer(EntityRecognizer):\n\n    expected_confidence_level = 0.7 # expected confidence level for this recognizer\n\n    def load(self) -&amp;gt; None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -&amp;gt; List[RecognizerResult]:\n        \"\"\"\n        Analyzes test to find tokens which represent numbers (either 123 or One Two Three).\n        \"\"\"\n        results = []\n\n        # iterate over the spaCy tokens, and call `token.like_num`\n        for token in nlp_artifacts.tokens:\n            if token.like_num:\n                result = RecognizerResult(\n                    entity_type=\"NUMBER\",\n                    start=token.idx,\n                    end=token.idx + len(token),\n                    score=self.expected_confidence_level\n                )\n                results.append(result)\n        return results\n</code></pre> <pre><code>new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n</code></pre> <p>Since this recognizer requires the <code>NlpArtifacts</code>, we would have to call it as part of the <code>AnalyzerEngine</code> flow:</p> <pre><code>text3 = \"Roberto lives in Five 10 Broad st.\"\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(new_numbers_recognizer)\n\nnumbers_results2 = analyzer.analyze(text=text3, language=\"en\")\nprint(\"Results:\")\nprint(\"\\n\".join([str(res) for res in numbers_results2]))\n</code></pre> <pre>\n<code>Results:\ntype: PERSON, start: 0, end: 7, score: 0.85\ntype: NUMBER, start: 17, end: 21, score: 0.7\ntype: NUMBER, start: 22, end: 24, score: 0.7\n</code>\n</pre> <p>The analyzer was able to pick up both numeric and alphabetical numbers, including other types of PII entities from other recognizers (PERSON in this case).</p> <pre><code>from presidio_analyzer.nlp_engine import NlpEngineProvider\n\n#import spacy\n#spacy.cli.download(\"es_core_news_md\")\n\n# Create configuration containing engine name and models\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n               {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n}\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, \n    supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es Morris\", language=\"es\")\nprint(\"Results from Spanish request:\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(\"Results from English request:\")\nprint(results_english)\n</code></pre> <pre>\n<code>Results from Spanish request:\n[]\nResults from English request:\n[type: PERSON, start: 11, end: 17, score: 0.85]\n</code>\n</pre> <p>See this documentation for more details on how to configure Presidio support additional NLP models and languages.</p> <pre><code># Define the regex pattern\nregex = r\"(\\b\\d{5}(?:\\-\\d{4})?\\b)\" # very weak regex pattern\nzipcode_pattern = Pattern(name=\"zip code (weak)\", regex=regex, score=0.01)\n\n# Define the recognizer with the defined pattern\nzipcode_recognizer = PatternRecognizer(supported_entity=\"US_ZIP_CODE\", patterns = [zipcode_pattern])\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test\nresults = analyzer.analyze(text=\"My zip code is 90210\",language=\"en\")\nprint(f\"Result:\\n {results}\")\n</code></pre> <pre>\n<code>Result:\n [type: US_ZIP_CODE, start: 15, end: 20, score: 0.01]\n</code>\n</pre> <p>So this is working, but would catch any 5 digit string. This is why we set the score to 0.01. Let's use context words to increase score:</p> <pre><code># Define the recognizer with the defined pattern and context words\nzipcode_recognizer = PatternRecognizer(supported_entity=\"US_ZIP_CODE\", \n                                       patterns = [zipcode_pattern],\n                                       context= [\"zip\",\"zipcode\"])\n</code></pre> <p>When creating an <code>AnalyzerEngine</code> we can provide our own context enhancement logic by passing it to <code>context_aware_enhancer</code> parameter. <code>AnalyzerEngine</code> will create <code>LemmaContextAwareEnhancer</code> by default if not passed, which will enhance score of each matched result if it's recognizer holds context words and those words are found in context of the matched entity.</p> <pre><code>registry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n</code></pre> <pre><code># Test\nresults = analyzer.analyze(text=\"My zip code is 90210\",language=\"en\")\nprint(\"Result:\")\nprint(results)\n</code></pre> <pre>\n<code>Result:\n[type: US_ZIP_CODE, start: 15, end: 20, score: 0.4]\n</code>\n</pre> <p>The confidence score is now 0.4, instead of 0.01. because <code>LemmaContextAwareEnhancer</code> default context similarity factor is 0.35 and default minimum score with context similarity is 0.4, we can change that by passing <code>context_similarity_factor</code> and <code>min_score_with_context_similarity</code> parameters of <code>LemmaContextAwareEnhancer</code> to other than values, for example:</p> <pre><code>registry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(\n    registry=registry,\n    context_aware_enhancer=\n        LemmaContextAwareEnhancer(context_similarity_factor=0.45, min_score_with_context_similarity=0.4))\n</code></pre> <pre><code># Test\nresults = analyzer.analyze(text=\"My zip code is 90210\",language=\"en\")\nprint(\"Result:\")\nprint(results)\n</code></pre> <pre>\n<code>Result:\n[type: US_ZIP_CODE, start: 15, end: 20, score: 0.46]\n</code>\n</pre> <p>The confidence score is now 0.46 because it got enhanced from 0.01 with 0.45 and is more the minimum of 0.4</p> <p>Presidio supports passing a list of outer context in analyzer level, this is useful if the text is coming from a specific column or a specific user input etc. notice how the \"zip\" context word doesn't appear in the text but still enhance the confidence score from 0.01 to 0.4:</p> <pre><code># Define the recognizer with the defined pattern and context words\nzipcode_recognizer = PatternRecognizer(supported_entity=\"US_ZIP_CODE\",\n                                       patterns = [zipcode_pattern],\n                                       context= [\"zip\",\"zipcode\"])\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test\nresult = analyzer.analyze(text=\"My code is 90210\",language=\"en\", context=[\"zip\"])\nprint(\"Result:\")\nprint(result)\n</code></pre> <pre>\n<code>Result:\n[type: US_ZIP_CODE, start: 11, end: 16, score: 0.4]\n</code>\n</pre> <pre><code>results = analyzer.analyze(text=\"My zip code is 90210\",language=\"en\", return_decision_process = True)\ndecision_process = results[0].analysis_explanation\n\npp = pprint.PrettyPrinter()\nprint(\"Decision process output:\\n\")\npp.pprint(decision_process.__dict__)\n</code></pre> <pre>\n<code>Decision process output:\n\n{'original_score': 0.01,\n 'pattern': '(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)',\n 'pattern_name': 'zip code (weak)',\n 'recognizer': 'PatternRecognizer',\n 'score': 0.4,\n 'score_context_improvement': 0.39,\n 'supportive_context_word': 'zip',\n 'textual_explanation': None,\n 'validation_result': None}\n</code>\n</pre> <p>When developing new recognizers, one can add information to this explanation and extend it with additional findings.</p> <p>We will use the built in recognizers that include the <code>URLRecognizer</code> and the NLP model <code>EntityRecognizer</code> and see the default functionality if we don't specify any list of words for the detector to allow to keep in the text.</p> <pre><code>websites_list = [\n    \"bing.com\",\n    \"microsoft.com\"\n]\ntext1 = \"Bill's favorite website is bing.com, David's is microsoft.com\"\nanalyzer = AnalyzerEngine()\nresult = analyzer.analyze(text = text1, language = 'en')\nprint(f\"Result: \\n {result}\")\n</code></pre> <pre>\n<code>Result: \n [type: PERSON, start: 0, end: 4, score: 0.85, type: URL, start: 27, end: 35, score: 0.85, type: PERSON, start: 37, end: 42, score: 0.85, type: URL, start: 48, end: 61, score: 0.85]\n</code>\n</pre> <p>To specify an allow list we just pass a list of values we want to keep as a parameter to call to <code>analyze</code>. Now we can see that in the results, <code>bing.com</code> is no longer being recognized as a PII item, only <code>microsoft.com</code> as well as the named entities are still recognized since we did include it in the allow list. </p> <pre><code>result = analyzer.analyze(text = text1, language = 'en', allow_list= [\"bing.com\", \"google.com\"])\nprint(f\"Result: \\n {result}\")\n</code></pre> <pre>\n<code>Result: \n [type: PERSON, start: 0, end: 4, score: 0.85, type: PERSON, start: 37, end: 42, score: 0.85, type: URL, start: 48, end: 61, score: 0.85]\n</code>\n</pre>"},{"location":"samples/python/customizing_presidio_analyzer/#customizing-the-pii-analysis-process-in-microsoft-presidio","title":"Customizing the PII analysis process in Microsoft Presidio","text":"<p>This notebooks covers different customization use cases to:</p> <ol> <li>Adapt Presidio to detect new types of PII entities</li> <li>Adapt Presidio to detect PII entities in a new language</li> <li>Embed new types of detection modules into Presidio, to improve the coverage of the service.</li> </ol>"},{"location":"samples/python/customizing_presidio_analyzer/#installation","title":"Installation","text":"<p>First, let's install presidio using <code>pip</code>. For detailed documentation, see the installation docs.</p> <p>Install from PyPI:</p>"},{"location":"samples/python/customizing_presidio_analyzer/#getting-started","title":"Getting started","text":"<p>The high level process in Presidio-Analyzer is the following: </p> <p>Load the <code>presidio-analyzer</code> modules. For more information, see the analyzer docs.</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-1-deny-list-based-pii-recognition","title":"Example 1: Deny-list based PII recognition","text":"<p>In this example, we will pass a short list of tokens which should be marked as PII if detected. First, let's define the tokens we want to treat as PII. In this case it would be a list of titles:</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-2-regex-based-pii-recognition","title":"Example 2: Regex based PII recognition","text":"<p>Another simple recognizer we can add is based on regular expressions.  Let's assume we want to be extremely conservative and treat any token which contains a number as PII.</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-3-rule-based-logic-recognizer","title":"Example 3: Rule based logic recognizer","text":"<p>Taking the numbers recognizer one step further, let's say we also would like to detect numbers within words, e.g. \"Number One\". We can leverage the underlying spaCy token attributes, or write our own logic to detect such entities.</p> <p>Notes:</p> <ul> <li> <p>In this example we would create a new class, which implements <code>EntityRecognizer</code>, the basic recognizer in Presidio. This abstract class requires us to implement the <code>load</code> method and <code>analyze</code> method. </p> </li> <li> <p>Each recognizer accepts an object of type <code>NlpArtifacts</code>, which holds pre-computed attributes on the input text.</p> </li> </ul> <p>A new recognizer should have this structure:</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-4-calling-an-external-service-for-pii-detection","title":"Example 4: Calling an external service for PII detection","text":"<p>In a similar way to example 3, we can write logic to call external services for PII detection.  For a detailed example, see this part of the documentation.</p> <p>This is a sample implementation of such remote recognizer.</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-5-supporting-new-languages","title":"Example 5: Supporting new languages","text":"<p>Two main parts in Presidio handle the text, and should be adapted if a new language is required: 1. The <code>NlpEngine</code> containing the NLP model which performs tokenization, lemmatization, Named Entity Recognition and other NLP tasks. 2. The different PII recognizers (<code>EntityRecognizer</code> objects) should be adapted or created.</p>"},{"location":"samples/python/customizing_presidio_analyzer/#adapting-the-nlp-engine","title":"Adapting the NLP engine","text":"<p>As its internal NLP engine, Presidio supports both spaCy and Stanza. Make sure you download the required models from spacy/stanza prior to using them. More details here. For example, to download the Spanish medium spaCy model: <code>python -m spacy download es_core_news_md</code></p> <p>In this example we will configure Presidio to use spaCy as its underlying NLP framework, with NLP models in English and Spanish:</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-6-using-context-words","title":"Example 6: Using context words","text":"<p>Presidio has a internal mechanism for leveraging context words. This mechanism would increse the detection confidence of a PII entity in case a specific word appears before or after it.</p> <p>In this example we would first implement a zip code recognizer without context, and then add context to see how the confidence changes. Zip regex patterns (essentially 5 digits) are very week, so we would want the initial confidence to be low, and increased with the existence of context words.</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-7-tracing-the-decision-process","title":"Example 7: Tracing the decision process","text":"<p>Presidio-analyzer's decision process exposes information on why a specific PII was detected. Such information could contain:</p> <ul> <li>Which recognizer detected the entity</li> <li>Which regex pattern was used</li> <li>Interpretability mechanisms in ML models</li> <li>Which context words improved the score</li> <li>Confidence scores before and after each step And more.</li> </ul> <p>For more information, refer to the decision process documentation.</p> <p>Let's use the decision process output to understand how the zip code value was detected:</p>"},{"location":"samples/python/customizing_presidio_analyzer/#example-8-passing-a-list-of-words-to-keep","title":"Example 8: passing a list of words to keep","text":""},{"location":"samples/python/encrypt_decrypt/","title":"Encrypt decrypt","text":"<pre><code># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</code></pre> <p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/encrypt_decrypt.ipynb</p> <pre><code>from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine\nfrom presidio_anonymizer.entities import RecognizerResult, OperatorResult, OperatorConfig\nfrom presidio_anonymizer.operators import Decrypt\n</code></pre> <pre><code>crypto_key = \"WmZq4t7w!z%C&amp;amp;F)J\"\n</code></pre> <pre><code>engine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text,\n# analyzer results (potentially coming from presidio-analyzer)\n# and an 'encrypt' operator to get an encrypted anonymization output:\nanonymize_result = engine.anonymize(\n    text=\"My name is James Bond\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),\n    ],\n    operators={\"PERSON\": OperatorConfig(\"encrypt\", {\"key\": crypto_key})},\n)\n\nanonymize_result\n</code></pre> <pre>\n<code>text: My name is M4lla0kBCzu6SwCONL6Y+ZqsPqhBp1Lhdc3t0FKnUwM=.\nitems:\n[\n    {'start': 11, 'end': 55, 'entity_type': 'PERSON', 'text': 'M4lla0kBCzu6SwCONL6Y+ZqsPqhBp1Lhdc3t0FKnUwM=', 'operator': 'encrypt'}\n]</code>\n</pre> <pre><code># Fetch the anonymized text from the result.\nanonymized_text = anonymize_result.text\n\n# Fetch the anonynized entities from the result.\nanonymized_entities = anonymize_result.items\n</code></pre> <pre><code># Initialize the engine:\nengine = DeanonymizeEngine()\n\n# Invoke the deanonymize function with the text, anonymizer results\n# and a 'decrypt' operator to get the original text as output.\ndeanonymized_result = engine.deanonymize(\n    text=anonymized_text,\n    entities=anonymized_entities,\n    operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": crypto_key})},\n)\n\ndeanonymized_result\n</code></pre> <pre>\n<code>text: My name is James Bond.\nitems:\n[\n    {'start': 11, 'end': 21, 'entity_type': 'PERSON', 'text': 'James Bond', 'operator': 'decrypt'}\n]</code>\n</pre> <pre><code># Alternatively, call the Decrypt operator directly:\n\n# Fetch the encrypted entitiy value from the previous stage\nencrypted_entity_value = anonymize_result.items[0].text\n\n# Restore the original entity value\nDecrypt().operate(text=encrypted_entity_value, params={\"key\": crypto_key})\n</code></pre> <pre>\n<code>'James Bond'</code>\n</pre>"},{"location":"samples/python/encrypt_decrypt/#encrypting-and-decrypting-identified-entities","title":"Encrypting and Decrypting identified entities","text":"<p>This sample shows how to use Presidio Anonymizer built-in functionality, to encrypt and decrypt identified entities. The encryption is using AES cypher in CBC mode and requires a cryptographic key as an input for both the encryption and the decryption.</p>"},{"location":"samples/python/encrypt_decrypt/#set-up-imports","title":"Set up imports","text":""},{"location":"samples/python/encrypt_decrypt/#define-a-cryptographic-key-for-both-encryption-and-decryption","title":"Define a cryptographic key (for both encryption and decryption)","text":""},{"location":"samples/python/encrypt_decrypt/#presidio-anonymizer-encrypt","title":"Presidio Anonymizer: Encrypt","text":""},{"location":"samples/python/encrypt_decrypt/#presidio-anonymizer-decrypt","title":"Presidio Anonymizer: Decrypt","text":""},{"location":"samples/python/example_dicom_image_redactor/","title":"Example dicom image redactor","text":"<p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/example_dicom_image_redactor.ipynb</p> <pre><code>!pip install presidio_analyzer presidio_anonymizer presidio_image_redactor\n!python -m spacy download en_core_web_lg\n</code></pre> <pre><code>import glob\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport pydicom\nfrom presidio_image_redactor import DicomImageRedactorEngine\n</code></pre> <pre><code>def compare_dicom_images(\n    instance_original: pydicom.dataset.FileDataset,\n    instance_redacted: pydicom.dataset.FileDataset,\n    figsize: tuple = (11, 11)\n) -&amp;gt; None:\n    \"\"\"Display the DICOM pixel arrays of both original and redacted as images.\n\n    Args:\n        instance_original (pydicom.dataset.FileDataset): A single DICOM instance (with text PHI).\n        instance_redacted (pydicom.dataset.FileDataset): A single DICOM instance (redacted PHI).\n        figsize (tuple): Figure size in inches (width, height).\n    \"\"\"\n    _, ax = plt.subplots(1, 2, figsize=figsize)\n    ax[0].imshow(instance_original.pixel_array, cmap=\"gray\")\n    ax[0].set_title('Original')\n    ax[1].imshow(instance_redacted.pixel_array, cmap=\"gray\")\n    ax[1].set_title('Redacted')\n</code></pre> <p>Instantiate the DICOM image redactor engine object.</p> <p>&gt; Note: The <code>DicomImageRedactorEngine</code> object can initialized with a custom <code>ImageAnalyzerEngine</code>, which may be useful in cases where DICOM metadata is insufficient.</p> <pre><code>engine = DicomImageRedactorEngine()\n</code></pre> <p>In cases where you already working with loaded DICOM data, the <code>.redact()</code> function is most appropriate.</p> <pre><code># Load in and process your DICOM file as needed\ndicom_instance = pydicom.dcmread('sample_data/0_ORIGINAL.dcm')\n</code></pre> <pre><code># Redact\nredacted_dicom_instance = engine.redact(dicom_instance, fill=\"contrast\")\n</code></pre> <pre><code>compare_dicom_images(dicom_instance, redacted_dicom_instance)\n</code></pre> <p>We can also set the \"fill\" to match the background color to blend in more with the image.</p> <pre><code>redacted_dicom_instance_2 = engine.redact(dicom_instance, fill=\"background\")\ncompare_dicom_images(dicom_instance, redacted_dicom_instance_2)\n</code></pre> <pre><code>redacted_dicom_instance = engine.redact(dicom_instance, use_metadata=False) # default is use_metadata=True\ncompare_dicom_images(dicom_instance, redacted_dicom_instance)\n</code></pre> <p>We can also return the bounding box information for the pixel regions that were redacted.</p> <pre><code>redacted_dicom_instance, bbox = engine.redact_and_return_bbox(dicom_instance)\ncompare_dicom_images(dicom_instance, redacted_dicom_instance)\nprint(f\"Number of redacted regions: {len(bbox)}\")\nprint(bbox)\n</code></pre> <pre>\n<code>Number of redacted regions: 4\n[{'top': 0, 'left': 0, 'width': 241, 'height': 37}, {'top': 0, 'left': 262, 'width': 230, 'height': 36}, {'top': 1, 'left': 588, 'width': 226, 'height': 35}, {'top': 47, 'left': 145, 'width': 218, 'height': 35}]\n</code>\n</pre> <pre><code># Single DICOM (.dcm) file or directory containing DICOM files\ninput_path = 'sample_data/'\n\n# Directory where the output will be written\noutput_parent_dir = 'output/'\n</code></pre> <pre><code># Redact text PHI from DICOM images\nengine.redact_from_directory(\n    input_dicom_path = input_path,\n    output_dir = output_parent_dir,\n    fill=\"contrast\",\n    save_bboxes=True # if True, saves the redacted region bounding box info to .json files in the output dir\n)\n</code></pre> <pre>\n<code>Output written to output\\sample_data\n</code>\n</pre> <p>Get file paths</p> <pre><code># Original DICOM images\np = Path(input_path).glob(\"**/*.dcm\")\noriginal_files = [x for x in p if x.is_file()]\n\n# Redacted DICOM images\np = Path(output_parent_dir).glob(\"**/*.dcm\")\nredacted_files = [x for x in p if x.is_file()]\n</code></pre> <p>Preview images</p> <pre><code>for i in range(0, len(original_files)):\n    original_file = pydicom.dcmread(original_files[i])\n    redacted_file = pydicom.dcmread(redacted_files[i])\n\n    compare_dicom_images(original_file, redacted_file)\n</code></pre>"},{"location":"samples/python/example_dicom_image_redactor/#de-identifying-sensitive-burnt-in-text-in-dicom-images","title":"De-identifying sensitive burnt-in text in DICOM images","text":"<p>This notebook covers how to: 1. Redact text Personal Health Information (PHI) present as pixels in DICOM images 2. Visually compare original DICOM images with their redacted versions</p> <p>&gt; This module only redacts pixel data and does not scrub text PHI which may exist in the DICOM metadata. To redact sensitive information from metadata, consider using another package such as the Tools for Health Data Anonymization.</p>"},{"location":"samples/python/example_dicom_image_redactor/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, make sure presidio and the latest version of Tesseract OCR are installed. For detailed documentation, see the installation docs.</p>"},{"location":"samples/python/example_dicom_image_redactor/#dataset","title":"Dataset","text":"<p>Sample DICOM files are available for use in this notebook in <code>./sample_data</code>. Copies of the original DICOM data were saved into the folder with permission from the dataset owners. Please see the original dataset information below: &gt; Rutherford, M., Mun, S.K., Levine, B., Bennett, W.C., Smith, K., Farmer, P., Jarosz, J., Wagner, U., Farahani, K., Prior, F. (2021). A DICOM dataset for evaluation of medical image de-identification (Pseudo-PHI-DICOM-Data) [Data set]. The Cancer Imaging Archive. DOI: https://doi.org/10.7937/s17z-r072</p>"},{"location":"samples/python/example_dicom_image_redactor/#1-setup","title":"1. Setup","text":""},{"location":"samples/python/example_dicom_image_redactor/#2-redacting-from-loaded-dicom-image-data","title":"2. Redacting from loaded DICOM image data","text":""},{"location":"samples/python/example_dicom_image_redactor/#22-verify-performance","title":"2.2 Verify performance","text":"<p>Let's look at the original input and compare against the de-identified output.</p>"},{"location":"samples/python/example_dicom_image_redactor/#23-adjust-parameters","title":"2.3 Adjust parameters","text":"<p>With the <code>use_metadata</code> parameter, we can toggle whether the DICOM metadata is used to augment the analyzer which determines which text to redact.</p>"},{"location":"samples/python/example_dicom_image_redactor/#3-redacting-from-dicom-files","title":"3. Redacting from DICOM files","text":"<p>Before instantiating your <code>DicomImageRedactorEngine</code> class, determine where you want your input to come from and where you want your output to be written to.</p> <p>&gt; Note: The output will mimic the folder structure of the input if the input is a directory. The redact method will operate on all DICOM (.dcm) files in the input directory and all its subdirectories.</p> <p>To protect against overwriting the original DICOM files, the <code>redact_from_file()</code> and <code>redact_from_directory()</code> methods will not run if the <code>output_dir</code> is a directory which already contains any content.</p>"},{"location":"samples/python/example_dicom_image_redactor/#31-run-de-identification","title":"3.1. Run de-identification","text":"<p>Use the <code>DicomImageRedactorEngine</code> class to process your DICOM images. If you have only one image to process and want to specify that directly instead of a directory, use <code>.redact_from_file()</code> instead of <code>.redact_from_directory()</code>.</p>"},{"location":"samples/python/example_dicom_image_redactor/#32-verify-performance","title":"3.2. Verify performance","text":"<p>Let's look at the original input and compare against the de-identified output.</p>"},{"location":"samples/python/example_dicom_image_redactor/#conclusion","title":"Conclusion","text":"<p>As seen in the DICOM image previews above, we see that our <code>DicomImageRedactorEngine</code> is able to successfully mask out text PHI present in the DICOM images without compromising image quality.</p> <p>Note: Performance is best when the burnt-in text is also present within the DICOM metadata. We recommend not scrubbing metadata until after performing image de-identification.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/","title":"Example dicom redactor evaluation","text":"<p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/example_dicom_redactor_evaluation.ipynb</p> <pre><code>!pip install presidio_analyzer presidio_anonymizer presidio_image_redactor\n!python -m spacy download en_core_web_lg\n</code></pre> <pre><code>import os\nimport json\nimport pandas as pd\nimport pydicom\n\nfrom presidio_image_redactor import DicomImagePiiVerifyEngine\n</code></pre> <pre><code># Set paths\ndata_dir = \"sample_data\"\ngt_path = \"sample_data/ground_truth.json\"\n</code></pre> <pre><code># Load ground truth JSON\nwith open(gt_path) as json_file:\n    gt = json.load(json_file)\n\n# Get list of files\ngt_dicom_files = list(gt.keys())\ngt_dicom_files\n</code></pre> <pre>\n<code>['sample_data/0_ORIGINAL.dcm',\n 'sample_data/1_ORIGINAL.dcm',\n 'sample_data/2_ORIGINAL.dcm',\n 'sample_data/3_ORIGINAL.dcm']</code>\n</pre> <pre><code>dicom_engine = DicomImagePiiVerifyEngine()\n</code></pre> <pre><code># Select one file to work with\nfile_of_interest = gt_dicom_files[0]\ngt_file_of_interest = gt[file_of_interest]\n</code></pre> <pre><code># Return image to visually inspect\ninstance = pydicom.dcmread(file_of_interest)\nverify_image, ocr_results, analyzer_results = dicom_engine.verify_dicom_instance(instance)\n</code></pre> <pre><code>def get_PHI_list(PHI: list) -&amp;gt; list:\n    \"\"\"Get list of PHI from ground truth for a single file.\n\n    Args:\n        PHI_dict (list): List of ground truth or detected text PHI.\n\n    Return:\n        PHI_list (list): List of PHI (just text).\n    \"\"\"\n    PHI_list = []\n    for item in PHI:\n        PHI_list.append(item['label'])\n\n    return PHI_list\n</code></pre> <pre><code>_, eval_results = dicom_engine.eval_dicom_instance(instance, gt_file_of_interest)\n</code></pre> <p>Results</p> <pre><code>print(f\"Precision: {eval_results['precision']}\")\nprint(f\"Recall: {eval_results['recall']}\")\nprint(f\"All Positives: {get_PHI_list(eval_results['all_positives'])}\")\nprint(f\"Ground Truth: {get_PHI_list(eval_results['ground_truth'])}\")\n</code></pre> <pre>\n<code>Precision: 1.0\nRecall: 1.0\nAll Positives: ['DAVIDSON', 'DOUGLAS', '[M]', '01.09.2012', '06.16.1976']\nGround Truth: ['DAVIDSON', 'DOUGLAS', '[M]', '01.09.2012', '06.16.1976']\n</code>\n</pre> <pre><code># Initialize lists to turn into results table\nlist_of_files = gt_dicom_files\nlist_of_gt = []\nlist_of_pos = []\nlist_of_recall = []\nlist_of_precision = []\n</code></pre> <p>Loop through all the files</p> <pre><code>for file in gt_dicom_files:\n    # Setup\n    ground_truth = gt[file]\n    instance = pydicom.dcmread(file)\n\n    # Evaluate\n    _, eval_results = dicom_engine.eval_dicom_instance(instance, ground_truth)\n\n    # Save results\n    list_of_gt.append(get_PHI_list(eval_results[\"ground_truth\"]))\n    list_of_pos.append(get_PHI_list(eval_results[\"all_positives\"]))\n    list_of_recall.append(eval_results[\"recall\"])\n    list_of_precision.append(eval_results[\"precision\"])\n</code></pre> <p>Create a summary results table</p> <pre><code># Organize results into a table\nall_results_dict = {\n    \"file\": list_of_files,\n    \"ground_truth\": list_of_gt,\n    \"all_positives\": list_of_pos,\n    \"recall\": list_of_recall,\n    \"precision\": list_of_precision\n}\n\ndf_results = pd.DataFrame(all_results_dict)\ndf_results\n</code></pre> file ground_truth all_positives recall precision 0 sample_data/0_ORIGINAL.dcm [DAVIDSON, DOUGLAS, [M], 01.09.2012, 06.16.1976] [DAVIDSON, DOUGLAS, [M], 01.09.2012, 06.16.1976] 1.0 1.0 1 sample_data/1_ORIGINAL.dcm [MARTIN, CHAD, [U], 01.01.2000] [MARTIN, CHAD, [U], 01.01.2000] 1.0 1.0 2 sample_data/2_ORIGINAL.dcm [KAUFMAN, SCOTT, [M], 03.09.2012, 07.22.1943] [KAUFMAN, 07.22.1943, SCOTT, [M], 03.09.2012] 1.0 1.0 3 sample_data/3_ORIGINAL.dcm [MEYER, STEPHANIE, [F], 02.25.2012, 07.16.1953] [MEYER, STEPHANIE, [F], 02.25.2012, 07.16.1953] 1.0 1.0 <p>For example, if we set <code>padding_width=1</code>, this can negatively impact the OCR step which identifies all text regardless of PHI status in an image if text is bordering the edges of the image. When the OCR fails to return all text, we cannot reliably detect PHI.</p> <pre><code># Select file\nfile_of_interest = gt_dicom_files[3]\ngt_file_of_interest = gt[file_of_interest]\ninstance = pydicom.dcmread(file_of_interest)\n\n# Run evaluation with minimal padding (0 padding not allowed)\n_, eval_results = dicom_engine.eval_dicom_instance(instance, gt_file_of_interest, padding_width=1)\n</code></pre> <p>Notice how low the recall is and how different the detected PHI list is here than in the summary table above which ran de-identification and evaluation with the default  <code>padding_width=25</code>.</p> <pre><code>print(f\"Precision: {eval_results['precision']}\")\nprint(f\"Recall: {eval_results['recall']}\")\nprint(f\"All Positives: {get_PHI_list(eval_results['all_positives'])}\")\nprint(f\"Ground Truth: {get_PHI_list(eval_results['ground_truth'])}\")\n</code></pre> <pre>\n<code>Precision: 1.0\nRecall: 0.2\nAll Positives: ['07.16.1953']\nGround Truth: ['MEYER', 'STEPHANIE', '[F]', '02.25.2012', '07.16.1953']\n</code>\n</pre>"},{"location":"samples/python/example_dicom_redactor_evaluation/#evaluate-dicom-de-identification-performance","title":"Evaluate DICOM de-identification performance","text":"<p>This notebook demonstrates how to use the <code>DicomImagePiiVerifyEngine</code> to evaluate how well the <code>DicomImageRedactorEngine</code> de-identifies text Personal Health Information (PHI) from DICOM images when ground truth labels are available.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, make sure presidio and the latest version of Tesseract OCR are installed. For detailed documentation, see the installation docs.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#dataset","title":"Dataset","text":"<p>Sample DICOM files are available for use in this notebook in <code>./sample_data</code>. Copies of the original DICOM data were saved into the folder with permission from the dataset owners. Please see the original dataset information below: &gt; Rutherford, M., Mun, S.K., Levine, B., Bennett, W.C., Smith, K., Farmer, P., Jarosz, J., Wagner, U., Farahani, K., Prior, F. (2021). A DICOM dataset for evaluation of medical image de-identification (Pseudo-PHI-DICOM-Data) [Data set]. The Cancer Imaging Archive. DOI: https://doi.org/10.7937/s17z-r072</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#load-ground-truth","title":"Load ground truth","text":"<p>Load the ground truth labels. For more information on the ground truth format, please see the evaluating DICOM de-identification page.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#initialize-the-verification-engine","title":"Initialize the verification engine","text":"<p>This engine will be used for both verification and evaluation.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#verify-detected-phi-for-one-dicom-image","title":"Verify detected PHI for one DICOM image","text":"<p>To visually identify what text is being detected as PHI on a DICOM image, use the <code>.verify_dicom_instance()</code> method.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#evaluate-de-identification-performance","title":"Evaluate de-identification performance","text":"<p>To evaluate how well the actual sensitive text (specified in the ground truth) are identified and redacted, use the <code>.evaluate_dicom_instance()</code> method.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#for-one-image","title":"For one image","text":"<p>Display the DICOM image with bounding boxes identifying the detected PHI.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#for-multiple-images","title":"For multiple images","text":""},{"location":"samples/python/example_dicom_redactor_evaluation/#experiment-with-settings","title":"Experiment with settings","text":"<p>You can experiment with different settings such as <code>padding_width</code>, <code>tolerance</code>, and any additional arguments to feed into the image analyzer in your DICOM verification engine and see the effect on performance.</p> <p>&gt; Changing tolerance does not affect the de-identification logic nor image redaction. Tolerance is only used for matching analyzer results to ground truth labels.</p>"},{"location":"samples/python/example_dicom_redactor_evaluation/#conclusion","title":"Conclusion","text":"<p>The <code>DicomImagePiiVerifyEngine</code> allows us to easily do a visual inspection on the identified PHI and also evaluate how well the de-identification worked compared to a provided ground truth.</p> <p>In the case of these sample images, the precision and recall of the Presidio <code>DicomImageRedactorEngine</code> redact function is 1.0 when we use the default values <code>padding_width=25</code> and <code>tolerance=50</code>.</p>"},{"location":"samples/python/example_pdf_annotation/","title":"Example pdf annotation","text":"<p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/example_pdf_annotation.ipynb</p> <pre><code>!pip install presidio_analyzer\n!pip install presidio_anonymizer\n!python -m spacy download en_core_web_lg\n!pip install pdfminer.six\n!pip install pikepdf\n</code></pre> <pre><code># For Presidio\nfrom presidio_analyzer import AnalyzerEngine, PatternRecognizer\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\n\n# For console output\nfrom pprint import pprint\n\n# For extracting text\nfrom pdfminer.high_level import extract_text, extract_pages\nfrom pdfminer.layout import LTTextContainer, LTChar, LTTextLine\n\n# For updating the PDF\nfrom pikepdf import Pdf, AttachedFileSpec, Name, Dictionary, Array\n</code></pre> <pre><code>analyzer = AnalyzerEngine()\n\nanalyzed_character_sets = []\n\nfor page_layout in extract_pages(\"./sample_data/sample.pdf\"):\n    for text_container in page_layout:\n        if isinstance(text_container, LTTextContainer):\n\n            # The element is a LTTextContainer, containing a paragraph of text.\n            text_to_anonymize = text_container.get_text()\n\n            # Analyze the text using the analyzer engine\n            analyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')\n\n            if text_to_anonymize.isspace() == False:\n                print(text_to_anonymize)\n                print(analyzer_results)\n\n            characters = list([])\n\n            # Grab the characters from the PDF\n            for text_line in filter(lambda t: isinstance(t, LTTextLine), text_container):\n                    for character in filter(lambda t: isinstance(t, LTChar), text_line):\n                            characters.append(character)\n\n\n            # Slice out the characters that match the analyzer results.\n            for result in analyzer_results:\n                start = result.start\n                end = result.end\n                analyzed_character_sets.append({\"characters\": characters[start:end], \"result\": result})\n</code></pre> <pre>\n<code>This is a test PDF, created by Microsoft Word. \n\n[]\nHi my name is Charles Darwin and my email is cdarwin@hmsbeagle.org \n\n[type: EMAIL_ADDRESS, start: 45, end: 66, score: 1.0, type: PERSON, start: 14, end: 28, score: 0.85, type: URL, start: 53, end: 66, score: 0.5]\nYou can contact me on 01234 567890. \n\n[type: PHONE_NUMBER, start: 22, end: 34, score: 0.4, type: US_DRIVER_LICENSE, start: 28, end: 34, score: 0.01]\n</code>\n</pre> <pre><code># Combine the bounding boxes into a single bounding box.\ndef combine_rect(rectA, rectB):\n    a, b = rectA, rectB\n    startX = min( a[0], b[0] )\n    startY = min( a[1], b[1] )\n    endX = max( a[2], b[2] )\n    endY = max( a[3], b[3] )\n    return (startX, startY, endX, endY)\n\nanalyzed_bounding_boxes = []\n\n# For each character set, combine the bounding boxes into a single bounding box.\nfor analyzed_character_set in analyzed_character_sets:\n    completeBoundingBox = analyzed_character_set[\"characters\"][0].bbox\n\n    for character in analyzed_character_set[\"characters\"]:\n        completeBoundingBox = combine_rect(completeBoundingBox, character.bbox)\n\n    analyzed_bounding_boxes.append({\"boundingBox\": completeBoundingBox, \"result\": analyzed_character_set[\"result\"]})\n</code></pre> <pre><code>pdf = Pdf.open(\"./sample_data/sample.pdf\")\n\nannotations = []\n\n# Create a highlight annotation for each bounding box.\nfor analyzed_bounding_box in analyzed_bounding_boxes:\n\n    boundingBox = analyzed_bounding_box[\"boundingBox\"]\n\n    # Create the annotation. \n    # We could also create a redaction annotation if the ongoing workflows supports them.\n    highlight = Dictionary(\n        Type=Name.Annot,\n        Subtype=Name.Highlight,\n        QuadPoints=[boundingBox[0], boundingBox[3],\n                    boundingBox[2], boundingBox[3],\n                    boundingBox[0], boundingBox[1],\n                    boundingBox[2], boundingBox[1]],\n        Rect=[boundingBox[0], boundingBox[1], boundingBox[2], boundingBox[3]],\n        C=[1, 0, 0],\n        CA=0.5,\n        T=analyzed_bounding_box[\"result\"].entity_type,\n    )\n\n    annotations.append(highlight)\n\n# Add the annotations to the PDF.\npdf.pages[0].Annots = pdf.make_indirect(annotations)\n\n# And save.\npdf.save(\"./sample_data/sample_annotated.pdf\")\n</code></pre> <pre><code>\n</code></pre>"},{"location":"samples/python/example_pdf_annotation/#annotating-pii-in-a-pdf","title":"Annotating PII in a PDF","text":"<p>This sample takes a PDF as an input, extracts the text, identifies PII using Presidio and annotates the PII using highlight annotations.</p>"},{"location":"samples/python/example_pdf_annotation/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, make sure the following packages are installed. For detailed documentation, see the installation docs.</p> <p>Install from PyPI:</p>"},{"location":"samples/python/example_pdf_annotation/#analyze-the-text-in-the-pdf","title":"Analyze the text in the PDF","text":"<p>To extract the text from the PDF, we use the pdf miner library. We extract the text from the PDF at the text container level. This is roughly equivalent to a paragraph. </p> <p>We then use Presidio Analyzer to identify the PII and it's location in the text.</p> <p>The Presidio analyzer is using pre-defined entity recognizers, and offers the option to create custom recognizers.</p>"},{"location":"samples/python/example_pdf_annotation/#create-phrase-bounding-boxes","title":"Create phrase bounding boxes","text":"<p>The next task is to take the character data, and inflate it into full phrase bounding boxes.</p> <p>For example, for an email address, we'll turn the bounding boxes for each character in the email address into one single bounding box.</p>"},{"location":"samples/python/example_pdf_annotation/#add-highlight-annotations","title":"Add highlight annotations","text":"<p>We finally iterate through all the analyzed bounding boxes and create highlight annotations for all of them.</p>"},{"location":"samples/python/example_pdf_annotation/#result","title":"Result","text":"<p>The output from the samples above creates a new PDF. This contains the original content, with text highlight annotations where the PII has been found.</p> <p>Each text annotation contains the name of the entity found.</p>"},{"location":"samples/python/example_pdf_annotation/#note","title":"Note","text":"<p>Before relying on this methodology to detect or markup PII from a PDF, please be aware of the following:</p>"},{"location":"samples/python/example_pdf_annotation/#text-extraction","title":"Text extraction","text":"<p>We purposely use a different library specifically for extracting text from the PDF. This is because text extraction is hard to get right, and it's worth using a library specifically developed with the purpose in mind.</p> <p>For more details, see:</p> <p>https://pdfminersix.readthedocs.io/en/latest/topic/converting_pdf_to_text.html</p> <p>That said, even with a purpose built library, there may be occasions where PII is present and visible in a PDF, but it is not detected by the sample code.</p> <p>This includes, but is not limited to:</p> <ul> <li>Text that cannot be reliable extracted to be analyzed. (e.g. incorrect spacing, wrong reading order)</li> <li>Text present in previous iterations of the PDF which is hidden from text extraction. (See incremental editing)</li> <li>Text present in images. (requires OCRing)</li> <li>Text present in annotations.</li> </ul>"},{"location":"samples/python/getting_entity_values/","title":"Getting entity values","text":"<pre><code># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</code></pre> <p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/getting_entity_values.ipynb</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\n</code></pre> <pre><code>analyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n</code></pre> <pre><code>text_to_analyze = \"Hi my name is Charles Darwin and my email is cdarwin@hmsbeagle.org\"\nanalyzer_results = analyzer.analyze(text_to_analyze, language=\"en\")\n</code></pre> <p>A naive approach for getting the text values:</p> <pre><code>[(text_to_analyze[res.start:res.end], res.start, res.end) for res in analyzer_results]\n</code></pre> <pre>\n<code>[('cdarwin@hmsbeagle.org', 45, 66),\n ('Charles Darwin', 14, 28),\n ('hmsbeagle.org', 53, 66)]</code>\n</pre> <p>Another option is to set up a custom operator* which runs an identity function (<code>lambda x: x</code>). This operator doesn't really anonymize, but replaces the identified value with itself. This is useful as the Anonymizer handles the overlaps automatically. </p> <p>&gt; In this example, the URL (hmsbeagle.org) is contained in the email address, so it's ommitted from the final result.</p> <p>* an <code>Operator</code> is usually either an <code>Anonymizer</code> or <code>Deanonymizer</code> on the presidio-anonymizer library/</p> <pre><code>anonymized_results = anonymizer.anonymize(\n        text=text_to_analyze,\n        analyzer_results=analyzer_results,            \n        operators={\"DEFAULT\": OperatorConfig(\"custom\", {\"lambda\": lambda x: x})}        \n    )\n</code></pre> <p>The operator defined here is <code>DEFAULT</code>, meaning it will be used for all entities. The <code>OperatorConfig</code> is a custom one and the labmda is the identity function.</p> <p>Output text, start and end locations for each detected entity</p> <pre><code>[(item.text, item.start, item.end) for item in anonymized_results.items]\n</code></pre> <pre>\n<code>[('cdarwin@hmsbeagle.org', 45, 66), ('Charles Darwin', 14, 28)]</code>\n</pre> <p>A third option would be to use the <code>keep</code> operator:</p> <pre><code>anonymized_results_with_keep = anonymizer.anonymize(\n        text=text_to_analyze,\n        analyzer_results=analyzer_results,            \n        operators={\"DEFAULT\": OperatorConfig(\"keep\")}        \n    )\n[(item.text, item.start, item.end) for item in anonymized_results_with_keep.items]\n</code></pre> <pre>\n<code>[('cdarwin@hmsbeagle.org', 45, 66), ('Charles Darwin', 14, 28)]</code>\n</pre>"},{"location":"samples/python/getting_entity_values/#getting-a-list-of-all-identified-texts","title":"Getting a list of all identified texts","text":"<p>This sample illustrates how to get a list of all the identified PII entities using Presidio Analyzer for detection and a custom Presidio Anonymizer operator.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/","title":"Image redaction allow list approach","text":"<pre><code>!pip install presidio_analyzer presidio_anonymizer presidio_image_redactor\n!python -m spacy download en_core_web_lg\n</code></pre> <pre><code>from PIL import Image\nimport pydicom\nfrom presidio_analyzer import Pattern, PatternRecognizer\nfrom presidio_image_redactor import ImageRedactorEngine, DicomImageRedactorEngine\nimport matplotlib.pyplot as plt\n</code></pre> <p>Initialize engines used for image redaction</p> <pre><code># Standard images\nengine = ImageRedactorEngine()\n\n# DICOM images\ndicom_engine = DicomImageRedactorEngine()\npadding_width = 3\nfill = \"background\"\n</code></pre> <pre><code>image = Image.open(\"../../image-redactor/ocr_text.png\")\ndisplay(image)\n</code></pre> <p>And this is what the image looks like with the standard, default behavior redaction.</p> <pre><code>redacted_image = engine.redact(image, (255, 192, 203))\ndisplay(redacted_image)\n</code></pre> <pre><code>instance = pydicom.dcmread(\"./sample_data/0_ORIGINAL.dcm\")\nplt.imshow(instance.pixel_array, cmap=\"gray\")\n</code></pre> <pre>\n<code>&lt;matplotlib.image.AxesImage at 0x1edb761bd00&gt;</code>\n</pre> <p>And this is what the image looks like with the standard, default behavior redaction.</p> <pre><code>results = dicom_engine.redact(instance, padding_width=padding_width, fill=fill)\nplt.imshow(results.pixel_array, cmap=\"gray\")\n</code></pre> <pre>\n<code>&lt;matplotlib.image.AxesImage at 0x1edb7da4610&gt;</code>\n</pre> <p>Redacted image when using the allow list approach</p> <pre><code>redacted_image = engine.redact(image, (255, 192, 203), allow_list=[\"David\", \"(212) 555-1234\"])\ndisplay(redacted_image)\n</code></pre> <p>Redacted DICOM image when using the allow list approach</p> <pre><code>results = dicom_engine.redact(instance, padding_width=padding_width, fill=fill, allow_list=[\"DAVIDSON\"])\nplt.imshow(results.pixel_array, cmap=\"gray\")\n</code></pre> <pre>\n<code>&lt;matplotlib.image.AxesImage at 0x1edaedf6340&gt;</code>\n</pre> <p>Create a custom recognizer to mark all text as sensitive</p> <pre><code>pattern_all_text = Pattern(name=\"any_text\", regex=r\"(?s).*\", score=0.5)\ncustom_recognizer = PatternRecognizer(\n    supported_entity=\"TEXT\",\n    patterns=[pattern_all_text],\n)\n</code></pre> <p>Then pass that custom recognizer into your redactor engine as an ad-hoc recognizer</p> <pre><code># Standard image\nredacted_image = engine.redact(\n    image,\n    (255, 192, 203),\n    ad_hoc_recognizers = [custom_recognizer], # you can pass in multiple ad-hoc recognizers\n    allow_list=[\"This\", \"project\",]\n)\ndisplay(redacted_image)\n</code></pre> <pre><code># DICOM image\nresults = dicom_engine.redact(\n    instance,\n    padding_width = padding_width,\n    fill = fill,\n    ad_hoc_recognizers = [custom_recognizer], # you can pass in multiple ad-hoc recognizers\n    allow_list = [\"DAVIDSON\", \"L\"]\n)\nplt.imshow(results.pixel_array, cmap=\"gray\")\n</code></pre> <pre>\n<code>&lt;matplotlib.image.AxesImage at 0x1edaee5f160&gt;</code>\n</pre> <p>Create a custom recognizer that marks all text as sensitive</p> <pre><code>pattern_all_text = Pattern(name=\"any_text\", regex=r\"(?s).*\", score=0.5)\ncustom_recognizer = PatternRecognizer(\n    supported_entity=\"TEXT\",\n    patterns=[pattern_all_text],\n)\n</code></pre> <p>Specify an empty allow list such that no text is allowed</p> <pre><code># Standard image\nredacted_image = engine.redact(\n    image,\n    (255, 192, 203),\n    ad_hoc_recognizers = [custom_recognizer],\n    allow_list=[]\n)\ndisplay(redacted_image)\n</code></pre> <pre><code># DICOM image\nresults = dicom_engine.redact(\n    instance,\n    padding_width = padding_width,\n    fill = fill,\n    ad_hoc_recognizers = [custom_recognizer],\n    allow_list = []\n)\nplt.imshow(results.pixel_array, cmap=\"gray\")\n</code></pre> <pre>\n<code>&lt;matplotlib.image.AxesImage at 0x1edaeee7580&gt;</code>\n</pre> <p>In this case, we see that all text picked up by the OCR is redacted. The \"L\" on the right side of the DICOM image and the single \"a\" in the standard image are still visible because they were not detected by the OCR.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#allow-list-approach-with-image-redaction","title":"Allow list approach with image redaction","text":"<p>This notebook covers how to use the <code>allow_list</code> argument to prevent certain words from being redacted from images and explains how you can use this to implement a strict redact all text approach.</p> <p>&gt; Note: Always place the <code>allow_list</code> argument last in your redact call as this is considered a text analyzer kwarg.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, make sure presidio and the latest version of Tesseract OCR are installed. For detailed documentation, see the installation docs.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#0-imports-and-initializations","title":"0. Imports and initializations","text":""},{"location":"samples/python/image_redaction_allow_list_approach/#1-example-images","title":"1. Example images","text":"<p>In this notebook, we will use the following examples images.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#11-standard-example-image","title":"1.1 Standard example image","text":""},{"location":"samples/python/image_redaction_allow_list_approach/#12-dicom-medical-image","title":"1.2 DICOM medical image","text":"<p>For more information on DICOM image redaction, please see example_dicom_image_redactor.ipynb and the Image redactor module documentation.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#2-scenario-prevent-some-words-from-being-redacted","title":"2. Scenario: Prevent some words from being redacted","text":"<p>Whether using the default recognizer, registering your own custom recognizer, or using ad-hoc recognizers to identify sensitive entities, there may be times where you do not want certain words redacted.</p> <p>In these cases, we can use the <code>allow_list</code> argument passed into the <code>ImageAnalyzerEngine</code> via our redact engine to preserve specified strings.</p> <p>&gt; Note: The <code>allow_list</code> argument should be positioned as the last argument in the redact call as it is considered a text analyzer kwarg.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#3-scenario-only-allow-specific-words-while-redacting-all-other-text","title":"3. Scenario : Only allow specific words while redacting all other text","text":"<p>In some cases, we want to preserve certain words and redact all other text in the image. We can create an ad-hoc recognizer that considers all text as sensitive and couple that with the allow list.</p> <p>&gt; Note: The <code>allow_list</code> argument should be positioned as the last argument in the redact call as it is considered a text analyzer kwarg.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#4-scenario-redact-all-text-on-the-image","title":"4. Scenario: Redact all text on the image","text":"<p>When it is critical to minimize False Negatives during the redaction process, we recommend using a \"redact all\" approach to redact all detected text.</p> <p>As with the other scenarios, good OCR performance is critical in ensuring the analyzer can pick up on all text in the image. False Negatives may still occur with images if the OCR fails to pick up on all the text.</p> <p>&gt; Note: The <code>allow_list</code> argument should be positioned as the last argument in the redact call as it is considered a text analyzer kwarg.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#conclusion","title":"Conclusion","text":"<p>The <code>allow_list</code> argument can be used in both standard and DICOM iamge redaction to allow specified words to avoid redaction. This can also be used to redact all detected text.</p> <p>While this approach allows for the greatest recall in terms of redacting sensitive text, it is dependent on the performance of the text detection which comes before analysis.</p>"},{"location":"samples/python/image_redaction_allow_list_approach/#tips-for-improved-performance","title":"Tips for improved performance","text":"<ol> <li>To avoid False Negative redaction, we recommend applying preprocessing techniques or experimenting with parameters to improve OCR performance or use an alternative approach to improve text detection. We are actively working on adding a preprocessing module to allow for easy application of image preprocessing methods.</li> <li>We recommend augmenting your allowlist to consider various casing and punctuation.</li> </ol>"},{"location":"samples/python/integrating_with_external_services/","title":"Integrating with external services","text":"<pre><code># download presidio\n!pip install presidio_analyzer presidio_anonymizer!python -m spacy download en_core_web_lg\n</code></pre> <p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/integrating_with_external_services.ipynb</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom text_analytics.example_text_analytics_recognizer import TextAnalyticsEntityCategory, TextAnalyticsRecognizer\n</code></pre> <ol> <li>Define which entities to get from Text Analytics</li> </ol> <pre><code>ta_entities = [\n    TextAnalyticsEntityCategory(name=\"Person\",\n                                entity_type=\"NAME\",\n                                supported_languages=[\"en\"]),\n    TextAnalyticsEntityCategory(name=\"Age\",\n                                entity_type=\"AGE\",\n                                subcategory = \"Age\", \n                                supported_languages=[\"en\"]),\n    TextAnalyticsEntityCategory(name=\"InternationlBankingAccountNumber\",\n                                entity_type=\"IBAN\",\n                                supported_languages=[\"en\"])]\n</code></pre> <p>For a full list of entities: https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/named-entity-types?tabs=personal</p> <ol> <li>Instantiate the remote recognizer object (In this case <code>TextAnalyticsRecognizer</code>)</li> </ol> <pre><code>text_analytics_recognizer = TextAnalyticsRecognizer(\n        text_analytics_key=\"&lt;your_text_analytics_key&gt;\",\n        text_analytics_endpoint=\"&lt;your_text_analytics_endpoint&gt;\",\n        text_analytics_categories = ta_entities)\n</code></pre> <ol> <li>Add the new recognizer to the list of recognizers and run the <code>PresidioAnalyzer</code></li> </ol> <pre><code>analyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(text_analytics_recognizer)\n\nresults = analyzer.analyze(\n    text=\"David is 30 years old. His IBAN: IL150120690000003111111\", language=\"en\"\n)\nprint(results)\n</code></pre>"},{"location":"samples/python/integrating_with_external_services/#integrating-external-modelsservices-with-presidio","title":"Integrating external models/services with Presidio","text":"<p>Presidio analyzer is comprised of a set of PII recognizers which can run local or remotely.  In this notebook we'll give an example of integrating an external service into Presidio-Analyzer.</p>"},{"location":"samples/python/integrating_with_external_services/#azure-text-analytics","title":"Azure Text Analytics","text":"<p>Azure Text Analytics is a cloud-based service that provides advanced natural language processing over raw text. One of its main functions includes  Named Entity Recognition (NER), which has the ability to identify different entities in text and categorize them into pre-defined classes or types.</p>"},{"location":"samples/python/integrating_with_external_services/#supported-entity-categories-in-the-text-analytics-api","title":"Supported entity categories in the Text Analytics API","text":"<p>Text Analytics supports multiple PII entity categories. The Text Analytics service runs a predictive model to identify and categorize named entities from an input document. The service's latest version includes the ability to detect personal (PII) and health (PHI) information. A list of all supported entities can be found in the official documentation.</p>"},{"location":"samples/python/integrating_with_external_services/#prerequisites","title":"Prerequisites","text":"<p>To use Text Analytics with Preisido, an Azure Text Analytics resource should first be created under an Azure subscription. Follow the official documentation for instructions. The key and endpoint, generated once the resource is created, should replace the placeholders <code>&lt;your_text_analytics_key&gt;</code> and <code>&lt;your_text_analytics_endpoint&gt;</code> in this notebook, respectively. </p>"},{"location":"samples/python/integrating_with_external_services/#text-analytics-recognizer","title":"Text Analytics Recognizer","text":"<p>In this example we will use the <code>TextAnalyticsRecognizer</code> sample implementation. This class extends Presidio's Remote Recognizer for calling the Text Analytics service REST API. For additional information of a remote recognizer, see the ExampleRemoteRecognizer sample. </p>"},{"location":"samples/python/keep_entities/","title":"Keep entities","text":"<pre><code># download presidio\n!pip install presidio_analyzer presidio_anonymizer!python -m spacy download en_core_web_lg\n</code></pre> <p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/keep_entities.ipynb</p> <pre><code>from presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import RecognizerResult, OperatorConfig\n</code></pre> <pre><code>engine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text,\n# analyzer results (potentially coming from presidio-analyzer)\n# and 'keep' operator on &lt;person&gt; PIIs\nanonymize_result = engine.anonymize(\n    text=\"My name is James Bond, I live in London\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),\n        RecognizerResult(entity_type=\"LOCATION\", start=33, end=39, score=0.8),\n    ],\n    operators={\n        \"PERSON\": OperatorConfig(\"keep\"),\n        \"DEFAULT\": OperatorConfig(\"replace\"),\n    },\n)\n</code></pre> <pre><code>anonymize_result\n</code></pre> <pre>\n<code>text: My name is James Bond, I live in &lt;LOCATION&gt;\nitems:\n[\n    {'start': 33, 'end': 43, 'entity_type': 'LOCATION', 'text': '&lt;LOCATION&gt;', 'operator': 'replace'},\n    {'start': 11, 'end': 21, 'entity_type': 'PERSON', 'text': 'James Bond', 'operator': 'keep'}\n]</code>\n</pre>"},{"location":"samples/python/keep_entities/#keeping-some-piis-from-being-anonymized","title":"Keeping some PIIs from being anonymized","text":"<p>This sample shows how to use Presidio's <code>keep</code> anonymizer to keep some of the identified PIIs in the output string</p>"},{"location":"samples/python/keep_entities/#set-up-imports","title":"Set up imports","text":""},{"location":"samples/python/keep_entities/#presidio-anonymizer-keep-person-names","title":"Presidio Anonymizer: Keep person names","text":"<p>This example input has 2 PIIs, an person name and a location. We configure the anonymizer to replace the location name with a placeholder, but keep the person name unmodified.</p>"},{"location":"samples/python/keep_entities/#result-name-unmodified-but-tracked","title":"Result: Name unmodified, but tracked","text":"<p>The person name is preserved in the result text, but remains tracked in the items list.</p>"},{"location":"samples/python/plot_custom_bboxes/","title":"Plot custom bboxes","text":"<pre><code>!pip install presidio_analyzer presidio_anonymizer presidio_image_redactor\n!python -m spacy download en_core_web_lg\n</code></pre> <pre><code>from PIL import Image\nimport pydicom\nfrom presidio_image_redactor import ImageAnalyzerEngine, ImagePiiVerifyEngine, DicomImagePiiVerifyEngine\nimport matplotlib.pyplot as plt\n</code></pre> <p>Initialize engines used for image redaction</p> <pre><code># Image analyzer engine\nimage_analyzer_engine = ImageAnalyzerEngine()\n\n# Verification engines\nverify_engine = ImagePiiVerifyEngine() # standard images\ndicom_verify_engine = DicomImagePiiVerifyEngine() # DICOM images\npadding_width = 3\n</code></pre> <pre><code>image = Image.open(\"../../image-redactor/ocr_text.png\")\ndisplay(image)\n</code></pre> <p>And this is what the image looks like with the standard, default behavior of the verification engine.</p> <pre><code>verify_image = verify_engine.verify(image, display_image=True)\n</code></pre> <pre><code>instance = pydicom.dcmread(\"./sample_data/0_ORIGINAL.dcm\")\nplt.imshow(instance.pixel_array, cmap=\"gray\")\n</code></pre> <pre>\n<code>&lt;matplotlib.image.AxesImage at 0x1f2a91512b0&gt;</code>\n</pre> <p>And this is what the image looks like with the standard, default behavior verification.</p> <pre><code>dicom_image, dicom_ocr_bboxes, dicom_analyzer_bboxes = dicom_verify_engine.verify_dicom_instance(\n    instance,\n    padding_width=padding_width,\n    display_image=True\n)\n</code></pre> <pre><code>len(dicom_ocr_bboxes)\n</code></pre> <pre>\n<code>9</code>\n</pre> <pre><code>len(dicom_analyzer_bboxes)\n</code></pre> <pre>\n<code>4</code>\n</pre> <p>Let's look at the format of the analyzer results bounding boxes returned by the DICOM verification engine. This the general format expected of custom bounding boxes passed into <code>add_custom_bbox()</code>.</p> <pre><code>type(dicom_analyzer_bboxes)\n</code></pre> <pre>\n<code>list</code>\n</pre> <pre><code>type(dicom_analyzer_bboxes[0])\n</code></pre> <pre>\n<code>dict</code>\n</pre> <pre><code>dicom_analyzer_bboxes[0]\n</code></pre> <pre>\n<code>{'entity_type': 'PERSON',\n 'score': 0.85,\n 'left': 3,\n 'top': 3,\n 'width': 241,\n 'height': 37,\n 'is_PII': True}</code>\n</pre> <p>For our custom bounding boxes, the \"entity_type\" and \"is_PII\" fields are optional and \"score\" is not used. However, the \"is_PII\" field is helpful in visually identifying which bounding boxes from your given bounding box list are considered PII.</p> <pre><code># Example provided bounding box list\ngiven_bboxes = [\n    {\n        'entity_type': 'PERSON',\n         'left': 3,\n         'top': 3,\n         'width': 241,\n         'height': 37,\n         'is_PII': True\n    },\n    {\n        'entity_type': 'PERSON',\n         'left': 179,\n         'top': 150,\n         'width': 300,\n         'height': 74,\n         'is_PII': False\n    }\n]\n</code></pre> <p>Let's plot the given bounding boxes.</p> <pre><code># Let's plot with the given bounding boxes\ntest_image = image_analyzer_engine.add_custom_bboxes(image, given_bboxes, show_text_annotation=True)\n</code></pre> <p>While the placement of our example bounding boxes here is not ideal, this shows how you can easily visualize if a provided set of bounding boxes match your expectations.</p>"},{"location":"samples/python/plot_custom_bboxes/#plot-custom-bounding-boxes","title":"Plot custom bounding boxes","text":"<p>This notebook covers how to use the image verification engines to plot custom bounding boxes. This can be helpful when you want to verify a provided set of bounding boxes against an image.</p>"},{"location":"samples/python/plot_custom_bboxes/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, make sure presidio and the latest version of Tesseract OCR are installed. For detailed documentation, see the installation docs.</p>"},{"location":"samples/python/plot_custom_bboxes/#0-imports-and-initializations","title":"0. Imports and initializations","text":""},{"location":"samples/python/plot_custom_bboxes/#1-example-images","title":"1. Example images","text":"<p>In this notebook, we will use the following examples images.</p>"},{"location":"samples/python/plot_custom_bboxes/#11-standard-example-image","title":"1.1 Standard example image","text":""},{"location":"samples/python/plot_custom_bboxes/#12-dicom-medical-image","title":"1.2 DICOM medical image","text":"<p>For more information on DICOM image redaction, please see example_dicom_image_redactor.ipynb and the Image redactor module documentation.</p>"},{"location":"samples/python/plot_custom_bboxes/#2-plot-custom-bounding-boxes","title":"2. Plot custom bounding boxes","text":"<p>There may be situations where you want to visually validate whether a set of given bounding boxes match your expectations on an image. In these cases, we can call <code>ImageAnalyzerEngine.add_custom_bbox()</code> instead of using the verify methods which include OCR and text analysis.</p>"},{"location":"samples/python/presidio_notebook/","title":"Presidio notebook","text":"<pre><code># download presidio\n!pip install presidio_analyzer presidio_anonymizer\n!python -m spacy download en_core_web_lg\n</code></pre> <p>Path to notebook: https://www.github.com/microsoft/presidio/blob/main/docs/samples/python/presidio_notebook.ipynb</p> <pre><code>from presidio_analyzer import AnalyzerEngine, PatternRecognizer\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\nimport json\nfrom pprint import pprint\n</code></pre> <pre><code>text_to_anonymize = \"His name is Mr. Jones and his phone number is 212-555-5555\"\n</code></pre> <pre><code>analyzer = AnalyzerEngine()\nanalyzer_results = analyzer.analyze(text=text_to_anonymize, entities=[\"PHONE_NUMBER\"], language='en')\n\nprint(analyzer_results)\n</code></pre> <pre><code>titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n                                      deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n\npronoun_recognizer = PatternRecognizer(supported_entity=\"PRONOUN\",\n                                       deny_list=[\"he\", \"He\", \"his\", \"His\", \"she\", \"She\", \"hers\", \"Hers\"])\n\nanalyzer.registry.add_recognizer(titles_recognizer)\nanalyzer.registry.add_recognizer(pronoun_recognizer)\n\nanalyzer_results = analyzer.analyze(text=text_to_anonymize,\n                            entities=[\"TITLE\", \"PRONOUN\"],\n                            language=\"en\")\nprint(analyzer_results)\n</code></pre> <p>Call Presidio Analyzer and get analyzed results with all the configured recognizers - default and new custom recognizers</p> <pre><code>analyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')\n\nanalyzer_results\n</code></pre> <pre><code>anonymizer = AnonymizerEngine()\n\nanonymized_results = anonymizer.anonymize(\n    text=text_to_anonymize,\n    analyzer_results=analyzer_results,    \n    operators={\"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"&lt;anonymized&gt;\"}), \n                        \"PHONE_NUMBER\": OperatorConfig(\"mask\", {\"type\": \"mask\", \"masking_char\" : \"*\", \"chars_to_mask\" : 12, \"from_end\" : True}),\n                        \"TITLE\": OperatorConfig(\"redact\", {})}\n)\n\nprint(f\"text: {anonymized_results.text}\")\nprint(\"detailed response:\")\n\npprint(json.loads(anonymized_results.to_json()))\n</code></pre> <pre><code>\n</code></pre>"},{"location":"samples/python/presidio_notebook/#analyze-text-for-pii-entities","title":"Analyze Text for PII Entities","text":"<p>Using Presidio Analyzer, analyze a text to identify PII entities.  The Presidio analyzer is using pre-defined entity recognizers, and offers the option to create custom recognizers.</p> <p>The following code sample will:</p> <ul> <li>Set up the Analyzer engine: load the NLP module (spaCy model by default) and other PII recognizers</li> <li>Call analyzer to get analyzed results for \"PHONE_NUMBER\" entity type</li> </ul>"},{"location":"samples/python/presidio_notebook/#create-custom-pii-entity-recognizers","title":"Create Custom PII Entity Recognizers","text":"<p>Presidio Analyzer comes with a pre-defined set of entity recognizers. It also allows adding new recognizers without changing the analyzer base code, by creating custom recognizers.  In the following example, we will create two new recognizers of type <code>PatternRecognizer</code> to identify titles and pronouns in the analyzed text. A <code>PatternRecognizer</code> is a PII entity recognizer which uses regular expressions or deny-lists.</p> <p>The following code sample will: - Create custom recognizers - Add the new custom recognizers to the analyzer - Call analyzer to get results from the new recognizers</p>"},{"location":"samples/python/presidio_notebook/#anonymize-text-with-identified-pii-entities","title":"Anonymize Text with Identified PII Entities","text":"<p>Presidio Anonymizer iterates over the Presidio Analyzer result, and provides anonymization capabilities for the identified text. The anonymizer provides 5 types of anonymizers - replace, redact, mask, hash and encrypt. The default is replace</p> <p>The following code sample will:</p> <ol> <li>Setup the anonymizer engine </li> <li>Create an anonymizer request - text to anonymize, list of anonymizers to apply and the results from the analyzer request</li> <li>Anonymize the text</li> </ol>"},{"location":"samples/python/streamlit/","title":"Simple demo website for Presidio","text":"<p>Here's a simple app, written in pure Python, to create a demo website for Presidio. The app is based on the streamlit package.</p> <p>A live version can be found here: https://huggingface.co/spaces/presidio/presidio_demo</p>"},{"location":"samples/python/streamlit/#requirements","title":"Requirements","text":"<ol> <li>Clone the repo and move to the <code>docs/samples/python/streamlit</code> folder</li> <li>Install dependencies (preferably in a virtual environment)</li> </ol> <pre><code>pip install -r requirements\n</code></pre> <p>Note: This would install additional packages such as <code>transformers</code> and <code>flair</code> which are not mandatory for using Presidio.</p> <ol> <li>Optional: Update the <code>analyzer_engine</code> and <code>anonymizer_engine</code> functions for your specific implementation (in <code>presidio_helpers.py</code>).</li> <li>Start the app:</li> </ol> <pre><code>streamlit run presidio_streamlit.py\n</code></pre> <ol> <li>Consider adding an <code>.env</code> file with the following environment variables, for further customizability: <pre><code>TA_KEY=YOUR_TEXT_ANALYTICS_KEY\nTA_ENDPOINT=YOUR_TEXT_ANALYTICS_ENDPOINT\nOPENAI_TYPE=\"Azure\" #or \"openai\"\nOPENAI_KEY=YOUR_OPENAI_KEY\nOPENAI_API_VERSION = \"2023-05-15\"\nAZURE_OPENAI_ENDPOINT=YOUR_AZURE_OPENAI_AZURE_OPENAI_ENDPOINT\nAZURE_OPENAI_DEPLOYMENT=text-davinci-003\nALLOW_OTHER_MODELS=true #true if the user could download new models\n</code></pre></li> </ol>"},{"location":"samples/python/streamlit/#output","title":"Output","text":"<p>Output should be similar to this screenshot: </p>"},{"location":"samples/python/text_analytics/","title":"Azure Text Analytics Integration","text":""},{"location":"samples/python/text_analytics/#introduction","title":"Introduction","text":"<p>Azure Text Analytics is a cloud-based service that provides advanced natural language processing over raw text. One of its main functions includes  Named Entity Recognition (NER), which has the ability to identify different entities in text and categorize them into pre-defined classes or types. This document will demonstrate Presidio integration with Azure Text Analytics.</p>"},{"location":"samples/python/text_analytics/#supported-entity-categories-in-the-text-analytics-api","title":"Supported entity categories in the Text Analytics API","text":"<p>Text Analytics supports multiple PII entity categories. The Text Analytics service runs a predictive model to identify and categorize named entities from an input document. The service's latest version includes the ability to detect personal (PII) and health (PHI) information. A list of all supported entities can be found in the official documentation.</p>"},{"location":"samples/python/text_analytics/#prerequisites","title":"Prerequisites","text":"<p>To use Text Analytics with Preisido, an Azure Text Analytics resource should first be created under an Azure subscription. Follow the official documentation for instructions. The key and endpoint, generated once the resource is created,  will be used when integrating with Text Analytics, using a Presidio Text Analytics recognizer.</p>"},{"location":"samples/python/text_analytics/#text-analytics-recognizer","title":"Text Analytics Recognizer","text":"<p>Sample implementation of a <code>TextAnalyticsRecognizer</code>. The sample suggests an implementation of a Remote Recognizer calling the Text Analytics service REST API. There is also an alternative solution of using <code>azure-ai-textanalytics</code> python package, replacing the client implementation in the example. The sample reads from a yaml file,  defining the entities that should be recognized by Text Analytics, and their corresponding Preisdio entity types. Use the supported entity categories reference above to extend the  list of entities defined in the file, with the entities that you want Text Analytics to recognize.</p>"},{"location":"samples/python/transformers_recognizer/","title":"Add a Transformers model based EntityRecognizer","text":"<p>Note</p> <p>This example demonstrates how to create a Presidio Recognizer. To integrate a transformers model as a Presidio NLP Engine, see this documentation.</p> <p>We allow these two options, as a user might want to have multiple NER models running in parallel. In this case, one can create multiple <code>EntityRecognizer</code> instances, each serving a different model. If you only plan to use one NER model, consider creating a <code>TransformersNlpEngine</code> instead of the <code>TransformersRecognizer</code> described in this document.</p> <p>When initializing the <code>TransformersRecognizer</code>, choose from the following options:</p> <ol> <li> <p>A string referencing an uploaded model to HuggingFace. See the different available options for models here.</p> </li> <li> <p>Initialize your own <code>TokenClassificationPipeline</code> instance using your custom transformers model and use it for inference.</p> </li> <li> <p>Provide the path to your own local custom trained model.</p> </li> </ol> <p>Note</p> <p>For each combination of model &amp; dataset, it is recommended to create a configuration object which includes setting necessary parameters for getting the correct results. Please reference this configuraion.py file for examples.</p>"},{"location":"samples/python/transformers_recognizer/#example-code","title":"Example Code","text":"<p>This example code uses a <code>TransformersRecognizer</code> for NER, and removes the default <code>SpacyRecognizer</code>. In order to be able to use spaCy features such as lemmas, we introduce the small (and faster) <code>en_core_web_sm</code> model.</p> <p>link to full TransformersRecognizer code</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nimport spacy\n\nmodel_path = \"obi/deid_roberta_i2b2\"\nsupported_entities = BERT_DEID_CONFIGURATION.get(\n    \"PRESIDIO_SUPPORTED_ENTITIES\")\ntransformers_recognizer = TransformersRecognizer(model_path=model_path,\n                                                 supported_entities=supported_entities)\n\n# This would download a large (~500Mb) model on the first run\ntransformers_recognizer.load_transformer(**BERT_DEID_CONFIGURATION)\n\n# Add transformers model to the registry\nregistry = RecognizerRegistry()\nregistry.add_recognizer(transformers_recognizer)\nregistry.remove_recognizer(\"SpacyRecognizer\")\n\n# Use small spacy model, for faster inference.\nif not spacy.util.is_package(\"en_core_web_sm\"):\n    spacy.cli.download(\"en_core_web_sm\")\n\nnlp_configuration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n}\n\nnlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n\nanalyzer = AnalyzerEngine(registry=registry, nlp_engine=nlp_engine)\n\nsample = \"My name is John and I live in NY\"\nresults = analyzer.analyze(sample, language=\"en\",\n                           return_decision_process=True,\n                           )\nprint(\"Found the following entities:\")\nfor result in results:\n    print(result, '----', sample[result.start:result.end])\n</code></pre>"},{"location":"tutorial/","title":"Tutorial: Customization in Microsoft Presidio","text":"<p>This tutorials covers different customization use cases to:</p> <ol> <li>Adapt Presidio to detect new types of PII entities.</li> <li>Adapt Presidio to detect PII entities in a new language.</li> <li>Embed new types of detection modules into Presidio, to improve the coverage of the service.</li> <li>Operate on identified entities: simple de-identification, custom operators and encryption.</li> </ol>"},{"location":"tutorial/#table-of-contents","title":"Table of contents","text":"<ul> <li>Getting started</li> <li>Deny-list based recognizers</li> <li>Regex based PII recognition</li> <li>Rule based logic recognizer</li> <li>Supporting new models and languages</li> <li>Calling an external service for PII detection</li> <li>Using context words</li> <li>Tracing the decision process</li> <li>Loading recognizers from file</li> <li>Ad-Hoc recognizers</li> <li>Simple anonymization</li> <li>Custom anonymization</li> <li>Encryption/Decryption</li> </ul>"},{"location":"tutorial/00_getting_started/","title":"Getting started","text":""},{"location":"tutorial/00_getting_started/#installation","title":"Installation","text":"<p>First, let's install presidio using <code>pip</code>. For detailed documentation, see the installation docs. Install from PyPI:</p> <pre><code>pip install presidio_analyzer\npip install presidio_anonymizer\npython -m spacy download en_core_web_lg\n</code></pre>"},{"location":"tutorial/00_getting_started/#simple-flow","title":"Simple flow","text":"<p>A simple call to Presidio Analyzer:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\ntext = \"His name is Mr. Jones and his phone number is 212-555-5555\"\n\nanalyzer = AnalyzerEngine()\nanalyzer_results = analyzer.analyze(text=text, language=\"en\")\n\nprint(analyzer_results)\n</code></pre> <p>Next, we'll go over ways to customize Presidio to specific needs by adding PII recognizers, using context words, NER models and more.</p>"},{"location":"tutorial/01_deny_list/","title":"Example 1: Deny-list based PII recognition","text":"<p>In this example, we will pass a short list of tokens which should be marked as PII if detected. First, let's define the tokens we want to treat as PII. In this case it would be a list of titles:</p> <pre><code>titles_list = [\n    \"Sir\",\n    \"Ma'am\",\n    \"Madam\",\n    \"Mr.\",\n    \"Mrs.\",\n    \"Ms.\",\n    \"Miss\",\n    \"Dr.\",\n    \"Professor\",\n]\n</code></pre> <p>Second, let's create a <code>PatternRecognizer</code> which would scan for those titles, by passing a <code>deny_list</code>:</p> <pre><code>from presidio_analyzer import PatternRecognizer\n\ntitles_recognizer = PatternRecognizer(supported_entity=\"TITLE\", deny_list=titles_list)\n</code></pre> <p>At this point we can call our recognizer directly:</p> <pre><code>from presidio_analyzer import PatternRecognizer\n\ntext1 = \"I suspect Professor Plum, in the Dining Room, with the candlestick\"\nresult = titles_recognizer.analyze(text1, entities=[\"TITLE\"])\nprint(f\"Result:\\n {result}\")\n</code></pre> <p>Finally, let's add this new recognizer to the list of recognizers used by the Presidio <code>AnalyzerEngine</code>:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(titles_recognizer)\n</code></pre> <p>When initializing the <code>AnalyzerEngine</code>, Presidio loads all available recognizers, including the <code>NlpEngine</code> used to detect entities, and extract tokens, lemmas and other linguistic features.</p> <p>Let's run the analyzer with the new recognizer in place:</p> <pre><code>results = analyzer.analyze(text=text1, language=\"en\")\n</code></pre> <pre><code>print(\"Results:\")\nprint(results)\n</code></pre> <p>As expected, both the name \"Plum\" and the title were identified as PII:</p> <pre><code>print(\"Identified these PII entities:\")\nfor result in results:\n    print(f\"- {text1[result.start:result.end]} as {result.entity_type}\")\n</code></pre>"},{"location":"tutorial/02_regex/","title":"Example 2: Regular-expressions based PII recognition","text":"<p>Another simple recognizer we can add is based on regular expressions. Let's assume we want to be extremely conservative and treat any token which contains a number as PII.</p> <pre><code>from presidio_analyzer import Pattern, PatternRecognizer\n\n# Define the regex pattern in a Presidio `Pattern` object:\nnumbers_pattern = Pattern(name=\"numbers_pattern\", regex=\"\\d+\", score=0.5)\n\n# Define the recognizer with one or more patterns\nnumber_recognizer = PatternRecognizer(\n    supported_entity=\"NUMBER\", patterns=[numbers_pattern]\n)\n</code></pre> <p>Testing the recognizer itself:</p> <pre><code>text2 = \"I live in 510 Broad st.\"\n\nnumbers_result = number_recognizer.analyze(text=text2, entities=[\"NUMBER\"])\n\nprint(\"Result:\")\nprint(numbers_result)\n</code></pre> <p>It's important to mention that recognizers are likely to have errors, both false-positive and false-negative, which would impact the entire performance of Presidio. Consider testing each recognizer on a representative dataset prior to integrating it into Presidio. For more info, see the best practices for developing recognizers documentation.</p>"},{"location":"tutorial/03_rule_based/","title":"Example 3: Rule based logic recognizer","text":"<p>Taking the numbers recognizer one step further, let's say we also would like to detect numbers within words, e.g. \"Number One\". We can leverage the underlying <code>spaCy</code> token attributes, or write our own logic to detect such entities.</p> <p>Notes:</p> <ul> <li> <p>In this example we would create a new class, which implements <code>EntityRecognizer</code>, the basic recognizer in Presidio. This abstract class requires us to implement the <code>load</code> method and <code>analyze</code> method.</p> </li> <li> <p>Each recognizer accepts an object of type <code>NlpArtifacts</code>, which holds pre-computed attributes on the input text.</p> </li> </ul> <p>A new recognizer should have this structure:</p> <pre><code>from typing import List\nfrom presidio_analyzer import EntityRecognizer, RecognizerResult\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n\n\nclass MyRecognizer(EntityRecognizer):\n    def load(self) -&gt; None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Logic for detecting a specific PII\n        \"\"\"\n        pass\n</code></pre> <p>For example, detecting numbers in either numerical or alphabetic (e.g. Forty five) form:</p> <pre><code>from typing import List\nfrom presidio_analyzer import EntityRecognizer, RecognizerResult\nfrom presidio_analyzer.nlp_engine import NlpArtifacts\n\n\nclass NumbersRecognizer(EntityRecognizer):\n\n    expected_confidence_level = 0.7  # expected confidence level for this recognizer\n\n    def load(self) -&gt; None:\n        \"\"\"No loading is required.\"\"\"\n        pass\n\n    def analyze(\n        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n    ) -&gt; List[RecognizerResult]:\n        \"\"\"\n        Analyzes test to find tokens which represent numbers (either 123 or One Two Three).\n        \"\"\"\n        results = []\n\n        # iterate over the spaCy tokens, and call `token.like_num`\n        for token in nlp_artifacts.tokens:\n            if token.like_num:\n                result = RecognizerResult(\n                    entity_type=\"NUMBER\",\n                    start=token.idx,\n                    end=token.idx + len(token),\n                    score=self.expected_confidence_level,\n                )\n                results.append(result)\n        return results\n\n\n# Instantiate the new NumbersRecognizer:\nnew_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n</code></pre> <p>Since this recognizer requires the <code>NlpArtifacts</code>, we would have to call it as part of the <code>AnalyzerEngine</code> flow:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\n\ntext3 = \"Roberto lives in Five 10 Broad st.\"\nanalyzer = AnalyzerEngine()\nanalyzer.registry.add_recognizer(new_numbers_recognizer)\n\nnumbers_results2 = analyzer.analyze(text=text3, language=\"en\")\nprint(\"Results:\")\nprint(\"\\n\".join([str(res) for res in numbers_results2]))\n</code></pre> <p>The analyzer was able to pick up both numeric and alphabetical numbers, including other types of PII entities from other recognizers (PERSON in this case).</p>"},{"location":"tutorial/04_external_services/","title":"Example 4: Calling an external service/framework for PII detection","text":"<p>In a similar way to example 3, we can write logic to call external services for PII detection. There are two types of external services we support:</p> <ol> <li>Remote services such as a PII detection model hosted somewhere. In this case, the recognizer would do the actual REST request and translate the results to a list of <code>RecognizerResult</code>.</li> <li>Calling PII models from other frameworks, such as transformers or Flair.</li> </ol>"},{"location":"tutorial/04_external_services/#calling-a-remote-service","title":"Calling a remote service","text":"<ol> <li> <p>Documentation on remote recognizers.</p> </li> <li> <p>A sample implementation of a remote recognizer.</p> </li> </ol>"},{"location":"tutorial/04_external_services/#calling-a-model-in-a-different-framework","title":"Calling a model in a different framework","text":"<ul> <li>This example shows a Presidio wrapper for a Flair model.</li> <li>Using a similar approach, we could create wrappers for HuggingFace models, Conditional Random Fields or any other framework.</li> </ul>"},{"location":"tutorial/05_languages/","title":"Example 5: Supporting new models and languages","text":"<p>Two main parts in Presidio handle the text, and should be adapted if a new language is required:</p> <ol> <li>The <code>NlpEngine</code> containing the NLP model which performs tokenization, lemmatization, Named Entity Recognition and other NLP tasks.</li> <li>The different PII recognizers (<code>EntityRecognizer</code> objects) should be adapted or created.</li> </ol>"},{"location":"tutorial/05_languages/#adapting-the-nlp-engine","title":"Adapting the NLP engine","text":"<p>As its internal NLP engine, Presidio supports both spaCy and Stanza. Make sure you download the required models from spacy/stanza prior to using them. More details here. For example, to download the Spanish medium spaCy model: <code>python -m spacy download es_core_news_md</code></p> <p>In this example we will configure Presidio to use spaCy as its underlying NLP framework, with NLP models in English and Spanish:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\n\n# import spacy\n# spacy.cli.download(\"es_core_news_md\")\n\n# Create configuration containing engine name and models\nconfiguration = {\n    \"nlp_engine_name\": \"spacy\",\n    \"models\": [\n        {\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n    ],\n}\n\n# Create NLP engine based on configuration\nprovider = NlpEngineProvider(nlp_configuration=configuration)\nnlp_engine_with_spanish = provider.create_engine()\n\n# Pass the created NLP engine and supported_languages to the AnalyzerEngine\nanalyzer = AnalyzerEngine(\n    nlp_engine=nlp_engine_with_spanish, supported_languages=[\"en\", \"es\"]\n)\n\n# Analyze in different languages\nresults_spanish = analyzer.analyze(text=\"Mi nombre es Morris\", language=\"es\")\nprint(\"Results from Spanish request:\")\nprint(results_spanish)\n\nresults_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\nprint(\"Results from English request:\")\nprint(results_english)\n</code></pre> <p>See this documentation for more details on setting up additional NLP models and languages.</p>"},{"location":"tutorial/05_languages/#using-external-modelsframeworks","title":"Using external models/frameworks","text":"<p>Some languages are not supported by spaCy/Stanza/huggingface, or have very limited support in those. In this case, other frameworks could be leveraged. (see example 4 for more information).</p> <p>Since Presidio requires a spaCy model to be passed, we propose to use a simple spaCy pipeline such as <code>en_core_web_sm</code> as the NLP engine's model, and a recognizer calling an external framework/service as the Named Entity Recognition (NER) model.</p>"},{"location":"tutorial/06_context/","title":"Example 6: Leveraging context words","text":"<p>Presidio has an internal mechanism for leveraging context words. This mechanism would increase the detection confidence of a PII entity in case a specific word appears before or after it.</p> <p>Furthermore, it is possible to create your own context enhancer, if you require a different logic for identifying context terms. The default context-aware enhancer in Presidio is the <code>LemmaContextAwareEnhancer</code> which compares each recognizer's context terms with the lemma of each token in the sentence.</p> <p>In this example we would first implement a zip code recognizer without context, and then add context to see how the confidence changes. Zip regex patterns (essentially 5 digits) are very weak, so we would want the initial confidence to be low, and increased with the existence of context words.</p>"},{"location":"tutorial/06_context/#example-adding-context-words-support-to-recognizers","title":"Example: Adding context words support to recognizers","text":"<p>First, let's create a simple <code>US_ZIP_CODE</code> recognizer:</p> <pre><code>from presidio_analyzer import (\n    Pattern,\n    PatternRecognizer,\n    RecognizerRegistry,\n    AnalyzerEngine,\n)\n\n# Define the regex pattern\nregex = r\"(\\b\\d{5}(?:\\-\\d{4})?\\b)\"  # very weak regex pattern\nzipcode_pattern = Pattern(name=\"zip code (weak)\", regex=regex, score=0.01)\n\n# Define the recognizer with the defined pattern\nzipcode_recognizer = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\", patterns=[zipcode_pattern]\n)\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\n\nprint(f\"Result:\\n {results}\")\n</code></pre> <p>So this is working, but would catch any 5 digit string. This is why we set the score to 0.01. Let's use context words to increase score:</p> <pre><code>from presidio_analyzer import PatternRecognizer\n\n# Define the recognizer with the defined pattern and context words\nzipcode_recognizer_w_context = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\",\n    patterns=[zipcode_pattern],\n    context=[\"zip\", \"zipcode\"],\n)\n</code></pre> <p>When creating an <code>AnalyzerEngine</code> we can provide our own context enhancement logic by passing it to <code>context_aware_enhancer</code> parameter. <code>AnalyzerEngine</code> will create <code>LemmaContextAwareEnhancer</code> by default if not passed, which will enhance score of each matched result if its recognizer holds context words and the lemma of those words are found in the surroundings of the matched entity.</p> <p>Creating the <code>AnalyzerEngine</code> and adding the new recognizer:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer_w_context)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\nprint(\"Result:\")\nprint(results)\n</code></pre> <p>The confidence score is now 0.4, instead of 0.01, since the <code>LemmaContextAwareEnhancer</code> default context similarity factor is 0.35 and default minimum score with context similarity is 0.4. We can change that by passing other values to the <code>context_similarity_factor</code> and <code>min_score_with_context_similarity</code> parameters of the <code>LemmaContextAwareEnhancer</code> object. For example:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\nfrom presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n\ncontext_aware_enhancer = LemmaContextAwareEnhancer(\n    context_similarity_factor=0.45, min_score_with_context_similarity=0.4\n)\n\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(\n    registry=registry, context_aware_enhancer=context_aware_enhancer\n)\n\n# Test\nresults = analyzer.analyze(text=\"My zip code is 90210\", language=\"en\")\nprint(\"Result:\")\nprint(results)\n</code></pre> <p>The confidence score is now 0.46 because it got enhanced from 0.01 with 0.45 and is more than the minimum of 0.4.</p> <p>In addition to surrounding words, additional context words could be passed on the request level. This is useful when there is context coming from metadata such as column names or a specific user input. In the following example, notice how the \"zip\" context word doesn't appear in the text but still enhances the confidence score from 0.01 to 0.4:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, PatternRecognizer\n\n# Define the recognizer with the defined pattern and context words\nzipcode_recognizer = PatternRecognizer(\n    supported_entity=\"US_ZIP_CODE\",\n    patterns=[zipcode_pattern],\n    context=[\"zip\", \"zipcode\"],\n)\nregistry = RecognizerRegistry()\nregistry.add_recognizer(zipcode_recognizer)\nanalyzer = AnalyzerEngine(registry=registry)\n\n# Test with an example record having a column name which could be injected as context\nrecord = {\"column_name\": \"zip\", \"text\": \"My code is 90210\"}\n\nresult = analyzer.analyze(\n    text=record[\"text\"], language=\"en\", context=[record[\"column_name\"]]\n)\n\nprint(\"Result:\")\nprint(result)\n</code></pre>"},{"location":"tutorial/07_decision_process/","title":"Example 7: Tracing the decision process","text":"<p>Presidio-analyzer's decision process exposes information on why a specific PII was detected. Such information could contain:</p> <ul> <li>Which recognizer detected the entity</li> <li>Which regex pattern was used</li> <li>Interpretability mechanisms in ML models</li> <li>Which context words improved the score</li> <li>Confidence scores before and after each step And more.</li> </ul> <p>For more information, refer to the decision process documentation.</p> <p>Let's use the decision process output to understand how the zip code value was detected:</p> <pre><code>from presidio_analyzer import AnalyzerEngine\nimport pprint\n\nanalyzer = AnalyzerEngine()\n\nresults = analyzer.analyze(\n    text=\"My zip code is 90210\", language=\"en\", return_decision_process=True\n)\n\ndecision_process = results[0].analysis_explanation\n\npp = pprint.PrettyPrinter()\nprint(\"Decision process output:\\n\")\npp.pprint(decision_process.__dict__)\n</code></pre> <p>When developing new recognizers, one can add information to this explanation and extend it with additional findings.</p>"},{"location":"tutorial/08_no_code/","title":"Example 8: Creating no-code pattern recognizers","text":"<p>No-code pattern recognizers can be helpful in two scenarios:</p> <ol> <li>There's an existing set of regular expressions / deny-lists which needs to be added to Presidio.</li> <li>Non-technical team members who require adding logic without writing code.</li> </ol> <p>Regular expression or deny-list based recognizers can be written in a YAML file, and added to the list of recognizers in Presidio.</p> <p>An example YAML file can be found here.</p> <p>For more information on the schema, see the <code>PatternRecognizer</code> definition on the API Docs).</p> <p>Once the YAML file is created, it can be loaded into the <code>RecognizerRegistry</code> instance.</p> <p>This example creates a <code>RecognizerRegistry</code> holding only the recognizers in the YAML file:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nyaml_file = \"recognizers.yaml\"\nregistry = RecognizerRegistry()\nregistry.add_recognizers_from_yaml(yaml_file)\n\nanalyzer = AnalyzerEngine(registry=registry)\nanalyzer.analyze(text=\"Mr. and Mrs. Smith\", language=\"en\")\n</code></pre> <p>This example adds the new recognizers to the predefined recognizers in Presidio:</p> <pre><code>from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n\nyaml_file = \"recognizers.yaml\"  # path to YAML file\nregistry = RecognizerRegistry()\nregistry.load_predefined_recognizers()  # Loads all the predefined recognizers (Credit card, phone number etc.)\n\nregistry.add_recognizers_from_yaml(yaml_file)\n\nanalyzer = AnalyzerEngine()\nanalyzer.analyze(text=\"Mr. Plum wrote a book\", language=\"en\")\n</code></pre>"},{"location":"tutorial/09_ad_hoc/","title":"Example 9: Ad-hoc recognizers","text":"<p>In addition to recognizers in code or in a YAML file, it is possible to create ad-hoc recognizers via the Presidio Analyzer API for regex and deny-list based logic. These recognizers, in JSON form, are added to the <code>/analyze</code> request and are only used in the context of this request.</p> <p>Note</p> <p>These ad-hoc recognizers could be useful if Presidio is already deployed, but requires additional detection logic to be added.</p> <ul> <li> <p>The json structure for a regex ad-hoc recognizer is the following:</p> <pre><code>{\n    \"text\": \"John Smith drivers license is AC432223. Zip code: 10023\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Zip code Recognizer\",\n        \"supported_language\": \"en\",\n        \"patterns\": [\n            {\n            \"name\": \"zip code (weak)\", \n            \"regex\": \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\", \n            \"score\": 0.01\n            }\n        ],\n        \"context\": [\"zip\", \"code\"],\n        \"supported_entity\":\"ZIP\"\n        }\n    ]\n}\n</code></pre> </li> <li> <p>The json structure for deny-list based recognizers is the following:</p> <pre><code>{\n    \"text\": \"Mr. John Smith's drivers license is AC432223\",\n    \"language\": \"en\",\n    \"ad_hoc_recognizers\":[\n        {\n        \"name\": \"Mr. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Mr\", \"Mr.\", \"Mister\"],\n        \"supported_entity\":\"MR_TITLE\"\n        },\n        {\n        \"name\": \"Ms. Recognizer\",\n        \"supported_language\": \"en\",\n        \"deny_list\": [\"Ms\", \"Ms.\", \"Miss\", \"Mrs\", \"Mrs.\"],\n        \"supported_entity\":\"MS_TITLE\"\n        }\n    ]\n}\n</code></pre> </li> </ul> <p>In both examples, the <code>/analyze</code> request is extended with a list of <code>ad_hoc_recognizers</code>, which could be either <code>patterns</code>, <code>deny_list</code> or both.</p> <p>Example call to the <code>/analyze</code> service:</p> <pre><code>{\n  \"text\": \"John Smith drivers license is AC432223 and the zip code is 12345\",\n  \"language\": \"en\",\n  \"return_decision_process\": false,\n  \"correlation_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"score_threshold\": 0.6,\n  \"entities\": [\n    \"US_DRIVER_LICENSE\",\n    \"ZIP\"\n  ],\n  \"trace\": false,\n  \"ad_hoc_recognizers\": [\n    {\n      \"name\": \"Zip code Recognizer\",\n      \"supported_language\": \"en\",\n      \"patterns\": [\n        {\n          \"name\": \"zip code (weak)\",\n          \"regex\": \"(\\\\b\\\\d{5}(?:\\\\-\\\\d{4})?\\\\b)\",\n          \"score\": 0.01\n        }\n      ],\n      \"context\": [\n        \"zip\",\n        \"code\"\n      ],\n      \"supported_entity\": \"ZIP\"\n    }\n  ]\n}\n</code></pre> <p>For more examples of deny-list recognizers, see this sample.</p>"},{"location":"tutorial/10_simple_anonymization/","title":"Example 10: Simple anonymization","text":"<p>Once we have the identified PII entities, we can perform different de-identification operations on them. For more information on the supported operators, see the anonymizer documentation.</p> <p>The anonymizer requires a configuration specifying the requested operation on each entity type. There's also a default operator which replaces a PII entity with the entity type name.</p> <p>Each operator has a unique configuration with the parameters needed to perform the operation (redact, hash, mask, replace, encrypt etc.)</p> <p>Here's an simple example of using presidio-anonymizer:</p> <pre><code>from presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import RecognizerResult\n\n# Analyzer output\nanalyzer_results = [\n    RecognizerResult(entity_type=\"PERSON\", start=11, end=15, score=0.8),\n    RecognizerResult(entity_type=\"PERSON\", start=17, end=27, score=0.8),\n]\n\n# Initialize the engine:\nengine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text,\n# analyzer results (potentially coming from presidio-analyzer) and\n# Operators to get the anonymization output:\nresult = engine.anonymize(\n    text=\"My name is Bond, James Bond\", analyzer_results=analyzer_results\n)\n\nprint(\"De-identified text\")\nprint(result.text)\n</code></pre> <p>To introduce additional operators, we can pass an <code>OperatorConfig</code>. In this example we:</p> <ol> <li>Mask the last 12 chars of a <code>PHONE_NUMBER</code> entity and replace them with <code>*</code></li> <li>Redact a <code>TITLE</code> entity</li> <li>Replace all other entities with the string <code>&lt;ANONYMIZED&gt;</code>.</li> </ol> <p>Defining the operators:</p> <pre><code># Define anonymization operators\noperators = {\n    \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"&lt;ANONYMIZED&gt;\"}),\n    \"PHONE_NUMBER\": OperatorConfig(\n        \"mask\",\n        {\n            \"type\": \"mask\",\n            \"masking_char\": \"*\",\n            \"chars_to_mask\": 12,\n            \"from_end\": True,\n        },\n    ),\n    \"TITLE\": OperatorConfig(\"redact\", {}),\n}\n</code></pre> <p>Full example:</p> <pre><code>from pprint import pprint\nimport json\n\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig, RecognizerResult\n\n\n# Analyzer output\nanalyzer_results = [\n    RecognizerResult(entity_type=\"PERSON\", start=11, end=15, score=0.8),\n    RecognizerResult(entity_type=\"PERSON\", start=17, end=27, score=0.8),\n]\n\ntext_to_anonymize = \"My name is Bond, James Bond\"\n\nanonymizer = AnonymizerEngine()\n\n# Define anonymization operators\noperators = {\n    \"DEFAULT\": OperatorConfig(\"replace\", {\"new_value\": \"&lt;ANONYMIZED&gt;\"}),\n    \"PHONE_NUMBER\": OperatorConfig(\n        \"mask\",\n        {\n            \"type\": \"mask\",\n            \"masking_char\": \"*\",\n            \"chars_to_mask\": 12,\n            \"from_end\": True,\n        },\n    ),\n    \"TITLE\": OperatorConfig(\"redact\", {}),\n}\n\nanonymized_results = anonymizer.anonymize(\n    text=text_to_anonymize, analyzer_results=analyzer_results, operators=operators\n)\n\nprint(f\"text: {anonymized_results.text}\")\nprint(\"detailed result:\")\n\npprint(json.loads(anonymized_results.to_json()))\n</code></pre>"},{"location":"tutorial/11_custom_anonymization/","title":"Example 11: Custom anonymization","text":"<p>Presidio-anonymizer can accept arbitrary operations to perform on identified entities. These operations can be passed in the form of a lambda function.</p> <p>In the following example, we use fake values to perform pseudonymization. First, let's look at the operator:</p> <pre><code>from faker import Faker\nfrom presidio_anonymizer.entities import OperatorConfig\n\nfake = Faker()\n\n# Create faker function (note that it has to receive a value)\ndef fake_name(x):\n    return fake.name()\n\n\n# Create custom operator for the PERSON entity\noperators = {\"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name})}\n</code></pre> <p>Full example:</p> <pre><code>from presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig, EngineResult, RecognizerResult\nfrom faker import Faker\n\n\nfake = Faker()\n\n# Create faker function (note that it has to receive a value)\ndef fake_name(x):\n    return fake.name()\n\n\n# Create custom operator for the PERSON entity\noperators = {\"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name})}\n\n# Analyzer output\nanalyzer_results = [RecognizerResult(entity_type=\"PERSON\", start=11, end=18, score=0.8)]\n\ntext_to_anonymize = \"My name is Raphael and I like to fish.\"\n\nanonymizer = AnonymizerEngine()\n\nanonymized_results = anonymizer.anonymize(\n    text=text_to_anonymize, analyzer_results=analyzer_results, operators=operators\n)\n\nprint(anonymized_results.text)\n</code></pre> <p>This is a simple example, but here are some examples for more advanced anonymization options:</p> <ul> <li>Identify the gender and create a random value from the same gender (e.g., Laura -&gt; Pam)</li> <li>Identifying the date pattern and perform date shift (01-01-2020 -&gt; 05-01-2020)</li> <li>Identify the age and bucket by decade (89 -&gt; 80-90)</li> </ul>"},{"location":"tutorial/12_encryption/","title":"Example 12: Encryption and decryption","text":"<p>This sample shows how to use Presidio Anonymizer built-in functionality, to encrypt and decrypt identified entities. The encryption is using AES cypher in CBC mode and requires a cryptographic key as an input for both the encryption and the decryption.</p>"},{"location":"tutorial/12_encryption/#set-up-imports","title":"Set up imports","text":"<pre><code>from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine\nfrom presidio_anonymizer.entities import (\n    RecognizerResult,\n    OperatorResult,\n    OperatorConfig,\n)\n</code></pre>"},{"location":"tutorial/12_encryption/#define-a-cryptographic-key-for-both-encryption-and-decryption","title":"Define a cryptographic key (for both encryption and decryption)","text":"<pre><code>crypto_key = \"WmZq4t7w!z%C&amp;F)J\"\n</code></pre>"},{"location":"tutorial/12_encryption/#presidio-anonymizer-encrypt","title":"Presidio Anonymizer: Encrypt","text":"<pre><code>engine = AnonymizerEngine()\n\n# Invoke the anonymize function with the text,\n# analyzer results (potentially coming from presidio-analyzer)\n# and an 'encrypt' operator to get an encrypted anonymization output:\nanonymize_result = engine.anonymize(\n    text=\"My name is James Bond\",\n    analyzer_results=[\n        RecognizerResult(entity_type=\"PERSON\", start=11, end=21, score=0.8),\n    ],\n    operators={\"PERSON\": OperatorConfig(\"encrypt\", {\"key\": crypto_key})},\n)\n\nanonymize_result\n</code></pre> <p>The output contains both the anonymized text, as well as the location of the encrypted entities. This is useful as we would need to decrypt only the entities and not the full text:</p> <pre><code># Fetch the anonymized text from the result.\nanonymized_text = anonymize_result.text\n\n# Fetch the anonynized entities from the result.\nanonymized_entities = anonymize_result.items\n</code></pre>"},{"location":"tutorial/12_encryption/#presidio-anonymizer-decrypt","title":"Presidio Anonymizer: Decrypt","text":"<pre><code># Initialize the engine:\nengine = DeanonymizeEngine()\n\n# Invoke the deanonymize function with the text, anonymizer results\n# and a 'decrypt' operator to get the original text as output.\ndeanonymized_result = engine.deanonymize(\n    text=anonymized_text,\n    entities=anonymized_entities,\n    operators={\"DEFAULT\": OperatorConfig(\"decrypt\", {\"key\": crypto_key})},\n)\n\ndeanonymized_result\n</code></pre>"},{"location":"tutorial/12_encryption/#alternatively-call-the-decrypt-operator-directly","title":"Alternatively, call the Decrypt operator directly","text":"<pre><code>from presidio_anonymizer.operators import Decrypt\n\n# Fetch the encrypted entity value from the previous stage\nencrypted_entity_value = anonymize_result.items[0].text\n\n# Restore the original entity value\nDecrypt().operate(text=encrypted_entity_value, params={\"key\": crypto_key})\n</code></pre>"},{"location":"tutorial/13_allow_list/","title":"Example 13: Allow-list to exclude words from being identified as PII","text":"<p>In this example, we will define a list of tokens that should not be marked as PII even if we want to identify others of that kind.</p> <p>In this example, we will pass a short list of tokens which should not be marked as PII even if detected by one of the recognizers.</p> <pre><code>websites_list = [\n    \"bing.com\",\n    \"microsoft.com\"\n]\n</code></pre> <p>We will use the built in recognizers that include the <code>URLRecognizer</code> and the NLP model <code>EntityRecognizer</code> and see the default functionality if we don't specify any list of words for the detector to allow to keep in the text.</p> <pre><code>from presidio_analyzer import AnalyzerEngine\ntext1 = \"My favorite website is bing.com, his is microsoft.com\"\nanalyzer = AnalyzerEngine()\nresult = analyzer.analyze(text = text1, language = 'en')\nprint(f\"Result: \\n {result}\")\n</code></pre> <p>To specify an allow list we just pass a list of values we want to keep as a parameter to call to <code>analyze</code>. Now we can see that in the results, <code>bing.com</code> is no longer being recognized as a PII item, only <code>microsoft.com</code> is still recognized since we did include it in the allow list.</p> <pre><code>result = analyzer.analyze(text = text1, language = 'en', allow_list = [\"bing.com\"] )\nprint(f\"Result:\\n {result}\")\n</code></pre>"}]}